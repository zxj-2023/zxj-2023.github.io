<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha256-XOqroi11tY4EFQMR9ZYwZWKj5ZXiftSx36RRuC3anlA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-bounce.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.20.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="zxj Blogs">
<meta property="og:type" content="website">
<meta property="og:title" content="Zhang XiJun">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Zhang XiJun">
<meta property="og:description" content="zxj Blogs">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="张熙浚">
<meta property="article:tag" content="zxj">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Zhang XiJun</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Zhang XiJun</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">BLOGS</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="张熙浚"
      src="/images/zxjavatar.gif">
  <p class="site-author-name" itemprop="name">张熙浚</p>
  <div class="site-description" itemprop="description">zxj Blogs</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">132</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">50</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">56</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/zxj-2023" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zxj-2023" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="http://wpa.qq.com/msgrd?v=3&uin=2902065320&site=qq&menu=yes" title="QQ → http:&#x2F;&#x2F;wpa.qq.com&#x2F;msgrd?v&#x3D;3&amp;uin&#x3D;2902065320&amp;site&#x3D;qq&amp;menu&#x3D;yes" rel="noopener me" target="_blank"><i class="fab fa-qq fa-fw"></i>QQ</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
    <div class="sidebar-inner sidebar-blogroll">
      <div class="links-of-blogroll animated">
        <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
          链接
        </div>
        <ul class="links-of-blogroll-list">
            <li class="links-of-blogroll-item">
              <a href="https://zxj-2023.github.io/" title="https:&#x2F;&#x2F;zxj-2023.github.io" rel="noopener" target="_blank">Zhang XiJun</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://theme-next.js.org/" title="https:&#x2F;&#x2F;theme-next.js.org" rel="noopener" target="_blank">NexT</a>
            </li>
        </ul>
      </div>
    </div>
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/08/28/%E5%AD%A6%E4%B9%A0/agent%E5%AE%9E%E6%88%98/deepresearch/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/08/28/%E5%AD%A6%E4%B9%A0/agent%E5%AE%9E%E6%88%98/deepresearch/" class="post-title-link" itemprop="url">deepresearch实战</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2025-08-28 00:00:00 / 修改时间：15:24:10" itemprop="dateCreated datePublished" datetime="2025-08-28T00:00:00+08:00">2025-08-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/agent%E5%AE%9E%E6%88%98/" itemprop="url" rel="index"><span itemprop="name">agent实战</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="理解deepresearch">理解deepresearch</h3>
<h3 id="参考资料">参考资料</h3>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1aXg5zzExm?buvid=XX7932A7E9DD8A1DA92D4974AB535749A9B44&amp;from_spmid=tm.recommend.0.0&amp;is_story_h5=false&amp;mid=25yhOI1fPeVYa5V16BFcAw%3D%3D&amp;plat_id=116&amp;share_from=ugc&amp;share_medium=android&amp;share_plat=android&amp;share_session_id=aec53502-2411-4bc1-8ccf-ce85bd6efc1e&amp;share_source=QQ&amp;share_tag=s_i&amp;spmid=united.player-video-detail.0.0&amp;timestamp=1753142992&amp;unique_k=tZcFKrS&amp;up_id=28357052&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">🦜🕸️
Open Deep Research V2：新架构！上下文工程最新实践
📋_哔哩哔哩_bilibili</a></p>
<p><a target="_blank" rel="noopener" href="https://academy.langchain.com/courses/take/deep-research-with-langgraph/lessons/67513388-course-overview">Project:
Deep Research with LangGraph - LangChain Academy</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/langchain-ai/open_deep_research">langchain-ai/open_deep_research
— langchain-ai/open_deep_research</a></p>
<p>官方博文<a target="_blank" rel="noopener" href="https://blog.langchain.com/open-deep-research/">开启深度研究 —
Open Deep Research</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/08/25/%E5%AE%9E%E4%B9%A0/Tosea.ai/aippt%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/08/25/%E5%AE%9E%E4%B9%A0/Tosea.ai/aippt%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/" class="post-title-link" itemprop="url">aippt技术调研</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-08-25 00:00:00" itemprop="dateCreated datePublished" datetime="2025-08-25T00:00:00+08:00">2025-08-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-08-28 17:16:27" itemprop="dateModified" datetime="2025-08-28T17:16:27+08:00">2025-08-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AE%9E%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">实习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AE%9E%E4%B9%A0/Tosea-ai/" itemprop="url" rel="index"><span itemprop="name">Tosea.ai</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="技术路线">技术路线</h3>
<p><strong>方案 A：html → PDF →
pptx</strong>（我尝试下来不大可行，无法再进行编辑了）</p>
<p><strong>方案 B：html → PPTXGenJS</strong></p>
<p><strong>方案 C：html → python-pptx</strong> (主流)</p>
<p><strong>方案 D：markdown → Slidev</strong></p>
<p>我尝试了当前市面的ppt生成产品，在网页里面展示还会有良好的动画效果，但是一旦导出成pptx，都是会变成静态页面，没有动画</p>
<p>reveal.js可以利用页面生成丰富的ppt动画效果；Slidev可以将markdown语法转化成ppt，可以导出为pdf或pptx，需要注意的是，PPTX
文件中的所有幻灯片都会被导出为图片。</p>
<p>PPTXGenJS和python-pptx的原理基本一致，区别一个是使用js一个是python，使用方法都是通过解析HTML标签内容，定义一个ppt实例，将html的内容一点点加入这个示例中，最后导出pptx。这样都仅能实现最基本的ppt演示，不会有复杂的结构，而且经常会出现一个问题——某个标签内文字太多往往会超出ppt演示范围</p>
<p>直接让AI来生成非常自由的PPT，最终的效果一般来说都比较烂，大部分都是预定义一个html模板，然后让AI来自动的选择模板往里面填充内容</p>
<h3 id="相关工具">相关工具</h3>
<p><a target="_blank" rel="noopener" href="https://cn.sli.dev/guide/">Slidev</a>
是一个为开发者设计的基于 Web 的幻灯片制作工具。它帮助您以 Markdown
的形式专注于编写幻灯片的内容，并制作出具有交互式演示功能的、高度可自定义的幻灯片。</p>
<p><a target="_blank" rel="noopener" href="https://github.com/hakimel/reveal.js">reveal.js</a>
是一个开源的 <strong>HTML 演示框架</strong>，用 JavaScript
写成。只要你会写 HTML/CSS/JS，就可以像做网页一样做出
<strong>酷炫、响应式、支持键盘/鼠标/触控交互</strong> 的幻灯片。</p>
<p><a target="_blank" rel="noopener" href="https://github.com/gitbrent/PptxGenJS">PptxGenJS</a>
允许您使用 JavaScript 生成专业的 PowerPoint 演示文稿——直接从
Node、React、Vite、Electron，甚至浏览器中生成。</p>
<p><a target="_blank" rel="noopener" href="https://github.com/scanny/python-pptx">python-pptx</a>
是一个用于创建、读取和更新 PowerPoint (.pptx)文件的 Python
库。典型的使用场景是从动态内容（如数据库查询、分析输出或 JSON 负载）生成
PowerPoint 演示文稿，可能是在响应 HTTP 请求时生成 PPTX 文件并下载。</p>
<blockquote>
<p>python-pptx使用方式：根据标签解析html文件，如h1，div等，然后一点点添加到定义的页中</p>
</blockquote>
<h3 id="市面同类产品">市面同类产品</h3>
<ol type="1">
<li><a target="_blank" rel="noopener" href="https://www.genspark.ai/agents?type=slides_agent">Genspark</a>
<ul>
<li>Genspark 是 <strong>MainFunc</strong> 公司（由前小度 CEO
景鲲和前小度 CTO 朱凯华联合创立）推出的 <strong>AI Agent
搜索引擎</strong>（或称“AI 原生搜索引擎”）。</li>
</ul></li>
<li><a target="_blank" rel="noopener" href="https://www.tiangong.cn/?from=ai-tab.cn">skywork</a>
<ul>
<li>Skywork 是昆仑万维（Kunlun Inc.）旗下 <strong>SkyWork AI</strong>
推出的一系列 <strong>开源大模型</strong> 与 <strong>AI
技术品牌</strong>。</li>
</ul></li>
<li><a target="_blank" rel="noopener" href="https://manus.im/app">manus</a></li>
<li><a target="_blank" rel="noopener" href="https://gamma.app/">Gamma</a> 是一个 <strong>“AI
驱动的在线内容工作站”</strong>：输入一句话、一段大纲或任何资料，它就能在
<strong>1-3 分钟内</strong> 帮你生成
<strong>高颜值、品牌化、可互动</strong> 的演示文稿、网站、社媒图文或
PDF，并可一键导出为 <strong>PPT / Google Slides / PDF /
网站链接</strong>。</li>
</ol>
<p>manus是每页ppt都是一个html文件，我猜测应该是使用像python-pptx的库生成</p>
<h3 id="ppt-mcp">ppt-mcp</h3>
<p>可以参考其中的工具实现，这两个我看下来都是使用python-pptx包</p>
<p><a target="_blank" rel="noopener" href="https://github.com/GongRzhe/Office-PowerPoint-MCP-Server">GongRzhe/Office-PowerPoint-MCP-Server:
A MCP (Model Context Protocol) server for PowerPoint manipulation using
python-pptx. This server provides tools for creating, editing, and
manipulating PowerPoint presentations through the MCP protocol.</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/ltc6539/mcp-ppt">ltc6539/mcp-ppt: A mcp
server supporting you to generate powerpoint using LLM and natural
language automatically.</a></p>
<h3 id="架构思考">架构思考</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">flowchart TB</span><br><span class="line">    subgraph &quot;Plan-and-Execute阶段&quot;</span><br><span class="line">        A[&quot;用户输入&quot;] --&gt; B[&quot;Planner Agent&quot;]</span><br><span class="line">        B --&gt; C[&quot;Agent Executor&quot;]</span><br><span class="line">        C --&gt; D[&quot;Replanner&quot;]</span><br><span class="line">        D --&gt;|&quot;需要更多信息&quot;| C</span><br><span class="line">        D --&gt;|&quot;信息充足&quot;| E[&quot;输出结构化信息&quot;]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;内容生成阶段&quot;</span><br><span class="line">        E --&gt; F[&quot;大纲设计节点&quot;]</span><br><span class="line">        F --&gt; G[&quot;页面内容生成节点&quot;]</span><br><span class="line">        G --&gt; H[&quot;HTML代码生成节点&quot;]</span><br><span class="line">    end</span><br><span class="line">    </span><br><span class="line">    subgraph &quot;文件转换阶段&quot;</span><br><span class="line">        H --&gt; I[&quot;html演示生成&quot;]</span><br><span class="line">        I --&gt; J[&quot;转换pptx节点&quot;]</span><br><span class="line">        J --&gt; K[&quot;输出PPTX文件&quot;]</span><br><span class="line">    end</span><br></pre></td></tr></table></figure>
<h3 id="利用apryse将pdf转成pptx">利用APRYSE将pdf转成pptx</h3>
<p>Apryse（曾用名 PDFTron）是一家加拿大公司推出的商用 SDK 家族，专注
“任何格式进、任何格式出” 的文档处理。</p>
<p>获取apikey<a target="_blank" rel="noopener" href="https://docs.apryse.com/core/guides/get-started/trial-key">Free
trial key for Apryse SDK | Apryse documentation</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.apryse.com/core/guides/get-started/python3">Python
3.X PDF Library for Windows, Linux and Mac | Apryse
documentation</a></p>
<p><strong>安装 Apryse SDK 的“结构化输出模块”（Structured Output
Module）</strong>。该模块是一个可选的扩展包，PDF → PPTX、PDF → Word
等高级转换功能都依赖它。<a target="_blank" rel="noopener" href="https://docs.apryse.com/core/guides/info/modules">库插件：OCR、CAD
转 PDF - 适用于服务器/桌面 SDK | Apryse 文档 — Library Add-ons: OCR, CAD
to PDF - for Server/Desktop SDK | Apryse documentation</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">from apryse_sdk import PDFNet, PDFDoc, Convert, StructuredOutputModule</span><br><span class="line"></span><br><span class="line"># 1. 初始化（许可证）</span><br><span class="line">PDFNet.Initialize(&quot;demo:1756369085114:&quot;)</span><br><span class="line"></span><br><span class="line"># 2. 告诉 SDK 模块放在哪里</span><br><span class="line">PDFNet.AddResourceSearchPath(r&quot;F:\project python\test\StructuredOutputWindows\Lib\Windows&quot;)</span><br><span class="line"></span><br><span class="line"># 3. 可选：确认模块已就位</span><br><span class="line">if not StructuredOutputModule.IsModuleAvailable():</span><br><span class="line">    raise RuntimeError(&quot;StructuredOutput module not found!&quot;)</span><br><span class="line"></span><br><span class="line"># 4. 正常调用</span><br><span class="line">doc = PDFDoc(&quot;input.pdf&quot;)</span><br><span class="line">Convert.ToPowerPoint(doc, &quot;output.pptx&quot;)</span><br></pre></td></tr></table></figure>
<h3 id="参考资料">参考资料</h3>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Zftnz6Ewx/?spm_id_from=333.788.recommend_more_video.0&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">动手实现一个做PPT的MCP服务器_哔哩哔哩_bilibili</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/08/19/%E5%AD%A6%E4%B9%A0/agent%E5%AE%9E%E6%88%98/agentic-rag/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/08/19/%E5%AD%A6%E4%B9%A0/agent%E5%AE%9E%E6%88%98/agentic-rag/" class="post-title-link" itemprop="url">agentic-rag实战</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-08-19 00:00:00" itemprop="dateCreated datePublished" datetime="2025-08-19T00:00:00+08:00">2025-08-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-08-28 15:06:56" itemprop="dateModified" datetime="2025-08-28T15:06:56+08:00">2025-08-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/agent%E5%AE%9E%E6%88%98/" itemprop="url" rel="index"><span itemprop="name">agent实战</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="前言">前言</h3>
<p><strong>这个agentic
rag主要是作用于检索部分，由是否需要调用检索工具判定是否进入检索阶段，当检索到相关的文章，则进行回答，否则对问题进行改写，再次检索</strong></p>
<p>代码见<a target="_blank" rel="noopener" href="https://github.com/zxj-2023/learn-rag-langchain/tree/main/agentic-rag">learn-rag-langchain/agentic-rag
at main · zxj-2023/learn-rag-langchain</a></p>
<p>在这个教程中，我们将构建一个检索代理。当您希望 LLM
决定是否从向量存储中检索上下文或直接响应用户时，检索代理非常有用。</p>
<p>完成教程后，我们将完成以下工作：</p>
<ol type="1">
<li>获取并预处理用于检索的文档。</li>
<li>为这些文档建立语义索引，并为代理创建一个检索工具。</li>
<li>构建一个能够决定何时使用检索工具的代理式 RAG 系统。</li>
</ol>
<figure>
<img src="/2025/08/19/%E5%AD%A6%E4%B9%A0/agent%E5%AE%9E%E6%88%98/agentic-rag/image-20250819165309335.png" alt="image-20250819165309335">
<figcaption aria-hidden="true">image-20250819165309335</figcaption>
</figure>
<h3 id="预处理文档">1. 预处理文档</h3>
<p>获取用于我们 RAG 系统的文档。我们将使用 Lilian Weng
优秀博客中最新的三页。我们将从使用 <code>WebBaseLoader</code>
工具获取页面内容开始：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from langchain_community.document_loaders import WebBaseLoader</span><br><span class="line"></span><br><span class="line">urls = [</span><br><span class="line">    &quot;https://lilianweng.github.io/posts/2024-11-28-reward-hacking/&quot;,</span><br><span class="line">    &quot;https://lilianweng.github.io/posts/2024-07-07-hallucination/&quot;,</span><br><span class="line">    &quot;https://lilianweng.github.io/posts/2024-04-12-diffusion-video/&quot;,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">docs = [WebBaseLoader(url).load() for url in urls]</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docs[0][0].page_content.strip()[:1000]</span><br></pre></td></tr></table></figure>
<p>将获取的文档分割成更小的块，以便索引到我们的向量存储中：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from langchain_text_splitters import RecursiveCharacterTextSplitter</span><br><span class="line"></span><br><span class="line">docs_list = [item for sublist in docs for item in sublist]</span><br><span class="line"></span><br><span class="line">text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(</span><br><span class="line">    chunk_size=100, chunk_overlap=50</span><br><span class="line">)</span><br><span class="line">doc_splits = text_splitter.split_documents(docs_list)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">doc_splits[0].page_content.strip()</span><br></pre></td></tr></table></figure>
<h3 id="创建检索工具">2. 创建检索工具</h3>
<p>现在我们已经有了分割的文档，我们可以将它们索引到一个向量存储中，我们将使用这个向量存储进行语义搜索。</p>
<p>使用内存向量存储和 OpenAI 嵌入：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">from langchain_chroma import Chroma  # 导入 Chroma</span><br><span class="line">from langchain_openai import OpenAIEmbeddings</span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line"># 确保安装了 langchain-chroma</span><br><span class="line"># pip install langchain-chroma</span><br><span class="line"></span><br><span class="line">embedding = OpenAIEmbeddings(</span><br><span class="line">    api_key=&quot;sk-&quot;, </span><br><span class="line">    base_url=&quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;,</span><br><span class="line">    model=&quot;text-embedding-v4&quot;,</span><br><span class="line">    check_embedding_ctx_length=False,</span><br><span class="line">    dimensions=1536,</span><br><span class="line">    chunk_size=5  # 设置较小的批次大小</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># 使用 Chroma 替代 InMemoryVectorStore</span><br><span class="line">vectorstore = Chroma.from_documents(</span><br><span class="line">    documents=doc_splits, </span><br><span class="line">    embedding=embedding,</span><br><span class="line">    persist_directory=&quot;./chroma_db&quot;  # 指定持久化目录</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 重新加载已存在的 Chroma 数据库</span><br><span class="line">vectorstore = Chroma(</span><br><span class="line">    persist_directory=&quot;./chroma_db&quot;,</span><br><span class="line">    embedding_function=embedding</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">retriever = vectorstore.as_retriever()</span><br></pre></td></tr></table></figure>
<p>使用 LangChain 的预构建 <code>create_retriever_tool</code>
创建检索工具</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from langchain.tools.retriever import create_retriever_tool</span><br><span class="line"></span><br><span class="line">retriever_tool = create_retriever_tool(</span><br><span class="line">    retriever,                    # 【输入】一个已经配置好的检索器（例如：向量数据库的检索器）</span><br><span class="line">    &quot;retrieve_blog_posts&quot;,        # 【工具名称】这个工具的唯一标识名（供模型内部调用）</span><br><span class="line">    &quot;Search and return information about Lilian Weng blog posts.&quot;  # 【工具描述】模型看到的说明，用于决定是否调用它</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">retriever_tool.invoke(&#123;&quot;query&quot;: &quot;types of reward hacking&quot;&#125;)</span><br></pre></td></tr></table></figure>
<h3 id="生成查询">3. 生成查询</h3>
<p>现在我们将开始构建我们智能体 RAG 图中的组件（节点和边）。</p>
<p>构建一个 <code>generate_query_or_respond</code> 节点。它将调用 LLM
来根据当前图状态（消息列表）生成响应。根据输入消息，它将决定使用检索工具进行检索，或直接响应用户。请注意，我们通过
<code>.bind_tools</code> 向聊天模型提供了先前创建的
<code>retriever_tool</code> 访问权限：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from langchain_community.chat_models import ChatTongyi</span><br><span class="line">llm = ChatTongyi(</span><br><span class="line">    model=&quot;qwen3-235b-a22b&quot;,</span><br><span class="line">    api_key=&quot;sk-&quot;,</span><br><span class="line">    base_url=&quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;,</span><br><span class="line">    model_kwargs=&#123;&quot;enable_thinking&quot;: False&#125;   # 关键在这里</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from langgraph.graph import MessagesState</span><br><span class="line"></span><br><span class="line">def generate_query_or_respond(state: MessagesState):</span><br><span class="line">    &quot;&quot;&quot;调用模型，根据当前状态生成响应。根据问题，模型将决定是使用检索工具进行检索，还是直接回复用户。</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    response = (</span><br><span class="line">        llm</span><br><span class="line">        .bind_tools([retriever_tool]).invoke(state[&quot;messages&quot;])</span><br><span class="line">    )</span><br><span class="line">    return &#123;&quot;messages&quot;: [response]&#125;</span><br></pre></td></tr></table></figure>
<p>提出一个需要语义搜索的问题：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">input = &#123;</span><br><span class="line">    &quot;messages&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;role&quot;: &quot;user&quot;,</span><br><span class="line">            &quot;content&quot;: &quot;What does Lilian Weng say about types of reward hacking?&quot;,</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line">generate_query_or_respond(input)[&quot;messages&quot;][-1].pretty_print()</span><br></pre></td></tr></table></figure>
<h3 id="评定文件">4.评定文件</h3>
<p>添加一个条件边 — <code>grade_documents</code> —
来判断检索到的文档是否与问题相关。</p>
<p>我们将使用一个具有结构化输出模式 <code>GradeDocuments</code>
的模型来对文档进行评分。 <code>grade_documents</code>
函数将根据评分决策（ <code>generate_answer</code> 或
<code>rewrite_question</code> ）返回要前往的节点的名称：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">from pydantic import BaseModel, Field</span><br><span class="line">from typing import Literal</span><br><span class="line"></span><br><span class="line"># 定义评分提示模板</span><br><span class="line">GRADE_PROMPT = (</span><br><span class="line">    &quot;你是一个评分员，负责评估检索到的文档与用户问题的相关性。\n &quot;</span><br><span class="line">    &quot;以下是检索到的文档内容：\n\n &#123;context&#125; \n\n&quot;</span><br><span class="line">    &quot;以下是用户的问题：&#123;question&#125; \n&quot;</span><br><span class="line">    &quot;如果文档包含与用户问题相关的关键词或语义含义，则将其评为相关。\n&quot;</span><br><span class="line">    &quot;请给出一个二元评分：&#x27;yes&#x27;（是）表示相关，&#x27;no&#x27;（否）表示不相关。&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># 定义用于评估文档相关性的 Pydantic 模型</span><br><span class="line">class GradeDocuments(BaseModel):</span><br><span class="line">    &quot;&quot;&quot;使用二元评分对文档进行相关性评估。&quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    binary_score: str = Field(</span><br><span class="line">        description=&quot;相关性评分：&#x27;yes&#x27; 表示相关，&#x27;no&#x27; 表示不相关&quot;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"># 初始化用于评分的聊天模型</span><br><span class="line">grader_model = llm</span><br><span class="line"></span><br><span class="line">def grade_documents(</span><br><span class="line">    state: MessagesState,</span><br><span class="line">) -&gt; Literal[&quot;generate_answer&quot;, &quot;rewrite_question&quot;]:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    判断检索到的文档是否与用户问题相关。</span><br><span class="line">    </span><br><span class="line">    参数:</span><br><span class="line">        state: 包含消息历史的状态对象，其中第一条消息是用户问题，</span><br><span class="line">               最后一条消息是检索到的文档内容。</span><br><span class="line">    </span><br><span class="line">    返回:</span><br><span class="line">        如果文档相关，返回 &quot;generate_answer&quot;；</span><br><span class="line">        如果不相关，返回 &quot;rewrite_question&quot;，表示需要重写问题并重新检索。</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    question = state[&quot;messages&quot;][0].content  # 获取用户问题</span><br><span class="line">    context = state[&quot;messages&quot;][-1].content  # 获取检索到的文档内容</span><br><span class="line"></span><br><span class="line">    # 将问题和文档内容填入提示模板</span><br><span class="line">    prompt = GRADE_PROMPT.format(question=question, context=context)</span><br><span class="line">    </span><br><span class="line">    # 调用模型，并以结构化输出（Pydantic 模型）的形式获取评分结果</span><br><span class="line">    response = (</span><br><span class="line">        grader_model</span><br><span class="line">        .with_structured_output(GradeDocuments)</span><br><span class="line">        .invoke([&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt&#125;])</span><br><span class="line">    )</span><br><span class="line">    #print(response)</span><br><span class="line">    score = response.binary_score  # 获取二元评分结果</span><br><span class="line"> </span><br><span class="line">    # 根据评分决定下一步操作</span><br><span class="line">    if score == &quot;yes&quot;:</span><br><span class="line">        return &quot;generate_answer&quot;  # 文档相关，生成答案</span><br><span class="line">    else:</span><br><span class="line">        return &quot;rewrite_question&quot;  # 文档不相关，重写问题后重新检索</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">from langchain_core.messages import convert_to_messages</span><br><span class="line"></span><br><span class="line">input = &#123;</span><br><span class="line">    &quot;messages&quot;: convert_to_messages(#将一系列消息转换为 BaseMessage 类型的消息列表。</span><br><span class="line">        [</span><br><span class="line">            &#123;</span><br><span class="line">                &quot;role&quot;: &quot;user&quot;,</span><br><span class="line">                &quot;content&quot;: &quot;What does Lilian Weng say about types of reward hacking?&quot;,</span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                &quot;role&quot;: &quot;assistant&quot;,</span><br><span class="line">                &quot;content&quot;: &quot;&quot;,</span><br><span class="line">                &quot;tool_calls&quot;: [</span><br><span class="line">                    &#123;</span><br><span class="line">                        &quot;id&quot;: &quot;1&quot;,</span><br><span class="line">                        &quot;name&quot;: &quot;retrieve_blog_posts&quot;,</span><br><span class="line">                        &quot;args&quot;: &#123;&quot;query&quot;: &quot;types of reward hacking&quot;&#125;,</span><br><span class="line">                    &#125;</span><br><span class="line">                ],</span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;&quot;role&quot;: &quot;tool&quot;, &quot;content&quot;: &quot;meow&quot;, &quot;tool_call_id&quot;: &quot;1&quot;&#125;,</span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line">&#125;</span><br><span class="line">grade_documents(input)</span><br></pre></td></tr></table></figure>
<h3 id="重写问题">5. 重写问题</h3>
<p>构建 <code>rewrite_question</code> 节点。</p>
<p>检索工具可能会返回潜在的不相关文档，这表明需要改进原始用户问题。为此，我们将调用
<code>rewrite_question</code> 节点：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">REWRITE_PROMPT = (</span><br><span class="line">    &quot;Look at the input and try to reason about the underlying semantic intent / meaning.\n&quot;</span><br><span class="line">    &quot;Here is the initial question:&quot;</span><br><span class="line">    &quot;\n ------- \n&quot;</span><br><span class="line">    &quot;&#123;question&#125;&quot;</span><br><span class="line">    &quot;\n ------- \n&quot;</span><br><span class="line">    &quot;Formulate an improved question:&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">def rewrite_question(state: MessagesState):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    重写用户最初的提问，以更好地表达其语义意图。</span><br><span class="line">    </span><br><span class="line">    参数:</span><br><span class="line">        state: 包含消息历史的状态对象，其中第一条消息是用户原始问题。</span><br><span class="line">    </span><br><span class="line">    返回:</span><br><span class="line">        一个字典，包含一条新的用户消息，内容为改写后的问题。</span><br><span class="line">        该消息将用于后续的检索步骤，以提高检索结果的相关性。</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    messages = state[&quot;messages&quot;]</span><br><span class="line">    question = messages[0].content  # 获取用户最初的提问</span><br><span class="line">    prompt = REWRITE_PROMPT.format(question=question)  # 将问题填入提示模板</span><br><span class="line">    response = llm.invoke([&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt&#125;])  # 调用模型生成改写后的问题</span><br><span class="line">    </span><br><span class="line">    # 返回新的消息结构，内容为改写后的问题</span><br><span class="line">    return &#123;&quot;messages&quot;: [&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: response.content&#125;]&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">input = &#123;</span><br><span class="line">    &quot;messages&quot;: convert_to_messages(</span><br><span class="line">        [</span><br><span class="line">            &#123;</span><br><span class="line">                &quot;role&quot;: &quot;user&quot;,</span><br><span class="line">                &quot;content&quot;: &quot;What does Lilian Weng say about types of reward hacking?&quot;,</span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                &quot;role&quot;: &quot;assistant&quot;,</span><br><span class="line">                &quot;content&quot;: &quot;&quot;,</span><br><span class="line">                &quot;tool_calls&quot;: [</span><br><span class="line">                    &#123;</span><br><span class="line">                        &quot;id&quot;: &quot;1&quot;,</span><br><span class="line">                        &quot;name&quot;: &quot;retrieve_blog_posts&quot;,</span><br><span class="line">                        &quot;args&quot;: &#123;&quot;query&quot;: &quot;types of reward hacking&quot;&#125;,</span><br><span class="line">                    &#125;</span><br><span class="line">                ],</span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;&quot;role&quot;: &quot;tool&quot;, &quot;content&quot;: &quot;meow&quot;, &quot;tool_call_id&quot;: &quot;1&quot;&#125;,</span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">response = rewrite_question(input)</span><br><span class="line">print(response[&quot;messages&quot;][-1][&quot;content&quot;])</span><br></pre></td></tr></table></figure>
<h3 id="生成答案">6. 生成答案</h3>
<p>构建 <code>generate_answer</code>
节点：如果我们通过了评分器的检查，我们可以根据原始问题和检索到的上下文生成最终答案：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">GENERATE_PROMPT = (</span><br><span class="line">    &quot;You are an assistant for question-answering tasks. &quot;</span><br><span class="line">    &quot;Use the following pieces of retrieved context to answer the question. &quot;</span><br><span class="line">    &quot;If you don&#x27;t know the answer, just say that you don&#x27;t know. &quot;</span><br><span class="line">    &quot;Use three sentences maximum and keep the answer concise.\n&quot;</span><br><span class="line">    &quot;Question: &#123;question&#125; \n&quot;</span><br><span class="line">    &quot;Context: &#123;context&#125;&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def generate_answer(state: MessagesState):</span><br><span class="line">    &quot;&quot;&quot;Generate an answer.&quot;&quot;&quot;</span><br><span class="line">    question = state[&quot;messages&quot;][0].content</span><br><span class="line">    context = state[&quot;messages&quot;][-1].content</span><br><span class="line">    prompt = GENERATE_PROMPT.format(question=question, context=context)</span><br><span class="line">    response = llm.invoke([&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt&#125;])</span><br><span class="line">    return &#123;&quot;messages&quot;: [response]&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">input = &#123;</span><br><span class="line">    &quot;messages&quot;: convert_to_messages(</span><br><span class="line">        [</span><br><span class="line">            &#123;</span><br><span class="line">                &quot;role&quot;: &quot;user&quot;,</span><br><span class="line">                &quot;content&quot;: &quot;What does Lilian Weng say about types of reward hacking?&quot;,</span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                &quot;role&quot;: &quot;assistant&quot;,</span><br><span class="line">                &quot;content&quot;: &quot;&quot;,</span><br><span class="line">                &quot;tool_calls&quot;: [</span><br><span class="line">                    &#123;</span><br><span class="line">                        &quot;id&quot;: &quot;1&quot;,</span><br><span class="line">                        &quot;name&quot;: &quot;retrieve_blog_posts&quot;,</span><br><span class="line">                        &quot;args&quot;: &#123;&quot;query&quot;: &quot;types of reward hacking&quot;&#125;,</span><br><span class="line">                    &#125;</span><br><span class="line">                ],</span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                &quot;role&quot;: &quot;tool&quot;,</span><br><span class="line">                &quot;content&quot;: &quot;reward hacking can be categorized into two types: environment or goal misspecification, and reward tampering&quot;,</span><br><span class="line">                &quot;tool_call_id&quot;: &quot;1&quot;,</span><br><span class="line">            &#125;,</span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">response = generate_answer(input)</span><br><span class="line">response[&quot;messages&quot;][-1].pretty_print()</span><br></pre></td></tr></table></figure>
<h3 id="组装图表">7. 组装图表</h3>
<p>以 <code>generate_query_or_respond</code> 开头，并确定是否需要调用
<code>retriever_tool</code></p>
<p>使用 <code>tools_condition</code> 跳转到下一步：</p>
<ul>
<li>如果 <code>generate_query_or_respond</code> 返回
<code>tool_calls</code> ，调用 <code>retriever_tool</code>
获取上下文</li>
<li>否则，直接回复用户</li>
</ul>
<p>对检索到的文档内容按与问题的相关性（ <code>grade_documents</code>
）进行评分，并路由到下一步：</p>
<ul>
<li>如果不相关，使用 <code>rewrite_question</code>
重写问题，然后再次调用 <code>generate_query_or_respond</code></li>
<li>如果相关，请继续到 <code>generate_answer</code>
并使用检索到的文档上下文生成最终响应 <code>ToolMessage</code></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">from langgraph.graph import StateGraph, START, END</span><br><span class="line">from langgraph.prebuilt import ToolNode</span><br><span class="line">from langgraph.prebuilt import tools_condition</span><br><span class="line"></span><br><span class="line"># 创建一个基于状态图（StateGraph）的流程，用于管理对话或任务的执行流程</span><br><span class="line">workflow = StateGraph(MessagesState)</span><br><span class="line"></span><br><span class="line"># 定义流程中将循环执行的各个节点</span><br><span class="line">workflow.add_node(generate_query_or_respond)          # 判断是生成检索查询还是直接回复用户</span><br><span class="line">workflow.add_node(&quot;retrieve&quot;, ToolNode([retriever_tool]))  # 检索节点：使用检索工具（retriever_tool）从知识库中查找相关文档</span><br><span class="line">workflow.add_node(rewrite_question)                  # 重写问题节点：当检索结果不相关时，优化并重写用户的问题</span><br><span class="line">workflow.add_node(generate_answer)                   # 生成答案节点：基于检索到的信息生成最终回答</span><br><span class="line"></span><br><span class="line"># 设置流程的起始点：从 `generate_query_or_respond` 节点开始</span><br><span class="line">workflow.add_edge(START, &quot;generate_query_or_respond&quot;)</span><br><span class="line"></span><br><span class="line"># 添加条件边：决定是否进行文档检索</span><br><span class="line">workflow.add_conditional_edges(</span><br><span class="line">    &quot;generate_query_or_respond&quot;,</span><br><span class="line">    # 使用 `tools_condition` 函数判断 LLM 的输出意图：</span><br><span class="line">    # 如果 LLM 决定调用 `retriever_tool` 工具，则进入检索；如果选择直接回复，则结束流程</span><br><span class="line">    tools_condition,</span><br><span class="line">    &#123;</span><br><span class="line">        # 将条件判断结果映射到图中的具体节点</span><br><span class="line">        &quot;tools&quot;: &quot;retrieve&quot;,   # 若需调用工具，则跳转到检索节点</span><br><span class="line">        END: END               # 若无需调用工具（即可以直接回答），则结束流程</span><br><span class="line">    &#125;,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># 在 `retrieve` 节点执行后，根据文档相关性判断下一步操作</span><br><span class="line">workflow.add_conditional_edges(</span><br><span class="line">    &quot;retrieve&quot;,</span><br><span class="line">    # 调用 `grade_documents` 函数评估检索到的文档是否与问题相关</span><br><span class="line">    grade_documents,</span><br><span class="line">    # 根据评分结果决定流向：</span><br><span class="line">    # - 如果相关，进入 `generate_answer`</span><br><span class="line">    # - 如果不相关，进入 `rewrite_question`</span><br><span class="line">    # （该逻辑在 `grade_documents` 函数中返回 &quot;generate_answer&quot; 或 &quot;rewrite_question&quot;）</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># 添加固定边：生成答案后流程结束</span><br><span class="line">workflow.add_edge(&quot;generate_answer&quot;, END)</span><br><span class="line"></span><br><span class="line"># 重写问题后，回到初始节点重新判断是否需要检索</span><br><span class="line">workflow.add_edge(&quot;rewrite_question&quot;, &quot;generate_query_or_respond&quot;)</span><br><span class="line"></span><br><span class="line"># 编译整个工作流，生成可执行的图结构</span><br><span class="line">graph = workflow.compile()</span><br></pre></td></tr></table></figure>
<figure>
<img src="/2025/08/19/%E5%AD%A6%E4%B9%A0/agent%E5%AE%9E%E6%88%98/agentic-rag/image-20250826170834571.png" alt="image-20250826170834571">
<figcaption aria-hidden="true">image-20250826170834571</figcaption>
</figure>
<h3 id="参考资料">参考资料</h3>
<p><a target="_blank" rel="noopener" href="https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_agentic_rag/#2-create-a-retriever-tool">《Agentic
RAG》 — Agentic RAG</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/08/18/%E5%AD%A6%E4%B9%A0/agent%E5%AE%9E%E6%88%98/Multi-Agent%20Workflows%E5%AE%9E%E6%88%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/08/18/%E5%AD%A6%E4%B9%A0/agent%E5%AE%9E%E6%88%98/Multi-Agent%20Workflows%E5%AE%9E%E6%88%98/" class="post-title-link" itemprop="url">Multi-Agent Workflows梳理与实战</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-08-18 00:00:00" itemprop="dateCreated datePublished" datetime="2025-08-18T00:00:00+08:00">2025-08-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-08-28 15:13:55" itemprop="dateModified" datetime="2025-08-28T15:13:55+08:00">2025-08-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/agent%E5%AE%9E%E6%88%98/" itemprop="url" rel="index"><span itemprop="name">agent实战</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="前言">前言</h3>
<p>代码见<a target="_blank" rel="noopener" href="https://github.com/zxj-2023/learn-rag-langchain/tree/main/multi-agent">learn-rag-langchain/multi-agent
at main · zxj-2023/learn-rag-langchain</a></p>
<h3 id="什么是多智能体">什么是多智能体</h3>
<p>当我们谈论”多智能体”时，我们指的是由llm驱动的多个独立的agent以<strong>特定方式</strong>连接在一起。</p>
<p>每个agent可以拥有自己的提示、LLM、工具和其他自定义代码，以最佳方式与其他智能体协作。</p>
<p>这种思维方式非常适合用图来表示，就像 <code>langgraph</code>
所提供的那样。在这种方法中，每个智能体都是图中的<strong>一个节点</strong>，而它们之间的<strong>连接则表示为一条边</strong>。<strong>控制流由边管理</strong>，它们通过向图的状态中<strong>添加信息来进行通信</strong>。</p>
<h3 id="多智能体架构梳理">多智能体架构梳理</h3>
<p>langgraph给我们提供了几种<strong>多智能体架构</strong></p>
<figure>
<img src="/2025/08/18/%E5%AD%A6%E4%B9%A0/agent%E5%AE%9E%E6%88%98/Multi-Agent%20Workflows%E5%AE%9E%E6%88%98/image-20250818163359517.png" alt="image-20250818163359517">
<figcaption aria-hidden="true">image-20250818163359517</figcaption>
</figure>
<p><strong>Network</strong>:
每个智能体可以与其他所有智能体通信。任何智能体都可以决定下一步调用哪个其他智能体。</p>
<p><a target="_blank" rel="noopener" href="https://langchain-ai.github.io/langgraph/tutorials/multi_agent/multi-agent-collaboration/">Multi-agent
network</a></p>
<p><strong>Supervisor</strong>：每个智能体与一个单一的监督者智能体通信。监督者智能体决定下一步应该调用哪个智能体。</p>
<p><a target="_blank" rel="noopener" href="https://langchain-ai.github.io/langgraph/tutorials/multi_agent/agent_supervisor/">代理监督者
— Agent Supervisor</a></p>
<p><strong>Hierarchical</strong>:
你可以定义一个具有监督者监督者的多代理系统。这是监督者架构的泛化，并允许更复杂的控制流程。</p>
<p><a target="_blank" rel="noopener" href="https://langchain-ai.github.io/langgraph/tutorials/multi_agent/hierarchical_agent_teams/">层级代理团队
— Hierarchical Agent Teams</a></p>
<p><strong>Custom multi-agent workflow</strong>:
每个代理只与代理子集通信。流程的部分是确定的，只有一些代理可以决定下一步调用哪些其他代理。</p>
<h3 id="agent-supervisor">Agent Supervisor</h3>
<p>在本教程中，你将构建一个包含两个代理的监督者系统——一个研究专家和一个数学专家。</p>
<h4 id="环境">环境</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -U langgraph langgraph-supervisor langchain-tavily &quot;langchain[openai]&quot;</span><br></pre></td></tr></table></figure>
<h4 id="创建工作代理">1. 创建工作代理</h4>
<p>首先，让我们创建我们的专业工作代理——研究代理和数学代理：</p>
<ul>
<li>研究代理将使用 Tavily API 访问网络搜索工具 <a target="_blank" rel="noopener" href="https://www.tavily.com/">Tavily - The Web Access Layer for AI
Agents</a></li>
<li>数学代理将访问简单的数学工具（ <code>add</code> ,
<code>multiply</code> , <code>divide</code> ）</li>
</ul>
<h5 id="研究代理">研究代理</h5>
<p>对于网络搜索，我们将使用 <code>TavilySearch</code> 工具来自
<code>langchain-tavily</code> :</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from langchain_tavily import TavilySearch</span><br><span class="line"></span><br><span class="line">web_search = TavilySearch(max_results=3,tavily_api_key=&quot;tvly-dev-&quot;)</span><br><span class="line">web_search_results = web_search.invoke(&quot;南京在哪&quot;)</span><br><span class="line"></span><br><span class="line">print(web_search_results[&quot;results&quot;][0][&quot;content&quot;])</span><br></pre></td></tr></table></figure>
<p>为了创建单个工作代理，我们将使用 LangGraph 的<a target="_blank" rel="noopener" href="https://langchain-ai.github.io/langgraph/agents/agents/">预构建代理</a>。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">from langgraph.prebuilt import create_react_agent</span><br><span class="line">from langchain_openai import ChatOpenAI</span><br><span class="line"></span><br><span class="line">llm=ChatOpenAI(</span><br><span class="line">    model=&quot;qwen3-235b-a22b-thinking-2507&quot;,</span><br><span class="line">    api_key=&quot;sk-&quot;,</span><br><span class="line">    base_url=&quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">research_agent = create_react_agent(</span><br><span class="line">    model=llm,</span><br><span class="line">    tools=[web_search],</span><br><span class="line">    prompt=(</span><br><span class="line">        &quot;你是一个研究代理。\n\n指令：\n- 仅协助与研究相关的任务，不得进行任何数学计算\n- 完成任务后，直接向主管回复\n- 仅回复你的工作结果，不得包含任何其他文字。&quot;</span><br><span class="line">    ),</span><br><span class="line">    name=&quot;research_agent&quot;,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>让我们运行代理来验证它的行为是否符合预期。<strong>我们将使用</strong>
<code>pretty_print_messages</code>
<strong>辅助工具来美观地渲染流式代理输出</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">from langchain_core.messages import convert_to_messages</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def pretty_print_message(message, indent=False):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    美化打印单条消息</span><br><span class="line">    </span><br><span class="line">    Args:</span><br><span class="line">        message: 要打印的消息对象</span><br><span class="line">        indent: 是否需要缩进打印</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    # 将消息转换为美观的HTML格式表示</span><br><span class="line">    pretty_message = message.pretty_repr(html=True)</span><br><span class="line">    if not indent:</span><br><span class="line">        # 如果不需要缩进，直接打印</span><br><span class="line">        print(pretty_message)</span><br><span class="line">        return</span><br><span class="line"></span><br><span class="line">    # 如果需要缩进，为每一行添加制表符前缀</span><br><span class="line">    indented = &quot;\n&quot;.join(&quot;\t&quot; + c for c in pretty_message.split(&quot;\n&quot;))</span><br><span class="line">    print(indented)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def pretty_print_messages(update, last_message=False):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    美化打印消息更新</span><br><span class="line">    </span><br><span class="line">    Args:</span><br><span class="line">        update: 包含消息更新的数据结构</span><br><span class="line">        last_message: 是否只打印最后一条消息</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    is_subgraph = False  # 标记是否为子图更新</span><br><span class="line">    </span><br><span class="line">    # 检查更新是否为元组格式（包含命名空间信息）</span><br><span class="line">    if isinstance(update, tuple):</span><br><span class="line">        ns, update = update</span><br><span class="line">        # 如果命名空间为空，跳过父图更新的打印</span><br><span class="line">        if len(ns) == 0:</span><br><span class="line">            return</span><br><span class="line"></span><br><span class="line">        # 提取图ID并打印子图更新信息</span><br><span class="line">        graph_id = ns[-1].split(&quot;:&quot;)[0]</span><br><span class="line">        print(f&quot;来自子图 &#123;graph_id&#125; 的更新:&quot;)</span><br><span class="line">        print(&quot;\n&quot;)</span><br><span class="line">        is_subgraph = True</span><br><span class="line"></span><br><span class="line">    # 遍历每个节点的更新</span><br><span class="line">    for node_name, node_update in update.items():</span><br><span class="line">        # 构造更新标签</span><br><span class="line">        update_label = f&quot;来自节点 &#123;node_name&#125; 的更新:&quot;</span><br><span class="line">        if is_subgraph:</span><br><span class="line">            # 如果是子图，添加缩进</span><br><span class="line">            update_label = &quot;\t&quot; + update_label</span><br><span class="line"></span><br><span class="line">        print(update_label)</span><br><span class="line">        print(&quot;\n&quot;)</span><br><span class="line"></span><br><span class="line">        # 将节点更新中的消息转换为消息对象列表</span><br><span class="line">        messages = convert_to_messages(node_update[&quot;messages&quot;])</span><br><span class="line">        # 如果只要求最后一条消息，则截取最后一条</span><br><span class="line">        if last_message:</span><br><span class="line">            messages = messages[-1:]</span><br><span class="line"></span><br><span class="line">        # 打印每条消息</span><br><span class="line">        for m in messages:</span><br><span class="line">            pretty_print_message(m, indent=is_subgraph)</span><br><span class="line">        print(&quot;\n&quot;)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">for chunk in research_agent.stream(</span><br><span class="line">    &#123;&quot;messages&quot;: [&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;南京在哪?&quot;&#125;]&#125;</span><br><span class="line">):</span><br><span class="line">    pretty_print_messages(chunk)</span><br></pre></td></tr></table></figure>
<h5 id="数学代理">数学代理</h5>
<p>对于数学代理工具，我们将使用纯 Python 函数：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">def add(a: float, b: float):</span><br><span class="line">    &quot;&quot;&quot;将两个数字相加。&quot;&quot;&quot;</span><br><span class="line">    return a + b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def multiply(a: float, b: float):</span><br><span class="line">    &quot;&quot;&quot;将两个数字相乘。&quot;&quot;&quot;</span><br><span class="line">    return a * b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def divide(a: float, b: float):</span><br><span class="line">    &quot;&quot;&quot;将两个数字相除。&quot;&quot;&quot;</span><br><span class="line">    return a / b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">math_agent = create_react_agent(</span><br><span class="line">    model=llm,</span><br><span class="line">    tools=[add, multiply, divide],</span><br><span class="line">    prompt=(</span><br><span class="line">        &quot;你是一个数学代理。\n\n&quot;</span><br><span class="line">        &quot;指令：\n&quot;</span><br><span class="line">        &quot;- 仅协助处理数学相关任务\n&quot;</span><br><span class="line">        &quot;- 完成任务后，直接回复给主管\n&quot;</span><br><span class="line">        &quot;- 仅回复你的工作结果，不要包含任何其他文字。&quot;</span><br><span class="line">    ),</span><br><span class="line">    name=&quot;math_agent&quot;,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>让我们运行数学代理：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">for chunk in math_agent.stream(</span><br><span class="line">    &#123;&quot;messages&quot;: [&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;what&#x27;s (3 + 5) x 7&quot;&#125;]&#125;</span><br><span class="line">):</span><br><span class="line">    pretty_print_messages(chunk)</span><br></pre></td></tr></table></figure>
<h4 id="创建监督者-langgraph-supervisor">2.创建监督者
<code>langgraph-supervisor</code></h4>
<p>为了实现我们的多智能体系统，我们将使用预构建的
<code>langgraph-supervisor</code> 库中的 <a target="_blank" rel="noopener" href="https://langchain-ai.github.io/langgraph/reference/supervisor/#langgraph_supervisor.supervisor.create_supervisor"><code>create_supervisor</code></a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">from langgraph_supervisor import create_supervisor</span><br><span class="line">from langchain.chat_models import init_chat_model</span><br><span class="line"></span><br><span class="line">supervisor = create_supervisor(</span><br><span class="line">    model=llm,</span><br><span class="line">    agents=[research_agent, math_agent],</span><br><span class="line">    prompt=(</span><br><span class="line">        &quot;你是一个管理两个代理的主管：\n&quot;</span><br><span class="line">        &quot;- 一个研究代理。将研究相关任务分配给这个代理\n&quot;</span><br><span class="line">        &quot;- 一个数学代理。将数学相关任务分配给这个代理\n&quot;</span><br><span class="line">        &quot;一次只分配工作给一个代理，不要并行调用代理。\n&quot;</span><br><span class="line">        &quot;不要自己做任何工作。&quot;</span><br><span class="line">    ),</span><br><span class="line">    add_handoff_back_messages=True,</span><br><span class="line">    output_mode=&quot;full_history&quot;,</span><br><span class="line">).compile()</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from IPython.display import display, Image</span><br><span class="line"></span><br><span class="line">display(Image(supervisor.get_graph().draw_mermaid_png()))</span><br></pre></td></tr></table></figure>
<figure>
<img src="/2025/08/18/%E5%AD%A6%E4%B9%A0/agent%E5%AE%9E%E6%88%98/Multi-Agent%20Workflows%E5%AE%9E%E6%88%98/image-20250819113009108.png" alt="image-20250819113009108">
<figcaption aria-hidden="true">image-20250819113009108</figcaption>
</figure>
<p>现在让我们用一个需要两个代理的查询来运行它：</p>
<p>研究代理将查找必要的 GDP 信息；数学代理将执行除法以找到纽约州 GDP
的百分比，如所请求</p>
<h4 id="从头创建监督者">3.从头创建监督者</h4>
<p>现在让我们从头实现这个多智能体系统。我们需要：</p>
<ol type="1">
<li>设置主管如何与各个代理进行沟通</li>
<li>创建监督代理</li>
<li>将监督代理和工作代理组合成一个多代理图。</li>
</ol>
<h5 id="设置代理通信">设置代理通信</h5>
<p>我们需要定义一种方式，让监督代理能够与工作代理进行通信。在多代理架构中，实现这一功能的一种常见方法是使用<strong>handoffs</strong>，即一个代理将控制权交给另一个代理。交接允许你指定：</p>
<ul>
<li><strong>destination</strong>:要转移到的目标代理</li>
<li><strong>payload</strong>:要传递给该智能体的信息</li>
</ul>
<p>我们将通过<strong>handoff
tools</strong>（转接工具）实现转接，并将这些工具交给监督代理：当监督代理调用这些工具时，它将控制权转交给工作代理，并将完整消息历史传递给该代理。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line">from typing import Annotated</span><br><span class="line">from langchain_core.tools import tool, InjectedToolCallId</span><br><span class="line">from langgraph.prebuilt import InjectedState</span><br><span class="line">from langgraph.graph import StateGraph, START, MessagesState</span><br><span class="line">from langgraph.types import Command</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def create_handoff_tool(*, agent_name: str, description: str | None = None):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    创建一个“交接”工具函数，用于在 LangGraph 的 Supervisor-Worker 架构中</span><br><span class="line">    把当前对话状态移交给指定名称的子 Agent。</span><br><span class="line"></span><br><span class="line">    参数</span><br><span class="line">    ----</span><br><span class="line">    agent_name : str</span><br><span class="line">        目标子 Agent 的名称，必须与 Supervisor 图中注册的节点名一致。</span><br><span class="line">    description : str | None</span><br><span class="line">        工具的描述文本。如果为 None，则使用默认描述 &quot;Ask &#123;agent_name&#125; for help.&quot;。</span><br><span class="line"></span><br><span class="line">    返回</span><br><span class="line">    ----</span><br><span class="line">    handoff_tool : Callable</span><br><span class="line">        一个已用 @tool 装饰的函数，可直接注入到 Supervisor 的工具列表。</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    # 动态生成工具名，例如 agent_name=&quot;math_agent&quot; -&gt; &quot;transfer_to_math_agent&quot;</span><br><span class="line">    name = f&quot;transfer_to_&#123;agent_name&#125;&quot;</span><br><span class="line"></span><br><span class="line">    # 如果调用者没有提供描述，则使用默认描述</span><br><span class="line">    description = description or f&quot;Ask &#123;agent_name&#125; for help.&quot;</span><br><span class="line"></span><br><span class="line">    # 用 LangGraph 的 @tool 装饰器注册工具</span><br><span class="line">    @tool(name, description=description)</span><br><span class="line">    def handoff_tool(</span><br><span class="line">        state: Annotated[MessagesState, InjectedState],</span><br><span class="line">        tool_call_id: Annotated[str, InjectedToolCallId],</span><br><span class="line">    ) -&gt; Command:</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        实际执行交接逻辑的工具函数。</span><br><span class="line"></span><br><span class="line">        参数</span><br><span class="line">        ----</span><br><span class="line">        state : MessagesState</span><br><span class="line">            当前对话状态，由 LangGraph 注入。</span><br><span class="line">        tool_call_id : str</span><br><span class="line">            本次工具调用的唯一 ID，由 LangGraph 注入。</span><br><span class="line"></span><br><span class="line">        返回</span><br><span class="line">        ----</span><br><span class="line">        Command</span><br><span class="line">            一个 LangGraph Command 对象，告诉框架：</span><br><span class="line">            - goto=agent_name        : 跳转到哪个子 Agent</span><br><span class="line">            - update                 : 更新后的状态</span><br><span class="line">            - graph=Command.PARENT   : 在父图（Supervisor）作用域内执行</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        # 构造一条工具消息，记录交接动作</span><br><span class="line">        tool_message = &#123;</span><br><span class="line">            &quot;role&quot;: &quot;tool&quot;,</span><br><span class="line">            &quot;content&quot;: f&quot;Successfully transferred to &#123;agent_name&#125;&quot;,</span><br><span class="line">            &quot;name&quot;: name,</span><br><span class="line">            &quot;tool_call_id&quot;: tool_call_id,</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        # 使用 Command 把对话状态连同新消息一起发送到目标 Agent</span><br><span class="line">        return Command(</span><br><span class="line">            goto=agent_name,</span><br><span class="line">            update=&#123;**state, &quot;messages&quot;: state[&quot;messages&quot;] + [tool_message]&#125;,</span><br><span class="line">            graph=Command.PARENT,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    # 返回已装饰的工具函数，供 Supervisor 添加进 tools 列表</span><br><span class="line">    return handoff_tool</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 创建研究代理的交接工具</span><br><span class="line">assign_to_research_agent = create_handoff_tool(</span><br><span class="line">    agent_name=&quot;research_agent&quot;,</span><br><span class="line">    description=&quot;Assign task to a researcher agent.&quot;,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># 创建数学代理的交接工具</span><br><span class="line">assign_to_math_agent = create_handoff_tool(</span><br><span class="line">    agent_name=&quot;math_agent&quot;,</span><br><span class="line">    description=&quot;Assign task to a math agent.&quot;,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h5 id="创建监督代理">创建监督代理</h5>
<p>然后，我们使用刚刚定义的交接工具来创建监督代理。我们将使用预构建的
<code>create_react_agent</code> :</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">supervisor_agent = create_react_agent(</span><br><span class="line">    model=llm,</span><br><span class="line">    tools=[assign_to_research_agent, assign_to_math_agent],</span><br><span class="line">    prompt=(</span><br><span class="line">        &quot;你是一个管理两个代理的主管：\n&quot;</span><br><span class="line">        &quot;- 一个研究代理。将研究相关任务分配给这个代理\n&quot;</span><br><span class="line">        &quot;- 一个数学代理。将数学相关任务分配给这个代理\n&quot;</span><br><span class="line">        &quot;一次只分配工作给一个代理，不要并行调用代理。\n&quot;</span><br><span class="line">        &quot;不要自己做任何工作。&quot;</span><br><span class="line">    ),</span><br><span class="line">    name=&quot;supervisor&quot;,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h5 id="创建多智能体图">创建多智能体图</h5>
<p>将这些内容整合起来，让我们为我们的整体多代理系统创建一个图。我们将添加监督代理和各个代理作为子图节点。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">from langgraph.graph import END</span><br><span class="line"></span><br><span class="line"># 定义多代理主管图</span><br><span class="line">supervisor = (</span><br><span class="line">    StateGraph(MessagesState)</span><br><span class="line">    # 注意：`destinations` 仅用于可视化，不影响运行时行为</span><br><span class="line">    .add_node(supervisor_agent, destinations=(&quot;research_agent&quot;, &quot;math_agent&quot;, END))</span><br><span class="line">    .add_node(research_agent)</span><br><span class="line">    .add_node(math_agent)</span><br><span class="line">    .add_edge(START, &quot;supervisor&quot;)</span><br><span class="line">    # 总是返回到主管</span><br><span class="line">    .add_edge(&quot;research_agent&quot;, &quot;supervisor&quot;)</span><br><span class="line">    .add_edge(&quot;math_agent&quot;, &quot;supervisor&quot;)</span><br><span class="line">    .compile()</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>在这个代码中，去 <code>research_agent</code> 和
<code>math_agent</code>
的条件边是通过<strong>工具调用</strong>实现的，而不是显式的条件边。</p>
<p>工作机制：</p>
<ol type="1">
<li><p><strong>工具作为交接手段</strong>：</p>
<ul>
<li><code>assign_to_research_agent</code> 和
<code>assign_to_math_agent</code> 这两个工具被添加到
<code>supervisor_agent</code> 中</li>
<li>当 supervisor_agent 决定需要某个代理帮助时，它会调用相应的工具</li>
</ul></li>
<li><p><strong>工具内部实现交接</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def handoff_tool(...) -&gt; Command:</span><br><span class="line">    return Command(</span><br><span class="line">        goto=agent_name,  # 这里指定了要跳转到哪个代理</span><br><span class="line">        update=&#123;...&#125;,</span><br><span class="line">        graph=Command.PARENT,</span><br><span class="line">    )</span><br></pre></td></tr></table></figure></li>
<li><p><strong>隐式的条件边</strong>：</p>
<ul>
<li>当 supervisor_agent 调用 <code>assign_to_research_agent</code>
工具时 → 自动跳转到 <code>research_agent</code></li>
<li>当 supervisor_agent 调用 <code>assign_to_math_agent</code> 工具时 →
自动跳转到 <code>math_agent</code></li>
</ul></li>
</ol>
</blockquote>
<blockquote>
<p>什么是 Command 机制</p>
<p>Command 机制是 LangGraph
提供的一种<strong>显式控制流程跳转</strong>的方式。它允许工具或节点直接指定下一步要执行什么操作，而不需要通过传统的条件边路由。</p>
<p>Command 的核心概念</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from langgraph.types import Command</span><br><span class="line"></span><br><span class="line">Command(</span><br><span class="line">    goto=agent_name,           # 要跳转到的目标节点</span><br><span class="line">    update=state_update,       # 要更新的状态</span><br><span class="line">    graph=Command.PARENT      # 在哪个图中执行（父图/子图）</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
</blockquote>
<p>请注意，我们已经从工作代理添加了明确的边回到主管——这意味着它们保证会将控制权返回给主管。如果你希望代理直接响应用户（即，将系统转变为路由器），你可以移除这些边。</p>
<h3 id="multi-agent-network">Multi-agent network</h3>
<p>一个单一智能体通常可以使用单个领域内的一小批工具来有效运作，但即使使用像
<code>gpt-4</code> 这样强大的模型，使用多个工具时也可能效果不佳。</p>
<p>处理复杂任务的一种方法是采用“分而治之”的方法：为每个任务或领域创建一个专门的智能体，并将任务路由到正确的“专家”。这是一个多智能体网络架构的例子。</p>
<figure>
<img src="/2025/08/18/%E5%AD%A6%E4%B9%A0/agent%E5%AE%9E%E6%88%98/Multi-Agent%20Workflows%E5%AE%9E%E6%88%98/image-20250819150039818.png" alt="image-20250819150039818">
<figcaption aria-hidden="true">image-20250819150039818</figcaption>
</figure>
<p><strong>这个多agent架构，就像多个agent进行讨论，所以也叫Multi Agent
Collaboration，但是给我的感觉，比较混乱，agent直接的路由很难去定义，agent一多就搞不清楚了</strong>，所以这里也不实战了。</p>
<h3 id="hierarchical-agent-teams">Hierarchical Agent Teams</h3>
<p>对于某些应用，如果工作按层次分布，系统可能会更有效。你可以通过组合不同的子图，并创建一个顶层监督者以及中层监督者来实现这一点。</p>
<figure>
<img src="/2025/08/18/%E5%AD%A6%E4%B9%A0/agent%E5%AE%9E%E6%88%98/Multi-Agent%20Workflows%E5%AE%9E%E6%88%98/image-20250819151813264.png" alt="image-20250819151813264">
<figcaption aria-hidden="true">image-20250819151813264</figcaption>
</figure>
<p>使用预设的supervisor构建</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"># 1. 定义研究团队的代理</span><br><span class="line">@tool</span><br><span class="line">def web_search(query: str) -&gt; str:</span><br><span class="line">    &quot;&quot;&quot;执行网络搜索&quot;&quot;&quot;</span><br><span class="line">    return f&quot;搜索结果：关于&#x27;&#123;query&#125;&#x27;的最新信息...&quot;</span><br><span class="line"></span><br><span class="line">@tool</span><br><span class="line">def analyze_data(data: str) -&gt; str:</span><br><span class="line">    &quot;&quot;&quot;分析数据&quot;&quot;&quot;</span><br><span class="line">    return f&quot;数据分析结果：&#123;data&#125;的趋势显示...&quot;</span><br><span class="line"></span><br><span class="line">research_agent = create_react_agent(</span><br><span class="line">    model=llm,</span><br><span class="line">    tools=[web_search, analyze_data],</span><br><span class="line">    prompt=&quot;你是一个研究专家，负责进行网络搜索和数据分析。&quot;,</span><br><span class="line">    name=&quot;research_specialist&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># 2. 定义数学团队的代理</span><br><span class="line">@tool</span><br><span class="line">def calculate_statistics(numbers: list[float]) -&gt; str:</span><br><span class="line">    &quot;&quot;&quot;计算统计值&quot;&quot;&quot;</span><br><span class="line">    if not numbers:</span><br><span class="line">        return &quot;错误：数据列表为空&quot;</span><br><span class="line">    avg = sum(numbers) / len(numbers)</span><br><span class="line">    return f&quot;统计结果：平均值=&#123;avg:.2f&#125;，数据点数量=&#123;len(numbers)&#125;&quot;</span><br><span class="line"></span><br><span class="line">@tool</span><br><span class="line">def solve_equation(equation: str) -&gt; str:</span><br><span class="line">    &quot;&quot;&quot;解方程&quot;&quot;&quot;</span><br><span class="line">    return f&quot;方程 &#123;equation&#125; 的解为：x = 42&quot;</span><br><span class="line"></span><br><span class="line">math_agent = create_react_agent(</span><br><span class="line">    model=llm,</span><br><span class="line">    tools=[calculate_statistics, solve_equation],</span><br><span class="line">    prompt=&quot;你是一个数学专家，负责统计计算和方程求解。&quot;,</span><br><span class="line">    name=&quot;math_specialist&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># 3. 创建研究团队主管</span><br><span class="line">research_supervisor = create_supervisor(</span><br><span class="line">    model=llm,</span><br><span class="line">    agents=[research_agent],</span><br><span class="line">    prompt=(</span><br><span class="line">        &quot;你是研究团队的主管。\n&quot;</span><br><span class="line">        &quot;你的团队有一个研究专家，负责网络搜索和数据分析。\n&quot;</span><br><span class="line">        &quot;根据任务需求，将工作分配给研究专家。\n&quot;</span><br><span class="line">        &quot;等待专家完成任务后，总结结果并报告给上级主管。&quot;</span><br><span class="line">    ),</span><br><span class="line">    name=&quot;research_supervisor&quot;</span><br><span class="line">).compile(name=&quot;research_supervisor&quot;)</span><br><span class="line"></span><br><span class="line"># 4. 创建数学团队主管</span><br><span class="line">math_supervisor = create_supervisor(</span><br><span class="line">    model=llm,</span><br><span class="line">    agents=[math_agent],</span><br><span class="line">    prompt=(</span><br><span class="line">        &quot;你是数学团队的主管。\n&quot;</span><br><span class="line">        &quot;你的团队有一个数学专家，负责统计计算和方程求解。\n&quot;</span><br><span class="line">        &quot;根据任务需求，将工作分配给数学专家。\n&quot;</span><br><span class="line">        &quot;等待专家完成任务后，总结结果并报告给上级主管。&quot;</span><br><span class="line">    ),</span><br><span class="line">    name=&quot;math_supervisor&quot;</span><br><span class="line">).compile(name=&quot;math_supervisor&quot;)</span><br><span class="line"></span><br><span class="line"># 5. 创建顶层主管</span><br><span class="line">top_supervisor = create_supervisor(</span><br><span class="line">    model=llm,</span><br><span class="line">    agents=[research_supervisor, math_supervisor],</span><br><span class="line">    prompt=(</span><br><span class="line">        &quot;你是顶层主管，管理两个专业团队：\n&quot;</span><br><span class="line">        &quot;- 研究团队：负责市场调研、数据分析等任务\n&quot;</span><br><span class="line">        &quot;- 数学团队：负责统计计算、方程求解等任务\n&quot;</span><br><span class="line">        &quot;根据任务的性质，将工作分配给相应的团队主管。\n&quot;</span><br><span class="line">        &quot;等待团队完成任务后，整合所有结果并给出最终报告。&quot;</span><br><span class="line">    ),</span><br><span class="line">    name=&quot;top_supervisor&quot;</span><br><span class="line">).compile(name=&quot;top_supervisor&quot;)</span><br></pre></td></tr></table></figure>
<h3 id="参考资料">参考资料</h3>
<p><a target="_blank" rel="noopener" href="https://blog.langchain.com/langgraph-multi-agent-workflows/">LangGraph：多智能体工作流
— LangGraph: Multi-Agent Workflows</a></p>
<p><a target="_blank" rel="noopener" href="https://langchain-ai.github.io/langgraph/concepts/multi_agent/">Overview</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/08/15/%E5%AD%A6%E4%B9%A0/python-web/react/react/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/08/15/%E5%AD%A6%E4%B9%A0/python-web/react/react/" class="post-title-link" itemprop="url">python web——react</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-08-15 00:00:00" itemprop="dateCreated datePublished" datetime="2025-08-15T00:00:00+08:00">2025-08-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-08-18 15:44:22" itemprop="dateModified" datetime="2025-08-18T15:44:22+08:00">2025-08-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/python-web/" itemprop="url" rel="index"><span itemprop="name">python-web</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/python-web/react/" itemprop="url" rel="index"><span itemprop="name">react</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="前言">前言</h3>
<p>为了后续自己搭建全栈项目做准备，对react做一定的了解</p>
<p>学习目标：大致看懂react的基本语法，可以在ai的协助下完成前端的搭建</p>
<h3 id="介绍">介绍</h3>
<p>React 是 Facebook（现 Meta）于 2013 年开源的一套用于构建用户界面的
JavaScript 库，现由 React 核心团队与社区共同维护。</p>
<h3 id="项目搭建">项目搭建</h3>
<p>项目创建</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npx create-react-app my-app</span><br></pre></td></tr></table></figure>
<blockquote>
<p>npx 是什么？</p>
<p>npm 5.2+ 自带的“包运行器”（Node Package eXecute）。类似uv</p>
<p>脚手架（Scaffold / Boilerplate）是什么？</p>
<ol type="1">
<li>定义：官方或社区提供的“项目模板生成器”，一条命令就能创建带目录结构、配置、脚本、依赖的完整项目骨架。</li>
<li>目的： • 省掉繁琐的初始化、Webpack/Rollup/Vite
配置、ESLint/TypeScript/测试等环境搭建。 •
统一团队规范，降低新人上手成本。</li>
</ol>
</blockquote>
<p>启动开发服务器</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> my-app</span><br><span class="line">npm start        <span class="comment"># 或 yarn start</span></span><br></pre></td></tr></table></figure>
<p>目录速览（核心）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">my-app</span><br><span class="line">├─ public/         # 静态资源，index.html 是页面模板</span><br><span class="line">├─ src/</span><br><span class="line">│  ├─ App.js       # 根组件</span><br><span class="line">│  ├─ index.js     # 应用入口（ReactDOM.createRoot）</span><br><span class="line">└─ package.json    # 依赖与脚本</span><br></pre></td></tr></table></figure>
<h3 id="jsx">JSX</h3>
<p>JSX（JavaScript XML 的缩写）是 React
引入的一种<strong>语法糖</strong>（syntactic sugar）。它让你在
JavaScript 文件里直接写<strong>类 HTML
标记</strong>，然后由构建工具（Babel、TypeScript、esbuild、swc）把它翻译成<strong>普通的
JavaScript 函数调用</strong>。</p>
<p>如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">// 1. 找到 public/index.html 中 id=&quot;root&quot; 的 DOM 节点，作为 React 应用的挂载点</span><br><span class="line">const root = ReactDOM.createRoot(document.getElementById(&#x27;root&#x27;));</span><br><span class="line"></span><br><span class="line">// 2. 将根组件 &lt;App /&gt; 渲染到该挂载点</span><br><span class="line">root.render(</span><br><span class="line">  // 3. &lt;React.StrictMode&gt; 是 React 提供的开发模式辅助工具</span><br><span class="line">  //    作用：在开发阶段自动检测潜在问题（如过时的 API、副作用重复执行等）</span><br><span class="line">  //    注意：它仅在开发环境生效，生产环境不会渲染任何额外 DOM</span><br><span class="line">  &lt;React.StrictMode&gt;</span><br><span class="line">    &#123;/* 4. 项目真正的根组件 App，所有业务逻辑都从这里开始 */&#125;</span><br><span class="line">    &lt;App /&gt;</span><br><span class="line">  &lt;/React.StrictMode&gt;</span><br><span class="line">);</span><br></pre></td></tr></table></figure>
<h3 id="箭头函数">箭头函数</h3>
<p>React（以及所有现代 JavaScript）里，“箭头”指的是
<strong>箭头函数（Arrow Function）</strong>，语法是：</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> 函数名 = <span class="function">(<span class="params">参数</span>) =&gt;</span> 返回值或语句块</span><br></pre></td></tr></table></figure>
<p>它的作用可以概括为 <strong>“更简洁的函数声明 + 词法作用域的
this”</strong>。</p>
<p>通俗理解：把小括号的内容变成箭头后的内容</p>
<h3 id="函数组件">函数组件</h3>
<p>函数组件 + JSX 的组合作用是： <strong>以函数的形式返回“虚拟 DOM
描述”，交由 React 渲染成真实 DOM</strong>，而不是直接返回 HTML
组件或字符串。</p>
<ol type="1">
<li>函数组件的“返回值”</li>
</ol>
<figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">function</span> <span class="title function_">Welcome</span>(<span class="params">props</span>) &#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="language-xml"><span class="tag">&lt;<span class="name">h1</span>&gt;</span>Hello &#123;props.name&#125;<span class="tag">&lt;/<span class="name">h1</span>&gt;</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>经过 Babel 编译后等价于：</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">function</span> <span class="title function_">Welcome</span>(<span class="params">props</span>) &#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="title class_">React</span>.<span class="title function_">createElement</span>(<span class="string">&#x27;h1&#x27;</span>, <span class="literal">null</span>, <span class="string">&#x27;Hello &#x27;</span>, props.<span class="property">name</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>React.createElement</code> 会生成一个<strong>纯 JS
对象</strong>（虚拟节点），而不是一段 HTML 字符串。</p>
<p><strong>使用示例</strong></p>
<figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1. 接收父组件传来的 props</span></span><br><span class="line"><span class="keyword">function</span> <span class="title function_">Card</span>(<span class="params">&#123; title, children &#125;</span>) &#123;</span><br><span class="line">  <span class="comment">// 2. 返回一段 JSX（最终会被编译成虚拟 DOM）</span></span><br><span class="line">  <span class="keyword">return</span> (</span><br><span class="line">    <span class="language-xml"><span class="tag">&lt;<span class="name">div</span> <span class="attr">className</span>=<span class="string">&quot;card&quot;</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">      <span class="tag">&lt;<span class="name">h2</span>&gt;</span>&#123;title&#125;<span class="tag">&lt;/<span class="name">h2</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">      &#123;children&#125;</span></span><br><span class="line"><span class="language-xml">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span></span><br><span class="line">  );</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>使用：</p>
<figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;<span class="title class_">Card</span> title=<span class="string">&quot;函数组件&quot;</span>&gt;</span><br><span class="line">  <span class="language-xml"><span class="tag">&lt;<span class="name">p</span>&gt;</span>Hello, world!<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span></span><br><span class="line">&lt;/<span class="title class_">Card</span>&gt;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>DOM（Document Object Model，文档对象模型）是浏览器在内存里把一份
HTML/XML
文档表示成<strong>树形结构</strong>的<strong>编程接口</strong>（API）。</p>
<p>每个节点（元素、文本、注释…）都是一个对象，拥有属性与方法，例如：</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> title = <span class="variable language_">document</span>.<span class="title function_">getElementById</span>(<span class="string">&#x27;title&#x27;</span>);</span><br><span class="line">title.<span class="property">textContent</span> = <span class="string">&#x27;Hi React&#x27;</span>;   <span class="comment">// 改文本</span></span><br><span class="line">title.<span class="property">style</span>.<span class="property">color</span> = <span class="string">&#x27;red&#x27;</span>;        <span class="comment">// 改样式</span></span><br></pre></td></tr></table></figure>
</blockquote>
<h3 id="插值写法">插值写法</h3>
<p>在 React 中，“插值”专指<strong>把一段 JavaScript 表达式的实时结果塞进
JSX</strong> 的写法。 核心符号只有一对花括号
<code>&#123; &#125;</code>，记住口诀：<strong>“JSX 里凡是 {} 包起来的，就是
JavaScript 运行后的值。”</strong></p>
<p><strong>基本文本插值</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">const name = &#x27;React&#x27;;</span><br><span class="line">&lt;h1&gt;Hello, &#123;name&#125;!&lt;/h1&gt;          // → Hello, React!</span><br></pre></td></tr></table></figure>
<p><strong>属性插值</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">function App() &#123;</span><br><span class="line">  const mytitle=&quot;hello&quot;</span><br><span class="line">  return (</span><br><span class="line">    &lt;div title=&#123;mytitle&#125;&gt;&lt;/div&gt;</span><br><span class="line">  );</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="数据渲染">数据渲染</h3>
<p><strong>条件渲染</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">function App() &#123;</span><br><span class="line">  const mytitle=&quot;hello&quot;</span><br><span class="line"></span><br><span class="line">  let mycontent=null</span><br><span class="line">  const flag=true</span><br><span class="line">  if(flag)&#123;</span><br><span class="line">    mycontent=&lt;h2&gt;hello&lt;/h2&gt;</span><br><span class="line">  &#125;</span><br><span class="line">  else&#123;</span><br><span class="line">    mycontent=&lt;h2&gt;world&lt;/h2&gt;</span><br><span class="line">  &#125;</span><br><span class="line">  return (</span><br><span class="line">    &lt;div title=&#123;mytitle&#125;&gt;&#123;mycontent&#125;&lt;/div&gt;</span><br><span class="line">  );</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>列表渲染</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">function App() &#123;</span><br><span class="line">  const list=[&#x27;1&#x27;,&#x27;2&#x27;,&#x27;3&#x27;]</span><br><span class="line">  const mycontent=list.map((item)=&gt;&#123;</span><br><span class="line">    return &lt;li&gt;&#123;item&#125;&lt;/li&gt;</span><br><span class="line">  &#125;)</span><br><span class="line">  return (</span><br><span class="line">    &lt;div&gt;&#123;mycontent&#125;&lt;/div&gt;</span><br><span class="line">  );</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<ol type="1">
<li><code>.map((item) =&gt; &#123; ... &#125;)</code> ‑
<code>Array.prototype.map</code>：遍历数组，把每个元素依次交给回调函数处理，并<strong>返回一个新数组</strong>。
‑ <code>(item)</code> 是每次循环拿到的当前元素。</li>
<li><code>return &lt;li&gt;&#123;item&#125;&lt;/li&gt;</code> ‑
每一次循环里，把当前元素 <code>item</code> 用 JSX 插值语法
<code>&#123;item&#125;</code> 放进 <code>&lt;li&gt;</code> 标签里。</li>
</ol>
</blockquote>
<h3 id="状态处理">状态处理</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import &#123; useState &#125; from &#x27;react&#x27;;</span><br><span class="line">function App() &#123;</span><br><span class="line">  const [mycontent,setmycontent]=useState(&quot;hello world&quot;);</span><br><span class="line">  function changeContent()&#123;</span><br><span class="line">    setmycontent(&quot;hello world2&quot;);</span><br><span class="line">  &#125;</span><br><span class="line">  return (</span><br><span class="line">    &lt;&gt;</span><br><span class="line">      &lt;div&gt;&#123;mycontent&#125;&lt;/div&gt;</span><br><span class="line">      &lt;button onClick=&#123;changeContent&#125;&gt;change&lt;/button&gt;</span><br><span class="line">    &lt;/&gt;</span><br><span class="line">  );</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>useState 是 React 提供的
<strong>Hook</strong>，让<strong>函数组件也能拥有内部状态</strong>（state）。可以通过更新函数，调用后触发重新渲染。</p>
<p><strong>对象的状态更新</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">import &#123; useState &#125; from &#x27;react&#x27;;</span><br><span class="line">function App() &#123;</span><br><span class="line">  const [mycontent,setmycontent]=useState(&#123;</span><br><span class="line">    title:&#x27;hello world&#x27;,</span><br><span class="line">    content :&#x27;hello world content&#x27;</span><br><span class="line">&#125;);</span><br><span class="line">  function changeContent()&#123;</span><br><span class="line">    setmycontent(&#123;</span><br><span class="line">      ...mycontent,</span><br><span class="line">      content:&#x27;new content&#x27;</span><br><span class="line">    &#125;);</span><br><span class="line">  &#125;</span><br><span class="line">  return (</span><br><span class="line">    &lt;&gt;</span><br><span class="line">      &lt;div title=&#123;mycontent.title&#125;&gt;&#123;mycontent.content&#125;&lt;/div&gt;</span><br><span class="line">      &lt;button onClick=&#123;changeContent&#125;&gt;change&lt;/button&gt;</span><br><span class="line">    &lt;/&gt;</span><br><span class="line">  );</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>...mycontent</code> 是 ES6 的 <strong>对象展开运算符（object
spread）</strong>。 一句话：把 <code>mycontent</code>
里所有“旧属性”先抄出来，然后再覆盖/新增你后面写的属性。</p>
<h3 id="react组件的使用">react组件的使用</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import &#123; useState &#125; from &#x27;react&#x27;;</span><br><span class="line">function App() &#123;</span><br><span class="line">return (</span><br><span class="line">    &lt;&gt;</span><br><span class="line">      &lt;img src=&#123;logo&#125; className=&quot;App-logo&quot; alt=&quot;logo&quot; style=&#123;&#123; width: &#x27;100px&#x27;,backgroundColor: &#x27;grey&#x27;&#125;&#125;/&gt;</span><br><span class="line">    &lt;/&gt;</span><br><span class="line">  );</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ol type="1">
<li><p><code>className</code> 代替 <code>class</code> 传统 HTML 写
<code>&lt;img class="App-logo"&gt;</code>；React 组件里必须用
<code>className</code>，因为 JSX 最终会被编译成 JavaScript 对象，而
<code>class</code> 是 JS 的保留关键字。</p></li>
<li><p>样式写成<strong>对象</strong></p></li>
</ol>
<p>HTML
写行内样式：<code>style="width:100px;background-color:grey"</code> React
必须写成对象：</p>
<figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">style=&#123;&#123;</span><br><span class="line">  <span class="attr">width</span>: <span class="string">&#x27;100px&#x27;</span>,</span><br><span class="line">  <span class="attr">backgroundColor</span>: <span class="string">&#x27;grey&#x27;</span>   <span class="comment">// 驼峰命名</span></span><br><span class="line">&#125;&#125;</span><br></pre></td></tr></table></figure>
<p>因为 JSX 属性最终会变成 JS
对象的键值对，键名必须合法（驼峰），值可以是任何 JS
值（数字、变量、计算结果）。</p>
<ol start="3" type="1">
<li>最终产物是<strong>虚拟 DOM 节点</strong></li>
</ol>
<p><code>&lt;img src=&#123;logo&#125; ... /&gt;</code> 在浏览器里不会直接变成
<code>&lt;img&gt;</code> 标签，而是先被编译成：</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="title class_">React</span>.<span class="title function_">createElement</span>(<span class="string">&#x27;img&#x27;</span>, &#123;</span><br><span class="line">  <span class="attr">src</span>: logo,</span><br><span class="line">  <span class="attr">className</span>: <span class="string">&#x27;App-logo&#x27;</span>,</span><br><span class="line">  <span class="attr">alt</span>: <span class="string">&#x27;logo&#x27;</span>,</span><br><span class="line">  <span class="attr">style</span>: &#123; <span class="attr">width</span>: <span class="string">&#x27;100px&#x27;</span>, <span class="attr">backgroundColor</span>: <span class="string">&#x27;grey&#x27;</span> &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>
<p>React 再拿这个对象去做 diff、更新真实 DOM，而不是直接 innerHTML。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">function App() &#123;</span><br><span class="line"></span><br><span class="line">  const imgdata=&#123;</span><br><span class="line">    className:&quot;App-logo&quot;,</span><br><span class="line">    style:&#123;</span><br><span class="line">      width:&#x27;100px&#x27;,</span><br><span class="line">      backgroundColor:&#x27;grey&#x27;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  return (</span><br><span class="line">    &lt;&gt;</span><br><span class="line">      &lt;img src=&#123;logo&#125; alt=&quot;logo&quot; &#123;...imgdata&#125;/&gt;</span><br><span class="line">    &lt;/&gt;</span><br><span class="line">  );</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>利用 <strong>JSX 展开运算符（spread attributes）</strong> 把
<code>imgdata</code> 里的所有键值一次性“拍平”到 <code>&lt;img&gt;</code>
标签上</p>
<h3 id="组件复用">组件复用</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">function Article(props) &#123;</span><br><span class="line">  return (</span><br><span class="line">    &lt;div&gt;</span><br><span class="line">      &lt;h2&gt;&#123;props.title&#125;&lt;/h2&gt;</span><br><span class="line">      &lt;p&gt;&#123;props.content&#125;&lt;/p&gt;</span><br><span class="line">    &lt;/div&gt;</span><br><span class="line">  );</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">function App() &#123;</span><br><span class="line">  return (</span><br><span class="line">    &lt;&gt;</span><br><span class="line">      &lt;Article title=&quot;标签1&quot; content=&quot;内容1&quot; /&gt;</span><br><span class="line">      &lt;Article title=&quot;标签2&quot; content=&quot;内容2&quot; /&gt;</span><br><span class="line">    &lt;/&gt;</span><br><span class="line">  );</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="组件通信">组件通信</h3>
<p>组件通信的 4 条主线</p>
<p>1️⃣ 父 → 子：props 2️⃣ 子 → 父：回调函数 3️⃣ 隔代/任意：Context 4️⃣
全局/远端：状态管理库（Zustand、Redux、React Query）</p>
<h4 id="父-子">父 → 子</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">function Parent() &#123;</span><br><span class="line">  const title = &#x27;Hello React&#x27;;</span><br><span class="line">  return &lt;Child title=&#123;title&#125; /&gt;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">function Child(&#123; title &#125;) &#123;</span><br><span class="line">  return &lt;h1&gt;&#123;title&#125;&lt;/h1&gt;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="子-父">子 → 父</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">function Parent() &#123;</span><br><span class="line">  const [count, setCount] = useState(0);</span><br><span class="line">  return (</span><br><span class="line">    &lt;&gt;</span><br><span class="line">      &lt;p&gt;父：&#123;count&#125;&lt;/p&gt;</span><br><span class="line">      &lt;Child onInc=&#123;() =&gt; setCount(c =&gt; c + 1)&#125; /&gt;</span><br><span class="line">    &lt;/&gt;</span><br><span class="line">  );</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">function Child(&#123; onInc &#125;) &#123;</span><br><span class="line">  return &lt;button onClick=&#123;onInc&#125;&gt;子按钮 +1&lt;/button&gt;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>父组件把“修改函数”通过 props
传给子组件，子组件在合适的时机调用它，把数据作为参数传回去。</strong></p>
<h3 id="react-hooks">react hooks</h3>
<p><strong>Hook 是什么？</strong> Hook 是 React 16.8 引入的
<strong>函数级 API</strong>，让函数组件拥有</p>
<ul>
<li>状态（useState）</li>
<li>生命周期（useEffect）</li>
<li>上下文（useContext）</li>
<li>自定义逻辑（自定义 Hook） 而不必写 class。</li>
</ul>
<h3 id="参考资料">参考资料</h3>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1kc411D7F9?spm_id_from=333.788.videopod.sections&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">20分钟学会React
Hooks 前端开发必看 AI编程工具 CodeGeeX 体验_哔哩哔哩_bilibili</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/08/14/%E5%AD%A6%E4%B9%A0/ai%E7%9B%B8%E5%85%B3/A2A%E5%8D%8F%E8%AE%AE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/08/14/%E5%AD%A6%E4%B9%A0/ai%E7%9B%B8%E5%85%B3/A2A%E5%8D%8F%E8%AE%AE/" class="post-title-link" itemprop="url">A2A协议</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-08-14 00:00:00" itemprop="dateCreated datePublished" datetime="2025-08-14T00:00:00+08:00">2025-08-14</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-08-28 15:09:26" itemprop="dateModified" datetime="2025-08-28T15:09:26+08:00">2025-08-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ai%E7%9B%B8%E5%85%B3/" itemprop="url" rel="index"><span itemprop="name">ai相关</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ai%E7%9B%B8%E5%85%B3/A2A%E5%8D%8F%E8%AE%AE/" itemprop="url" rel="index"><span itemprop="name">A2A协议</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="前言">前言</h3>
<p>就是client调用agent那一块，感觉还是比较困惑，我看例子是要通过定义给的execut和cancel函数，那就意味着agent提供者都要去自己去定义这些怎么执行的函数，还有描述agent的skill和card，工作量明显比mcp大了很多，可能这也是现在a2a传播没有mcp好的一大原因吧，我的理解，不知道对不对</p>
<hr>
<p>思考：现在利用a2a搭建多agent的现实例子多吗，从概念上，我认为a2a的思路是没问题的，但感觉下来，现在大多数的多agent的实现方式还是像langgraph中<strong>条件边</strong>来控制使用哪个agent，是不是因为a2a对于中小开发者搭建起来还是有些复杂，但我还是认为他这种于mcp类似，模块化，可以自定义的形式会是后续方向。就像现在的mcp
client，可以在市场上下载自己想要的mcp，利用a2a协议，用户可以在市场上下载想用的agent，搭建自己的多agent管家，现在市场上有类似的产品吗？</p>
<p>a2a协议其实与mcp类似，对象不同，一个是mcp client与mcp
server（tool），一个是agent client与agent
server。具体实现中，需要完成对agent
server的信息暴露与executor的编写，以便让client正确调用agent，调用前要启动服务。</p>
<p>一个agent
server所要包含的要素包括：1.AgentSkill，用于描述agent可以实现的能力</p>
<p>2.AgentCard，描述agent的信息，包括运行的url，输入和返回的数据类型，所包含的skills</p>
<p>3.AgentExecutor，定义了如何执行智能体，通过定义execute方法，以便正确调用agent
server</p>
<p>4.通过DefaultRequestHandler，封装调用agent的接口，不用再手写接口，只要提供一个
executor 和一个 store
即可，收到对话内容后，<code>DefaultRequestHandler</code>
会把对话打包成任务，交给 <code>HelloWorldAgentExecutor</code>
去执行。、</p>
<p>5.通过A2AStarletteApplication打包成应用（如fastapi），他的作用如下：1.把这个
handler 注册成真正的 HTTP 路由，于是外部就能通过 <code>POST /</code>
调用上述 JSON-RPC 方法。2.对外暴露名片</p>
<h3 id="什么是a2a协议">什么是A2A协议</h3>
<p>A2A 协议（Agent2Agent Protocol，智能体间通信协议）是 Google 在 2025
年 4 月发布并开源的首个 AI
智能体交互标准。它通过统一的通信规范，解决不同团队、不同框架、不同供应商开发的
AI 智能体如何“对话”和协同工作的问题。</p>
<blockquote>
<p>与mcp区分，<strong>MCP</strong> 解决
<strong>“单个智能体如何调用外部工具/数据”</strong>
的问题，而<strong>A2A</strong> 解决
<strong>“多个智能体如何协同完成任务”</strong> 的问题。</p>
</blockquote>
<figure>
<img src="/2025/08/14/%E5%AD%A6%E4%B9%A0/ai%E7%9B%B8%E5%85%B3/A2A%E5%8D%8F%E8%AE%AE/image-20250809222720192.png" alt="image-20250809222720192">
<figcaption aria-hidden="true">image-20250809222720192</figcaption>
</figure>
<h3 id="为什么要使用a2a协议">为什么要使用A2A协议</h3>
<p>随着 AI 应用深化，单一“万能”模型难以兼顾所有领域。A2A
鼓励构建“小而专”的智能体生态：</p>
<ul>
<li>每个智能体专注一个领域（如订票、报税、图像处理）。</li>
<li>通过 A2A
协议，它们像乐高积木一样自由组合，快速响应新的业务需求。</li>
</ul>
<p>比如你让一个agent使用多个工具，不仅会浪费tokens，也会降低其调用工具的准确性。所有，专业的领域使用专业的agent，而agent间的通信便要依靠A2A协议</p>
<h3 id="环境配置">环境配置</h3>
<p><strong>克隆仓库</strong></p>
<p>如果你还没有克隆，请克隆 A2A Samples 仓库：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/a2aproject/a2a-samples.git -b main --depth 1</span><br><span class="line">cd a2a-samples</span><br></pre></td></tr></table></figure>
<p><strong>Python 环境和 SDK 安装</strong></p>
<p>我们推荐为 Python 项目使用虚拟环境。A2A Python SDK 使用
<code>uv</code> 进行依赖管理，但你也可以使用 <code>pip</code> 与
<code>venv</code>。</p>
<ol type="1">
<li><p><strong>创建并激活虚拟环境：</strong></p>
<p>使用 <code>venv</code>（标准库）：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python -m venv .venv</span><br><span class="line">source .venv/bin/activate</span><br></pre></td></tr></table></figure></li>
<li><p><strong>安装所需的 Python 依赖项以及 A2A SDK
及其依赖项：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r samples/python/requirements.txt</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="agent-skills-agent-card">Agent Skills &amp; Agent Card</h3>
<h4 id="agent-skills">Agent Skills</h4>
<p>一个<strong>代理技能</strong>描述了代理可以执行的具体能力或功能。它是告诉客户端代理擅长哪些任务的构建模块。</p>
<p><code>AgentSkill</code> 的关键属性（定义在 <code>a2a.types</code>
中）：</p>
<ul>
<li><code>id</code>: 技能的唯一标识符。</li>
<li><code>name</code>: 人类可读的名称。</li>
<li><code>description</code>：对技能功能的更详细说明。</li>
<li><code>tags</code>：用于分类和发现的关键词。</li>
<li><code>examples</code>：示例提示或使用案例。</li>
<li><code>inputModes</code> / <code>outputModes</code>:
支持的输入和输出媒体类型（例如，“text/plain”，“application/json”）。</li>
</ul>
<p>在 <code>__main__.py</code> 中，你可以看到如何为 Helloworld
代理定义一个技能：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">skill = AgentSkill(</span><br><span class="line">    id=&#x27;hello_world&#x27;,</span><br><span class="line">    name=&#x27;Returns hello world&#x27;,</span><br><span class="line">    description=&#x27;just returns hello world&#x27;,</span><br><span class="line">    tags=[&#x27;hello world&#x27;],</span><br><span class="line">    examples=[&#x27;hi&#x27;, &#x27;hello world&#x27;],</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>这个技能非常简单：它的名称是 “Returns hello
world”，并且主要处理文本。</p>
<h4 id="agent-card">Agent Card</h4>
<p><strong>代理卡</strong>是一个 A2A 服务器提供的 JSON 文档，通常位于
<code>.well-known/agent-card.json</code>
端点。它就像代理的数字名片。</p>
<p><code>AgentCard</code> 的关键属性（定义在 <code>a2a.types</code>
中）：</p>
<ul>
<li><code>name</code>, <code>description</code>, <code>version</code>:
基本身份信息。</li>
<li><code>url</code>：A2A 服务可访问的端点。</li>
<li><code>capabilities</code>：指定支持的 A2A 功能，如
<code>streaming</code> 或 <code>pushNotifications</code>。</li>
<li><code>defaultInputModes</code> / <code>defaultOutputModes</code>:
代理的默认媒体类型。</li>
<li><code>skills</code>: 代理提供的 <code>AgentSkill</code>
对象列表。</li>
</ul>
<p><code>helloworld</code> 示例定义其 Agent Card 如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># This will be the public-facing agent card</span><br><span class="line">public_agent_card = AgentCard(</span><br><span class="line">    name=&#x27;Hello World Agent&#x27;,</span><br><span class="line">    description=&#x27;Just a hello world agent&#x27;,</span><br><span class="line">    url=&#x27;http://localhost:9999/&#x27;,</span><br><span class="line">    version=&#x27;1.0.0&#x27;,</span><br><span class="line"> 	# 默认输入模式：Agent 能够接收的输入类型列表，这里仅支持纯文本</span><br><span class="line">    default_input_modes=[&#x27;text&#x27;],</span><br><span class="line">    # 默认输出模式：Agent 能够产生的输出类型列表，这里仅返回纯文本</span><br><span class="line">    default_output_modes=[&#x27;text&#x27;],</span><br><span class="line">    # 能力声明：告知调用方 Agent 支持的能力，例如是否支持流式输出（streaming）</span><br><span class="line">    capabilities=AgentCapabilities(streaming=True),</span><br><span class="line">    skills=[skill],  # Only the basic skill for the public card</span><br><span class="line">    supports_authenticated_extended_card=True,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>这张卡片告诉我们代理名为 “Hello World Agent”，运行在
<code>http://localhost:9999/</code>，支持文本交互，并具有
<code>hello_world</code>
技能。它还表明支持公开认证，意味着无需特定凭证。</p>
<h3 id="agent-executor">Agent Executor</h3>
<p>A2A 代理处理请求和生成响应/事件的核心逻辑由一个 <strong>Agent
Executor</strong> 负责。A2A Python SDK 提供了一个抽象基类
<code>a2a.server.agent_execution.AgentExecutor</code> 供你实现。</p>
<p><strong><code>AgentExecutor</code> 接口</strong></p>
<p><code>AgentExecutor</code> 类定义了两个主要方法：</p>
<ul>
<li><code>async def execute(self, context: RequestContext, event_queue: EventQueue)</code>
: 处理期望响应或事件流的传入请求。它处理用户输入（可通过
<code>context</code> 获取）并使用 <code>event_queue</code> 发送
<code>Message</code>、<code>Task</code>、<code>TaskStatusUpdateEvent</code>
或 <code>TaskArtifactUpdateEvent</code> 对象。</li>
<li><code>async def cancel(self, context: RequestContext, event_queue: EventQueue)</code>
: 处理取消正在进行的任务的请求。</li>
</ul>
<p><code>RequestContext</code>
提供有关传入请求的信息，例如用户消息和任何现有的任务详情。<code>EventQueue</code>
由执行器使用，用于将事件发送回客户端。</p>
<p><strong>Helloworld AgentExecutor</strong></p>
<p>让我们看看 <code>agent_executor.py</code>。它定义了
<code>HelloWorldAgentExecutor</code>。</p>
<ol type="1">
<li><p><strong>代理（<code>HelloWorldAgent</code>）</strong>：这是一个简单的辅助类，封装了实际的“业务逻辑”。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">class HelloWorldAgent:</span><br><span class="line">    &quot;&quot;&quot;Hello World Agent.&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    async def invoke(self) -&gt; str:</span><br><span class="line">        return &#x27;Hello World&#x27;</span><br></pre></td></tr></table></figure>
<p>它有一个简单的 <code>invoke</code> 方法，返回字符串”Hello
World”。</p></li>
<li><p><strong>执行器（<code>HelloWorldAgentExecutor</code>）</strong>：这个类实现了
<code>AgentExecutor</code> 接口。</p>
<ul>
<li><p><strong><code>__init__</code></strong>:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">class HelloWorldAgentExecutor(AgentExecutor):</span><br><span class="line">    &quot;&quot;&quot;Test AgentProxy Implementation.&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.agent = HelloWorldAgent()</span><br></pre></td></tr></table></figure>
<p>它实例化了 <code>HelloWorldAgent</code>。</p></li>
<li><p><strong><code>execute</code></strong>:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">async def execute(</span><br><span class="line">    self,</span><br><span class="line">    context: RequestContext,</span><br><span class="line">    event_queue: EventQueue,</span><br><span class="line">) -&gt; None:</span><br><span class="line">    result = await self.agent.invoke()</span><br><span class="line">    await event_queue.enqueue_event(new_agent_text_message(result))</span><br></pre></td></tr></table></figure>
<p>当收到一个 <code>message/send</code> 或 <code>message/stream</code>
请求时（这两种请求在这个简化的执行器中均由 <code>execute</code>
处理）：</p>
<ol type="1">
<li>它调用 <code>self.agent.invoke()</code> 来获取 “Hello World”
字符串。</li>
<li>它使用 <code>new_agent_text_message</code> 工具函数创建一个 A2A
<code>Message</code> 对象。</li>
<li>它将此消息入队到 <code>event_queue</code>。底层的
<code>DefaultRequestHandler</code>
随后会处理这个队列以向客户端发送响应。对于像这样的一条消息，在流关闭之前，它将导致一个
<code>message/send</code> 的单一响应或一个 <code>message/stream</code>
的单一事件。</li>
</ol></li>
<li><p><strong><code>cancel</code></strong>: Helloworld 示例的
<code>cancel</code>
方法简单地抛出一个异常，表明这个基本代理不支持取消操作。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">async def cancel(</span><br><span class="line">    self, context: RequestContext, event_queue: EventQueue</span><br><span class="line">) -&gt; None:</span><br><span class="line">    raise Exception(&#x27;cancel not supported&#x27;)</span><br></pre></td></tr></table></figure></li>
</ul></li>
</ol>
<p><code>AgentExecutor</code> 充当 A2A
协议（由请求处理器和服务器应用程序管理）与您的代理特定逻辑之间的桥梁。它接收关于请求的上下文信息，并使用事件队列来通信结果或更新。</p>
<h3 id="启动server">启动server</h3>
<p>现在我们已经有了 Agent Card 和 Agent Executor，可以设置并启动 A2A
服务器。</p>
<p>A2A Python SDK 提供了一个 <code>A2AStarletteApplication</code>
类，简化了运行符合 A2A 标准的 HTTP 服务器。它使用 <a target="_blank" rel="noopener" href="https://www.starlette.io/">Starlette</a> 作为 Web 框架，通常与 <a target="_blank" rel="noopener" href="https://www.uvicorn.org/">Uvicorn</a> 等 ASGI 服务器一起运行。</p>
<p>让我们再次查看
<code>__main__.py</code>，看看服务器是如何初始化和启动的。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line">import uvicorn</span><br><span class="line"></span><br><span class="line">from a2a.server.apps import A2AStarletteApplication</span><br><span class="line">from a2a.server.request_handlers import DefaultRequestHandler</span><br><span class="line">from a2a.server.tasks import InMemoryTaskStore</span><br><span class="line">from a2a.types import (</span><br><span class="line">    AgentCapabilities,</span><br><span class="line">    AgentCard,</span><br><span class="line">    AgentSkill,</span><br><span class="line">)</span><br><span class="line">from agent_executor import (</span><br><span class="line">    HelloWorldAgentExecutor,  # type: ignore[import-untyped]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    skill = AgentSkill(</span><br><span class="line">        id=&#x27;hello_world&#x27;,</span><br><span class="line">        name=&#x27;返回 hello world&#x27;,</span><br><span class="line">        description=&#x27;简单地返回 hello world&#x27;,</span><br><span class="line">        tags=[&#x27;hello world&#x27;],</span><br><span class="line">        examples=[&#x27;hi&#x27;, &#x27;hello world&#x27;],</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    extended_skill = AgentSkill(</span><br><span class="line">        id=&#x27;super_hello_world&#x27;,</span><br><span class="line">        name=&#x27;返回 SUPER Hello World&#x27;,</span><br><span class="line">        description=&#x27;仅限已认证用户使用的更热情的问候。&#x27;,</span><br><span class="line">        tags=[&#x27;hello world&#x27;, &#x27;super&#x27;, &#x27;extended&#x27;],</span><br><span class="line">        examples=[&#x27;super hi&#x27;, &#x27;give me a super hello&#x27;],</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    # 这是面向公众的 Agent 卡片</span><br><span class="line">    public_agent_card = AgentCard(</span><br><span class="line">        name=&#x27;Hello World Agent&#x27;,</span><br><span class="line">        description=&#x27;只是一个 hello world 代理&#x27;,</span><br><span class="line">        url=&#x27;http://localhost:9999/&#x27;,</span><br><span class="line">        version=&#x27;1.0.0&#x27;,</span><br><span class="line">        default_input_modes=[&#x27;text&#x27;],</span><br><span class="line">        default_output_modes=[&#x27;text&#x27;],</span><br><span class="line">        capabilities=AgentCapabilities(streaming=True),</span><br><span class="line">        skills=[skill],  # 公开卡片仅包含基础技能</span><br><span class="line">        supports_authenticated_extended_card=True,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    # 这是已认证用户的扩展 Agent 卡片</span><br><span class="line">    # 额外包含 &#x27;extended_skill&#x27;</span><br><span class="line">    specific_extended_agent_card = public_agent_card.model_copy(</span><br><span class="line">        update=&#123;</span><br><span class="line">            &#x27;name&#x27;: &#x27;Hello World Agent - Extended Edition&#x27;,  # 使用不同名称以便区分</span><br><span class="line">            &#x27;description&#x27;: &#x27;面向已认证用户的完整功能 hello world 代理。&#x27;,</span><br><span class="line">            &#x27;version&#x27;: &#x27;1.0.1&#x27;,  # 甚至可以是不同的版本</span><br><span class="line">            # capabilities 及其他字段（如 url、default_input_modes、default_output_modes、</span><br><span class="line">            # supports_authenticated_extended_card）均从 public_agent_card 继承，</span><br><span class="line">            # 除非在此处另行指定。</span><br><span class="line">            &#x27;skills&#x27;: [</span><br><span class="line">                skill,</span><br><span class="line">                extended_skill,</span><br><span class="line">            ],  # 扩展卡片包含两个技能</span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    request_handler = DefaultRequestHandler(</span><br><span class="line">        agent_executor=HelloWorldAgentExecutor(),</span><br><span class="line">        task_store=InMemoryTaskStore(),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    server = A2AStarletteApplication(</span><br><span class="line">        agent_card=public_agent_card,</span><br><span class="line">        http_handler=request_handler,</span><br><span class="line">        extended_agent_card=specific_extended_agent_card,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    # 使用 uvicorn 启动服务，监听 0.0.0.0:9999</span><br><span class="line">    uvicorn.run(server.build(), host=&#x27;0.0.0.0&#x27;, port=9999)</span><br></pre></td></tr></table></figure>
<p>我们来分解一下：</p>
<ol type="1">
<li><strong><code>DefaultRequestHandler</code></strong>:
<ul>
<li>SDK 提供了 <code>DefaultRequestHandler</code>。这个处理器接收你的
<code>AgentExecutor</code>
实现（这里，<code>HelloWorldAgentExecutor</code>）和一个
<code>TaskStore</code>（这里，<code>InMemoryTaskStore</code>）。</li>
<li>它将传入的 A2A RPC 调用路由到你的执行器的适当方法上（比如
<code>execute</code> 或 <code>cancel</code>）。</li>
<li><code>TaskStore</code> 被 <code>DefaultRequestHandler</code>
用来管理任务的生命周期，特别是对于有状态交互、流式传输和重新订阅。即使你的代理执行器很简单，处理器也需要一个任务存储。</li>
</ul></li>
<li><strong><code>A2AStarletteApplication</code></strong>:
<ul>
<li><code>A2AStarletteApplication</code> 类使用 <code>agent_card</code>
和 <code>request_handler</code>（在其构造函数中称为
<code>http_handler</code>）进行实例化。</li>
<li><code>agent_card</code> 至关重要，因为服务器将在
<code>/.well-known/agent-card.json</code>
端点（默认情况下）上公开它。</li>
<li><code>request_handler</code> 负责通过与其 <code>AgentExecutor</code>
交互来处理所有传入的 A2A 方法调用。</li>
</ul></li>
<li><strong><code>uvicorn.run(server_app_builder.build(), ...)</code></strong>:
<ul>
<li><code>A2AStarletteApplication</code> 有一个 <code>build()</code>
方法，用于构建实际的 Starlette 应用程序。</li>
<li>然后使用 <code>uvicorn.run()</code> 运行该应用程序，使您的代理可通过
HTTP 访问。</li>
<li><code>host='0.0.0.0'</code>
使服务器可在您机器上的所有网络接口上访问。</li>
<li><code>port=9999</code> 指定监听的端口。这需要与
<code>AgentCard</code> 中的 <code>url</code> 匹配。</li>
</ul></li>
<li><code>specific_extended_agent_card</code>
<ul>
<li><strong>给同一个 Agent
准备“两张不同权限的名片”</strong>，分别用于“普通访客”和“已认证用户”。、</li>
</ul></li>
</ol>
<h3 id="与服务器交互">与服务器交互</h3>
<p>Helloworld A2A 服务器运行后，让我们向它发送一些请求。SDK
包含一个客户端（<code>A2AClient</code>），可以简化这些交互。</p>
<p>让我们看一下 <code>test_client.py</code> 的关键部分：</p>
<ol type="1">
<li><p><strong>获取代理卡 &amp; 初始化客户端</strong> ：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">base_url = &#x27;http://localhost:9999&#x27;</span><br><span class="line"></span><br><span class="line">async with httpx.AsyncClient() as httpx_client:</span><br><span class="line">    # 初始化 A2ACardResolver</span><br><span class="line">    resolver = A2ACardResolver(</span><br><span class="line">        httpx_client=httpx_client,</span><br><span class="line">        base_url=base_url,</span><br><span class="line">        # agent_card_path 使用默认值，extended_agent_card_path 也使用默认值</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p><code>A2ACardResolver</code> 类是一个便捷工具。它首先从服务器端的
<code>/.well-known/agent-card.json</code> 端点（基于提供的基 URL）获取
<code>AgentCard</code>，然后使用它初始化客户端。</p></li>
<li><p><strong>发送非流式消息 (<code>send_message</code>)</strong>:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">client = A2AClient(</span><br><span class="line">    httpx_client=httpx_client, </span><br><span class="line">    agent_card=final_agent_card_to_use#这个card为经过认证处理后暴露的card</span><br><span class="line">)</span><br><span class="line">logger.info(&#x27;A2AClient initialized.&#x27;)</span><br><span class="line"></span><br><span class="line">send_message_payload: dict[str, Any] = &#123;</span><br><span class="line">    &#x27;message&#x27;: &#123;</span><br><span class="line">        &#x27;role&#x27;: &#x27;user&#x27;,</span><br><span class="line">        &#x27;parts&#x27;: [</span><br><span class="line">            &#123;&#x27;kind&#x27;: &#x27;text&#x27;, &#x27;text&#x27;: &#x27;how much is 10 USD in INR?&#x27;&#125;</span><br><span class="line">        ],</span><br><span class="line">        &#x27;messageId&#x27;: uuid4().hex,</span><br><span class="line">    &#125;,</span><br><span class="line">&#125;</span><br><span class="line">request = SendMessageRequest(</span><br><span class="line">    id=str(uuid4()), params=MessageSendParams(**send_message_payload)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">response = await client.send_message(request)</span><br><span class="line">print(response.model_dump(mode=&#x27;json&#x27;, exclude_none=True))</span><br></pre></td></tr></table></figure>
<ul>
<li><code>send_message_payload</code> 构建了
<code>MessageSendParams</code> 的数据。</li>
<li>这些数据被封装在 <code>SendMessageRequest</code> 中。</li>
<li>它包含一个 <code>message</code> 对象，其中 <code>role</code>
设置为”用户”，内容在 <code>parts</code> 中。</li>
<li>Helloworld 代理的 <code>execute</code> 方法将入队一条”Hello
World”消息。<code>DefaultRequestHandler</code>
将获取这条消息并将其作为响应发送。</li>
<li><code>response</code> 将是一个 <code>SendMessageResponse</code>
对象，其中包含 <code>SendMessageSuccessResponse</code>（以代理的
<code>Message</code> 作为结果）或
<code>JSONRPCErrorResponse</code>。</li>
</ul></li>
<li><p><strong>处理任务 ID（Helloworld 的说明性注释）</strong>:</p>
<p>Helloworld 客户端（<code>test_client.py</code>）不会直接尝试
<code>get_task</code> 或 <code>cancel_task</code>，因为简单的 Helloworld
代理的 <code>execute</code> 方法，通过 <code>message/send</code>
调用时，会导致 <code>DefaultRequestHandler</code> 返回一个直接的
<code>Message</code> 响应，而不是 <code>Task</code>
对象。更复杂的、明确管理任务的代理（如 LangGraph 示例）会从
<code>message/send</code> 返回一个 <code>Task</code> 对象，然后其
<code>id</code> 可用于 <code>get_task</code> 或
<code>cancel_task</code>。</p></li>
<li><p><strong>发送流式消息（<code>send_message_streaming</code>）</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">streaming_request = SendStreamingMessageRequest(</span><br><span class="line">    id=str(uuid4()), params=MessageSendParams(**send_message_payload)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">stream_response = client.send_message_streaming(streaming_request)</span><br><span class="line"></span><br><span class="line">async for chunk in stream_response:</span><br><span class="line">    print(chunk.model_dump(mode=&#x27;json&#x27;, exclude_none=True))</span><br></pre></td></tr></table></figure>
<ul>
<li>此方法调用代理的 <code>message/stream</code>
端点。<code>DefaultRequestHandler</code> 将调用
<code>HelloWorldAgentExecutor.execute</code> 方法。</li>
<li><code>execute</code> 方法将一个”Hello
World”消息入队，然后关闭事件队列。</li>
<li>客户端将接收这条单条消息为一个
<code>SendStreamingMessageResponse</code> 事件，然后流将终止。</li>
<li><code>stream_response</code> 是一个
<code>AsyncGenerator</code>。</li>
</ul></li>
</ol>
<h3 id="参考资料">参考资料</h3>
<p><a target="_blank" rel="noopener" href="https://github.com/a2aproject/a2a-samples?tab=readme-ov-file">a2aproject/a2a-samples:
Samples using the Agent2Agent (A2A) Protocol</a></p>
<p><a target="_blank" rel="noopener" href="https://a2a-protocol.org/latest/">Agent2Agent (A2A)
Protocol</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/a2aproject/a2a-python">a2aproject/a2a-python:
Agent2Agent (A2A) 协议的官方 Python SDK — a2aproject/a2a-python:
Official Python SDK for the Agent2Agent (A2A) Protocol</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/08/14/%E5%AD%A6%E4%B9%A0/ai%E6%A1%86%E6%9E%B6/langmem/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/08/14/%E5%AD%A6%E4%B9%A0/ai%E6%A1%86%E6%9E%B6/langmem/" class="post-title-link" itemprop="url">Langmem快速入门</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2025-08-14 00:00:00 / 修改时间：17:42:03" itemprop="dateCreated datePublished" datetime="2025-08-14T00:00:00+08:00">2025-08-14</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ai%E6%A1%86%E6%9E%B6/" itemprop="url" rel="index"><span itemprop="name">ai框架</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ai%E6%A1%86%E6%9E%B6/langmem/" itemprop="url" rel="index"><span itemprop="name">langmem</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="前言">前言</h3>
<p>本文简单测试了一下langgraph官方提供的记忆管理工具，发现还是存在bug，我在a线程先让他记住我是张熙浚，然后又告诉他我不是张熙浚我是张俊细，在线程b询问他我是谁时，他还是认为我是张熙浚。记忆的管理部分确实是一个很大的问题，但中小开发者我认为还是直接使用人家造好的轮子方便些（我尝试去阅读了他的记忆管理工具的源码，以我目前的水平，想手搓花费的精力还是太多了）</p>
<p>我还有一个疑惑，我的理解是，当前记忆的存储基本上依赖于agent的决定，所以并不稳定，我也搞不清楚他什么时候会把哪些信息存入记忆，可以设置
schemas结构，控制存储的内容，但是长期记忆仅存储指定的这些信息，感觉还是有些鸡肋啊</p>
<p>代码见<a target="_blank" rel="noopener" href="https://github.com/zxj-2023/learn-rag-langchain/tree/main/langmem">learn-rag-langchain/langmem
at main · zxj-2023/learn-rag-langchain</a></p>
<h3 id="介绍">介绍</h3>
<p>LangMem 是 LangChain 推出的开源 SDK，通过一套存储-提取-优化机制，让
Agent
能够在多轮、多天甚至多用户之间持续学习、记住用户偏好并不断改进回答。</p>
<p>LangMem 的记忆工具按两个层次的集成模式组织：</p>
<ol type="1">
<li>核心 API</li>
</ol>
<p>LangMem
的核心是提供无副作用地转换记忆状态的函数。这些原语是记忆操作的构建块：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langmem/reference/memory/#langmem.create_memory_manager"><strong>记忆管理器</strong></a>：根据新的对话信息，提取新记忆、更新或删除过时记忆，并从现有记忆中进行整合和泛化。</li>
<li><a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langmem/reference/prompt_optimization/#langmem.create_prompt_optimizer"><strong>提示优化器</strong></a>：根据对话信息（可选反馈）更新提示规则和核心行为。</li>
</ul>
<p>这些核心函数不依赖于任何特定的数据库或存储系统。您可以在任何应用程序中使用它们。</p>
<ol start="2" type="1">
<li>有状态集成</li>
</ol>
<p>上一层依赖于 LangGraph 的长期记忆存储。这些组件使用上述核心 API
来转换存储中存在的记忆，并在新对话信息传入时根据需要进行更新/插入或删除：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langmem/reference/memory/#langmem.create_memory_store_manager"><strong>存储管理器</strong></a>：自动持久化提取的记忆。</li>
<li><a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langmem/reference/tools/#langmem.create_manage_memory_tool"><strong>记忆管理工具</strong></a>：让智能体直接访问记忆操作。</li>
</ul>
<figure>
<img src="/2025/08/14/%E5%AD%A6%E4%B9%A0/ai%E6%A1%86%E6%9E%B6/langmem/image-20250814152044798.png" alt="image-20250814152044798">
<figcaption aria-hidden="true">image-20250814152044798</figcaption>
</figure>
<p>langmem可以通过两种方式创建记忆</p>
<ol type="1">
<li><strong>在热路径中：</strong> Agent 使用工具主动保存笔记。</li>
<li><strong>在后台：</strong>记忆从对话中自动“潜意识地”提取。</li>
</ol>
<h3 id="热路径快速入门指南">热路径快速入门指南</h3>
<p>在本指南中，我们将创建一个 LangGraph Agent，它通过 LangMem 的
<code>manage_memory</code> 工具来主动管理自己的长期记忆。</p>
<h4 id="create_manage_memory_tool">create_manage_memory_tool</h4>
<p>create_manage_memory_tool通过创建一个工具（Tool），这个工具可以被
agent用来<strong>管理持久化记忆</strong>。这些记忆可以在不同的对话、会话甚至应用重启后依然存在。</p>
<ol type="1">
<li><p><strong>持久化存储 (Persistent Storage):</strong> 它利用了
LangGraph 提供的 <code>BaseStore</code>
接口。这使得数据可以存储在内存、数据库（如
Postgres）等地方，而不是仅仅存在于程序的运行时内存中。</p></li>
<li><p><strong>命名空间 (Namespace):</strong>
为了组织和隔离不同用户或不同类型的记忆，数据被存储在层级化的命名空间中。例如，<code>("memories", "user-123")</code>
可以确保用户 “user-123”
的记忆与其他用户或系统记忆分开。命名空间可以包含占位符（如
<code>&#123;langgraph_user_id&#125;</code>），在实际执行时会被具体的配置值替换。</p></li>
<li><p><strong>记忆 (Memory):</strong>
在这个上下文中，一个“记忆”就是存储在 <code>BaseStore</code>
中的一个数据项（<code>Item</code>）。它有一个唯一的
<code>key</code>（通常是 UUID），一个 <code>namespace</code>，一个
<code>value</code>（存储实际内容），以及创建和更新时间戳。</p></li>
<li><p><strong>工具 (Tool):</strong> 在 AI
应用中，工具是代理（Agent）可以调用的函数或能力。这个函数创建的工具就是一个封装好的、可以被
Agent 调用的函数，用于执行创建、更新、删除记忆的操作。</p></li>
</ol>
<h4 id="什么时候agent会调用记忆工具">什么时候agent会调用记忆工具</h4>
<figure>
<img src="/2025/08/14/%E5%AD%A6%E4%B9%A0/ai%E6%A1%86%E6%9E%B6/langmem/image-20250814163150589.png" alt="image-20250814163150589">
<figcaption aria-hidden="true">image-20250814163150589</figcaption>
</figure>
<p>ai是这样回答的，ReAct架构的agent是否调用工具由他自己决定</p>
<h4 id="实战">实战</h4>
<p><strong>导入库</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from langgraph.checkpoint.memory import MemorySaver</span><br><span class="line">from langgraph.prebuilt import create_react_agent</span><br><span class="line">from langgraph.store.memory import InMemoryStore</span><br><span class="line">from langgraph.utils.config import get_store </span><br><span class="line">from langmem import (</span><br><span class="line">    # 让智能体创建、更新和删除记忆 </span><br><span class="line">    create_manage_memory_tool,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><strong>返回记忆提示词</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def prompt(state):</span><br><span class="line">    &quot;&quot;&quot;为LLM准备消息。&quot;&quot;&quot;</span><br><span class="line">    # 从配置的上下文变量中获取存储; </span><br><span class="line">    store = get_store() # 与提供给 `create_react_agent` 的相同</span><br><span class="line">    memories = store.search(</span><br><span class="line">        # 在与我们为智能体配置的相同命名空间内搜索</span><br><span class="line">        (&quot;memories&quot;,),</span><br><span class="line">        query=state[&quot;messages&quot;][-1].content,</span><br><span class="line">    )</span><br><span class="line">    system_msg = f&quot;&quot;&quot;You are a helpful assistant.</span><br><span class="line"></span><br><span class="line">## Memories</span><br><span class="line">&lt;memories&gt;</span><br><span class="line">&#123;memories&#125;</span><br><span class="line">&lt;/memories&gt;</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">    return [&#123;&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: system_msg&#125;, *state[&quot;messages&quot;]]</span><br></pre></td></tr></table></figure>
<p><strong>定义store与checkpoint</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">from langchain import embeddings</span><br><span class="line">from langchain_openai import OpenAIEmbeddings</span><br><span class="line">embedding=OpenAIEmbeddings(</span><br><span class="line">    api_key=&quot;sk-&quot;, </span><br><span class="line">    base_url=&quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;,</span><br><span class="line">    model=&quot;text-embedding-v4&quot;,</span><br><span class="line">    check_embedding_ctx_length = False,</span><br><span class="line">    dimensions=1536 </span><br><span class="line">)</span><br><span class="line">store = InMemoryStore(</span><br><span class="line">    index=&#123; # 存储提取的记忆 </span><br><span class="line">        &quot;dims&quot;: 1536,</span><br><span class="line">        &quot;embed&quot;: embedding,</span><br><span class="line">    &#125;</span><br><span class="line">) </span><br><span class="line">checkpointer = MemorySaver() # 检查点图状态 </span><br></pre></td></tr></table></figure>
<p><strong>定义agent</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">from langchain_openai import ChatOpenAI</span><br><span class="line">model_qwen=ChatOpenAI(</span><br><span class="line">    api_key=&quot;sk-&quot;, </span><br><span class="line">    base_url=&quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;,</span><br><span class="line">    model=&quot;qwen3-30b-a3b-instruct-2507&quot;,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">agent = create_react_agent( </span><br><span class="line">    model=model_qwen,</span><br><span class="line">    prompt=prompt,</span><br><span class="line">    tools=[ # 添加记忆工具 </span><br><span class="line">        # 智能体可以调用 &quot;manage_memory&quot; 来</span><br><span class="line">        # 通过ID创建、更新和删除记忆</span><br><span class="line">        # 命名空间为记忆添加作用域。要</span><br><span class="line">        # 为每个用户限定记忆范围，使用 (&quot;memories&quot;, &quot;&#123;user_id&#125;&quot;): </span><br><span class="line">        create_manage_memory_tool(namespace=(&quot;memories&quot;,)),</span><br><span class="line">    ],</span><br><span class="line">    # 我们的记忆将存储在这个提供的BaseStore实例中</span><br><span class="line">    store=store,</span><br><span class="line">    # 图的&quot;状态&quot;将在每个节点完成执行后进行检查点</span><br><span class="line">    # 用于跟踪聊天历史和持久执行</span><br><span class="line">    checkpointer=checkpointer, </span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><strong>可视化图</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">agent.get_graph().draw_mermaid_png(output_file_path=&quot;agent.png&quot;)</span><br></pre></td></tr></table></figure>
<p><strong>在线程a让agent记住我们的偏好</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">config = &#123;&quot;configurable&quot;: &#123;&quot;thread_id&quot;: &quot;thread-a&quot;&#125;&#125; </span><br><span class="line">agent.invoke( </span><br><span class="line">    &#123; </span><br><span class="line">        &quot;messages&quot;: [ </span><br><span class="line">            &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;我喜欢黑色的显示模式&quot;&#125; </span><br><span class="line">        ] </span><br><span class="line">    &#125;, </span><br><span class="line">    # 我们将通过使用具有相同thread_id的config</span><br><span class="line">    # 来继续对话(thread-a)</span><br><span class="line">    config=config, </span><br><span class="line">) </span><br><span class="line">print(response[&quot;messages&quot;][-1].content) </span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">是的，我知道！你偏好黑色显示模式。我会在后续交互中保持这一设置。</span><br></pre></td></tr></table></figure>
<p><strong>在线程b查看是否记住</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 新线程 = 新对话！</span><br><span class="line">new_config = &#123;&quot;configurable&quot;: &#123;&quot;thread_id&quot;: &quot;thread-b&quot;&#125;&#125; </span><br><span class="line"># 智能体只能回忆起</span><br><span class="line"># 它使用manage_memories工具明确保存的内容</span><br><span class="line">response = agent.invoke( </span><br><span class="line">    &#123;&quot;messages&quot;: [&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;你好。你还记得我吗？你知道我有什么偏好吗？&quot;&#125;]&#125;,</span><br><span class="line">    config=new_config, </span><br><span class="line">) </span><br><span class="line">print(response[&quot;messages&quot;][-1].content) </span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">你好！虽然我无法记住你作为个体的详细信息，但我可以访问一些关于你的偏好信息。根据之前的记录，我知道你偏好使用黑色显示模式。如果你还有其他偏好或希望我记住什么，请告诉我，我会帮你记录下来。</span><br></pre></td></tr></table></figure>
<h3 id="后台快速入门指南">后台快速入门指南</h3>
<p>本指南将向您展示如何使用 <a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langmem/background_quickstart/"><code>create_memory_store_manager</code></a>
在后台提取和整合记忆。当记忆在后台处理时，智能体将正常继续运行。</p>
<ol type="1">
<li><p><strong>Runnable:</strong> LangChain/LangGraph
中的核心抽象，代表一个可以被调用（<code>invoke</code>/<code>ainvoke</code>）来处理输入并产生输出的单元。<code>MemoryStoreManager</code>
本身就是一个 Runnable。</p></li>
<li><p><strong>BaseStore:</strong> LangGraph
提供的持久化存储接口。Manager
会使用它来读取（搜索）和写入（创建、更新、删除）记忆。</p></li>
<li><p><strong>Memory (记忆):</strong> 在 Manager
的上下文中，记忆通常是指从对话中提取的、值得保存的片段信息（如用户偏好、事实等）。它们存储在
<code>BaseStore</code> 中，有自己的 <code>namespace</code> 和
<code>key</code>。</p></li>
<li><p><strong>Schema (模式):</strong> 一个 Pydantic
模型，用于定义记忆的结构。这允许你强制记忆遵循特定的格式（例如，包含
<code>category</code>, <code>preference</code>, <code>context</code>
字段）。如果未提供
<code>schemas</code>，则默认使用非结构化的字符串。</p></li>
<li><p><strong>Namespace (命名空间):</strong> 用于组织存储在
<code>BaseStore</code> 中的记忆。支持使用占位符（如
<code>&#123;langgraph_user_id&#125;</code>）进行动态配置。</p></li>
<li><p>自动化流程:</p>
<p>Manager 会自动执行以下步骤：</p>
<ul>
<li><strong>搜索 (Search):</strong> 根据新对话内容，在
<code>BaseStore</code> 中查找相关的现有记忆。</li>
<li><strong>分析/提取 (Analyze/Extract):</strong> 使用 LLM
分析新对话和检索到的记忆，决定是否需要创建新记忆、更新现有记忆或删除过时记忆。</li>
<li><strong>应用更改 (Apply Changes):</strong>
将分析结果（记忆的增删改）写回到 <code>BaseStore</code>。</li>
</ul></li>
</ol>
<h4 id="实战-1">实战</h4>
<p><strong>导入库</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from langchain.chat_models import init_chat_model </span><br><span class="line">from langgraph.func import entrypoint </span><br><span class="line">from langgraph.store.memory import InMemoryStore </span><br><span class="line"></span><br><span class="line">from langmem import ReflectionExecutor, create_memory_store_manager </span><br></pre></td></tr></table></figure>
<p><strong>定义store</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">from langchain_openai import OpenAIEmbeddings</span><br><span class="line">embedding=OpenAIEmbeddings(</span><br><span class="line">    api_key=&quot;sk-&quot;, </span><br><span class="line">    base_url=&quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;,</span><br><span class="line">    model=&quot;text-embedding-v4&quot;,</span><br><span class="line">    check_embedding_ctx_length = False,</span><br><span class="line">    dimensions=1536 </span><br><span class="line">)</span><br><span class="line">store = InMemoryStore(</span><br><span class="line">    index=&#123; # 存储提取的记忆 </span><br><span class="line">        &quot;dims&quot;: 1536,</span><br><span class="line">        &quot;embed&quot;: embedding,</span><br><span class="line">    &#125;</span><br><span class="line">) </span><br></pre></td></tr></table></figure>
<p><strong>创建记忆管理器</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 创建记忆管理器 Runnable 来从对话中提取记忆</span><br><span class="line">memory_manager = create_memory_store_manager( </span><br><span class="line">    model_qwen, </span><br><span class="line">    # 将记忆存储在 &quot;memories&quot; 命名空间（即目录）中</span><br><span class="line">    namespace=(&quot;memories&quot;,),  </span><br><span class="line">    instructions=&quot;用中文存储记忆。&quot;</span><br><span class="line">) </span><br><span class="line"></span><br><span class="line"># 包装 memory_manager 以处理延迟的后台处理</span><br><span class="line">executor = ReflectionExecutor(memory_manager) </span><br></pre></td></tr></table></figure>
<p>对每条消息都进行记忆处理存在以下缺点： -
当消息快速连续到达时，会产生冗余工作 -
在对话中途进行处理时，上下文不完整 - 不必要的 token 消耗</p>
<p><a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langmem/reference/utils/#langmem.ReflectionExecutor"><code>ReflectionExecutor</code></a>
可以延迟记忆处理并取消冗余工作。</p>
<p><strong>创建工作流</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">from langchain_openai import ChatOpenAI</span><br><span class="line">model_qwen=ChatOpenAI(</span><br><span class="line">    api_key=&quot;sk-&quot;, </span><br><span class="line">    base_url=&quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;,</span><br><span class="line">    model=&quot;qwen3-30b-a3b-instruct-2507&quot;,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">@entrypoint(store=store)  # 创建一个 LangGraph 工作流</span><br><span class="line">async def chat(message: str): </span><br><span class="line">    response = model_qwen.invoke(message) </span><br><span class="line"></span><br><span class="line">    # memory_manager 从对话历史中提取记忆</span><br><span class="line">    # 我们将以 OpenAI 的消息格式提供它</span><br><span class="line">    to_process = &#123;&quot;messages&quot;: [&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: message&#125;] + [response]&#125; </span><br><span class="line">    await memory_manager.ainvoke(to_process)  </span><br><span class="line">    return response.content </span><br><span class="line"></span><br><span class="line"># 正常运行对话</span><br><span class="line">response = await chat.ainvoke( </span><br><span class="line">    &quot;记住我是张熙浚&quot;, </span><br><span class="line">) </span><br><span class="line">print(response) </span><br></pre></td></tr></table></figure>
<p><strong>查看记忆</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(store.search((&quot;memories&quot;,)))</span><br></pre></td></tr></table></figure>
<h3 id="参考资料">参考资料</h3>
<p><a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langmem/">简介 - LangChain
框架</a></p>
<p><a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langmem/concepts/conceptual_guide/#semantic-memory-facts-and-knowledge">核心概念
- LangChain 框架</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/08/12/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83qwen3-32b/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/08/12/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83qwen3-32b/" class="post-title-link" itemprop="url">分布式训练qwen3-32b</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-08-12 00:00:00" itemprop="dateCreated datePublished" datetime="2025-08-12T00:00:00+08:00">2025-08-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-08-28 14:58:37" itemprop="dateModified" datetime="2025-08-28T14:58:37+08:00">2025-08-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%A8%A1%E5%9E%8B/" itemprop="url" rel="index"><span itemprop="name">模型</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%A8%A1%E5%9E%8B/%E5%BE%AE%E8%B0%83/" itemprop="url" rel="index"><span itemprop="name">微调</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="训练框架-llama-factor">训练框架-<a target="_blank" rel="noopener" href="https://github.com/hiyouga/LLaMA-Factory">LLaMA-Factor</a></h3>
<p><a target="_blank" rel="noopener" href="https://llamafactory.readthedocs.io/zh-cn/latest/getting_started/installation.html">安装
- LLaMA Factory</a></p>
<p><strong>docker部署镜像</strong>，以便后续传入内网</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git</span><br><span class="line">cd LLaMA-Factory</span><br><span class="line"></span><br><span class="line">docker build -f ./docker/docker-cuda/Dockerfile \</span><br><span class="line">    --build-arg PIP_INDEX=https://pypi.org/simple \</span><br><span class="line">    --build-arg EXTRAS=metrics \</span><br><span class="line">    -t llamafactory:latest .</span><br><span class="line"></span><br><span class="line">docker run -dit --ipc=host --gpus=all \</span><br><span class="line">    -p 7860:7860 \</span><br><span class="line">    -p 8001:8000 \    # 主机 8001 → 容器 8000，主机8000端口被占用了</span><br><span class="line">    --name llamafactory \</span><br><span class="line">    -v /aisys/:/aisys/ \</span><br><span class="line">    docker.1ms.run/hiyouga/llamafactory</span><br><span class="line"></span><br><span class="line">docker run -dit --ipc=host --gpus=all -p 7860:7860 -p 8001:8000 -v /aisys/:/aisys/ --name llamafactory docker.1ms.run/hiyouga/llamafactory</span><br><span class="line"></span><br><span class="line">docker exec -it llamafactory bash</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker pull docker.1ms.run/hiyouga/llamafactory                                    </span><br><span class="line"></span><br><span class="line">docker save docker.1ms.run/hiyouga/llamafactory:latest -o llamafactory-image.tar</span><br><span class="line"></span><br><span class="line">docker load -i llamafactory-image.tar</span><br></pre></td></tr></table></figure>
<p><strong>LLaMA Board 可视化微调（由 <a target="_blank" rel="noopener" href="https://github.com/gradio-app/gradio">Gradio</a>
驱动）</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">llamafactory-cli webui</span><br></pre></td></tr></table></figure>
<ul>
<li>Web UI 访问：<code>http://localhost:7860</code></li>
<li>API 服务访问：<code>http://localhost:8001</code></li>
</ul>
<h3 id="数据集-easy-dataset">数据集-<a target="_blank" rel="noopener" href="https://github.com/ConardLi/easy-dataset">easy-dataset</a></h3>
<p><strong>docker部署镜像</strong>，以便后续传入内网</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/ConardLi/easy-dataset.git</span><br><span class="line">cd easy-dataset</span><br><span class="line"></span><br><span class="line">docker build -t easy-dataset .</span><br><span class="line"></span><br><span class="line">docker load -i easy-dataset.tar</span><br><span class="line"></span><br><span class="line">docker run -d \</span><br><span class="line">  -p 1717:1717 \</span><br><span class="line">  -v /aisys/repo_dev/xizhang/lora_database:/app/local-db \</span><br><span class="line">  -v /aisys/repo_dev/xizhang/lora_databse_prisma:/app/prisma \</span><br><span class="line">  --name easy-dataset \</span><br><span class="line">  easy-dataset</span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">docker exec -it easy-dataset sh</span><br><span class="line"></span><br><span class="line">docker stop easy-dataset</span><br><span class="line">docker rm easy-dataset</span><br><span class="line"></span><br><span class="line">#实时跟踪</span><br><span class="line"> docker logs -f easy-dataset</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>注意：</strong> 请将
<code>&#123;YOUR_LOCAL_DB_PATH&#125;</code>、<code>&#123;LOCAL_PRISMA_PATH&#125;</code>
替换为你希望存储本地数据库的实际路径，建议直接使用当前代码仓库目录下的
<code>local-db</code> 和 <code>prisma</code> 文件夹，这样可以和 NPM
启动时的数据库路径保持一致。</p>
</blockquote>
<blockquote>
<p><strong>注意：</strong>
如果需要挂载数据库文件（PRISMA），需要提前执行
<code>npm run db:push</code> 初始化数据库文件。</p>
</blockquote>
<p>使用开源项目制作数据集</p>
<p>打开浏览器，访问 <code>http://localhost:1717</code></p>
<h3 id="上传内网">上传内网</h3>
<p>使用scp</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp -r &quot;F:\project python\实习\微调\universal-llm_latest.tar&quot; root@10.117.128.50:/aisys/repo_dev/xizhang/images</span><br></pre></td></tr></table></figure>
<p><strong>SCP</strong> 全称是 <strong>Secure Copy
Protocol</strong>（安全复制协议），是一种用于在计算机之间<strong>安全地复制文件</strong>的网络协议。</p>
<p>它基于 <strong>SSH</strong>（Secure
Shell）协议工作，因此所有传输的数据都是<strong>加密的</strong>，可以防止被窃听或篡改，非常适合在不安全的网络（如互联网）中使用。</p>
<h3 id="模型部署与调用">模型部署与调用</h3>
<h4 id="制作模型运行镜像">制作模型运行镜像</h4>
<p>qwen3部署版本要求如下</p>
<p>使用 Python 3.10 或以上版本， PyTorch 2.6 或以上版本</p>
<p><code>transformers&gt;=4.51.0</code> 版本</p>
<p>使用 <code>sglang&gt;=0.4.6.post1</code> 或
<code>vllm&gt;=0.8.5</code> 来创建一个与 OpenAI 兼容的 API 端点</p>
<h4 id="镜像信息">镜像信息</h4>
<table>
<colgroup>
<col style="width: 15%">
<col style="width: 18%">
<col style="width: 21%">
<col style="width: 45%">
</colgroup>
<thead>
<tr class="header">
<th>类别</th>
<th>组件</th>
<th>版本 / 来源</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>OS</strong></td>
<td>Ubuntu</td>
<td>22.04 LTS (Jammy)</td>
<td>上游镜像继承</td>
</tr>
<tr class="even">
<td><strong>Python</strong></td>
<td>CPython</td>
<td>3.11</td>
<td>镜像自带</td>
</tr>
<tr class="odd">
<td><strong>PyTorch</strong></td>
<td>PyTorch</td>
<td>2.6.0+cu126</td>
<td>官方 wheel，CUDA 12.6</td>
</tr>
<tr class="even">
<td><strong>CUDA</strong></td>
<td>Runtime</td>
<td>12.6.3</td>
<td>与宿主机 535 驱动兼容</td>
</tr>
<tr class="odd">
<td><strong>cuDNN</strong></td>
<td>cuDNN</td>
<td>9</td>
<td>包含在镜像</td>
</tr>
<tr class="even">
<td><strong>核心库</strong></td>
<td>transformers</td>
<td>≥4.51.0</td>
<td>官方最新</td>
</tr>
<tr class="odd">
<td></td>
<td>tokenizers</td>
<td>≥0.21</td>
<td>transformers 依赖</td>
</tr>
<tr class="even">
<td></td>
<td>accelerate</td>
<td>≥1.0.0</td>
<td>训练 / 推理加速</td>
</tr>
<tr class="odd">
<td></td>
<td>sentencepiece</td>
<td>≥0.2.0</td>
<td>Qwen3 分词器必需</td>
</tr>
<tr class="even">
<td></td>
<td>protobuf</td>
<td>≥5.28.0</td>
<td>序列化 / 模型加载</td>
</tr>
<tr class="odd">
<td></td>
<td>tiktoken</td>
<td>≥0.8.0</td>
<td>OpenAI 格式分词</td>
</tr>
<tr class="even">
<td><strong>推理框架</strong></td>
<td>vLLM</td>
<td>≥0.8.5</td>
<td>支持 tensor-parallel、PagedAttention</td>
</tr>
<tr class="odd">
<td></td>
<td>SGLang</td>
<td>≥0.4.6.post1</td>
<td>支持 outline 解码、MoE 优化</td>
</tr>
<tr class="even">
<td><strong>可选加速</strong></td>
<td>flash-attn</td>
<td>≥2.7</td>
<td>长上下文 / 大 batch 推理</td>
</tr>
<tr class="odd">
<td><strong>权重下载</strong></td>
<td>modelscope</td>
<td>最新</td>
<td>国内镜像加速</td>
</tr>
<tr class="even">
<td><strong>工具链</strong></td>
<td>git / git-lfs</td>
<td>最新</td>
<td>拉取 HuggingFace 权重</td>
</tr>
<tr class="odd">
<td></td>
<td>curl / jq / vim</td>
<td>最新</td>
<td>调试 &amp; 健康检查</td>
</tr>
</tbody>
</table>
<p><strong>基础镜像</strong><code>pytorch/pytorch:2.6.0-cuda12.6-cudnn9-devel</code>
是 <strong>PyTorch 官方在 Docker Hub
上提供的“全家桶”开发镜像</strong>，发布日期 2025-01-29，镜像大小约 13
GB，定位是 <strong>“开箱即用”的 GPU 训练 / 推理 / 调试环境</strong></p>
<h4 id="dockerfile">dockerfile</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"># ---------- 1. 基础镜像 ----------</span><br><span class="line">FROM pytorch/pytorch:2.6.0-cuda12.6-cudnn9-devel</span><br><span class="line"></span><br><span class="line"># ---------- 2. 国内镜像源 ----------</span><br><span class="line">RUN sed -i &#x27;s|http://archive.ubuntu.com|https://mirrors.tuna.tsinghua.edu.cn|g&#x27; /etc/apt/sources.list &amp;&amp; \</span><br><span class="line">    sed -i &#x27;s|http://security.ubuntu.com|https://mirrors.tuna.tsinghua.edu.cn|g&#x27; /etc/apt/sources.list &amp;&amp; \</span><br><span class="line">    pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple &amp;&amp; \</span><br><span class="line">    pip config set global.trusted-host pypi.tuna.tsinghua.edu.cn</span><br><span class="line"></span><br><span class="line"># ---------- 3. 系统依赖 ----------</span><br><span class="line">RUN apt-get update &amp;&amp; \</span><br><span class="line">    DEBIAN_FRONTEND=noninteractive apt-get install -y \</span><br><span class="line">    git git-lfs build-essential ninja-build curl wget vim jq &amp;&amp; \</span><br><span class="line">    rm -rf /var/lib/apt/lists/*</span><br><span class="line"></span><br><span class="line"># ---------- 4. Python 依赖 ----------</span><br><span class="line">RUN pip install --no-cache-dir --upgrade pip setuptools wheel &amp;&amp; \</span><br><span class="line">    pip install --no-cache-dir \</span><br><span class="line">    &quot;torch==2.6.0+cu126&quot; \</span><br><span class="line">    &quot;transformers&gt;=4.51.0&quot; \</span><br><span class="line">    &quot;tokenizers&gt;=0.21&quot; \</span><br><span class="line">    &quot;accelerate&gt;=1.0.0&quot; \</span><br><span class="line">    &quot;sentencepiece&gt;=0.2.0&quot; \</span><br><span class="line">    &quot;protobuf&gt;=5.28.0&quot; \</span><br><span class="line">    &quot;tiktoken&gt;=0.8.0&quot; \</span><br><span class="line">    &quot;vllm&gt;=0.8.5&quot; \</span><br><span class="line">    &quot;sglang[all]&gt;=0.4.6.post1&quot; \</span><br><span class="line">    &quot;modelscope&quot; \</span><br><span class="line">    &quot;fastapi&quot; &quot;uvicorn[standard]&quot; &quot;pydantic&quot;</span><br><span class="line"></span><br><span class="line"># ---------- 5. 可选性能加速 ----------</span><br><span class="line">RUN pip install --no-cache-dir &quot;flash-attn&gt;=2.7&quot; --no-build-isolation || true</span><br><span class="line"></span><br><span class="line"># ---------- 6. 国内 HuggingFace 镜像 ----------</span><br><span class="line">ENV HF_ENDPOINT=https://hf-mirror.com</span><br><span class="line"></span><br><span class="line"># ---------- 7. 工作目录 ----------</span><br><span class="line">WORKDIR /app</span><br><span class="line">EXPOSE 4000 4001 4002</span><br><span class="line"></span><br><span class="line"># ---------- 8. 默认命令 ----------</span><br><span class="line">CMD [&quot;/bin/bash&quot;]</span><br></pre></td></tr></table></figure>
<h4 id="运行容器">运行容器</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">docker run -it \</span><br><span class="line">  --name llm-service \</span><br><span class="line">  --gpus all \</span><br><span class="line">  -p 4000:4000 \</span><br><span class="line">  -p 4001:4001 \</span><br><span class="line">  -p 4002:4002 \</span><br><span class="line">  -v /aisys/repo_dev/xizhang/models:/app/models \</span><br><span class="line">  -v /aisys/repo_dev/xizhang/models/cache:/app/models/.cache \</span><br><span class="line">  --shm-size=8g \</span><br><span class="line">  universal-llm:latest bash</span><br></pre></td></tr></table></figure>
<h4 id="vllm部署qwen3">vllm部署qwen3</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">vllm serve /app/models/qwen3-32b-lora-new \</span><br><span class="line">    --port 4001 \</span><br><span class="line">    --tensor-parallel-size 4 \</span><br><span class="line">    --max-model-len 1024 \</span><br><span class="line">    --reasoning-parser qwen3 \</span><br><span class="line">    --gpu-memory-utilization 0.8 \</span><br><span class="line">    --max-num-seqs 8 \</span><br><span class="line">    --host 0.0.0.0</span><br></pre></td></tr></table></figure>
<table>
<colgroup>
<col style="width: 20%">
<col style="width: 40%">
<col style="width: 40%">
</colgroup>
<thead>
<tr class="header">
<th>参数</th>
<th>含义</th>
<th>推荐/注意</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>--port 8000</code></td>
<td>服务监听端口</td>
<td>与 <code>-p 8000:8000</code> 保持一致；如需多实例，可改 8001/8002
…</td>
</tr>
<tr class="even">
<td><code>--tensor-parallel-size 4</code></td>
<td>把模型权重切成 4 份，跨 4 张 GPU 并行计算</td>
<td>必须 ≤ 实际 GPU 数量；Qwen3-32B 在 4×L20
上显存刚好够，<strong>不可再大</strong></td>
</tr>
<tr class="odd">
<td><code>--max-model-len 1024</code></td>
<td>单次推理最大 token 数（含 prompt + 生成）</td>
<td>若场景需要 4k/8k/32k，可调到 4096/8192；显存占用 ∝ 长度²</td>
</tr>
<tr class="even">
<td><code>--reasoning-parser qwen3</code></td>
<td>vLLM ≥0.8.5 新增开关，解析 Qwen3 的
<code>&lt;think&gt;…&lt;/think&gt;</code> 标签，把推理过程单独返回</td>
<td>仅在 <strong>Qwen3</strong> 系列模型有效，其他模型请去掉</td>
</tr>
<tr class="odd">
<td><code>--gpu-memory-utilization 0.8</code></td>
<td>显存使用上限 80 %；剩余 20 % 留给 CUDA kernel、KV cache 膨胀</td>
<td>若出现 OOM，可降到 0.7；若想多并发，可尝试 0.85（风险 OOM）</td>
</tr>
<tr class="even">
<td><code>--max-num-seqs 8</code></td>
<td>同一时刻最多并发处理的 <strong>请求条数</strong></td>
<td>与 <code>--max-model-len</code> 和显存同时决定；若长度 ↑，此值需
↓</td>
</tr>
<tr class="odd">
<td><code>--host 0.0.0.0</code></td>
<td>监听所有网卡，使容器外可访问</td>
<td>生产环境可改为内网 IP 或 127.0.0.1 提高安全性</td>
</tr>
</tbody>
</table>
<h4 id="测试">测试</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">curl http://localhost:4001/v1/chat/completions \</span><br><span class="line">   -H &quot;Content-Type: application/json&quot; \</span><br><span class="line">   -d &#x27;&#123;</span><br><span class="line">       &quot;model&quot;: &quot;/app/models/qwen3-32b-lora-new&quot;,</span><br><span class="line">       &quot;messages&quot;: [</span><br><span class="line">           &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;请用中文介绍一下你自己&quot;&#125;</span><br><span class="line">       ],</span><br><span class="line">       &quot;temperature&quot;: 0.7,</span><br><span class="line">       &quot;max_tokens&quot;: 512</span><br><span class="line">   &#125;&#x27;</span><br></pre></td></tr></table></figure>
<h4 id="调用">调用</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">from openai import OpenAI</span><br><span class="line"></span><br><span class="line"># 指向本地 vLLM</span><br><span class="line">client = OpenAI(</span><br><span class="line">    base_url=&quot;http://localhost:8000/v1&quot;,</span><br><span class="line">    api_key=&quot;dummy&quot;          # vLLM 不做鉴权，随便填</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">resp = client.chat.completions.create(</span><br><span class="line">    model=&quot;qwen3-32b&quot;,       # 必须和 vLLM 启动路径或 --served-model-name 保持一致</span><br><span class="line">    messages=[</span><br><span class="line">        &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;9.9 和 9.11 哪个大？&quot;&#125;</span><br><span class="line">    ],</span><br><span class="line">    max_tokens=512,</span><br><span class="line">    temperature=0.7,</span><br><span class="line">    stream=False             # True 可开流式</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">print(resp.choices[0].message.content)</span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="https://qwen.readthedocs.io/en/latest/getting_started/quickstart.html">快速入门
- Qwen — Quickstart - Qwen</a></p>
<p><a target="_blank" rel="noopener" href="https://www.modelscope.cn/models/Qwen/Qwen3-32B">通义千问3-32B ·
模型库</a></p>
<h3 id="微调数据集">微调数据集</h3>
<h4 id="alpaca和sharegpt的区别">alpaca和sharegpt的区别</h4>
<p>▶ Alpaca 典型字段</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;instruction&quot;</span><span class="punctuation">:</span> <span class="string">&quot;把下面句子翻译成英文&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;input&quot;</span><span class="punctuation">:</span> <span class="string">&quot;今天天气真好&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;output&quot;</span><span class="punctuation">:</span> <span class="string">&quot;The weather is nice today.&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;system&quot;</span><span class="punctuation">:</span> <span class="string">&quot;你是一个翻译助手&quot;</span><span class="punctuation">,</span>   <span class="comment">// 可选</span></span><br><span class="line">  <span class="attr">&quot;history&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">]</span>                 <span class="comment">// 可选，放前几轮</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<ul>
<li>一条数据 = 一次独立任务</li>
<li>字段固定：<code>instruction / input / output</code> 三板斧</li>
</ul>
<p>▶ ShareGPT 典型字段</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;conversations&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">    <span class="punctuation">&#123;</span><span class="attr">&quot;from&quot;</span><span class="punctuation">:</span> <span class="string">&quot;human&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;value&quot;</span><span class="punctuation">:</span> <span class="string">&quot;我今天心情不好&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span><span class="attr">&quot;from&quot;</span><span class="punctuation">:</span> <span class="string">&quot;gpt&quot;</span><span class="punctuation">,</span>   <span class="attr">&quot;value&quot;</span><span class="punctuation">:</span> <span class="string">&quot;怎么啦？想聊聊吗&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span><span class="attr">&quot;from&quot;</span><span class="punctuation">:</span> <span class="string">&quot;human&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;value&quot;</span><span class="punctuation">:</span> <span class="string">&quot;论文又被拒了&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span><span class="attr">&quot;from&quot;</span><span class="punctuation">:</span> <span class="string">&quot;gpt&quot;</span><span class="punctuation">,</span>   <span class="attr">&quot;value&quot;</span><span class="punctuation">:</span> <span class="string">&quot;理解你的挫败感…&quot;</span><span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;system&quot;</span><span class="punctuation">:</span> <span class="string">&quot;你是贴心聊天机器人&quot;</span><span class="punctuation">,</span>   <span class="comment">// 可选</span></span><br><span class="line">  <span class="attr">&quot;tools&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span>...<span class="punctuation">]</span>                   <span class="comment">// 可选，放函数描述</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<ul>
<li>一条数据 = 一段完整的多轮对话</li>
<li>角色交替：<code>human / gpt / function / observation</code> 等</li>
</ul>
<table>
<colgroup>
<col style="width: 9%">
<col style="width: 47%">
<col style="width: 42%">
</colgroup>
<thead>
<tr class="header">
<th>维度</th>
<th>Alpaca</th>
<th>ShareGPT</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>来源</strong></td>
<td>斯坦福 Alpaca 项目，为了低成本做指令微调</td>
<td>ShareGPT 网站爬取的真实 ChatGPT 对话</td>
</tr>
<tr class="even">
<td><strong>目标</strong></td>
<td>让模型学会“看到指令+输入→给出答案”</td>
<td>让模型学会“像 ChatGPT 一样多轮对话”</td>
</tr>
</tbody>
</table>
<h4 id="详解">详解</h4>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;instruction&quot;: &quot;电阻率测定法的环境温湿度控制对检测结果的可信度有何影响？&quot;,</span><br><span class="line">    &quot;input&quot;: &quot;- \&quot;检测依据：DL<span class="operator">/</span>T <span class="number">421</span><span class="number">-2009</span> 电力用油体积电阻率测定法\&quot;\n- \&quot;检测地点及环境条件：油品检测室 温度：<span class="number">16</span>℃ 湿度：<span class="number">57</span><span class="operator">%</span>RH\&quot;\n- \&quot;电阻率（\&quot;&quot;,</span><br><span class="line">    &quot;output&quot;: &quot;&lt;think&gt;### \n首先，理解问题的核心：电阻率测定法中，环境温湿度控制如何影响检测结果的可信度，这涉及到测量过程的准确性和可靠性。\n\n然后，分析关键信息：检测依据是DL/T 421-2009标准，它规定了电阻率测定的方法；环境条件被记录为温度16℃和湿度57%RH；检测结果显示电阻率为1.04×10^10 Ω·cm，符合DL/T 571-2014标准的要求（≥6×10^9 Ω·cm）。\n\n接着，推理温湿度控制的影响：环境温湿度是测量过程中的关键变量，控制这些条件确保&quot;,</span><br><span class="line">    &quot;system&quot;: &quot;作为电力能源报告解读专家，我在生成答案时，将严格遵循以下格式：\n根据“信息来源”，“信息来源”是原文中可直接支撑结论的句子、数据或图表编号给出“结论与推理”——用上述逐条复现的信息为唯一依据，推导出最终答案。&quot;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>instruction为问题；input为上下文；output包含思维链与答案；system为系统提示词</p>
<h3 id="微调参数设置">微调参数设置</h3>
<h4 id="deepspeed-stagedeepspeed-阶段"><strong>DeepSpeed
stage（DeepSpeed 阶段）</strong></h4>
<p><strong>deepSpeed 的 ZeRO 分布式优化阶段</strong>，用于在多 GPU
上高效训练大模型。</p>
<table>
<colgroup>
<col style="width: 14%">
<col style="width: 43%">
<col style="width: 42%">
</colgroup>
<thead>
<tr class="header">
<th>Stage</th>
<th>功能</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Stage 0</strong></td>
<td>不做任何优化</td>
<td>基础分布式训练（DDP），显存占用高</td>
</tr>
<tr class="even">
<td><strong>Stage 1</strong></td>
<td>梯度分片（Gradient Sharding）</td>
<td>将梯度切分到不同 GPU，减少显存</td>
</tr>
<tr class="odd">
<td><strong>Stage 2</strong></td>
<td>参数 + 梯度分片</td>
<td>进一步降低显存，但需通信同步</td>
</tr>
<tr class="even">
<td><strong>Stage 3</strong></td>
<td>✅ <strong>参数 + 梯度 + 优化器状态分片</strong></td>
<td>最强显存优化，支持超大模型</td>
</tr>
</tbody>
</table>
<h4 id="使用-deepspeed-offload使用-offload">使用 DeepSpeed offload（使用
offload）</h4>
<p>将 <strong>部分或全部模型参数、优化器状态卸载到 CPU
内存</strong>，进一步释放 GPU 显存。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">llamafactory-cli train \</span><br><span class="line">    --stage sft \</span><br><span class="line">    --do_train True \</span><br><span class="line">    --model_name_or_path /aisys/repo_dev/xizhang/models/qwen3-32b-lora-new \</span><br><span class="line">    --preprocessing_num_workers 16 \</span><br><span class="line">    --finetuning_type lora \</span><br><span class="line">    --template qwen3 \</span><br><span class="line">    --flash_attn auto \</span><br><span class="line">    --dataset_dir /aisys/repo_dev/xizhang/lora_database/P9er76jCWCFW \</span><br><span class="line">    --dataset [Easy Dataset] [P9er76jCWCFW] Alpaca \</span><br><span class="line">    --cutoff_len 4096 \</span><br><span class="line">    --learning_rate 5e-05 \</span><br><span class="line">    --num_train_epochs 3.0 \</span><br><span class="line">    --max_samples 100000 \</span><br><span class="line">    --per_device_train_batch_size 2 \</span><br><span class="line">    --gradient_accumulation_steps 1 \</span><br><span class="line">    --lr_scheduler_type cosine \</span><br><span class="line">    --max_grad_norm 1.0 \</span><br><span class="line">    --logging_steps 5 \</span><br><span class="line">    --save_steps 200 \</span><br><span class="line">    --warmup_steps 0 \</span><br><span class="line">    --packing False \</span><br><span class="line">    --enable_thinking True \</span><br><span class="line">    --report_to none \</span><br><span class="line">    --output_dir saves/Qwen3-32B-Thinking/lora/train_2025-08-28-03-04-52 \</span><br><span class="line">    --bf16 True \</span><br><span class="line">    --plot_loss True \</span><br><span class="line">    --trust_remote_code True \</span><br><span class="line">    --ddp_timeout 180000000 \</span><br><span class="line">    --include_num_input_tokens_seen True \</span><br><span class="line">    --optim adamw_torch \</span><br><span class="line">    --lora_rank 8 \</span><br><span class="line">    --lora_alpha 16 \</span><br><span class="line">    --lora_dropout 0 \</span><br><span class="line">    --lora_target all \</span><br><span class="line">    --val_size 0.15 \</span><br><span class="line">    --eval_strategy steps \</span><br><span class="line">    --eval_steps 200 \</span><br><span class="line">    --per_device_eval_batch_size 2 \</span><br><span class="line">    --deepspeed cache/ds_z3_config.json</span><br></pre></td></tr></table></figure>
<h3 id="训练结果">训练结果</h3>
<figure>
<img src="/2025/08/12/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83qwen3-32b/image-20250827091214108.png" alt="image-20250827091214108">
<figcaption aria-hidden="true">image-20250827091214108</figcaption>
</figure>
<h3 id="评估">评估</h3>
<p>不知道为什么使用llamafactory的评估会爆显存，我怀疑是因为那个webui评估可能不支持多卡，就进行一下人工评估吧</p>
<p>输入</p>
<figure>
<img src="/2025/08/12/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83qwen3-32b/image-20250828144300339.png" alt="image-20250828144300339">
<figcaption aria-hidden="true">image-20250828144300339</figcaption>
</figure>
<p>微调模型</p>
<figure>
<img src="/2025/08/12/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83qwen3-32b/image-20250828145106050.png" alt="image-20250828145106050">
<figcaption aria-hidden="true">image-20250828145106050</figcaption>
</figure>
<p>初始模型</p>
<figure>
<img src="/2025/08/12/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83qwen3-32b/image-20250828145750115.png" alt="image-20250828145750115">
<figcaption aria-hidden="true">image-20250828145750115</figcaption>
</figure>
<h3 id="在内网计算节点访问swanlab-cloud">在内网计算节点访问SwanLab
Cloud</h3>
<p><a target="_blank" rel="noopener" href="https://docs.swanlab.cn/guide_cloud/experiment_track/ssh-portforwarding.html">在内网计算节点访问SwanLab
Cloud | SwanLab官方文档</a></p>
<h3 id="如何计算训练步数">如何计算训练步数</h3>
<h4 id="训练集样本量">1. 训练集样本量</h4>
<p><strong>公式</strong> 训练集样本量 = 总数据量 × (1 − 验证集比例)</p>
<p><strong>示例</strong> 总数据 2876 条，验证集占 15% 2876 × (1 − 0.15)
= 2876 × 0.85 = <strong>2446 条</strong></p>
<h4 id="每次参数更新处理的样本数effective-batch-size">2.
每次参数更新处理的样本数（effective batch size）</h4>
<p><strong>公式</strong> 每次更新样本数 = 单设备批次大小 × GPU 数 ×
梯度累积步数</p>
<p><strong>示例</strong></p>
<ul>
<li>per_device_train_batch_size = 1</li>
<li>GPU 数 = 2</li>
<li>gradient_accumulation_steps = 8</li>
</ul>
<p>1 × 2 × 8 = <strong>16 条</strong></p>
<blockquote>
<p>通俗理解： GPU 一次只能看 1 条 → 2 卡并行就是 2 条 → 累积 8
次才更新一次参数，所以一次更新真正看了 16 条数据。</p>
</blockquote>
<h4 id="每轮epoch的训练步数">3. 每轮（epoch）的训练步数</h4>
<p><strong>公式</strong> 每轮步数 = ⌊ 训练集样本量 ÷ 每次更新样本数 ⌋
（⌊ ⌋ 表示向下取整）</p>
<p><strong>示例</strong> 2446 ÷ 16 = 152.875 → <strong>152
步</strong></p>
<h4 id="总训练步数">4. 总训练步数</h4>
<p><strong>公式</strong> 总步数 = 每轮步数 × 训练轮数 (epochs)</p>
<p><strong>示例</strong> 152 × 3 = <strong>456 步</strong></p>
<h3 id="如何计算一个模型占用的显存">如何计算一个模型占用的显存</h3>
<h4 id="基础模型的权重">基础模型的权重</h4>
<ul>
<li>定义：预训练模型的参数矩阵，即选择的预训练模型所占用显存的大小。</li>
<li><strong>计算公式</strong>： <strong>显存占用 = 模型参数数量 ×
单个参数的字节数</strong></li>
</ul>
<h4 id="常见模型精度下的单个参数显存占用">常见模型精度下的单个参数显存占用：</h4>
<p>表格</p>
<p>复制</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">精度类型</th>
<th style="text-align: left;">二进制位数</th>
<th style="text-align: left;">字节数</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">FP32</td>
<td style="text-align: left;">32位</td>
<td style="text-align: left;">4字节</td>
</tr>
<tr class="even">
<td style="text-align: left;">FP16</td>
<td style="text-align: left;">16位</td>
<td style="text-align: left;">2字节</td>
</tr>
<tr class="odd">
<td style="text-align: left;">BF16</td>
<td style="text-align: left;">16位</td>
<td style="text-align: left;">2字节（指数位同FP32）</td>
</tr>
<tr class="even">
<td style="text-align: left;">INT8</td>
<td style="text-align: left;">8位</td>
<td style="text-align: left;">1字节</td>
</tr>
<tr class="odd">
<td style="text-align: left;">INT4</td>
<td style="text-align: left;">4位</td>
<td style="text-align: left;">0.5字节</td>
</tr>
<tr class="even">
<td style="text-align: left;">INT2</td>
<td style="text-align: left;">2位</td>
<td style="text-align: left;">0.25字节</td>
</tr>
</tbody>
</table>
<p>例如</p>
<ul>
<li><strong>模型选择</strong>：Qwen2.5-7B-Instruct</li>
<li><strong>参数规模</strong>：70亿（7B）</li>
<li><strong>计算精度</strong>：BF16（2字节/参数）</li>
<li><strong>预估显存占用</strong>： <strong>70亿 × 2字节 = 140亿字节 =
14GB</strong></li>
</ul>
<h4 id="框架开销framework-overhead">框架开销（Framework Overhead）</h4>
<ul>
<li><strong>定义</strong>：LLaMAFactory 底层使用的深度学习框架（如
PyTorch）本身的显存占用。</li>
<li><strong>包含内容</strong>：
<ul>
<li>张量缓存</li>
<li>线程资源</li>
<li>内核调度开销</li>
<li>自动微分图结构等</li>
</ul></li>
<li><strong>计算方法</strong>：难以精确计算</li>
<li><strong>估算方法</strong>：通常占用不大，默认估算为 <strong>1
GB</strong></li>
</ul>
<h4 id="lora-适配器lora-adapters">LoRA 适配器（LoRA Adapters）</h4>
<ul>
<li><p><strong>定义</strong>：在 LoRA
微调中，不直接修改原始模型的庞大权重，而是插入轻量级的“LoRA适配器模块”来学习微调所需的变化。</p></li>
<li><p><strong>计算方法</strong>：</p>
<p>显存占用=LoRA层数×秩（Rank）×(输入维度+输出维度)×2<em>B</em></p></li>
<li><p><strong>估算方法</strong>：</p>
<ul>
<li>与 LoRA 的秩（Rank）大小相关</li>
<li>一般占用不大，常规配置下通常不超过 <strong>0.5
GB</strong>，保守估计为 <strong>0.5 GB</strong></li>
</ul></li>
</ul>
<h4 id="激活值activations">激活值（Activations）</h4>
<ul>
<li><p><strong>定义</strong>：前向传播过程中各层的输出张量（如隐藏层状态、注意力矩阵等），即模型“处理数据时产生的所有中间结果”。</p></li>
<li><p><strong>计算方法</strong>：</p>
<p>显存占用=批量大小×序列长度×隐藏层维度×模型层数×单个元素字节数</p></li>
<li><p><strong>估算方法</strong>：</p>
<ul>
<li>单次处理的 Token 量每增加 <strong>1K</strong>，显存约增加
<strong>2.5 GB</strong></li>
<li>与单 GPU 的批量大小和数据集的截断长度（序列长度）正相关</li>
<li>在固定其他配置（基础模型权重、框架开销、LoRA适配器）后，剩余显存即为激活值占用</li>
</ul></li>
</ul>
<h3 id="加速方式">加速方式</h3>
<table>
<colgroup>
<col style="width: 10%">
<col style="width: 14%">
<col style="width: 37%">
<col style="width: 37%">
</colgroup>
<thead>
<tr class="header">
<th>加速方式</th>
<th>全称 / 来源</th>
<th>核心原理与特点</th>
<th>适用场景与注意事项</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>auto</strong></td>
<td>自动选择</td>
<td>由框架（如 transformers、LLaMA-Factory、DeepSpeed
等）根据当前硬件、驱动、CUDA 版本自动挑选最快的可用算子或路径。
优点：零配置、开箱即用；缺点：不一定能启用最新、最快的内核。</td>
<td>初次实验、不想手动调参时首选。</td>
</tr>
<tr class="even">
<td><strong>flashattn2</strong></td>
<td>FlashAttention-2</td>
<td>通过 IO-Aware 的算法和 GPU Tensor Core 优化，将标准 Multi-Head
Attention 的显存访问次数大幅降低，从而显著加快训练/推理速度（通常
2-4×），并减少显存占用。 需要 A100、H100、RTX 30/40 系列等
Ampere/Lovelace 架构；依赖 CUDA≥11.8、PyTorch≥2.0 且需安装
<code>flash-attn</code> wheel。</td>
<td>训练/微调 LLM 时首选；序列越长收益越大。若编译失败可退回 xformers
或原生实现。</td>
</tr>
<tr class="odd">
<td><strong>unsloth</strong></td>
<td>Unsloth 开源库</td>
<td>针对 Llama、Mistral、Qwen 等架构，使用动态量化、手工 fused-kernel
和梯度检查点优化，使 LoRA 微调在消费级 GPU 上也能跑更大
batch/更长序列。官方宣称速度提升 2-5×，显存节省 50-70%。
安装简单：<code>pip install unsloth</code>（会自动替换部分 PyTorch
层）。</td>
<td>单卡 4090/3090 上 LoRA 微调 7B-13B
模型效果最佳；目前仅支持有限模型。</td>
</tr>
<tr class="even">
<td><strong>liger_kernel</strong></td>
<td>Liger-Kernel（微软开源）</td>
<td>以 Triton 编写的高性能 fused-kernel
合集（SwiGLU、RMSNorm、CrossEntropy、RoPE 等），在保持数值精度的同时减少
kernel launch 和显存写回，训练吞吐量可提升 10-20%。 纯 Python/Triton
实现，无需额外 CUDA 编译。</td>
<td>对训练框架侵入性小，可与 FlashAttention 并存；适合想“无痛”提速
10-20% 的场景。</td>
</tr>
</tbody>
</table>
<h3 id="参考资料">参考资料</h3>
<p><a target="_blank" rel="noopener" href="https://llamafactory.readthedocs.io/zh-cn/latest/">LLaMA
Factory</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/08/12/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/qwen3-8b%E5%BE%AE%E8%B0%83%E5%AE%9E%E6%88%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/08/12/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/qwen3-8b%E5%BE%AE%E8%B0%83%E5%AE%9E%E6%88%98/" class="post-title-link" itemprop="url">qwen3-8b微调实战</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-08-12 00:00:00" itemprop="dateCreated datePublished" datetime="2025-08-12T00:00:00+08:00">2025-08-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-08-13 15:10:05" itemprop="dateModified" datetime="2025-08-13T15:10:05+08:00">2025-08-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%A8%A1%E5%9E%8B/" itemprop="url" rel="index"><span itemprop="name">模型</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%A8%A1%E5%9E%8B/%E5%BE%AE%E8%B0%83/" itemprop="url" rel="index"><span itemprop="name">微调</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="前言">前言</h3>
<p>在完成微调前备知识的学习后，正式开始使用unsloth对Qwen3-8B-unsloth-bnb-4bit模型的lora微调实战</p>
<h3 id="模型加载">模型加载</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">from unsloth import FastLanguageModel</span><br><span class="line">import torch</span><br><span class="line"></span><br><span class="line">max_seq_length = 8192</span><br><span class="line">dtype = None</span><br><span class="line">load_in_4bit = True</span><br><span class="line"></span><br><span class="line">model, tokenizer = FastLanguageModel.from_pretrained(</span><br><span class="line">    model_name = &quot;/workspace/qwen3-8b&quot;,</span><br><span class="line">    max_seq_length = max_seq_length,</span><br><span class="line">    dtype = dtype,</span><br><span class="line">    load_in_4bit = load_in_4bit,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<blockquote>
<p><code>FastLanguageModel</code> 是 <strong>Unsloth
框架的核心入口类</strong>，即<strong>“把 Hugging Face 的 transformers
模型‘加速’成支持 QLoRA 微调、显存占用减半、速度提升 2-5
倍的封装器。”</strong></p>
<p><code>max_seq_length = 8192</code><strong>作用</strong>：告诉框架
<strong>“后续所有输入序列的最大长度”</strong>。<strong>内部一次性为位置编码、注意力掩码、KV-Cache
等开辟的张量尺寸</strong>，因此显存随它
<strong>平方级增长</strong>。</p>
<p><code>dtype = None</code><strong>作用</strong>：让 Unsloth
<strong>自动选择最合适的浮点精度</strong>。</p>
<p><code>load_in_4bit = True</code><strong>作用</strong>：把模型<strong>权重量化成
4-bit</strong>，显存降到 1/4，QLoRA 微调必备。</p>
</blockquote>
<h3 id="查看模型与分词器信息">查看模型与分词器信息</h3>
<h4 id="模型信息">模型信息</h4>
<p>运行</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model</span><br></pre></td></tr></table></figure>
<p>通过阅读模型信息我们可以了解到：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(embed_tokens): Embedding(151936, 4096, padding_idx=151654)</span><br></pre></td></tr></table></figure>
<p><strong>模型有 15 万个 token 的字典，每个字/词被翻译成 4096
维向量，第 151 654 号 token 被官方指定为填充符。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">(layers): ModuleList(</span><br><span class="line">      (0-2): 3 x Qwen3DecoderLayer(</span><br><span class="line">        (self_attn): Qwen3Attention(</span><br><span class="line">          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)</span><br><span class="line">          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)</span><br><span class="line">          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)</span><br><span class="line">          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)</span><br><span class="line">          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)</span><br><span class="line">          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)</span><br><span class="line">          (rotary_emb): LlamaRotaryEmbedding()</span><br><span class="line">        )</span><br><span class="line">        (mlp): Qwen3MLP(</span><br><span class="line">          (gate_proj): Linear(in_features=4096, out_features=12288, bias=False)</span><br><span class="line">          (up_proj): Linear(in_features=4096, out_features=12288, bias=False)</span><br><span class="line">          (down_proj): Linear(in_features=12288, out_features=4096, bias=False)</span><br><span class="line">          (act_fn): SiLU()</span><br><span class="line">        )</span><br><span class="line">        (input_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)</span><br><span class="line">        (post_attention_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)</span><br><span class="line">      )</span><br></pre></td></tr></table></figure>
<p>共有36层<strong>Qwen3DecoderLayer</strong>，每层包含<strong>Qwen3Attention</strong>，<strong>Qwen3MLP</strong>（<strong>一个
SwiGLU
前馈网络</strong>），<strong>Qwen3RMSNorm</strong>（两个<strong>归一化层</strong>，对
4096 维的隐藏向量做“均方根归一化”，防止梯度爆炸、稳定训练。）</p>
<figure>
<img src="/2025/08/12/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/qwen3-8b%E5%BE%AE%E8%B0%83%E5%AE%9E%E6%88%98/image-20250812153659843.png" alt="image-20250812153659843">
<figcaption aria-hidden="true">image-20250812153659843</figcaption>
</figure>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/cavalier-chen/p/18937098">大模型-qwen3
模型结构解读-66 - jack-chen666 - 博客园</a></p>
<blockquote>
<p><strong>LoRA可以插到哪里呢？</strong></p>
<p><strong>凡是打印里每层 Decoder 中出现的
<code>Linear4bit</code>（q/k/v/o + gate/up/down）就是 LoRA
可插、且默认会被插入的位置。</strong></p>
</blockquote>
<h4 id="分词器信息">分词器信息</h4>
<p>运行</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokenizer</span><br></pre></td></tr></table></figure>
<p>查看tokenizer信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Qwen2TokenizerFast(name_or_path=&#x27;/workspace/qwen3-8b&#x27;, vocab_size=151643, model_max_length=40960, is_fast=True, padding_side=&#x27;left&#x27;, truncation_side=&#x27;right&#x27;, special_tokens=&#123;&#x27;eos_token&#x27;: &#x27;&lt;|im_end|&gt;&#x27;, &#x27;pad_token&#x27;: &#x27;&lt;|vision_pad|&gt;&#x27;, &#x27;additional_special_tokens&#x27;: [&#x27;&lt;|im_start|&gt;&#x27;, &#x27;&lt;|im_end|&gt;&#x27;, &#x27;&lt;|object_ref_start|&gt;&#x27;, &#x27;&lt;|object_ref_end|&gt;&#x27;, &#x27;&lt;|box_start|&gt;&#x27;, &#x27;&lt;|box_end|&gt;&#x27;, &#x27;&lt;|quad_start|&gt;&#x27;, &#x27;&lt;|quad_end|&gt;&#x27;, &#x27;&lt;|vision_start|&gt;&#x27;, &#x27;&lt;|vision_end|&gt;&#x27;, &#x27;&lt;|vision_pad|&gt;&#x27;, &#x27;&lt;|image_pad|&gt;&#x27;, &#x27;&lt;|video_pad|&gt;&#x27;]&#125;, clean_up_tokenization_spaces=False, added_tokens_decoder=&#123;</span><br><span class="line">	151643: AddedToken(&quot;&lt;|endoftext|&gt;&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),</span><br><span class="line">	151644: AddedToken(&quot;&lt;|im_start|&gt;&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),</span><br><span class="line">	151645: AddedToken(&quot;&lt;|im_end|&gt;&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),</span><br><span class="line">	151646: AddedToken(&quot;&lt;|object_ref_start|&gt;&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),</span><br><span class="line">	截取部分</span><br><span class="line">&#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>vocab_size=151643：<strong>模型真正能理解和生成的子词/符号有这 151643
种，其余位置是预留空白。</strong></p>
<p>model_max_length=40960：<strong>理论最大输入长度 40k
token</strong>（实际受显存限制）</p>
<p>is_fast=True：表示 <strong>tokenizer 使用的是 Hugging Face 的「Rust
高速实现」</strong>（即 <em>tokenizers</em> 库）</p>
<p>special_tokens：打印的 <code>special_tokens</code> 字典 &amp;
<code>added_tokens_decoder</code> 已经把 <strong>151643-151668</strong>
全部列出，共 <strong>26 个</strong>。</p>
<h3 id="模拟一次模型处理流程">模拟一次模型处理流程</h3>
<p>将对话内容通过tokenizer进行处理</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">messages = [</span><br><span class="line">    &#123;&quot;role&quot; : &quot;user&quot;, &quot;content&quot; : &quot;你好，好久不见！&quot;&#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">text = tokenizer.apply_chat_template(</span><br><span class="line">    messages,</span><br><span class="line">    tokenize = False,</span><br><span class="line">    add_generation_prompt = True, </span><br><span class="line">    enable_thinking = False, # 设置不思考</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><code>apply_chat_template</code> 是把「人类对话格式的 Python
列表」一键翻译成 <strong>模型能直接理解的带特殊标记的文本字符串（或
token id 序列）</strong> 的“官方模板引擎”。</p>
<p>转化后的格式为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x27;&lt;|im_start|&gt;user\n你好，好久不见！&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n&lt;think&gt;\n\n&lt;/think&gt;\n\n&#x27;</span><br></pre></td></tr></table></figure>
<p>然后将转化后的字符串<strong>转成 GPU 上的 PyTorch token
张量，准备直接送进模型推理或训练。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">inputs = tokenizer(text, return_tensors=&quot;pt&quot;).to(&quot;cuda&quot;)</span><br></pre></td></tr></table></figure>
<p>以上代码共做了三步：</p>
<ol type="1">
<li><strong>tokenizer(text)</strong> 把前面
<code>apply_chat_template</code> 得到的字符串按词表切成 <strong>token id
列表</strong>。</li>
<li><strong>return_tensors=“pt”</strong> 把列表包成 <strong>PyTorch
张量</strong>（shape = [1, seq_len]）。</li>
<li><strong>.to(“cuda”)</strong> 把张量搬到 <strong>GPU
显存</strong>。</li>
</ol>
<p>输出如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;input_ids&#x27;: tensor([[151644,    872,    198, 108386,   3837, 111920, 101571,   6313, 151645,</span><br><span class="line">            198, 151644,  77091,    198, 151667,    271, 151668,    271]],</span><br><span class="line">       device=&#x27;cuda:0&#x27;), &#x27;attention_mask&#x27;: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device=&#x27;cuda:0&#x27;)&#125;</span><br></pre></td></tr></table></figure>
<table>
<colgroup>
<col style="width: 24%">
<col style="width: 11%">
<col style="width: 63%">
</colgroup>
<thead>
<tr class="header">
<th>键</th>
<th>形状</th>
<th>每个数字的含义</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>input_ids</strong></td>
<td><code>[1, 17]</code></td>
<td>17 个 token 的 ID 列表，已放到 GPU</td>
</tr>
<tr class="even">
<td><strong>attention_mask</strong></td>
<td><code>[1, 17]</code></td>
<td>17 个 <strong>1</strong>，表示“这些位置都是有效 token，无填充”</td>
</tr>
</tbody>
</table>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">outputs = model.generate(</span><br><span class="line">    input_ids=inputs.input_ids,</span><br><span class="line">    attention_mask=inputs.attention_mask,</span><br><span class="line">    max_new_tokens=max_seq_length,</span><br><span class="line">    use_cache=True,#启用 KV-Cache，避免重复计算，显存换时间</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>让模型在 GPU 上 <strong>根据已有 token
继续生成文本</strong>，直到达到 <code>max_new_tokens</code>
或遇到终止符。</p>
<p>outputs格式和inputs类似，使用nput_ids表示后续字符</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">response = tokenizer.batch_decode(outputs)</span><br></pre></td></tr></table></figure>
<p>把模型输出的 <strong>token id
序列</strong>（<code>outputs</code>）一次性还原成
<strong>人类可读的字符串</strong>。</p>
<p>输出如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x27;&lt;|im_start|&gt;user\n你好，好久不见！&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n&lt;think&gt;\n\n&lt;/think&gt;\n\n你好！好久不见！最近过得怎么样？有什么新鲜事想和我分享吗？😊&lt;|im_end|&gt;&#x27;</span><br></pre></td></tr></table></figure>
<p>这里展示的是没有思考过程的，最简单对话流程，若设置思考模式，完整代码如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">text = tokenizer.apply_chat_template(</span><br><span class="line">    messages,</span><br><span class="line">    tools = tools,#同样，可以设置function calling</span><br><span class="line">    tokenize = False,</span><br><span class="line">    add_generation_prompt = True, </span><br><span class="line">    enable_thinking = True, # 设置思考</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">inputs = tokenizer(text, return_tensors=&quot;pt&quot;).to(&quot;cuda&quot;)</span><br><span class="line"></span><br><span class="line">outputs = model.generate(</span><br><span class="line">    input_ids=inputs.input_ids,</span><br><span class="line">    attention_mask=inputs.attention_mask,</span><br><span class="line">    max_new_tokens=max_seq_length,</span><br><span class="line">    use_cache=True,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">response = tokenizer.batch_decode(outputs)</span><br></pre></td></tr></table></figure>
<p>当然，除了使用上述底层API进行对话外，Unsloth还提供了更加便捷的流式输出模型对话信息的函数，基本对话效果如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">messages = [</span><br><span class="line">    &#123;&quot;role&quot; : &quot;user&quot;, &quot;content&quot; : &quot;你好，好久不见！&quot;&#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">text = tokenizer.apply_chat_template(</span><br><span class="line">    messages,</span><br><span class="line">    tokenize = False,</span><br><span class="line">    add_generation_prompt = True, </span><br><span class="line">    enable_thinking = False, </span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">_ = model.generate(</span><br><span class="line">    **tokenizer(text, return_tensors = &quot;pt&quot;).to(&quot;cuda&quot;),</span><br><span class="line">    max_new_tokens = 256, # Increase for longer outputs!</span><br><span class="line">    temperature = 0.7, top_p = 0.8, top_k = 20, # For non thinking</span><br><span class="line">    streamer = TextStreamer(tokenizer, skip_prompt = True),#实时流式输出：每解码一个 token 就立刻打印到终端</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="准备数据集">准备数据集</h3>
<h4 id="下载数据集">下载数据集</h4>
<p>选取的两个数据集</p>
<ol type="1">
<li>我们使用 Open Math Reasoning 数据集，该数据集曾被用于赢得 AIMO（AI
数学奥林匹克 - 第二届进步奖）挑战！我们从中抽取了 10%
可验证的推理轨迹，这些轨迹是基于 DeepSeek R1 模型生成的，并且准确率超过
95%。数据集地址：https://huggingface.co/datasets/unsloth/OpenMathReasoning-mini</li>
<li>我们还利用了 Maxime Labonne 的 FineTome-100k
数据集，该数据集风格类似 ShareGPT。但我们需要将其转换为 HuggingFace
通用的多轮对话格式。数据集地址：https://huggingface.co/datasets/mlabonne/FineTome-100k</li>
</ol>
<p>在实际微调过程中，大多都会使用huggingface的datasets库进行数据集下载和管理，实际下载流程如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!pip install --upgrade datasets huggingface_hub</span><br></pre></td></tr></table></figure>
<p><code>datasets</code> 是 Hugging Face
提供的一个高效数据处理库，专为机器学习和大语言模型（LLM）训练而设计。它支持加载、处理、转换和保存各种格式的数据（如
JSON、CSV、Parquet 等），并能与 <code>transformers</code>
模型无缝集成。通过
<code>datasets</code>，开发者可以快速完成数据清洗、切分、tokenization
等常见任务，大大提升训练效率，特别适合用于指令微调、对话生成、Function
Calling 等任务的数据预处理。</p>
<p>然后分别下载并导入这两个库：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">reasoning_dataset = load_dataset(&quot;unsloth/OpenMathReasoning-mini&quot;, split = &quot;cot&quot;)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>cot全称为<strong>Chain-of-Thought，思维链</strong>，是「<strong>一步一步把思考过程写出来</strong>」的解题方式，而不是直接给出最终答案。</p>
<p><strong>只下 cot
是因为任务只需要“带推理过程”的那部分数据，其他子集对当前微调目标无用，避免冗余下载。</strong></p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">non_reasoning_dataset = load_dataset(&quot;mlabonne/FineTome-100k&quot;, split = &quot;train&quot;)</span><br></pre></td></tr></table></figure>
<h4 id="查看数据集">查看数据集</h4>
<p>然后输入数据集名称，即可查看数据集基本信息：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">reasoning_dataset</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Dataset(&#123;</span><br><span class="line">    features: [&#x27;expected_answer&#x27;, &#x27;problem_type&#x27;, &#x27;problem_source&#x27;, &#x27;generation_model&#x27;, &#x27;pass_rate_72b_tir&#x27;, &#x27;problem&#x27;, &#x27;generated_solution&#x27;, &#x27;inference_mode&#x27;],</span><br><span class="line">    num_rows: 19252</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<p><strong>一共 19 252 条</strong>
<strong>CoT（思维链）数学题</strong>，每条包含 8
个字段，可直接用来训练/评估模型的逐步推理能力。</p>
<p>generated_solution：模型自己写的 逐步推理 + 最终答案（就是你想要的
CoT）</p>
<p>expected_answer：标准答案（通常是一个简洁数字或表达式）</p>
<p>generation_model：生成这条 CoT 的“教师模型”名字，比如 qwen2-72b</p>
<p>加上索引则可以直接查看对应数据集信息：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">reasoning_dataset[0]</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;expected_answer&#x27;: &#x27;14&#x27;,</span><br><span class="line"> &#x27;problem_type&#x27;: &#x27;has_answer_extracted&#x27;,</span><br><span class="line"> &#x27;problem_source&#x27;: &#x27;aops_c4_high_school_math&#x27;,</span><br><span class="line"> &#x27;generation_model&#x27;: &#x27;DeepSeek-R1&#x27;,</span><br><span class="line"> &#x27;pass_rate_72b_tir&#x27;: &#x27;0.96875&#x27;,</span><br><span class="line"> &#x27;problem&#x27;: &#x27;Given $\\sqrt&#123;x^2+165&#125;-\\sqrt&#123;x^2-52&#125;=7$ and $x$ is positive, find all possible values of $x$.&#x27;,</span><br><span class="line"> &#x27;generated_solution&#x27;: &quot;&lt;think&gt;\nOkay, let&#x27;s see. I need to solve the equation √(x² + 165) - √(x² - 52) = 7, a截取部分&quot;,</span><br><span class="line"> &#x27;inference_mode&#x27;: &#x27;cot&#x27;&#125;</span><br></pre></td></tr></table></figure>
<p>能够看出这是一个基于DeepSeek
R1回答的数学数据集，其中<code>problem</code>是问题，<code>generated_solution</code>是数学推导过程（即思考过程），而<code>expected_answer</code>则是最终的答案。该数据集总共接近2万条数据</p>
<p>而对话数据集如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">non_reasoning_dataset</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Dataset(&#123;</span><br><span class="line">    features: [&#x27;conversations&#x27;, &#x27;source&#x27;, &#x27;score&#x27;],</span><br><span class="line">    num_rows: 100000</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">non_reasoning_dataset[0]</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;conversations&#x27;: [&#123;&#x27;from&#x27;: &#x27;human&#x27;,</span><br><span class="line">   &#x27;value&#x27;: &#x27;Explain what boolean operators are, what they do, and provide examples of how they can be used in programming. Additionally, describe the concept of operator precedence and prov截取&#x27;&#125;,</span><br><span class="line">  &#123;&#x27;from&#x27;: &#x27;gpt&#x27;,</span><br><span class="line">   &#x27;value&#x27;: &#x27;Boolean operators are logical operators used in programming to manipulate boolean values. The截取&#x27;&#125;],</span><br><span class="line"> &#x27;source&#x27;: &#x27;infini-instruct-top-500k&#x27;,</span><br><span class="line"> &#x27;score&#x27;: 5.212620735168457&#125;</span><br></pre></td></tr></table></figure>
<p>其中每一条数据都是一个对话，包含一组或者多组ChatGPT的聊天信息，其中<code>from</code>代表是用户消息还是大模型回复消息，而<code>value</code>则是对应的文本。该对话数据集总共包含10万条数据</p>
<p>能够看出dataset是一种类似json的数据格式，每条数据都以字段格式进行存储，在实际微调过程中，我们需要先将数据集的目标字段进行提取和拼接，然后加载到Qwen3模型的提示词模板中，并最终带入Unsloth进行微调。</p>
<h3 id="数据集清洗">数据集清洗</h3>
<h4 id="对话数据集的清洗">对话数据集的清洗</h4>
<p>接下来尝试对上述两个格式各异的数据集进行数据清洗，主要是围绕数据集进行<strong>数据格式</strong>的调整，便于后续<strong>带入Qwen3提示词模板</strong>。对于dataset格式的数据对象来说，可以先创建满足格式调整的函数，然后使用map方法对数据集格式进行调整。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def generate_conversation(examples):</span><br><span class="line">    problems  = examples[&quot;problem&quot;]</span><br><span class="line">    solutions = examples[&quot;generated_solution&quot;]</span><br><span class="line">    conversations = []</span><br><span class="line">    for problem, solution in zip(problems, solutions):</span><br><span class="line">        conversations.append([</span><br><span class="line">            &#123;&quot;role&quot; : &quot;user&quot;,      &quot;content&quot; : problem&#125;,</span><br><span class="line">            &#123;&quot;role&quot; : &quot;assistant&quot;, &quot;content&quot; : solution&#125;,</span><br><span class="line">        ])</span><br><span class="line">    return &#123; &quot;conversations&quot;: conversations, &#125;</span><br></pre></td></tr></table></figure>
<p>这里先创建generate_conversation函数，用于对reasoning_dataset中的每一条数据进行格式调整，即通过新创建一个新的特征conversations，来以对话形式保存历史问答数据：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">reasoning_data = reasoning_dataset.map(</span><br><span class="line">    generate_conversation,  # 处理函数</span><br><span class="line">    batched=True            # 批量处理，加快速度</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>map：对数据集中的每一批样本调用 generate_conversation</p>
<p>batched=True：一次传入一批（几百到几千条）样本，避免逐行慢速 Python
循环</p>
</blockquote>
<p>接下来将其带入Qwen3的提示词模板中进行转化：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">reasoning_conversations = tokenizer.apply_chat_template(</span><br><span class="line">    reasoning_data[&quot;conversations&quot;],</span><br><span class="line">    tokenize = False,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>之后即可带入这些数据进行微调。能看出每条数据的格式都和Unsloth底层对话API创建的数据格式类似，之后我们或许可以借助Unsloth底层对话API来创建微调数据集。</p>
<h4 id="推理数据集的推理">推理数据集的推理</h4>
<p>然后继续处理non_reasoning_conversations数据集，由于该数据集采用了sharegpt对话格式，因此可以直接借助Unsloth的standardize_sharegpt库进行数据集的格式转化，转化效果如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from unsloth.chat_templates import standardize_sharegpt</span><br></pre></td></tr></table></figure>
<blockquote>
<p>standardize_sharegpt的作用</p>
<p><strong>把“ShareGPT 格式”的对话数据一键转成 Unsloth / Hugging Face
通用的 <code>role/content</code> 列表，后续就能直接用
<code>apply_chat_template</code> 生成训练文本。</strong></p>
<p>1️⃣ ShareGPT 原始长什么样？</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;from&quot;</span><span class="punctuation">:</span> <span class="string">&quot;human&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;value&quot;</span><span class="punctuation">:</span> <span class="string">&quot;1+1=?&quot;</span><span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;from&quot;</span><span class="punctuation">:</span> <span class="string">&quot;gpt&quot;</span><span class="punctuation">,</span>  <span class="attr">&quot;value&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2&quot;</span><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>2️⃣ 转换后长什么样？</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;role&quot;</span><span class="punctuation">:</span> <span class="string">&quot;user&quot;</span><span class="punctuation">,</span>      <span class="attr">&quot;content&quot;</span><span class="punctuation">:</span> <span class="string">&quot;1+1=?&quot;</span><span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;role&quot;</span><span class="punctuation">:</span> <span class="string">&quot;assistant&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;content&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2&quot;</span><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataset = standardize_sharegpt(non_reasoning_dataset)</span><br></pre></td></tr></table></figure>
<p>接下来即可直接带入Qwen3对话模板中进行格式调整：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">non_reasoning_conversations = tokenizer.apply_chat_template(</span><br><span class="line">    dataset[&quot;conversations&quot;],</span><br><span class="line">    tokenize = False,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h4 id="数据集采样">数据集采样</h4>
<p>自此即完成了每个数据集的格式调整工作，不过这两个数据集并不均衡，能看得出非推理类数据集的长度更长。我们假设希望模型保留一定的推理能力，但又特别希望它作为一个聊天模型来使用。</p>
<p>因此，我们需要定义一个
<strong>仅聊天数据的比例</strong>。<strong>目标是从两个数据集中构建一个混合训练集</strong>。这里我们可以设定一个
25% 推理数据、75% 聊天数据的比例：也就是说，从推理数据集中抽取
25%（或者说，抽取占比为 100% - 聊天数据占比
的部分），最后将这两个数据集合并起来即可。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">chat_percentage = 0.75</span><br><span class="line"></span><br><span class="line">import pandas as pd</span><br><span class="line">#先把非推理对话列表转成 Pandas Series，方便后续抽样</span><br><span class="line">non_reasoning_subset = pd.Series(non_reasoning_conversations)</span><br><span class="line"></span><br><span class="line">non_reasoning_subset = non_reasoning_subset.sample(#sample(...)为无放回随机抽样</span><br><span class="line">    int(len(reasoning_conversations) * (1.0 - chat_percentage)),#计算 需要抽多少条非推理样本</span><br><span class="line">    random_state = 2407,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>这里我们需要先将上述list格式的数据转化为pd.Series数据，然后进行采样，并最终将其转化为dataset类型对象。（此外也可以先转化为dataset对象类型，然后再进行采样）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">data = pd.concat([</span><br><span class="line">    pd.Series(reasoning_conversations),</span><br><span class="line">    pd.Series(non_reasoning_subset)</span><br><span class="line">])</span><br><span class="line">data.name = &quot;text&quot;</span><br><span class="line"></span><br><span class="line">from datasets import Dataset</span><br><span class="line"></span><br><span class="line">combined_dataset = Dataset.from_pandas(pd.DataFrame(data))</span><br><span class="line">combined_dataset = combined_dataset.shuffle(seed = 3407)#用固定种子随机打乱顺序</span><br></pre></td></tr></table></figure>
<blockquote>
<p>pd.concat([…])：纵向拼接 → 一条长 Series，顺序：先推理，后非推理</p>
<p>Dataset.from_pandas(…)：把 Pandas Series 转成 Hugging Face
Dataset</p>
</blockquote>
<p><strong>把“推理对话”和“抽样后的非推理对话”合并成一个</strong>
<strong>随机打乱</strong> <strong>的 <code>Dataset</code>
对象，后面可直接拿去训练。</strong></p>
<h4 id="查看数据集-1">查看数据集</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">combined_dataset[0]</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;text&#x27;: &quot;&lt;|im_start|&gt;user\nCalculate the pH during a titration when 9.54 mL of a 0.15 M HCl solution has reacted with 22.88 mL of a 0.14 M NaOH solution?&lt;|im_end|&gt;\n&lt;|im_st截取&quot;,</span><br><span class="line"> &#x27;__index_level_0__&#x27;: 49038&#125;</span><br></pre></td></tr></table></figure>
<p>其中text字段就是后续带入微调的字段。</p>
<h4 id="数据集保存">数据集保存</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">combined_dataset.save_to_disk(&quot;/workspace/cleaned_qwen3_dataset&quot;)</span><br></pre></td></tr></table></figure>
<p>后续使用时即可使用如下代码进行读取：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from datasets import load_from_disk</span><br><span class="line">combined_dataset = load_from_disk(&quot;cleaned_qwen3_dataset&quot;)</span><br></pre></td></tr></table></figure>
<h3 id="qwen3推理能力高效微调流程">Qwen3推理能力高效微调流程</h3>
<p>准备完数据之后，即可开始进行微调。这里我们先进行少量数据微调测试，程序能够基本跑通后，我们再进行大规模数据集微调。</p>
<h4 id="进行lora参数注入">进行LoRA参数注入</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">model = FastLanguageModel.get_peft_model(</span><br><span class="line">    model,</span><br><span class="line">    r = 32,           # 秩（LoRA 低秩矩阵的列数）。越大可学习参数越多，显存也越高。常用 8/16/32/64/128</span><br><span class="line">    target_modules = [&quot;q_proj&quot;, &quot;k_proj&quot;, &quot;v_proj&quot;, &quot;o_proj&quot;,</span><br><span class="line">                      &quot;gate_proj&quot;, &quot;up_proj&quot;, &quot;down_proj&quot;],  # 在哪些线性层插入 LoRA 适配器（Attention + MLP）</span><br><span class="line">    lora_alpha = 32,  # 缩放因子。经验值 = rank 或 2×rank，控制更新强度</span><br><span class="line">    lora_dropout = 0, # LoRA 本身的 dropout 比例；0 省显存且速度最快</span><br><span class="line">    bias = &quot;none&quot;,    # 是否训练原 Linear 的偏置。设为 &quot;none&quot; 不训练，进一步节省显存</span><br><span class="line">    use_gradient_checkpointing = &quot;unsloth&quot;,  # 梯度检查点：True 省显存，&quot;unsloth&quot; 再省 30 %，超长上下文必开</span><br><span class="line">    random_state = 3407,  # 随机种子，保证 LoRA 初始化可复现</span><br><span class="line">    use_rslora = False,   # 默认 False，True 则启用 Rank-Stabilized LoRA（训练更稳，但显存稍高）</span><br><span class="line">    loftq_config = None,  # LoftQ 量化初始化，None 表示不用；若配置可进一步压缩初始权重</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>这一步<strong>“LoRA
参数注入”</strong>就是：<strong>在不改动原模型权重的前提下，给指定层插入少量</strong>
<strong>可训练低秩矩阵</strong> <strong>（LoRA 适配器），从而只更新 &lt;
1 % 的参数，完成高效微调。</strong></p>
<blockquote>
<p>不是“在原有层之外再增加一层”，而是<strong>把 LoRA
的“小矩阵”插到</strong> <strong>原有线性层内部</strong>：</p>
<ul>
<li>原层结构（冻结）： <code>x → Linear4bit(W) → y</code></li>
<li>注入后结构（冻结 + 可训练）：
<code>x → [Linear4bit(W)  +  LoRA(A·B)] → y</code></li>
</ul>
<p><code>A</code> 和 <code>B</code> 两个低秩矩阵被
<strong>注册为同一层的新参数</strong>，<strong>不新建网络层</strong>，参数在
<strong>前向时相加</strong>，<strong>反向只更新 A 和 B</strong>。</p>
</blockquote>
<h4 id="设置微调参数">设置微调参数</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">from trl import SFTTrainer, SFTConfig</span><br><span class="line"></span><br><span class="line">trainer = SFTTrainer(</span><br><span class="line">    model=model,                       # 已插入 LoRA 的 4-bit 模型</span><br><span class="line">    tokenizer=tokenizer,               # 对应 tokenizer（含 chat 模板）</span><br><span class="line">    train_dataset=combined_dataset,    # 训练集：聊天+推理对话</span><br><span class="line">    eval_dataset=None,                 # 如需验证，把验证集放进来即可</span><br><span class="line"></span><br><span class="line">    args=SFTConfig(</span><br><span class="line">        dataset_text_field=&quot;text&quot;,      # 训练集中每条样本的字段名（对话列表）</span><br><span class="line">        per_device_train_batch_size=2,  # 每张显卡上的 batch_size（显存决定）</span><br><span class="line">        gradient_accumulation_steps=4,  # 4 次累积 → 全局有效 batch = 2×4 = 8</span><br><span class="line">        warmup_steps=5,                # 前 5 步线性预热学习率</span><br><span class="line">        max_steps=30,                  # 训练 30 步（调试阶段）；正式可用 num_train_epochs</span><br><span class="line">        learning_rate=2e-4,            # LoRA 常用 2e-4；长训降到 2e-5</span><br><span class="line">        logging_steps=1,               # 每 1 步打印一次日志</span><br><span class="line">        optim=&quot;adamw_8bit&quot;,            # 8-bit AdamW，省显存</span><br><span class="line">        weight_decay=0.01,             # L2 正则</span><br><span class="line">        lr_scheduler_type=&quot;linear&quot;,    # 线性衰减到 0</span><br><span class="line">        seed=3407,                     # 固定随机种子</span><br><span class="line">        report_to=&quot;swanlab&quot;,             # 把指标推送到 swanlab</span><br><span class="line">    ),</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="https://github.com/huggingface/trl">TRL</a> (Transformers
Reinforcement Learning，用强化学习训练Transformers模型)
是一个领先的Python库，旨在通过监督微调（SFT）、近端策略优化（PPO）和直接偏好优化（DPO）等先进技术，对基础模型进行训练后优化。TRL
建立在 🤗 Transformers
生态系统之上，支持多种模型架构和模态，并且能够在各种硬件配置上进行扩展。</p>
<p>其中<code>SFTTrainer</code>：一个专门为指令微调设计的训练器，封装了
Hugging Face 的
<code>Trainer</code>，而<code>SFTConfig</code>：配置训练参数的专用类，功能类似
<code>TrainingArguments</code>。而SFTConfig核心参数解释如下：</p>
<table>
<colgroup>
<col style="width: 34%">
<col style="width: 65%">
</colgroup>
<thead>
<tr class="header">
<th>参数名</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>dataset_text_field="text"</code></td>
<td>数据集中用于训练的字段名称，如 <code>text</code> 或
<code>prompt</code></td>
</tr>
<tr class="even">
<td><code>per_device_train_batch_size=2</code></td>
<td>每张 GPU 上的 batch size 是 2</td>
</tr>
<tr class="odd">
<td><code>gradient_accumulation_steps=4</code></td>
<td>梯度累计 4 次后才进行一次反向传播（等效于总 batch size = 2 × 4 =
8）</td>
</tr>
<tr class="even">
<td><code>warmup_steps=5</code></td>
<td>前 5 步进行 warmup（缓慢提升学习率）</td>
</tr>
<tr class="odd">
<td><code>max_steps=30</code></td>
<td>最多训练 30 步（适合调试或快速实验）</td>
</tr>
<tr class="even">
<td><code>learning_rate=2e-4</code></td>
<td>初始学习率（短训练可用较高值）</td>
</tr>
<tr class="odd">
<td><code>logging_steps=1</code></td>
<td>每训练 1 步就打印一次日志</td>
</tr>
<tr class="even">
<td><code>optim="adamw_8bit"</code></td>
<td>使用 8-bit AdamW 优化器（节省内存，Unsloth 支持）</td>
</tr>
<tr class="odd">
<td><code>weight_decay=0.01</code></td>
<td>权重衰减，用于防止过拟合</td>
</tr>
<tr class="even">
<td><code>lr_scheduler_type="linear"</code></td>
<td>线性学习率调度器（从高到低线性下降）</td>
</tr>
<tr class="odd">
<td><code>seed=3407</code></td>
<td>固定随机种子，确保结果可复现</td>
</tr>
<tr class="even">
<td><code>report_to="none"</code></td>
<td>不使用 WandB 或 TensorBoard 等日志平台（可改为
<code>"wandb"</code>）</td>
</tr>
</tbody>
</table>
<blockquote>
<ol type="1">
<li><p><strong>per_device_train_batch_size=2</strong>
<strong>每次前向只用了 2 条样本</strong> → 显存占用小，单卡就能跑。</p>
<p><strong>batch_size
决定「每一步真正喂给模型的样本数量」，越大训练越稳，但对显存要求越高。</strong></p></li>
<li><p><strong>gradient_accumulation_steps=4</strong> <strong>把这 2
条样本算出的梯度先攒起来，攒够 4 次再一次性做反向传播</strong> →
等效于一次性看了 <strong>2 × 4 = 8 条样本</strong>，但显存仍按 2
条算。</p></li>
</ol>
</blockquote>
<p>此时基本训练过程为： 1. 从 <code>combined_dataset</code>
中取出一批样本（2 条） 2. 重复上面过程 4
次（<code>gradient_accumulation_steps=4</code>） 3.
将累计的梯度用于更新模型一次参数（等效于一次大 batch 更新） 4.
重复上述过程，直到 <code>max_steps=30</code> 停止</p>
<h4 id="设置训练可视化swanlab">设置训练可视化swanlab</h4>
<p><a target="_blank" rel="noopener" href="https://docs.swanlab.cn/guide_cloud/integration/integration-huggingface-trl.html">🤗HuggingFace
Trl | SwanLab官方文档</a></p>
<p>只需要在你的训练代码中，找到HF的<code>Config</code>部分（比如<code>SFTConfig</code>、<code>GRPOConfig</code>等），添加<code>report_to="swanlab"</code>参数，即可完成集成。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from trl import SFTConfig, SFTTrainer</span><br><span class="line"></span><br><span class="line">args = SFTConfig(</span><br><span class="line">    ...,</span><br><span class="line">    report_to=&quot;swanlab&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer = Trainer(..., args=args)</span><br></pre></td></tr></table></figure>
<p>默认下，项目名会使用你运行代码的<code>目录名</code>。</p>
<p>如果你想自定义项目名，可以设置<code>SWANLAB_PROJECT</code>环境变量：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">os.environ[&quot;SWANLAB_PROJECT&quot;]=&quot;qwen2-sft&quot;</span><br></pre></td></tr></table></figure>
<h4 id="微调执行流程">微调执行流程</h4>
<p>一切准备就绪后，接下来即可开始进行微调。由于本次微调总共只运行30个step，整个过程并不会很长，实际执行过程如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trainer_stats = trainer.train()</span><br></pre></td></tr></table></figure>
<h4 id="保存模型">保存模型</h4>
<p><strong>1. 保存 LoRA Adapter</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 保存 LoRA adapter（仅几十 MB）</span><br><span class="line">save_path = &quot;./lora-adapter&quot;</span><br><span class="line">model.save_pretrained(save_path)          # LoRA 权重</span><br><span class="line">tokenizer.save_pretrained(save_path)      # 词表</span><br></pre></td></tr></table></figure>
<p>以后加载：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> unsloth <span class="keyword">import</span> FastLanguageModel</span><br><span class="line">model, tokenizer = FastLanguageModel.from_pretrained(</span><br><span class="line">    model_name = <span class="string">&quot;base-model-name-or-path&quot;</span>,</span><br><span class="line">    max_seq_length = <span class="number">2048</span>,</span><br><span class="line">    load_in_4bit = <span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line">model = FastLanguageModel.get_peft_model(model, ...)  <span class="comment"># 同训练时参数</span></span><br><span class="line">model.load_adapter(save_path)   <span class="comment"># 把 LoRA 权重挂回去</span></span><br></pre></td></tr></table></figure>
<p><strong>2.合并 LoRA → 完整模型</strong></p>
<p>如果你想把 <strong>LoRA 权重合并到基座</strong>
得到一个独立的大模型（方便推理、上传 Hub）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 合并权重</span></span><br><span class="line">merged_model = model.merge_and_unload()   <span class="comment"># 返回普通 transformers 模型</span></span><br><span class="line">merged_model.save_pretrained(<span class="string">&quot;./merged-model&quot;</span>)</span><br><span class="line">tokenizer.save_pretrained(<span class="string">&quot;./merged-model&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>合并后就是完整的大模型（GB 级），可直接用
<code>AutoModelForCausalLM.from_pretrained("./merged-model")</code>
加载，不依赖 Unsloth。</p>
<h3 id="微调结果">微调结果</h3>
<h4 id="可视化结果">可视化结果</h4>
<figure>
<img src="/2025/08/12/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/qwen3-8b%E5%BE%AE%E8%B0%83%E5%AE%9E%E6%88%98/image-20250813111238359.png" alt="image-20250813111238359">
<figcaption aria-hidden="true">image-20250813111238359</figcaption>
</figure>
<p><a target="_blank" rel="noopener" href="https://swanlab.cn/@zxj123/Fine-tune-Qwen-8B/runs/e2l6g6s3v7dlb7hmfircv/chart">图表
｜ Fine-tune-Qwen-8B/rat-2</a></p>
<table>
<colgroup>
<col style="width: 18%">
<col style="width: 22%">
<col style="width: 28%">
<col style="width: 30%">
</colgroup>
<thead>
<tr class="header">
<th>指标名称</th>
<th>含义</th>
<th>单位/范围提示</th>
<th>常见关注点</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>train/loss</td>
<td>训练损失（Training Loss）</td>
<td>标量，越小越好</td>
<td>是否持续下降、是否震荡、是否过拟合</td>
</tr>
<tr class="even">
<td>train/grad_norm</td>
<td>梯度范数（Gradient Norm）</td>
<td>标量，通常 0.01–1.0 为合理区间</td>
<td>是否爆炸（&gt;10）或消失（&lt;1e-4）</td>
</tr>
<tr class="odd">
<td>train/learning_rate</td>
<td>学习率（Learning Rate）</td>
<td>标量，如 1e-4、5e-4 等</td>
<td>是否过大导致震荡、过小导致收敛慢</td>
</tr>
<tr class="even">
<td>train/epoch</td>
<td>已训练的轮次（Epoch）</td>
<td>标量，1.0 表示完整遍历一次训练集</td>
<td>当前已训练多少轮、是否还需继续训练</td>
</tr>
<tr class="odd">
<td>train/global_step</td>
<td>全局步数（Global Step）</td>
<td>整数，每个 batch +1</td>
<td>与 epoch 对应，计算已见样本量</td>
</tr>
</tbody>
</table>
<h4 id="对话测试">对话测试</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">messages = [</span><br><span class="line">    &#123;&quot;role&quot; : &quot;user&quot;, &quot;content&quot; : &quot;解决(x + 2)^2 = 0.&quot;&#125;</span><br><span class="line">]</span><br><span class="line">text = tokenizer.apply_chat_template(</span><br><span class="line">    messages,</span><br><span class="line">    tokenize = False,</span><br><span class="line">    add_generation_prompt = True, # Must add for generation</span><br><span class="line">    enable_thinking = True, # Disable thinking</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">from transformers import TextStreamer</span><br><span class="line">_ = model.generate(</span><br><span class="line">    **tokenizer(text, return_tensors = &quot;pt&quot;).to(&quot;cuda&quot;),</span><br><span class="line">    max_new_tokens = 20488, # Increase for longer outputs!</span><br><span class="line">    temperature = 0.6, top_p = 0.95, top_k = 20, # For thinking</span><br><span class="line">    streamer = TextStreamer(tokenizer, skip_prompt = True),</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/08/11/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/LoRA%E5%85%B6%E4%BB%96%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E6%96%B9%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/08/11/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/LoRA%E5%85%B6%E4%BB%96%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E6%96%B9%E6%B3%95/" class="post-title-link" itemprop="url">LoRA其他的模型微调方法</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-08-11 00:00:00" itemprop="dateCreated datePublished" datetime="2025-08-11T00:00:00+08:00">2025-08-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-08-14 10:58:52" itemprop="dateModified" datetime="2025-08-14T10:58:52+08:00">2025-08-14</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%A8%A1%E5%9E%8B/" itemprop="url" rel="index"><span itemprop="name">模型</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%A8%A1%E5%9E%8B/%E5%BE%AE%E8%B0%83/" itemprop="url" rel="index"><span itemprop="name">微调</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <figure>
<img src="/2025/08/11/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/LoRA%E5%85%B6%E4%BB%96%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E6%96%B9%E6%B3%95/image-20250814105851771.png" alt="image-20250814105851771">
<figcaption aria-hidden="true">image-20250814105851771</figcaption>
</figure>
<p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzkzODI1NzQyNA==&amp;mid=2247494667&amp;idx=1&amp;sn=c3af7d2472de61752ef8b8df28746f2e&amp;poc_token=HCyNmWijps0ViWD6wPgqFiDYUZVRSs7xUDRfowWE">大模型微调技巧：LoRA
与 QLoRA讲解</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/javatiange/article/details/149964743?fromshare=blogdetail&amp;sharetype=blogdetail&amp;sharerId=149964743&amp;sharerefer=PC&amp;sharesource=2501_91530961&amp;sharefrom=from_link">一文详解：8种常见的大模型微调方法，看这篇就够了！-CSDN博客</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/682082440">大模型微调技术 -
知乎</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/14/">14</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">张熙浚</span>
  </div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="本站访问数 fa fa-user 次"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="本站总访问量 fa fa-eye 次"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script size="400" alpha="0.6" zIndex="-1" src="https://cdnjs.cloudflare.com/ajax/libs/ribbon.js/1.0.2/ribbon.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.1.0/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script src="/js/pjax.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  <script src="/js/third-party/pace.js"></script>


  
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
