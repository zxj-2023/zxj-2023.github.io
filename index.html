<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha256-XOqroi11tY4EFQMR9ZYwZWKj5ZXiftSx36RRuC3anlA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-bounce.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.20.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="zxj Blogs">
<meta property="og:type" content="website">
<meta property="og:title" content="Zhang XiJun">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Zhang XiJun">
<meta property="og:description" content="zxj Blogs">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="张熙浚">
<meta property="article:tag" content="zxj">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Zhang XiJun</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Zhang XiJun</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">BLOGS</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="张熙浚"
      src="/images/zxjavatar.gif">
  <p class="site-author-name" itemprop="name">张熙浚</p>
  <div class="site-description" itemprop="description">zxj Blogs</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">128</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">49</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">56</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/zxj-2023" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zxj-2023" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="http://wpa.qq.com/msgrd?v=3&uin=2902065320&site=qq&menu=yes" title="QQ → http:&#x2F;&#x2F;wpa.qq.com&#x2F;msgrd?v&#x3D;3&amp;uin&#x3D;2902065320&amp;site&#x3D;qq&amp;menu&#x3D;yes" rel="noopener me" target="_blank"><i class="fab fa-qq fa-fw"></i>QQ</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
    <div class="sidebar-inner sidebar-blogroll">
      <div class="links-of-blogroll animated">
        <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
          链接
        </div>
        <ul class="links-of-blogroll-list">
            <li class="links-of-blogroll-item">
              <a href="https://zxj-2023.github.io/" title="https:&#x2F;&#x2F;zxj-2023.github.io" rel="noopener" target="_blank">Zhang XiJun</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://theme-next.js.org/" title="https:&#x2F;&#x2F;theme-next.js.org" rel="noopener" target="_blank">NexT</a>
            </li>
        </ul>
      </div>
    </div>
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/08/15/%E5%AD%A6%E4%B9%A0/python-web/react/react/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/08/15/%E5%AD%A6%E4%B9%A0/python-web/react/react/" class="post-title-link" itemprop="url">python web——react</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-08-15 00:00:00" itemprop="dateCreated datePublished" datetime="2025-08-15T00:00:00+08:00">2025-08-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-08-18 15:44:22" itemprop="dateModified" datetime="2025-08-18T15:44:22+08:00">2025-08-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/python-web/" itemprop="url" rel="index"><span itemprop="name">python-web</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/python-web/react/" itemprop="url" rel="index"><span itemprop="name">react</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="前言">前言</h3>
<p>为了后续自己搭建全栈项目做准备，对react做一定的了解</p>
<p>学习目标：大致看懂react的基本语法，可以在ai的协助下完成前端的搭建</p>
<h3 id="介绍">介绍</h3>
<p>React 是 Facebook（现 Meta）于 2013 年开源的一套用于构建用户界面的
JavaScript 库，现由 React 核心团队与社区共同维护。</p>
<h3 id="项目搭建">项目搭建</h3>
<p>项目创建</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npx create-react-app my-app</span><br></pre></td></tr></table></figure>
<blockquote>
<p>npx 是什么？</p>
<p>npm 5.2+ 自带的“包运行器”（Node Package eXecute）。类似uv</p>
<p>脚手架（Scaffold / Boilerplate）是什么？</p>
<ol type="1">
<li>定义：官方或社区提供的“项目模板生成器”，一条命令就能创建带目录结构、配置、脚本、依赖的完整项目骨架。</li>
<li>目的： • 省掉繁琐的初始化、Webpack/Rollup/Vite
配置、ESLint/TypeScript/测试等环境搭建。 •
统一团队规范，降低新人上手成本。</li>
</ol>
</blockquote>
<p>启动开发服务器</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> my-app</span><br><span class="line">npm start        <span class="comment"># 或 yarn start</span></span><br></pre></td></tr></table></figure>
<p>目录速览（核心）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">my-app</span><br><span class="line">├─ public/         # 静态资源，index.html 是页面模板</span><br><span class="line">├─ src/</span><br><span class="line">│  ├─ App.js       # 根组件</span><br><span class="line">│  ├─ index.js     # 应用入口（ReactDOM.createRoot）</span><br><span class="line">└─ package.json    # 依赖与脚本</span><br></pre></td></tr></table></figure>
<h3 id="jsx">JSX</h3>
<p>JSX（JavaScript XML 的缩写）是 React
引入的一种<strong>语法糖</strong>（syntactic sugar）。它让你在
JavaScript 文件里直接写<strong>类 HTML
标记</strong>，然后由构建工具（Babel、TypeScript、esbuild、swc）把它翻译成<strong>普通的
JavaScript 函数调用</strong>。</p>
<p>如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">// 1. 找到 public/index.html 中 id=&quot;root&quot; 的 DOM 节点，作为 React 应用的挂载点</span><br><span class="line">const root = ReactDOM.createRoot(document.getElementById(&#x27;root&#x27;));</span><br><span class="line"></span><br><span class="line">// 2. 将根组件 &lt;App /&gt; 渲染到该挂载点</span><br><span class="line">root.render(</span><br><span class="line">  // 3. &lt;React.StrictMode&gt; 是 React 提供的开发模式辅助工具</span><br><span class="line">  //    作用：在开发阶段自动检测潜在问题（如过时的 API、副作用重复执行等）</span><br><span class="line">  //    注意：它仅在开发环境生效，生产环境不会渲染任何额外 DOM</span><br><span class="line">  &lt;React.StrictMode&gt;</span><br><span class="line">    &#123;/* 4. 项目真正的根组件 App，所有业务逻辑都从这里开始 */&#125;</span><br><span class="line">    &lt;App /&gt;</span><br><span class="line">  &lt;/React.StrictMode&gt;</span><br><span class="line">);</span><br></pre></td></tr></table></figure>
<h3 id="箭头函数">箭头函数</h3>
<p>React（以及所有现代 JavaScript）里，“箭头”指的是
<strong>箭头函数（Arrow Function）</strong>，语法是：</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> 函数名 = <span class="function">(<span class="params">参数</span>) =&gt;</span> 返回值或语句块</span><br></pre></td></tr></table></figure>
<p>它的作用可以概括为 <strong>“更简洁的函数声明 + 词法作用域的
this”</strong>。</p>
<p>通俗理解：把小括号的内容变成箭头后的内容</p>
<h3 id="函数组件">函数组件</h3>
<p>函数组件 + JSX 的组合作用是： <strong>以函数的形式返回“虚拟 DOM
描述”，交由 React 渲染成真实 DOM</strong>，而不是直接返回 HTML
组件或字符串。</p>
<ol type="1">
<li>函数组件的“返回值”</li>
</ol>
<figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">function</span> <span class="title function_">Welcome</span>(<span class="params">props</span>) &#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="language-xml"><span class="tag">&lt;<span class="name">h1</span>&gt;</span>Hello &#123;props.name&#125;<span class="tag">&lt;/<span class="name">h1</span>&gt;</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>经过 Babel 编译后等价于：</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">function</span> <span class="title function_">Welcome</span>(<span class="params">props</span>) &#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="title class_">React</span>.<span class="title function_">createElement</span>(<span class="string">&#x27;h1&#x27;</span>, <span class="literal">null</span>, <span class="string">&#x27;Hello &#x27;</span>, props.<span class="property">name</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>React.createElement</code> 会生成一个<strong>纯 JS
对象</strong>（虚拟节点），而不是一段 HTML 字符串。</p>
<p><strong>使用示例</strong></p>
<figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1. 接收父组件传来的 props</span></span><br><span class="line"><span class="keyword">function</span> <span class="title function_">Card</span>(<span class="params">&#123; title, children &#125;</span>) &#123;</span><br><span class="line">  <span class="comment">// 2. 返回一段 JSX（最终会被编译成虚拟 DOM）</span></span><br><span class="line">  <span class="keyword">return</span> (</span><br><span class="line">    <span class="language-xml"><span class="tag">&lt;<span class="name">div</span> <span class="attr">className</span>=<span class="string">&quot;card&quot;</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">      <span class="tag">&lt;<span class="name">h2</span>&gt;</span>&#123;title&#125;<span class="tag">&lt;/<span class="name">h2</span>&gt;</span></span></span><br><span class="line"><span class="language-xml">      &#123;children&#125;</span></span><br><span class="line"><span class="language-xml">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span></span><br><span class="line">  );</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>使用：</p>
<figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;<span class="title class_">Card</span> title=<span class="string">&quot;函数组件&quot;</span>&gt;</span><br><span class="line">  <span class="language-xml"><span class="tag">&lt;<span class="name">p</span>&gt;</span>Hello, world!<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span></span><br><span class="line">&lt;/<span class="title class_">Card</span>&gt;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>DOM（Document Object Model，文档对象模型）是浏览器在内存里把一份
HTML/XML
文档表示成<strong>树形结构</strong>的<strong>编程接口</strong>（API）。</p>
<p>每个节点（元素、文本、注释…）都是一个对象，拥有属性与方法，例如：</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> title = <span class="variable language_">document</span>.<span class="title function_">getElementById</span>(<span class="string">&#x27;title&#x27;</span>);</span><br><span class="line">title.<span class="property">textContent</span> = <span class="string">&#x27;Hi React&#x27;</span>;   <span class="comment">// 改文本</span></span><br><span class="line">title.<span class="property">style</span>.<span class="property">color</span> = <span class="string">&#x27;red&#x27;</span>;        <span class="comment">// 改样式</span></span><br></pre></td></tr></table></figure>
</blockquote>
<h3 id="插值写法">插值写法</h3>
<p>在 React 中，“插值”专指<strong>把一段 JavaScript 表达式的实时结果塞进
JSX</strong> 的写法。 核心符号只有一对花括号
<code>&#123; &#125;</code>，记住口诀：<strong>“JSX 里凡是 {} 包起来的，就是
JavaScript 运行后的值。”</strong></p>
<p><strong>基本文本插值</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">const name = &#x27;React&#x27;;</span><br><span class="line">&lt;h1&gt;Hello, &#123;name&#125;!&lt;/h1&gt;          // → Hello, React!</span><br></pre></td></tr></table></figure>
<p><strong>属性插值</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">function App() &#123;</span><br><span class="line">  const mytitle=&quot;hello&quot;</span><br><span class="line">  return (</span><br><span class="line">    &lt;div title=&#123;mytitle&#125;&gt;&lt;/div&gt;</span><br><span class="line">  );</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="数据渲染">数据渲染</h3>
<p><strong>条件渲染</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">function App() &#123;</span><br><span class="line">  const mytitle=&quot;hello&quot;</span><br><span class="line"></span><br><span class="line">  let mycontent=null</span><br><span class="line">  const flag=true</span><br><span class="line">  if(flag)&#123;</span><br><span class="line">    mycontent=&lt;h2&gt;hello&lt;/h2&gt;</span><br><span class="line">  &#125;</span><br><span class="line">  else&#123;</span><br><span class="line">    mycontent=&lt;h2&gt;world&lt;/h2&gt;</span><br><span class="line">  &#125;</span><br><span class="line">  return (</span><br><span class="line">    &lt;div title=&#123;mytitle&#125;&gt;&#123;mycontent&#125;&lt;/div&gt;</span><br><span class="line">  );</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>列表渲染</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">function App() &#123;</span><br><span class="line">  const list=[&#x27;1&#x27;,&#x27;2&#x27;,&#x27;3&#x27;]</span><br><span class="line">  const mycontent=list.map((item)=&gt;&#123;</span><br><span class="line">    return &lt;li&gt;&#123;item&#125;&lt;/li&gt;</span><br><span class="line">  &#125;)</span><br><span class="line">  return (</span><br><span class="line">    &lt;div&gt;&#123;mycontent&#125;&lt;/div&gt;</span><br><span class="line">  );</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<ol type="1">
<li><code>.map((item) =&gt; &#123; ... &#125;)</code> ‑
<code>Array.prototype.map</code>：遍历数组，把每个元素依次交给回调函数处理，并<strong>返回一个新数组</strong>。
‑ <code>(item)</code> 是每次循环拿到的当前元素。</li>
<li><code>return &lt;li&gt;&#123;item&#125;&lt;/li&gt;</code> ‑
每一次循环里，把当前元素 <code>item</code> 用 JSX 插值语法
<code>&#123;item&#125;</code> 放进 <code>&lt;li&gt;</code> 标签里。</li>
</ol>
</blockquote>
<h3 id="状态处理">状态处理</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import &#123; useState &#125; from &#x27;react&#x27;;</span><br><span class="line">function App() &#123;</span><br><span class="line">  const [mycontent,setmycontent]=useState(&quot;hello world&quot;);</span><br><span class="line">  function changeContent()&#123;</span><br><span class="line">    setmycontent(&quot;hello world2&quot;);</span><br><span class="line">  &#125;</span><br><span class="line">  return (</span><br><span class="line">    &lt;&gt;</span><br><span class="line">      &lt;div&gt;&#123;mycontent&#125;&lt;/div&gt;</span><br><span class="line">      &lt;button onClick=&#123;changeContent&#125;&gt;change&lt;/button&gt;</span><br><span class="line">    &lt;/&gt;</span><br><span class="line">  );</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>useState 是 React 提供的
<strong>Hook</strong>，让<strong>函数组件也能拥有内部状态</strong>（state）。可以通过更新函数，调用后触发重新渲染。</p>
<p><strong>对象的状态更新</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">import &#123; useState &#125; from &#x27;react&#x27;;</span><br><span class="line">function App() &#123;</span><br><span class="line">  const [mycontent,setmycontent]=useState(&#123;</span><br><span class="line">    title:&#x27;hello world&#x27;,</span><br><span class="line">    content :&#x27;hello world content&#x27;</span><br><span class="line">&#125;);</span><br><span class="line">  function changeContent()&#123;</span><br><span class="line">    setmycontent(&#123;</span><br><span class="line">      ...mycontent,</span><br><span class="line">      content:&#x27;new content&#x27;</span><br><span class="line">    &#125;);</span><br><span class="line">  &#125;</span><br><span class="line">  return (</span><br><span class="line">    &lt;&gt;</span><br><span class="line">      &lt;div title=&#123;mycontent.title&#125;&gt;&#123;mycontent.content&#125;&lt;/div&gt;</span><br><span class="line">      &lt;button onClick=&#123;changeContent&#125;&gt;change&lt;/button&gt;</span><br><span class="line">    &lt;/&gt;</span><br><span class="line">  );</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>...mycontent</code> 是 ES6 的 <strong>对象展开运算符（object
spread）</strong>。 一句话：把 <code>mycontent</code>
里所有“旧属性”先抄出来，然后再覆盖/新增你后面写的属性。</p>
<h3 id="react组件的使用">react组件的使用</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import &#123; useState &#125; from &#x27;react&#x27;;</span><br><span class="line">function App() &#123;</span><br><span class="line">return (</span><br><span class="line">    &lt;&gt;</span><br><span class="line">      &lt;img src=&#123;logo&#125; className=&quot;App-logo&quot; alt=&quot;logo&quot; style=&#123;&#123; width: &#x27;100px&#x27;,backgroundColor: &#x27;grey&#x27;&#125;&#125;/&gt;</span><br><span class="line">    &lt;/&gt;</span><br><span class="line">  );</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ol type="1">
<li><p><code>className</code> 代替 <code>class</code> 传统 HTML 写
<code>&lt;img class="App-logo"&gt;</code>；React 组件里必须用
<code>className</code>，因为 JSX 最终会被编译成 JavaScript 对象，而
<code>class</code> 是 JS 的保留关键字。</p></li>
<li><p>样式写成<strong>对象</strong></p></li>
</ol>
<p>HTML
写行内样式：<code>style="width:100px;background-color:grey"</code> React
必须写成对象：</p>
<figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">style=&#123;&#123;</span><br><span class="line">  <span class="attr">width</span>: <span class="string">&#x27;100px&#x27;</span>,</span><br><span class="line">  <span class="attr">backgroundColor</span>: <span class="string">&#x27;grey&#x27;</span>   <span class="comment">// 驼峰命名</span></span><br><span class="line">&#125;&#125;</span><br></pre></td></tr></table></figure>
<p>因为 JSX 属性最终会变成 JS
对象的键值对，键名必须合法（驼峰），值可以是任何 JS
值（数字、变量、计算结果）。</p>
<ol start="3" type="1">
<li>最终产物是<strong>虚拟 DOM 节点</strong></li>
</ol>
<p><code>&lt;img src=&#123;logo&#125; ... /&gt;</code> 在浏览器里不会直接变成
<code>&lt;img&gt;</code> 标签，而是先被编译成：</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="title class_">React</span>.<span class="title function_">createElement</span>(<span class="string">&#x27;img&#x27;</span>, &#123;</span><br><span class="line">  <span class="attr">src</span>: logo,</span><br><span class="line">  <span class="attr">className</span>: <span class="string">&#x27;App-logo&#x27;</span>,</span><br><span class="line">  <span class="attr">alt</span>: <span class="string">&#x27;logo&#x27;</span>,</span><br><span class="line">  <span class="attr">style</span>: &#123; <span class="attr">width</span>: <span class="string">&#x27;100px&#x27;</span>, <span class="attr">backgroundColor</span>: <span class="string">&#x27;grey&#x27;</span> &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>
<p>React 再拿这个对象去做 diff、更新真实 DOM，而不是直接 innerHTML。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">function App() &#123;</span><br><span class="line"></span><br><span class="line">  const imgdata=&#123;</span><br><span class="line">    className:&quot;App-logo&quot;,</span><br><span class="line">    style:&#123;</span><br><span class="line">      width:&#x27;100px&#x27;,</span><br><span class="line">      backgroundColor:&#x27;grey&#x27;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  return (</span><br><span class="line">    &lt;&gt;</span><br><span class="line">      &lt;img src=&#123;logo&#125; alt=&quot;logo&quot; &#123;...imgdata&#125;/&gt;</span><br><span class="line">    &lt;/&gt;</span><br><span class="line">  );</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>利用 <strong>JSX 展开运算符（spread attributes）</strong> 把
<code>imgdata</code> 里的所有键值一次性“拍平”到 <code>&lt;img&gt;</code>
标签上</p>
<h3 id="组件复用">组件复用</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">function Article(props) &#123;</span><br><span class="line">  return (</span><br><span class="line">    &lt;div&gt;</span><br><span class="line">      &lt;h2&gt;&#123;props.title&#125;&lt;/h2&gt;</span><br><span class="line">      &lt;p&gt;&#123;props.content&#125;&lt;/p&gt;</span><br><span class="line">    &lt;/div&gt;</span><br><span class="line">  );</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">function App() &#123;</span><br><span class="line">  return (</span><br><span class="line">    &lt;&gt;</span><br><span class="line">      &lt;Article title=&quot;标签1&quot; content=&quot;内容1&quot; /&gt;</span><br><span class="line">      &lt;Article title=&quot;标签2&quot; content=&quot;内容2&quot; /&gt;</span><br><span class="line">    &lt;/&gt;</span><br><span class="line">  );</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="组件通信">组件通信</h3>
<p>组件通信的 4 条主线</p>
<p>1️⃣ 父 → 子：props 2️⃣ 子 → 父：回调函数 3️⃣ 隔代/任意：Context 4️⃣
全局/远端：状态管理库（Zustand、Redux、React Query）</p>
<h4 id="父-子">父 → 子</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">function Parent() &#123;</span><br><span class="line">  const title = &#x27;Hello React&#x27;;</span><br><span class="line">  return &lt;Child title=&#123;title&#125; /&gt;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">function Child(&#123; title &#125;) &#123;</span><br><span class="line">  return &lt;h1&gt;&#123;title&#125;&lt;/h1&gt;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="子-父">子 → 父</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">function Parent() &#123;</span><br><span class="line">  const [count, setCount] = useState(0);</span><br><span class="line">  return (</span><br><span class="line">    &lt;&gt;</span><br><span class="line">      &lt;p&gt;父：&#123;count&#125;&lt;/p&gt;</span><br><span class="line">      &lt;Child onInc=&#123;() =&gt; setCount(c =&gt; c + 1)&#125; /&gt;</span><br><span class="line">    &lt;/&gt;</span><br><span class="line">  );</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">function Child(&#123; onInc &#125;) &#123;</span><br><span class="line">  return &lt;button onClick=&#123;onInc&#125;&gt;子按钮 +1&lt;/button&gt;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>父组件把“修改函数”通过 props
传给子组件，子组件在合适的时机调用它，把数据作为参数传回去。</strong></p>
<h3 id="react-hooks">react hooks</h3>
<p><strong>Hook 是什么？</strong> Hook 是 React 16.8 引入的
<strong>函数级 API</strong>，让函数组件拥有</p>
<ul>
<li>状态（useState）</li>
<li>生命周期（useEffect）</li>
<li>上下文（useContext）</li>
<li>自定义逻辑（自定义 Hook） 而不必写 class。</li>
</ul>
<h3 id="参考资料">参考资料</h3>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1kc411D7F9?spm_id_from=333.788.videopod.sections&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">20分钟学会React
Hooks 前端开发必看 AI编程工具 CodeGeeX 体验_哔哩哔哩_bilibili</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/08/14/%E5%AD%A6%E4%B9%A0/ai%E7%9B%B8%E5%85%B3/A2A%E5%8D%8F%E8%AE%AE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/08/14/%E5%AD%A6%E4%B9%A0/ai%E7%9B%B8%E5%85%B3/A2A%E5%8D%8F%E8%AE%AE/" class="post-title-link" itemprop="url">A2A协议</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-08-14 00:00:00" itemprop="dateCreated datePublished" datetime="2025-08-14T00:00:00+08:00">2025-08-14</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-08-18 15:53:22" itemprop="dateModified" datetime="2025-08-18T15:53:22+08:00">2025-08-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ai%E7%9B%B8%E5%85%B3/" itemprop="url" rel="index"><span itemprop="name">ai相关</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ai%E7%9B%B8%E5%85%B3/A2A%E5%8D%8F%E8%AE%AE/" itemprop="url" rel="index"><span itemprop="name">A2A协议</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="前言">前言</h3>
<p>思考：现在利用a2a搭建多agent的现实例子多吗，从概念上，我认为a2a的思路是没问题的，但感觉下来，现在大多数的多agent的实现方式还是像langgraph中<strong>条件边</strong>来控制使用哪个agent，是不是因为a2a对于中小开发者搭建起来还是有些复杂，但我还是认为他这种于mcp类似，模块化，可以自定义的形式会是后续方向。就像现在的mcp
client，可以在市场上下载自己想要的mcp，利用a2a协议，用户可以在市场上下载想用的agent，搭建自己的多agent管家，现在市场上有类似的产品吗？</p>
<p>a2a协议其实与mcp类似，对象不同，一个是mcp client与mcp
server（tool），一个是agent client与agent
server。具体实现中，需要完成对agent
server的信息暴露与executor的编写，以便让client正确调用agent，调用前要启动服务。</p>
<p>一个agent
server所要包含的要素包括：1.AgentSkill，用于描述agent可以实现的能力</p>
<p>2.AgentCard，描述agent的信息，包括运行的url，输入和返回的数据类型，所包含的skills</p>
<p>3.AgentExecutor，定义了如何执行智能体，通过定义execute方法，以便正确调用agent
server</p>
<p>4.通过DefaultRequestHandler，封装调用agent的接口，不用再手写接口，只要提供一个
executor 和一个 store
即可，收到对话内容后，<code>DefaultRequestHandler</code>
会把对话打包成任务，交给 <code>HelloWorldAgentExecutor</code>
去执行。、</p>
<p>5.通过A2AStarletteApplication打包成应用（如fastapi），他的作用如下：1.把这个
handler 注册成真正的 HTTP 路由，于是外部就能通过 <code>POST /</code>
调用上述 JSON-RPC 方法。2.对外暴露名片</p>
<h3 id="什么是a2a协议">什么是A2A协议</h3>
<p>A2A 协议（Agent2Agent Protocol，智能体间通信协议）是 Google 在 2025
年 4 月发布并开源的首个 AI
智能体交互标准。它通过统一的通信规范，解决不同团队、不同框架、不同供应商开发的
AI 智能体如何“对话”和协同工作的问题。</p>
<blockquote>
<p>与mcp区分，<strong>MCP</strong> 解决
<strong>“单个智能体如何调用外部工具/数据”</strong>
的问题，而<strong>A2A</strong> 解决
<strong>“多个智能体如何协同完成任务”</strong> 的问题。</p>
</blockquote>
<figure>
<img src="/2025/08/14/%E5%AD%A6%E4%B9%A0/ai%E7%9B%B8%E5%85%B3/A2A%E5%8D%8F%E8%AE%AE/image-20250809222720192.png" alt="image-20250809222720192">
<figcaption aria-hidden="true">image-20250809222720192</figcaption>
</figure>
<h3 id="为什么要使用a2a协议">为什么要使用A2A协议</h3>
<p>随着 AI 应用深化，单一“万能”模型难以兼顾所有领域。A2A
鼓励构建“小而专”的智能体生态：</p>
<ul>
<li>每个智能体专注一个领域（如订票、报税、图像处理）。</li>
<li>通过 A2A
协议，它们像乐高积木一样自由组合，快速响应新的业务需求。</li>
</ul>
<p>比如你让一个agent使用多个工具，不仅会浪费tokens，也会降低其调用工具的准确性。所有，专业的领域使用专业的agent，而agent间的通信便要依靠A2A协议</p>
<h3 id="环境配置">环境配置</h3>
<p><strong>克隆仓库</strong></p>
<p>如果你还没有克隆，请克隆 A2A Samples 仓库：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/a2aproject/a2a-samples.git -b main --depth 1</span><br><span class="line">cd a2a-samples</span><br></pre></td></tr></table></figure>
<p><strong>Python 环境和 SDK 安装</strong></p>
<p>我们推荐为 Python 项目使用虚拟环境。A2A Python SDK 使用
<code>uv</code> 进行依赖管理，但你也可以使用 <code>pip</code> 与
<code>venv</code>。</p>
<ol type="1">
<li><p><strong>创建并激活虚拟环境：</strong></p>
<p>使用 <code>venv</code>（标准库）：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python -m venv .venv</span><br><span class="line">source .venv/bin/activate</span><br></pre></td></tr></table></figure></li>
<li><p><strong>安装所需的 Python 依赖项以及 A2A SDK
及其依赖项：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r samples/python/requirements.txt</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="agent-skills-agent-card">Agent Skills &amp; Agent Card</h3>
<h4 id="agent-skills">Agent Skills</h4>
<p>一个<strong>代理技能</strong>描述了代理可以执行的具体能力或功能。它是告诉客户端代理擅长哪些任务的构建模块。</p>
<p><code>AgentSkill</code> 的关键属性（定义在 <code>a2a.types</code>
中）：</p>
<ul>
<li><code>id</code>: 技能的唯一标识符。</li>
<li><code>name</code>: 人类可读的名称。</li>
<li><code>description</code>：对技能功能的更详细说明。</li>
<li><code>tags</code>：用于分类和发现的关键词。</li>
<li><code>examples</code>：示例提示或使用案例。</li>
<li><code>inputModes</code> / <code>outputModes</code>:
支持的输入和输出媒体类型（例如，“text/plain”，“application/json”）。</li>
</ul>
<p>在 <code>__main__.py</code> 中，你可以看到如何为 Helloworld
代理定义一个技能：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">skill = AgentSkill(</span><br><span class="line">    id=&#x27;hello_world&#x27;,</span><br><span class="line">    name=&#x27;Returns hello world&#x27;,</span><br><span class="line">    description=&#x27;just returns hello world&#x27;,</span><br><span class="line">    tags=[&#x27;hello world&#x27;],</span><br><span class="line">    examples=[&#x27;hi&#x27;, &#x27;hello world&#x27;],</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>这个技能非常简单：它的名称是 “Returns hello
world”，并且主要处理文本。</p>
<h4 id="agent-card">Agent Card</h4>
<p><strong>代理卡</strong>是一个 A2A 服务器提供的 JSON 文档，通常位于
<code>.well-known/agent-card.json</code>
端点。它就像代理的数字名片。</p>
<p><code>AgentCard</code> 的关键属性（定义在 <code>a2a.types</code>
中）：</p>
<ul>
<li><code>name</code>, <code>description</code>, <code>version</code>:
基本身份信息。</li>
<li><code>url</code>：A2A 服务可访问的端点。</li>
<li><code>capabilities</code>：指定支持的 A2A 功能，如
<code>streaming</code> 或 <code>pushNotifications</code>。</li>
<li><code>defaultInputModes</code> / <code>defaultOutputModes</code>:
代理的默认媒体类型。</li>
<li><code>skills</code>: 代理提供的 <code>AgentSkill</code>
对象列表。</li>
</ul>
<p><code>helloworld</code> 示例定义其 Agent Card 如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># This will be the public-facing agent card</span><br><span class="line">public_agent_card = AgentCard(</span><br><span class="line">    name=&#x27;Hello World Agent&#x27;,</span><br><span class="line">    description=&#x27;Just a hello world agent&#x27;,</span><br><span class="line">    url=&#x27;http://localhost:9999/&#x27;,</span><br><span class="line">    version=&#x27;1.0.0&#x27;,</span><br><span class="line"> 	# 默认输入模式：Agent 能够接收的输入类型列表，这里仅支持纯文本</span><br><span class="line">    default_input_modes=[&#x27;text&#x27;],</span><br><span class="line">    # 默认输出模式：Agent 能够产生的输出类型列表，这里仅返回纯文本</span><br><span class="line">    default_output_modes=[&#x27;text&#x27;],</span><br><span class="line">    # 能力声明：告知调用方 Agent 支持的能力，例如是否支持流式输出（streaming）</span><br><span class="line">    capabilities=AgentCapabilities(streaming=True),</span><br><span class="line">    skills=[skill],  # Only the basic skill for the public card</span><br><span class="line">    supports_authenticated_extended_card=True,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>这张卡片告诉我们代理名为 “Hello World Agent”，运行在
<code>http://localhost:9999/</code>，支持文本交互，并具有
<code>hello_world</code>
技能。它还表明支持公开认证，意味着无需特定凭证。</p>
<h3 id="agent-executor">Agent Executor</h3>
<p>A2A 代理处理请求和生成响应/事件的核心逻辑由一个 <strong>Agent
Executor</strong> 负责。A2A Python SDK 提供了一个抽象基类
<code>a2a.server.agent_execution.AgentExecutor</code> 供你实现。</p>
<p><strong><code>AgentExecutor</code> 接口</strong></p>
<p><code>AgentExecutor</code> 类定义了两个主要方法：</p>
<ul>
<li><code>async def execute(self, context: RequestContext, event_queue: EventQueue)</code>
: 处理期望响应或事件流的传入请求。它处理用户输入（可通过
<code>context</code> 获取）并使用 <code>event_queue</code> 发送
<code>Message</code>、<code>Task</code>、<code>TaskStatusUpdateEvent</code>
或 <code>TaskArtifactUpdateEvent</code> 对象。</li>
<li><code>async def cancel(self, context: RequestContext, event_queue: EventQueue)</code>
: 处理取消正在进行的任务的请求。</li>
</ul>
<p><code>RequestContext</code>
提供有关传入请求的信息，例如用户消息和任何现有的任务详情。<code>EventQueue</code>
由执行器使用，用于将事件发送回客户端。</p>
<p><strong>Helloworld AgentExecutor</strong></p>
<p>让我们看看 <code>agent_executor.py</code>。它定义了
<code>HelloWorldAgentExecutor</code>。</p>
<ol type="1">
<li><p><strong>代理（<code>HelloWorldAgent</code>）</strong>：这是一个简单的辅助类，封装了实际的“业务逻辑”。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">class HelloWorldAgent:</span><br><span class="line">    &quot;&quot;&quot;Hello World Agent.&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    async def invoke(self) -&gt; str:</span><br><span class="line">        return &#x27;Hello World&#x27;</span><br></pre></td></tr></table></figure>
<p>它有一个简单的 <code>invoke</code> 方法，返回字符串”Hello
World”。</p></li>
<li><p><strong>执行器（<code>HelloWorldAgentExecutor</code>）</strong>：这个类实现了
<code>AgentExecutor</code> 接口。</p>
<ul>
<li><p><strong><code>__init__</code></strong>:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">class HelloWorldAgentExecutor(AgentExecutor):</span><br><span class="line">    &quot;&quot;&quot;Test AgentProxy Implementation.&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.agent = HelloWorldAgent()</span><br></pre></td></tr></table></figure>
<p>它实例化了 <code>HelloWorldAgent</code>。</p></li>
<li><p><strong><code>execute</code></strong>:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">async def execute(</span><br><span class="line">    self,</span><br><span class="line">    context: RequestContext,</span><br><span class="line">    event_queue: EventQueue,</span><br><span class="line">) -&gt; None:</span><br><span class="line">    result = await self.agent.invoke()</span><br><span class="line">    await event_queue.enqueue_event(new_agent_text_message(result))</span><br></pre></td></tr></table></figure>
<p>当收到一个 <code>message/send</code> 或 <code>message/stream</code>
请求时（这两种请求在这个简化的执行器中均由 <code>execute</code>
处理）：</p>
<ol type="1">
<li>它调用 <code>self.agent.invoke()</code> 来获取 “Hello World”
字符串。</li>
<li>它使用 <code>new_agent_text_message</code> 工具函数创建一个 A2A
<code>Message</code> 对象。</li>
<li>它将此消息入队到 <code>event_queue</code>。底层的
<code>DefaultRequestHandler</code>
随后会处理这个队列以向客户端发送响应。对于像这样的一条消息，在流关闭之前，它将导致一个
<code>message/send</code> 的单一响应或一个 <code>message/stream</code>
的单一事件。</li>
</ol></li>
<li><p><strong><code>cancel</code></strong>: Helloworld 示例的
<code>cancel</code>
方法简单地抛出一个异常，表明这个基本代理不支持取消操作。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">async def cancel(</span><br><span class="line">    self, context: RequestContext, event_queue: EventQueue</span><br><span class="line">) -&gt; None:</span><br><span class="line">    raise Exception(&#x27;cancel not supported&#x27;)</span><br></pre></td></tr></table></figure></li>
</ul></li>
</ol>
<p><code>AgentExecutor</code> 充当 A2A
协议（由请求处理器和服务器应用程序管理）与您的代理特定逻辑之间的桥梁。它接收关于请求的上下文信息，并使用事件队列来通信结果或更新。</p>
<h3 id="启动server">启动server</h3>
<p>现在我们已经有了 Agent Card 和 Agent Executor，可以设置并启动 A2A
服务器。</p>
<p>A2A Python SDK 提供了一个 <code>A2AStarletteApplication</code>
类，简化了运行符合 A2A 标准的 HTTP 服务器。它使用 <a target="_blank" rel="noopener" href="https://www.starlette.io/">Starlette</a> 作为 Web 框架，通常与 <a target="_blank" rel="noopener" href="https://www.uvicorn.org/">Uvicorn</a> 等 ASGI 服务器一起运行。</p>
<p>让我们再次查看
<code>__main__.py</code>，看看服务器是如何初始化和启动的。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line">import uvicorn</span><br><span class="line"></span><br><span class="line">from a2a.server.apps import A2AStarletteApplication</span><br><span class="line">from a2a.server.request_handlers import DefaultRequestHandler</span><br><span class="line">from a2a.server.tasks import InMemoryTaskStore</span><br><span class="line">from a2a.types import (</span><br><span class="line">    AgentCapabilities,</span><br><span class="line">    AgentCard,</span><br><span class="line">    AgentSkill,</span><br><span class="line">)</span><br><span class="line">from agent_executor import (</span><br><span class="line">    HelloWorldAgentExecutor,  # type: ignore[import-untyped]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    skill = AgentSkill(</span><br><span class="line">        id=&#x27;hello_world&#x27;,</span><br><span class="line">        name=&#x27;返回 hello world&#x27;,</span><br><span class="line">        description=&#x27;简单地返回 hello world&#x27;,</span><br><span class="line">        tags=[&#x27;hello world&#x27;],</span><br><span class="line">        examples=[&#x27;hi&#x27;, &#x27;hello world&#x27;],</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    extended_skill = AgentSkill(</span><br><span class="line">        id=&#x27;super_hello_world&#x27;,</span><br><span class="line">        name=&#x27;返回 SUPER Hello World&#x27;,</span><br><span class="line">        description=&#x27;仅限已认证用户使用的更热情的问候。&#x27;,</span><br><span class="line">        tags=[&#x27;hello world&#x27;, &#x27;super&#x27;, &#x27;extended&#x27;],</span><br><span class="line">        examples=[&#x27;super hi&#x27;, &#x27;give me a super hello&#x27;],</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    # 这是面向公众的 Agent 卡片</span><br><span class="line">    public_agent_card = AgentCard(</span><br><span class="line">        name=&#x27;Hello World Agent&#x27;,</span><br><span class="line">        description=&#x27;只是一个 hello world 代理&#x27;,</span><br><span class="line">        url=&#x27;http://localhost:9999/&#x27;,</span><br><span class="line">        version=&#x27;1.0.0&#x27;,</span><br><span class="line">        default_input_modes=[&#x27;text&#x27;],</span><br><span class="line">        default_output_modes=[&#x27;text&#x27;],</span><br><span class="line">        capabilities=AgentCapabilities(streaming=True),</span><br><span class="line">        skills=[skill],  # 公开卡片仅包含基础技能</span><br><span class="line">        supports_authenticated_extended_card=True,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    # 这是已认证用户的扩展 Agent 卡片</span><br><span class="line">    # 额外包含 &#x27;extended_skill&#x27;</span><br><span class="line">    specific_extended_agent_card = public_agent_card.model_copy(</span><br><span class="line">        update=&#123;</span><br><span class="line">            &#x27;name&#x27;: &#x27;Hello World Agent - Extended Edition&#x27;,  # 使用不同名称以便区分</span><br><span class="line">            &#x27;description&#x27;: &#x27;面向已认证用户的完整功能 hello world 代理。&#x27;,</span><br><span class="line">            &#x27;version&#x27;: &#x27;1.0.1&#x27;,  # 甚至可以是不同的版本</span><br><span class="line">            # capabilities 及其他字段（如 url、default_input_modes、default_output_modes、</span><br><span class="line">            # supports_authenticated_extended_card）均从 public_agent_card 继承，</span><br><span class="line">            # 除非在此处另行指定。</span><br><span class="line">            &#x27;skills&#x27;: [</span><br><span class="line">                skill,</span><br><span class="line">                extended_skill,</span><br><span class="line">            ],  # 扩展卡片包含两个技能</span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    request_handler = DefaultRequestHandler(</span><br><span class="line">        agent_executor=HelloWorldAgentExecutor(),</span><br><span class="line">        task_store=InMemoryTaskStore(),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    server = A2AStarletteApplication(</span><br><span class="line">        agent_card=public_agent_card,</span><br><span class="line">        http_handler=request_handler,</span><br><span class="line">        extended_agent_card=specific_extended_agent_card,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    # 使用 uvicorn 启动服务，监听 0.0.0.0:9999</span><br><span class="line">    uvicorn.run(server.build(), host=&#x27;0.0.0.0&#x27;, port=9999)</span><br></pre></td></tr></table></figure>
<p>我们来分解一下：</p>
<ol type="1">
<li><strong><code>DefaultRequestHandler</code></strong>:
<ul>
<li>SDK 提供了 <code>DefaultRequestHandler</code>。这个处理器接收你的
<code>AgentExecutor</code>
实现（这里，<code>HelloWorldAgentExecutor</code>）和一个
<code>TaskStore</code>（这里，<code>InMemoryTaskStore</code>）。</li>
<li>它将传入的 A2A RPC 调用路由到你的执行器的适当方法上（比如
<code>execute</code> 或 <code>cancel</code>）。</li>
<li><code>TaskStore</code> 被 <code>DefaultRequestHandler</code>
用来管理任务的生命周期，特别是对于有状态交互、流式传输和重新订阅。即使你的代理执行器很简单，处理器也需要一个任务存储。</li>
</ul></li>
<li><strong><code>A2AStarletteApplication</code></strong>:
<ul>
<li><code>A2AStarletteApplication</code> 类使用 <code>agent_card</code>
和 <code>request_handler</code>（在其构造函数中称为
<code>http_handler</code>）进行实例化。</li>
<li><code>agent_card</code> 至关重要，因为服务器将在
<code>/.well-known/agent-card.json</code>
端点（默认情况下）上公开它。</li>
<li><code>request_handler</code> 负责通过与其 <code>AgentExecutor</code>
交互来处理所有传入的 A2A 方法调用。</li>
</ul></li>
<li><strong><code>uvicorn.run(server_app_builder.build(), ...)</code></strong>:
<ul>
<li><code>A2AStarletteApplication</code> 有一个 <code>build()</code>
方法，用于构建实际的 Starlette 应用程序。</li>
<li>然后使用 <code>uvicorn.run()</code> 运行该应用程序，使您的代理可通过
HTTP 访问。</li>
<li><code>host='0.0.0.0'</code>
使服务器可在您机器上的所有网络接口上访问。</li>
<li><code>port=9999</code> 指定监听的端口。这需要与
<code>AgentCard</code> 中的 <code>url</code> 匹配。</li>
</ul></li>
<li><code>specific_extended_agent_card</code>
<ul>
<li><strong>给同一个 Agent
准备“两张不同权限的名片”</strong>，分别用于“普通访客”和“已认证用户”。、</li>
</ul></li>
</ol>
<h3 id="与服务器交互">与服务器交互</h3>
<p>Helloworld A2A 服务器运行后，让我们向它发送一些请求。SDK
包含一个客户端（<code>A2AClient</code>），可以简化这些交互。</p>
<p>让我们看一下 <code>test_client.py</code> 的关键部分：</p>
<ol type="1">
<li><p><strong>获取代理卡 &amp; 初始化客户端</strong> ：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">base_url = &#x27;http://localhost:9999&#x27;</span><br><span class="line"></span><br><span class="line">async with httpx.AsyncClient() as httpx_client:</span><br><span class="line">    # 初始化 A2ACardResolver</span><br><span class="line">    resolver = A2ACardResolver(</span><br><span class="line">        httpx_client=httpx_client,</span><br><span class="line">        base_url=base_url,</span><br><span class="line">        # agent_card_path 使用默认值，extended_agent_card_path 也使用默认值</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p><code>A2ACardResolver</code> 类是一个便捷工具。它首先从服务器端的
<code>/.well-known/agent-card.json</code> 端点（基于提供的基 URL）获取
<code>AgentCard</code>，然后使用它初始化客户端。</p></li>
<li><p><strong>发送非流式消息 (<code>send_message</code>)</strong>:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">client = A2AClient(</span><br><span class="line">    httpx_client=httpx_client, </span><br><span class="line">    agent_card=final_agent_card_to_use#这个card为经过认证处理后暴露的card</span><br><span class="line">)</span><br><span class="line">logger.info(&#x27;A2AClient initialized.&#x27;)</span><br><span class="line"></span><br><span class="line">send_message_payload: dict[str, Any] = &#123;</span><br><span class="line">    &#x27;message&#x27;: &#123;</span><br><span class="line">        &#x27;role&#x27;: &#x27;user&#x27;,</span><br><span class="line">        &#x27;parts&#x27;: [</span><br><span class="line">            &#123;&#x27;kind&#x27;: &#x27;text&#x27;, &#x27;text&#x27;: &#x27;how much is 10 USD in INR?&#x27;&#125;</span><br><span class="line">        ],</span><br><span class="line">        &#x27;messageId&#x27;: uuid4().hex,</span><br><span class="line">    &#125;,</span><br><span class="line">&#125;</span><br><span class="line">request = SendMessageRequest(</span><br><span class="line">    id=str(uuid4()), params=MessageSendParams(**send_message_payload)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">response = await client.send_message(request)</span><br><span class="line">print(response.model_dump(mode=&#x27;json&#x27;, exclude_none=True))</span><br></pre></td></tr></table></figure>
<ul>
<li><code>send_message_payload</code> 构建了
<code>MessageSendParams</code> 的数据。</li>
<li>这些数据被封装在 <code>SendMessageRequest</code> 中。</li>
<li>它包含一个 <code>message</code> 对象，其中 <code>role</code>
设置为”用户”，内容在 <code>parts</code> 中。</li>
<li>Helloworld 代理的 <code>execute</code> 方法将入队一条”Hello
World”消息。<code>DefaultRequestHandler</code>
将获取这条消息并将其作为响应发送。</li>
<li><code>response</code> 将是一个 <code>SendMessageResponse</code>
对象，其中包含 <code>SendMessageSuccessResponse</code>（以代理的
<code>Message</code> 作为结果）或
<code>JSONRPCErrorResponse</code>。</li>
</ul></li>
<li><p><strong>处理任务 ID（Helloworld 的说明性注释）</strong>:</p>
<p>Helloworld 客户端（<code>test_client.py</code>）不会直接尝试
<code>get_task</code> 或 <code>cancel_task</code>，因为简单的 Helloworld
代理的 <code>execute</code> 方法，通过 <code>message/send</code>
调用时，会导致 <code>DefaultRequestHandler</code> 返回一个直接的
<code>Message</code> 响应，而不是 <code>Task</code>
对象。更复杂的、明确管理任务的代理（如 LangGraph 示例）会从
<code>message/send</code> 返回一个 <code>Task</code> 对象，然后其
<code>id</code> 可用于 <code>get_task</code> 或
<code>cancel_task</code>。</p></li>
<li><p><strong>发送流式消息（<code>send_message_streaming</code>）</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">streaming_request = SendStreamingMessageRequest(</span><br><span class="line">    id=str(uuid4()), params=MessageSendParams(**send_message_payload)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">stream_response = client.send_message_streaming(streaming_request)</span><br><span class="line"></span><br><span class="line">async for chunk in stream_response:</span><br><span class="line">    print(chunk.model_dump(mode=&#x27;json&#x27;, exclude_none=True))</span><br></pre></td></tr></table></figure>
<ul>
<li>此方法调用代理的 <code>message/stream</code>
端点。<code>DefaultRequestHandler</code> 将调用
<code>HelloWorldAgentExecutor.execute</code> 方法。</li>
<li><code>execute</code> 方法将一个”Hello
World”消息入队，然后关闭事件队列。</li>
<li>客户端将接收这条单条消息为一个
<code>SendStreamingMessageResponse</code> 事件，然后流将终止。</li>
<li><code>stream_response</code> 是一个
<code>AsyncGenerator</code>。</li>
</ul></li>
</ol>
<h3 id="参考资料">参考资料</h3>
<p><a target="_blank" rel="noopener" href="https://github.com/a2aproject/a2a-samples?tab=readme-ov-file">a2aproject/a2a-samples:
Samples using the Agent2Agent (A2A) Protocol</a></p>
<p><a target="_blank" rel="noopener" href="https://a2a-protocol.org/latest/">Agent2Agent (A2A)
Protocol</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/a2aproject/a2a-python">a2aproject/a2a-python:
Agent2Agent (A2A) 协议的官方 Python SDK — a2aproject/a2a-python:
Official Python SDK for the Agent2Agent (A2A) Protocol</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/08/14/%E5%AD%A6%E4%B9%A0/ai%E6%A1%86%E6%9E%B6/langmem/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/08/14/%E5%AD%A6%E4%B9%A0/ai%E6%A1%86%E6%9E%B6/langmem/" class="post-title-link" itemprop="url">Langmem快速入门</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2025-08-14 00:00:00 / 修改时间：17:42:03" itemprop="dateCreated datePublished" datetime="2025-08-14T00:00:00+08:00">2025-08-14</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ai%E6%A1%86%E6%9E%B6/" itemprop="url" rel="index"><span itemprop="name">ai框架</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ai%E6%A1%86%E6%9E%B6/langmem/" itemprop="url" rel="index"><span itemprop="name">langmem</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="前言">前言</h3>
<p>本文简单测试了一下langgraph官方提供的记忆管理工具，发现还是存在bug，我在a线程先让他记住我是张熙浚，然后又告诉他我不是张熙浚我是张俊细，在线程b询问他我是谁时，他还是认为我是张熙浚。记忆的管理部分确实是一个很大的问题，但中小开发者我认为还是直接使用人家造好的轮子方便些（我尝试去阅读了他的记忆管理工具的源码，以我目前的水平，想手搓花费的精力还是太多了）</p>
<p>我还有一个疑惑，我的理解是，当前记忆的存储基本上依赖于agent的决定，所以并不稳定，我也搞不清楚他什么时候会把哪些信息存入记忆，可以设置
schemas结构，控制存储的内容，但是长期记忆仅存储指定的这些信息，感觉还是有些鸡肋啊</p>
<p>代码见<a target="_blank" rel="noopener" href="https://github.com/zxj-2023/learn-rag-langchain/tree/main/langmem">learn-rag-langchain/langmem
at main · zxj-2023/learn-rag-langchain</a></p>
<h3 id="介绍">介绍</h3>
<p>LangMem 是 LangChain 推出的开源 SDK，通过一套存储-提取-优化机制，让
Agent
能够在多轮、多天甚至多用户之间持续学习、记住用户偏好并不断改进回答。</p>
<p>LangMem 的记忆工具按两个层次的集成模式组织：</p>
<ol type="1">
<li>核心 API</li>
</ol>
<p>LangMem
的核心是提供无副作用地转换记忆状态的函数。这些原语是记忆操作的构建块：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langmem/reference/memory/#langmem.create_memory_manager"><strong>记忆管理器</strong></a>：根据新的对话信息，提取新记忆、更新或删除过时记忆，并从现有记忆中进行整合和泛化。</li>
<li><a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langmem/reference/prompt_optimization/#langmem.create_prompt_optimizer"><strong>提示优化器</strong></a>：根据对话信息（可选反馈）更新提示规则和核心行为。</li>
</ul>
<p>这些核心函数不依赖于任何特定的数据库或存储系统。您可以在任何应用程序中使用它们。</p>
<ol start="2" type="1">
<li>有状态集成</li>
</ol>
<p>上一层依赖于 LangGraph 的长期记忆存储。这些组件使用上述核心 API
来转换存储中存在的记忆，并在新对话信息传入时根据需要进行更新/插入或删除：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langmem/reference/memory/#langmem.create_memory_store_manager"><strong>存储管理器</strong></a>：自动持久化提取的记忆。</li>
<li><a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langmem/reference/tools/#langmem.create_manage_memory_tool"><strong>记忆管理工具</strong></a>：让智能体直接访问记忆操作。</li>
</ul>
<figure>
<img src="/2025/08/14/%E5%AD%A6%E4%B9%A0/ai%E6%A1%86%E6%9E%B6/langmem/image-20250814152044798.png" alt="image-20250814152044798">
<figcaption aria-hidden="true">image-20250814152044798</figcaption>
</figure>
<p>langmem可以通过两种方式创建记忆</p>
<ol type="1">
<li><strong>在热路径中：</strong> Agent 使用工具主动保存笔记。</li>
<li><strong>在后台：</strong>记忆从对话中自动“潜意识地”提取。</li>
</ol>
<h3 id="热路径快速入门指南">热路径快速入门指南</h3>
<p>在本指南中，我们将创建一个 LangGraph Agent，它通过 LangMem 的
<code>manage_memory</code> 工具来主动管理自己的长期记忆。</p>
<h4 id="create_manage_memory_tool">create_manage_memory_tool</h4>
<p>create_manage_memory_tool通过创建一个工具（Tool），这个工具可以被
agent用来<strong>管理持久化记忆</strong>。这些记忆可以在不同的对话、会话甚至应用重启后依然存在。</p>
<ol type="1">
<li><p><strong>持久化存储 (Persistent Storage):</strong> 它利用了
LangGraph 提供的 <code>BaseStore</code>
接口。这使得数据可以存储在内存、数据库（如
Postgres）等地方，而不是仅仅存在于程序的运行时内存中。</p></li>
<li><p><strong>命名空间 (Namespace):</strong>
为了组织和隔离不同用户或不同类型的记忆，数据被存储在层级化的命名空间中。例如，<code>("memories", "user-123")</code>
可以确保用户 “user-123”
的记忆与其他用户或系统记忆分开。命名空间可以包含占位符（如
<code>&#123;langgraph_user_id&#125;</code>），在实际执行时会被具体的配置值替换。</p></li>
<li><p><strong>记忆 (Memory):</strong>
在这个上下文中，一个“记忆”就是存储在 <code>BaseStore</code>
中的一个数据项（<code>Item</code>）。它有一个唯一的
<code>key</code>（通常是 UUID），一个 <code>namespace</code>，一个
<code>value</code>（存储实际内容），以及创建和更新时间戳。</p></li>
<li><p><strong>工具 (Tool):</strong> 在 AI
应用中，工具是代理（Agent）可以调用的函数或能力。这个函数创建的工具就是一个封装好的、可以被
Agent 调用的函数，用于执行创建、更新、删除记忆的操作。</p></li>
</ol>
<h4 id="什么时候agent会调用记忆工具">什么时候agent会调用记忆工具</h4>
<figure>
<img src="/2025/08/14/%E5%AD%A6%E4%B9%A0/ai%E6%A1%86%E6%9E%B6/langmem/image-20250814163150589.png" alt="image-20250814163150589">
<figcaption aria-hidden="true">image-20250814163150589</figcaption>
</figure>
<p>ai是这样回答的，ReAct架构的agent是否调用工具由他自己决定</p>
<h4 id="实战">实战</h4>
<p><strong>导入库</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from langgraph.checkpoint.memory import MemorySaver</span><br><span class="line">from langgraph.prebuilt import create_react_agent</span><br><span class="line">from langgraph.store.memory import InMemoryStore</span><br><span class="line">from langgraph.utils.config import get_store </span><br><span class="line">from langmem import (</span><br><span class="line">    # 让智能体创建、更新和删除记忆 </span><br><span class="line">    create_manage_memory_tool,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><strong>返回记忆提示词</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def prompt(state):</span><br><span class="line">    &quot;&quot;&quot;为LLM准备消息。&quot;&quot;&quot;</span><br><span class="line">    # 从配置的上下文变量中获取存储; </span><br><span class="line">    store = get_store() # 与提供给 `create_react_agent` 的相同</span><br><span class="line">    memories = store.search(</span><br><span class="line">        # 在与我们为智能体配置的相同命名空间内搜索</span><br><span class="line">        (&quot;memories&quot;,),</span><br><span class="line">        query=state[&quot;messages&quot;][-1].content,</span><br><span class="line">    )</span><br><span class="line">    system_msg = f&quot;&quot;&quot;You are a helpful assistant.</span><br><span class="line"></span><br><span class="line">## Memories</span><br><span class="line">&lt;memories&gt;</span><br><span class="line">&#123;memories&#125;</span><br><span class="line">&lt;/memories&gt;</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">    return [&#123;&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: system_msg&#125;, *state[&quot;messages&quot;]]</span><br></pre></td></tr></table></figure>
<p><strong>定义store与checkpoint</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">from langchain import embeddings</span><br><span class="line">from langchain_openai import OpenAIEmbeddings</span><br><span class="line">embedding=OpenAIEmbeddings(</span><br><span class="line">    api_key=&quot;sk-&quot;, </span><br><span class="line">    base_url=&quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;,</span><br><span class="line">    model=&quot;text-embedding-v4&quot;,</span><br><span class="line">    check_embedding_ctx_length = False,</span><br><span class="line">    dimensions=1536 </span><br><span class="line">)</span><br><span class="line">store = InMemoryStore(</span><br><span class="line">    index=&#123; # 存储提取的记忆 </span><br><span class="line">        &quot;dims&quot;: 1536,</span><br><span class="line">        &quot;embed&quot;: embedding,</span><br><span class="line">    &#125;</span><br><span class="line">) </span><br><span class="line">checkpointer = MemorySaver() # 检查点图状态 </span><br></pre></td></tr></table></figure>
<p><strong>定义agent</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">from langchain_openai import ChatOpenAI</span><br><span class="line">model_qwen=ChatOpenAI(</span><br><span class="line">    api_key=&quot;sk-&quot;, </span><br><span class="line">    base_url=&quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;,</span><br><span class="line">    model=&quot;qwen3-30b-a3b-instruct-2507&quot;,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">agent = create_react_agent( </span><br><span class="line">    model=model_qwen,</span><br><span class="line">    prompt=prompt,</span><br><span class="line">    tools=[ # 添加记忆工具 </span><br><span class="line">        # 智能体可以调用 &quot;manage_memory&quot; 来</span><br><span class="line">        # 通过ID创建、更新和删除记忆</span><br><span class="line">        # 命名空间为记忆添加作用域。要</span><br><span class="line">        # 为每个用户限定记忆范围，使用 (&quot;memories&quot;, &quot;&#123;user_id&#125;&quot;): </span><br><span class="line">        create_manage_memory_tool(namespace=(&quot;memories&quot;,)),</span><br><span class="line">    ],</span><br><span class="line">    # 我们的记忆将存储在这个提供的BaseStore实例中</span><br><span class="line">    store=store,</span><br><span class="line">    # 图的&quot;状态&quot;将在每个节点完成执行后进行检查点</span><br><span class="line">    # 用于跟踪聊天历史和持久执行</span><br><span class="line">    checkpointer=checkpointer, </span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><strong>可视化图</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">agent.get_graph().draw_mermaid_png(output_file_path=&quot;agent.png&quot;)</span><br></pre></td></tr></table></figure>
<p><strong>在线程a让agent记住我们的偏好</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">config = &#123;&quot;configurable&quot;: &#123;&quot;thread_id&quot;: &quot;thread-a&quot;&#125;&#125; </span><br><span class="line">agent.invoke( </span><br><span class="line">    &#123; </span><br><span class="line">        &quot;messages&quot;: [ </span><br><span class="line">            &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;我喜欢黑色的显示模式&quot;&#125; </span><br><span class="line">        ] </span><br><span class="line">    &#125;, </span><br><span class="line">    # 我们将通过使用具有相同thread_id的config</span><br><span class="line">    # 来继续对话(thread-a)</span><br><span class="line">    config=config, </span><br><span class="line">) </span><br><span class="line">print(response[&quot;messages&quot;][-1].content) </span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">是的，我知道！你偏好黑色显示模式。我会在后续交互中保持这一设置。</span><br></pre></td></tr></table></figure>
<p><strong>在线程b查看是否记住</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 新线程 = 新对话！</span><br><span class="line">new_config = &#123;&quot;configurable&quot;: &#123;&quot;thread_id&quot;: &quot;thread-b&quot;&#125;&#125; </span><br><span class="line"># 智能体只能回忆起</span><br><span class="line"># 它使用manage_memories工具明确保存的内容</span><br><span class="line">response = agent.invoke( </span><br><span class="line">    &#123;&quot;messages&quot;: [&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;你好。你还记得我吗？你知道我有什么偏好吗？&quot;&#125;]&#125;,</span><br><span class="line">    config=new_config, </span><br><span class="line">) </span><br><span class="line">print(response[&quot;messages&quot;][-1].content) </span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">你好！虽然我无法记住你作为个体的详细信息，但我可以访问一些关于你的偏好信息。根据之前的记录，我知道你偏好使用黑色显示模式。如果你还有其他偏好或希望我记住什么，请告诉我，我会帮你记录下来。</span><br></pre></td></tr></table></figure>
<h3 id="后台快速入门指南">后台快速入门指南</h3>
<p>本指南将向您展示如何使用 <a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langmem/background_quickstart/"><code>create_memory_store_manager</code></a>
在后台提取和整合记忆。当记忆在后台处理时，智能体将正常继续运行。</p>
<ol type="1">
<li><p><strong>Runnable:</strong> LangChain/LangGraph
中的核心抽象，代表一个可以被调用（<code>invoke</code>/<code>ainvoke</code>）来处理输入并产生输出的单元。<code>MemoryStoreManager</code>
本身就是一个 Runnable。</p></li>
<li><p><strong>BaseStore:</strong> LangGraph
提供的持久化存储接口。Manager
会使用它来读取（搜索）和写入（创建、更新、删除）记忆。</p></li>
<li><p><strong>Memory (记忆):</strong> 在 Manager
的上下文中，记忆通常是指从对话中提取的、值得保存的片段信息（如用户偏好、事实等）。它们存储在
<code>BaseStore</code> 中，有自己的 <code>namespace</code> 和
<code>key</code>。</p></li>
<li><p><strong>Schema (模式):</strong> 一个 Pydantic
模型，用于定义记忆的结构。这允许你强制记忆遵循特定的格式（例如，包含
<code>category</code>, <code>preference</code>, <code>context</code>
字段）。如果未提供
<code>schemas</code>，则默认使用非结构化的字符串。</p></li>
<li><p><strong>Namespace (命名空间):</strong> 用于组织存储在
<code>BaseStore</code> 中的记忆。支持使用占位符（如
<code>&#123;langgraph_user_id&#125;</code>）进行动态配置。</p></li>
<li><p>自动化流程:</p>
<p>Manager 会自动执行以下步骤：</p>
<ul>
<li><strong>搜索 (Search):</strong> 根据新对话内容，在
<code>BaseStore</code> 中查找相关的现有记忆。</li>
<li><strong>分析/提取 (Analyze/Extract):</strong> 使用 LLM
分析新对话和检索到的记忆，决定是否需要创建新记忆、更新现有记忆或删除过时记忆。</li>
<li><strong>应用更改 (Apply Changes):</strong>
将分析结果（记忆的增删改）写回到 <code>BaseStore</code>。</li>
</ul></li>
</ol>
<h4 id="实战-1">实战</h4>
<p><strong>导入库</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from langchain.chat_models import init_chat_model </span><br><span class="line">from langgraph.func import entrypoint </span><br><span class="line">from langgraph.store.memory import InMemoryStore </span><br><span class="line"></span><br><span class="line">from langmem import ReflectionExecutor, create_memory_store_manager </span><br></pre></td></tr></table></figure>
<p><strong>定义store</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">from langchain_openai import OpenAIEmbeddings</span><br><span class="line">embedding=OpenAIEmbeddings(</span><br><span class="line">    api_key=&quot;sk-&quot;, </span><br><span class="line">    base_url=&quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;,</span><br><span class="line">    model=&quot;text-embedding-v4&quot;,</span><br><span class="line">    check_embedding_ctx_length = False,</span><br><span class="line">    dimensions=1536 </span><br><span class="line">)</span><br><span class="line">store = InMemoryStore(</span><br><span class="line">    index=&#123; # 存储提取的记忆 </span><br><span class="line">        &quot;dims&quot;: 1536,</span><br><span class="line">        &quot;embed&quot;: embedding,</span><br><span class="line">    &#125;</span><br><span class="line">) </span><br></pre></td></tr></table></figure>
<p><strong>创建记忆管理器</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 创建记忆管理器 Runnable 来从对话中提取记忆</span><br><span class="line">memory_manager = create_memory_store_manager( </span><br><span class="line">    model_qwen, </span><br><span class="line">    # 将记忆存储在 &quot;memories&quot; 命名空间（即目录）中</span><br><span class="line">    namespace=(&quot;memories&quot;,),  </span><br><span class="line">    instructions=&quot;用中文存储记忆。&quot;</span><br><span class="line">) </span><br><span class="line"></span><br><span class="line"># 包装 memory_manager 以处理延迟的后台处理</span><br><span class="line">executor = ReflectionExecutor(memory_manager) </span><br></pre></td></tr></table></figure>
<p>对每条消息都进行记忆处理存在以下缺点： -
当消息快速连续到达时，会产生冗余工作 -
在对话中途进行处理时，上下文不完整 - 不必要的 token 消耗</p>
<p><a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langmem/reference/utils/#langmem.ReflectionExecutor"><code>ReflectionExecutor</code></a>
可以延迟记忆处理并取消冗余工作。</p>
<p><strong>创建工作流</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">from langchain_openai import ChatOpenAI</span><br><span class="line">model_qwen=ChatOpenAI(</span><br><span class="line">    api_key=&quot;sk-&quot;, </span><br><span class="line">    base_url=&quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;,</span><br><span class="line">    model=&quot;qwen3-30b-a3b-instruct-2507&quot;,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">@entrypoint(store=store)  # 创建一个 LangGraph 工作流</span><br><span class="line">async def chat(message: str): </span><br><span class="line">    response = model_qwen.invoke(message) </span><br><span class="line"></span><br><span class="line">    # memory_manager 从对话历史中提取记忆</span><br><span class="line">    # 我们将以 OpenAI 的消息格式提供它</span><br><span class="line">    to_process = &#123;&quot;messages&quot;: [&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: message&#125;] + [response]&#125; </span><br><span class="line">    await memory_manager.ainvoke(to_process)  </span><br><span class="line">    return response.content </span><br><span class="line"></span><br><span class="line"># 正常运行对话</span><br><span class="line">response = await chat.ainvoke( </span><br><span class="line">    &quot;记住我是张熙浚&quot;, </span><br><span class="line">) </span><br><span class="line">print(response) </span><br></pre></td></tr></table></figure>
<p><strong>查看记忆</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(store.search((&quot;memories&quot;,)))</span><br></pre></td></tr></table></figure>
<h3 id="参考资料">参考资料</h3>
<p><a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langmem/">简介 - LangChain
框架</a></p>
<p><a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langmem/concepts/conceptual_guide/#semantic-memory-facts-and-knowledge">核心概念
- LangChain 框架</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/08/12/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/qwen3-8b%E5%BE%AE%E8%B0%83%E5%AE%9E%E6%88%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/08/12/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/qwen3-8b%E5%BE%AE%E8%B0%83%E5%AE%9E%E6%88%98/" class="post-title-link" itemprop="url">qwen3-8b微调实战</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-08-12 00:00:00" itemprop="dateCreated datePublished" datetime="2025-08-12T00:00:00+08:00">2025-08-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-08-13 15:10:05" itemprop="dateModified" datetime="2025-08-13T15:10:05+08:00">2025-08-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%A8%A1%E5%9E%8B/" itemprop="url" rel="index"><span itemprop="name">模型</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%A8%A1%E5%9E%8B/%E5%BE%AE%E8%B0%83/" itemprop="url" rel="index"><span itemprop="name">微调</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="前言">前言</h3>
<p>在完成微调前备知识的学习后，正式开始使用unsloth对Qwen3-8B-unsloth-bnb-4bit模型的lora微调实战</p>
<h3 id="模型加载">模型加载</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">from unsloth import FastLanguageModel</span><br><span class="line">import torch</span><br><span class="line"></span><br><span class="line">max_seq_length = 8192</span><br><span class="line">dtype = None</span><br><span class="line">load_in_4bit = True</span><br><span class="line"></span><br><span class="line">model, tokenizer = FastLanguageModel.from_pretrained(</span><br><span class="line">    model_name = &quot;/workspace/qwen3-8b&quot;,</span><br><span class="line">    max_seq_length = max_seq_length,</span><br><span class="line">    dtype = dtype,</span><br><span class="line">    load_in_4bit = load_in_4bit,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<blockquote>
<p><code>FastLanguageModel</code> 是 <strong>Unsloth
框架的核心入口类</strong>，即<strong>“把 Hugging Face 的 transformers
模型‘加速’成支持 QLoRA 微调、显存占用减半、速度提升 2-5
倍的封装器。”</strong></p>
<p><code>max_seq_length = 8192</code><strong>作用</strong>：告诉框架
<strong>“后续所有输入序列的最大长度”</strong>。<strong>内部一次性为位置编码、注意力掩码、KV-Cache
等开辟的张量尺寸</strong>，因此显存随它
<strong>平方级增长</strong>。</p>
<p><code>dtype = None</code><strong>作用</strong>：让 Unsloth
<strong>自动选择最合适的浮点精度</strong>。</p>
<p><code>load_in_4bit = True</code><strong>作用</strong>：把模型<strong>权重量化成
4-bit</strong>，显存降到 1/4，QLoRA 微调必备。</p>
</blockquote>
<h3 id="查看模型与分词器信息">查看模型与分词器信息</h3>
<h4 id="模型信息">模型信息</h4>
<p>运行</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model</span><br></pre></td></tr></table></figure>
<p>通过阅读模型信息我们可以了解到：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(embed_tokens): Embedding(151936, 4096, padding_idx=151654)</span><br></pre></td></tr></table></figure>
<p><strong>模型有 15 万个 token 的字典，每个字/词被翻译成 4096
维向量，第 151 654 号 token 被官方指定为填充符。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">(layers): ModuleList(</span><br><span class="line">      (0-2): 3 x Qwen3DecoderLayer(</span><br><span class="line">        (self_attn): Qwen3Attention(</span><br><span class="line">          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)</span><br><span class="line">          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)</span><br><span class="line">          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)</span><br><span class="line">          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)</span><br><span class="line">          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)</span><br><span class="line">          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)</span><br><span class="line">          (rotary_emb): LlamaRotaryEmbedding()</span><br><span class="line">        )</span><br><span class="line">        (mlp): Qwen3MLP(</span><br><span class="line">          (gate_proj): Linear(in_features=4096, out_features=12288, bias=False)</span><br><span class="line">          (up_proj): Linear(in_features=4096, out_features=12288, bias=False)</span><br><span class="line">          (down_proj): Linear(in_features=12288, out_features=4096, bias=False)</span><br><span class="line">          (act_fn): SiLU()</span><br><span class="line">        )</span><br><span class="line">        (input_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)</span><br><span class="line">        (post_attention_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)</span><br><span class="line">      )</span><br></pre></td></tr></table></figure>
<p>共有36层<strong>Qwen3DecoderLayer</strong>，每层包含<strong>Qwen3Attention</strong>，<strong>Qwen3MLP</strong>（<strong>一个
SwiGLU
前馈网络</strong>），<strong>Qwen3RMSNorm</strong>（两个<strong>归一化层</strong>，对
4096 维的隐藏向量做“均方根归一化”，防止梯度爆炸、稳定训练。）</p>
<figure>
<img src="/2025/08/12/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/qwen3-8b%E5%BE%AE%E8%B0%83%E5%AE%9E%E6%88%98/image-20250812153659843.png" alt="image-20250812153659843">
<figcaption aria-hidden="true">image-20250812153659843</figcaption>
</figure>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/cavalier-chen/p/18937098">大模型-qwen3
模型结构解读-66 - jack-chen666 - 博客园</a></p>
<blockquote>
<p><strong>LoRA可以插到哪里呢？</strong></p>
<p><strong>凡是打印里每层 Decoder 中出现的
<code>Linear4bit</code>（q/k/v/o + gate/up/down）就是 LoRA
可插、且默认会被插入的位置。</strong></p>
</blockquote>
<h4 id="分词器信息">分词器信息</h4>
<p>运行</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokenizer</span><br></pre></td></tr></table></figure>
<p>查看tokenizer信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Qwen2TokenizerFast(name_or_path=&#x27;/workspace/qwen3-8b&#x27;, vocab_size=151643, model_max_length=40960, is_fast=True, padding_side=&#x27;left&#x27;, truncation_side=&#x27;right&#x27;, special_tokens=&#123;&#x27;eos_token&#x27;: &#x27;&lt;|im_end|&gt;&#x27;, &#x27;pad_token&#x27;: &#x27;&lt;|vision_pad|&gt;&#x27;, &#x27;additional_special_tokens&#x27;: [&#x27;&lt;|im_start|&gt;&#x27;, &#x27;&lt;|im_end|&gt;&#x27;, &#x27;&lt;|object_ref_start|&gt;&#x27;, &#x27;&lt;|object_ref_end|&gt;&#x27;, &#x27;&lt;|box_start|&gt;&#x27;, &#x27;&lt;|box_end|&gt;&#x27;, &#x27;&lt;|quad_start|&gt;&#x27;, &#x27;&lt;|quad_end|&gt;&#x27;, &#x27;&lt;|vision_start|&gt;&#x27;, &#x27;&lt;|vision_end|&gt;&#x27;, &#x27;&lt;|vision_pad|&gt;&#x27;, &#x27;&lt;|image_pad|&gt;&#x27;, &#x27;&lt;|video_pad|&gt;&#x27;]&#125;, clean_up_tokenization_spaces=False, added_tokens_decoder=&#123;</span><br><span class="line">	151643: AddedToken(&quot;&lt;|endoftext|&gt;&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),</span><br><span class="line">	151644: AddedToken(&quot;&lt;|im_start|&gt;&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),</span><br><span class="line">	151645: AddedToken(&quot;&lt;|im_end|&gt;&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),</span><br><span class="line">	151646: AddedToken(&quot;&lt;|object_ref_start|&gt;&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),</span><br><span class="line">	截取部分</span><br><span class="line">&#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>vocab_size=151643：<strong>模型真正能理解和生成的子词/符号有这 151643
种，其余位置是预留空白。</strong></p>
<p>model_max_length=40960：<strong>理论最大输入长度 40k
token</strong>（实际受显存限制）</p>
<p>is_fast=True：表示 <strong>tokenizer 使用的是 Hugging Face 的「Rust
高速实现」</strong>（即 <em>tokenizers</em> 库）</p>
<p>special_tokens：打印的 <code>special_tokens</code> 字典 &amp;
<code>added_tokens_decoder</code> 已经把 <strong>151643-151668</strong>
全部列出，共 <strong>26 个</strong>。</p>
<h3 id="模拟一次模型处理流程">模拟一次模型处理流程</h3>
<p>将对话内容通过tokenizer进行处理</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">messages = [</span><br><span class="line">    &#123;&quot;role&quot; : &quot;user&quot;, &quot;content&quot; : &quot;你好，好久不见！&quot;&#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">text = tokenizer.apply_chat_template(</span><br><span class="line">    messages,</span><br><span class="line">    tokenize = False,</span><br><span class="line">    add_generation_prompt = True, </span><br><span class="line">    enable_thinking = False, # 设置不思考</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><code>apply_chat_template</code> 是把「人类对话格式的 Python
列表」一键翻译成 <strong>模型能直接理解的带特殊标记的文本字符串（或
token id 序列）</strong> 的“官方模板引擎”。</p>
<p>转化后的格式为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x27;&lt;|im_start|&gt;user\n你好，好久不见！&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n&lt;think&gt;\n\n&lt;/think&gt;\n\n&#x27;</span><br></pre></td></tr></table></figure>
<p>然后将转化后的字符串<strong>转成 GPU 上的 PyTorch token
张量，准备直接送进模型推理或训练。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">inputs = tokenizer(text, return_tensors=&quot;pt&quot;).to(&quot;cuda&quot;)</span><br></pre></td></tr></table></figure>
<p>以上代码共做了三步：</p>
<ol type="1">
<li><strong>tokenizer(text)</strong> 把前面
<code>apply_chat_template</code> 得到的字符串按词表切成 <strong>token id
列表</strong>。</li>
<li><strong>return_tensors=“pt”</strong> 把列表包成 <strong>PyTorch
张量</strong>（shape = [1, seq_len]）。</li>
<li><strong>.to(“cuda”)</strong> 把张量搬到 <strong>GPU
显存</strong>。</li>
</ol>
<p>输出如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;input_ids&#x27;: tensor([[151644,    872,    198, 108386,   3837, 111920, 101571,   6313, 151645,</span><br><span class="line">            198, 151644,  77091,    198, 151667,    271, 151668,    271]],</span><br><span class="line">       device=&#x27;cuda:0&#x27;), &#x27;attention_mask&#x27;: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device=&#x27;cuda:0&#x27;)&#125;</span><br></pre></td></tr></table></figure>
<table>
<colgroup>
<col style="width: 24%">
<col style="width: 11%">
<col style="width: 63%">
</colgroup>
<thead>
<tr class="header">
<th>键</th>
<th>形状</th>
<th>每个数字的含义</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>input_ids</strong></td>
<td><code>[1, 17]</code></td>
<td>17 个 token 的 ID 列表，已放到 GPU</td>
</tr>
<tr class="even">
<td><strong>attention_mask</strong></td>
<td><code>[1, 17]</code></td>
<td>17 个 <strong>1</strong>，表示“这些位置都是有效 token，无填充”</td>
</tr>
</tbody>
</table>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">outputs = model.generate(</span><br><span class="line">    input_ids=inputs.input_ids,</span><br><span class="line">    attention_mask=inputs.attention_mask,</span><br><span class="line">    max_new_tokens=max_seq_length,</span><br><span class="line">    use_cache=True,#启用 KV-Cache，避免重复计算，显存换时间</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>让模型在 GPU 上 <strong>根据已有 token
继续生成文本</strong>，直到达到 <code>max_new_tokens</code>
或遇到终止符。</p>
<p>outputs格式和inputs类似，使用nput_ids表示后续字符</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">response = tokenizer.batch_decode(outputs)</span><br></pre></td></tr></table></figure>
<p>把模型输出的 <strong>token id
序列</strong>（<code>outputs</code>）一次性还原成
<strong>人类可读的字符串</strong>。</p>
<p>输出如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x27;&lt;|im_start|&gt;user\n你好，好久不见！&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n&lt;think&gt;\n\n&lt;/think&gt;\n\n你好！好久不见！最近过得怎么样？有什么新鲜事想和我分享吗？😊&lt;|im_end|&gt;&#x27;</span><br></pre></td></tr></table></figure>
<p>这里展示的是没有思考过程的，最简单对话流程，若设置思考模式，完整代码如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">text = tokenizer.apply_chat_template(</span><br><span class="line">    messages,</span><br><span class="line">    tools = tools,#同样，可以设置function calling</span><br><span class="line">    tokenize = False,</span><br><span class="line">    add_generation_prompt = True, </span><br><span class="line">    enable_thinking = True, # 设置思考</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">inputs = tokenizer(text, return_tensors=&quot;pt&quot;).to(&quot;cuda&quot;)</span><br><span class="line"></span><br><span class="line">outputs = model.generate(</span><br><span class="line">    input_ids=inputs.input_ids,</span><br><span class="line">    attention_mask=inputs.attention_mask,</span><br><span class="line">    max_new_tokens=max_seq_length,</span><br><span class="line">    use_cache=True,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">response = tokenizer.batch_decode(outputs)</span><br></pre></td></tr></table></figure>
<p>当然，除了使用上述底层API进行对话外，Unsloth还提供了更加便捷的流式输出模型对话信息的函数，基本对话效果如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">messages = [</span><br><span class="line">    &#123;&quot;role&quot; : &quot;user&quot;, &quot;content&quot; : &quot;你好，好久不见！&quot;&#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">text = tokenizer.apply_chat_template(</span><br><span class="line">    messages,</span><br><span class="line">    tokenize = False,</span><br><span class="line">    add_generation_prompt = True, </span><br><span class="line">    enable_thinking = False, </span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">_ = model.generate(</span><br><span class="line">    **tokenizer(text, return_tensors = &quot;pt&quot;).to(&quot;cuda&quot;),</span><br><span class="line">    max_new_tokens = 256, # Increase for longer outputs!</span><br><span class="line">    temperature = 0.7, top_p = 0.8, top_k = 20, # For non thinking</span><br><span class="line">    streamer = TextStreamer(tokenizer, skip_prompt = True),#实时流式输出：每解码一个 token 就立刻打印到终端</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="准备数据集">准备数据集</h3>
<h4 id="下载数据集">下载数据集</h4>
<p>选取的两个数据集</p>
<ol type="1">
<li>我们使用 Open Math Reasoning 数据集，该数据集曾被用于赢得 AIMO（AI
数学奥林匹克 - 第二届进步奖）挑战！我们从中抽取了 10%
可验证的推理轨迹，这些轨迹是基于 DeepSeek R1 模型生成的，并且准确率超过
95%。数据集地址：https://huggingface.co/datasets/unsloth/OpenMathReasoning-mini</li>
<li>我们还利用了 Maxime Labonne 的 FineTome-100k
数据集，该数据集风格类似 ShareGPT。但我们需要将其转换为 HuggingFace
通用的多轮对话格式。数据集地址：https://huggingface.co/datasets/mlabonne/FineTome-100k</li>
</ol>
<p>在实际微调过程中，大多都会使用huggingface的datasets库进行数据集下载和管理，实际下载流程如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!pip install --upgrade datasets huggingface_hub</span><br></pre></td></tr></table></figure>
<p><code>datasets</code> 是 Hugging Face
提供的一个高效数据处理库，专为机器学习和大语言模型（LLM）训练而设计。它支持加载、处理、转换和保存各种格式的数据（如
JSON、CSV、Parquet 等），并能与 <code>transformers</code>
模型无缝集成。通过
<code>datasets</code>，开发者可以快速完成数据清洗、切分、tokenization
等常见任务，大大提升训练效率，特别适合用于指令微调、对话生成、Function
Calling 等任务的数据预处理。</p>
<p>然后分别下载并导入这两个库：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">reasoning_dataset = load_dataset(&quot;unsloth/OpenMathReasoning-mini&quot;, split = &quot;cot&quot;)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>cot全称为<strong>Chain-of-Thought，思维链</strong>，是「<strong>一步一步把思考过程写出来</strong>」的解题方式，而不是直接给出最终答案。</p>
<p><strong>只下 cot
是因为任务只需要“带推理过程”的那部分数据，其他子集对当前微调目标无用，避免冗余下载。</strong></p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">non_reasoning_dataset = load_dataset(&quot;mlabonne/FineTome-100k&quot;, split = &quot;train&quot;)</span><br></pre></td></tr></table></figure>
<h4 id="查看数据集">查看数据集</h4>
<p>然后输入数据集名称，即可查看数据集基本信息：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">reasoning_dataset</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Dataset(&#123;</span><br><span class="line">    features: [&#x27;expected_answer&#x27;, &#x27;problem_type&#x27;, &#x27;problem_source&#x27;, &#x27;generation_model&#x27;, &#x27;pass_rate_72b_tir&#x27;, &#x27;problem&#x27;, &#x27;generated_solution&#x27;, &#x27;inference_mode&#x27;],</span><br><span class="line">    num_rows: 19252</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<p><strong>一共 19 252 条</strong>
<strong>CoT（思维链）数学题</strong>，每条包含 8
个字段，可直接用来训练/评估模型的逐步推理能力。</p>
<p>generated_solution：模型自己写的 逐步推理 + 最终答案（就是你想要的
CoT）</p>
<p>expected_answer：标准答案（通常是一个简洁数字或表达式）</p>
<p>generation_model：生成这条 CoT 的“教师模型”名字，比如 qwen2-72b</p>
<p>加上索引则可以直接查看对应数据集信息：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">reasoning_dataset[0]</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;expected_answer&#x27;: &#x27;14&#x27;,</span><br><span class="line"> &#x27;problem_type&#x27;: &#x27;has_answer_extracted&#x27;,</span><br><span class="line"> &#x27;problem_source&#x27;: &#x27;aops_c4_high_school_math&#x27;,</span><br><span class="line"> &#x27;generation_model&#x27;: &#x27;DeepSeek-R1&#x27;,</span><br><span class="line"> &#x27;pass_rate_72b_tir&#x27;: &#x27;0.96875&#x27;,</span><br><span class="line"> &#x27;problem&#x27;: &#x27;Given $\\sqrt&#123;x^2+165&#125;-\\sqrt&#123;x^2-52&#125;=7$ and $x$ is positive, find all possible values of $x$.&#x27;,</span><br><span class="line"> &#x27;generated_solution&#x27;: &quot;&lt;think&gt;\nOkay, let&#x27;s see. I need to solve the equation √(x² + 165) - √(x² - 52) = 7, a截取部分&quot;,</span><br><span class="line"> &#x27;inference_mode&#x27;: &#x27;cot&#x27;&#125;</span><br></pre></td></tr></table></figure>
<p>能够看出这是一个基于DeepSeek
R1回答的数学数据集，其中<code>problem</code>是问题，<code>generated_solution</code>是数学推导过程（即思考过程），而<code>expected_answer</code>则是最终的答案。该数据集总共接近2万条数据</p>
<p>而对话数据集如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">non_reasoning_dataset</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Dataset(&#123;</span><br><span class="line">    features: [&#x27;conversations&#x27;, &#x27;source&#x27;, &#x27;score&#x27;],</span><br><span class="line">    num_rows: 100000</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">non_reasoning_dataset[0]</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;conversations&#x27;: [&#123;&#x27;from&#x27;: &#x27;human&#x27;,</span><br><span class="line">   &#x27;value&#x27;: &#x27;Explain what boolean operators are, what they do, and provide examples of how they can be used in programming. Additionally, describe the concept of operator precedence and prov截取&#x27;&#125;,</span><br><span class="line">  &#123;&#x27;from&#x27;: &#x27;gpt&#x27;,</span><br><span class="line">   &#x27;value&#x27;: &#x27;Boolean operators are logical operators used in programming to manipulate boolean values. The截取&#x27;&#125;],</span><br><span class="line"> &#x27;source&#x27;: &#x27;infini-instruct-top-500k&#x27;,</span><br><span class="line"> &#x27;score&#x27;: 5.212620735168457&#125;</span><br></pre></td></tr></table></figure>
<p>其中每一条数据都是一个对话，包含一组或者多组ChatGPT的聊天信息，其中<code>from</code>代表是用户消息还是大模型回复消息，而<code>value</code>则是对应的文本。该对话数据集总共包含10万条数据</p>
<p>能够看出dataset是一种类似json的数据格式，每条数据都以字段格式进行存储，在实际微调过程中，我们需要先将数据集的目标字段进行提取和拼接，然后加载到Qwen3模型的提示词模板中，并最终带入Unsloth进行微调。</p>
<h3 id="数据集清洗">数据集清洗</h3>
<h4 id="对话数据集的清洗">对话数据集的清洗</h4>
<p>接下来尝试对上述两个格式各异的数据集进行数据清洗，主要是围绕数据集进行<strong>数据格式</strong>的调整，便于后续<strong>带入Qwen3提示词模板</strong>。对于dataset格式的数据对象来说，可以先创建满足格式调整的函数，然后使用map方法对数据集格式进行调整。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def generate_conversation(examples):</span><br><span class="line">    problems  = examples[&quot;problem&quot;]</span><br><span class="line">    solutions = examples[&quot;generated_solution&quot;]</span><br><span class="line">    conversations = []</span><br><span class="line">    for problem, solution in zip(problems, solutions):</span><br><span class="line">        conversations.append([</span><br><span class="line">            &#123;&quot;role&quot; : &quot;user&quot;,      &quot;content&quot; : problem&#125;,</span><br><span class="line">            &#123;&quot;role&quot; : &quot;assistant&quot;, &quot;content&quot; : solution&#125;,</span><br><span class="line">        ])</span><br><span class="line">    return &#123; &quot;conversations&quot;: conversations, &#125;</span><br></pre></td></tr></table></figure>
<p>这里先创建generate_conversation函数，用于对reasoning_dataset中的每一条数据进行格式调整，即通过新创建一个新的特征conversations，来以对话形式保存历史问答数据：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">reasoning_data = reasoning_dataset.map(</span><br><span class="line">    generate_conversation,  # 处理函数</span><br><span class="line">    batched=True            # 批量处理，加快速度</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>map：对数据集中的每一批样本调用 generate_conversation</p>
<p>batched=True：一次传入一批（几百到几千条）样本，避免逐行慢速 Python
循环</p>
</blockquote>
<p>接下来将其带入Qwen3的提示词模板中进行转化：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">reasoning_conversations = tokenizer.apply_chat_template(</span><br><span class="line">    reasoning_data[&quot;conversations&quot;],</span><br><span class="line">    tokenize = False,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>之后即可带入这些数据进行微调。能看出每条数据的格式都和Unsloth底层对话API创建的数据格式类似，之后我们或许可以借助Unsloth底层对话API来创建微调数据集。</p>
<h4 id="推理数据集的推理">推理数据集的推理</h4>
<p>然后继续处理non_reasoning_conversations数据集，由于该数据集采用了sharegpt对话格式，因此可以直接借助Unsloth的standardize_sharegpt库进行数据集的格式转化，转化效果如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from unsloth.chat_templates import standardize_sharegpt</span><br></pre></td></tr></table></figure>
<blockquote>
<p>standardize_sharegpt的作用</p>
<p><strong>把“ShareGPT 格式”的对话数据一键转成 Unsloth / Hugging Face
通用的 <code>role/content</code> 列表，后续就能直接用
<code>apply_chat_template</code> 生成训练文本。</strong></p>
<p>1️⃣ ShareGPT 原始长什么样？</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;from&quot;</span><span class="punctuation">:</span> <span class="string">&quot;human&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;value&quot;</span><span class="punctuation">:</span> <span class="string">&quot;1+1=?&quot;</span><span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;from&quot;</span><span class="punctuation">:</span> <span class="string">&quot;gpt&quot;</span><span class="punctuation">,</span>  <span class="attr">&quot;value&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2&quot;</span><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>2️⃣ 转换后长什么样？</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;role&quot;</span><span class="punctuation">:</span> <span class="string">&quot;user&quot;</span><span class="punctuation">,</span>      <span class="attr">&quot;content&quot;</span><span class="punctuation">:</span> <span class="string">&quot;1+1=?&quot;</span><span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;role&quot;</span><span class="punctuation">:</span> <span class="string">&quot;assistant&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;content&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2&quot;</span><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataset = standardize_sharegpt(non_reasoning_dataset)</span><br></pre></td></tr></table></figure>
<p>接下来即可直接带入Qwen3对话模板中进行格式调整：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">non_reasoning_conversations = tokenizer.apply_chat_template(</span><br><span class="line">    dataset[&quot;conversations&quot;],</span><br><span class="line">    tokenize = False,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h4 id="数据集采样">数据集采样</h4>
<p>自此即完成了每个数据集的格式调整工作，不过这两个数据集并不均衡，能看得出非推理类数据集的长度更长。我们假设希望模型保留一定的推理能力，但又特别希望它作为一个聊天模型来使用。</p>
<p>因此，我们需要定义一个
<strong>仅聊天数据的比例</strong>。<strong>目标是从两个数据集中构建一个混合训练集</strong>。这里我们可以设定一个
25% 推理数据、75% 聊天数据的比例：也就是说，从推理数据集中抽取
25%（或者说，抽取占比为 100% - 聊天数据占比
的部分），最后将这两个数据集合并起来即可。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">chat_percentage = 0.75</span><br><span class="line"></span><br><span class="line">import pandas as pd</span><br><span class="line">#先把非推理对话列表转成 Pandas Series，方便后续抽样</span><br><span class="line">non_reasoning_subset = pd.Series(non_reasoning_conversations)</span><br><span class="line"></span><br><span class="line">non_reasoning_subset = non_reasoning_subset.sample(#sample(...)为无放回随机抽样</span><br><span class="line">    int(len(reasoning_conversations) * (1.0 - chat_percentage)),#计算 需要抽多少条非推理样本</span><br><span class="line">    random_state = 2407,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>这里我们需要先将上述list格式的数据转化为pd.Series数据，然后进行采样，并最终将其转化为dataset类型对象。（此外也可以先转化为dataset对象类型，然后再进行采样）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">data = pd.concat([</span><br><span class="line">    pd.Series(reasoning_conversations),</span><br><span class="line">    pd.Series(non_reasoning_subset)</span><br><span class="line">])</span><br><span class="line">data.name = &quot;text&quot;</span><br><span class="line"></span><br><span class="line">from datasets import Dataset</span><br><span class="line"></span><br><span class="line">combined_dataset = Dataset.from_pandas(pd.DataFrame(data))</span><br><span class="line">combined_dataset = combined_dataset.shuffle(seed = 3407)#用固定种子随机打乱顺序</span><br></pre></td></tr></table></figure>
<blockquote>
<p>pd.concat([…])：纵向拼接 → 一条长 Series，顺序：先推理，后非推理</p>
<p>Dataset.from_pandas(…)：把 Pandas Series 转成 Hugging Face
Dataset</p>
</blockquote>
<p><strong>把“推理对话”和“抽样后的非推理对话”合并成一个</strong>
<strong>随机打乱</strong> <strong>的 <code>Dataset</code>
对象，后面可直接拿去训练。</strong></p>
<h4 id="查看数据集-1">查看数据集</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">combined_dataset[0]</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;text&#x27;: &quot;&lt;|im_start|&gt;user\nCalculate the pH during a titration when 9.54 mL of a 0.15 M HCl solution has reacted with 22.88 mL of a 0.14 M NaOH solution?&lt;|im_end|&gt;\n&lt;|im_st截取&quot;,</span><br><span class="line"> &#x27;__index_level_0__&#x27;: 49038&#125;</span><br></pre></td></tr></table></figure>
<p>其中text字段就是后续带入微调的字段。</p>
<h4 id="数据集保存">数据集保存</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">combined_dataset.save_to_disk(&quot;/workspace/cleaned_qwen3_dataset&quot;)</span><br></pre></td></tr></table></figure>
<p>后续使用时即可使用如下代码进行读取：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from datasets import load_from_disk</span><br><span class="line">combined_dataset = load_from_disk(&quot;cleaned_qwen3_dataset&quot;)</span><br></pre></td></tr></table></figure>
<h3 id="qwen3推理能力高效微调流程">Qwen3推理能力高效微调流程</h3>
<p>准备完数据之后，即可开始进行微调。这里我们先进行少量数据微调测试，程序能够基本跑通后，我们再进行大规模数据集微调。</p>
<h4 id="进行lora参数注入">进行LoRA参数注入</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">model = FastLanguageModel.get_peft_model(</span><br><span class="line">    model,</span><br><span class="line">    r = 32,           # 秩（LoRA 低秩矩阵的列数）。越大可学习参数越多，显存也越高。常用 8/16/32/64/128</span><br><span class="line">    target_modules = [&quot;q_proj&quot;, &quot;k_proj&quot;, &quot;v_proj&quot;, &quot;o_proj&quot;,</span><br><span class="line">                      &quot;gate_proj&quot;, &quot;up_proj&quot;, &quot;down_proj&quot;],  # 在哪些线性层插入 LoRA 适配器（Attention + MLP）</span><br><span class="line">    lora_alpha = 32,  # 缩放因子。经验值 = rank 或 2×rank，控制更新强度</span><br><span class="line">    lora_dropout = 0, # LoRA 本身的 dropout 比例；0 省显存且速度最快</span><br><span class="line">    bias = &quot;none&quot;,    # 是否训练原 Linear 的偏置。设为 &quot;none&quot; 不训练，进一步节省显存</span><br><span class="line">    use_gradient_checkpointing = &quot;unsloth&quot;,  # 梯度检查点：True 省显存，&quot;unsloth&quot; 再省 30 %，超长上下文必开</span><br><span class="line">    random_state = 3407,  # 随机种子，保证 LoRA 初始化可复现</span><br><span class="line">    use_rslora = False,   # 默认 False，True 则启用 Rank-Stabilized LoRA（训练更稳，但显存稍高）</span><br><span class="line">    loftq_config = None,  # LoftQ 量化初始化，None 表示不用；若配置可进一步压缩初始权重</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>这一步<strong>“LoRA
参数注入”</strong>就是：<strong>在不改动原模型权重的前提下，给指定层插入少量</strong>
<strong>可训练低秩矩阵</strong> <strong>（LoRA 适配器），从而只更新 &lt;
1 % 的参数，完成高效微调。</strong></p>
<blockquote>
<p>不是“在原有层之外再增加一层”，而是<strong>把 LoRA
的“小矩阵”插到</strong> <strong>原有线性层内部</strong>：</p>
<ul>
<li>原层结构（冻结）： <code>x → Linear4bit(W) → y</code></li>
<li>注入后结构（冻结 + 可训练）：
<code>x → [Linear4bit(W)  +  LoRA(A·B)] → y</code></li>
</ul>
<p><code>A</code> 和 <code>B</code> 两个低秩矩阵被
<strong>注册为同一层的新参数</strong>，<strong>不新建网络层</strong>，参数在
<strong>前向时相加</strong>，<strong>反向只更新 A 和 B</strong>。</p>
</blockquote>
<h4 id="设置微调参数">设置微调参数</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">from trl import SFTTrainer, SFTConfig</span><br><span class="line"></span><br><span class="line">trainer = SFTTrainer(</span><br><span class="line">    model=model,                       # 已插入 LoRA 的 4-bit 模型</span><br><span class="line">    tokenizer=tokenizer,               # 对应 tokenizer（含 chat 模板）</span><br><span class="line">    train_dataset=combined_dataset,    # 训练集：聊天+推理对话</span><br><span class="line">    eval_dataset=None,                 # 如需验证，把验证集放进来即可</span><br><span class="line"></span><br><span class="line">    args=SFTConfig(</span><br><span class="line">        dataset_text_field=&quot;text&quot;,      # 训练集中每条样本的字段名（对话列表）</span><br><span class="line">        per_device_train_batch_size=2,  # 每张显卡上的 batch_size（显存决定）</span><br><span class="line">        gradient_accumulation_steps=4,  # 4 次累积 → 全局有效 batch = 2×4 = 8</span><br><span class="line">        warmup_steps=5,                # 前 5 步线性预热学习率</span><br><span class="line">        max_steps=30,                  # 训练 30 步（调试阶段）；正式可用 num_train_epochs</span><br><span class="line">        learning_rate=2e-4,            # LoRA 常用 2e-4；长训降到 2e-5</span><br><span class="line">        logging_steps=1,               # 每 1 步打印一次日志</span><br><span class="line">        optim=&quot;adamw_8bit&quot;,            # 8-bit AdamW，省显存</span><br><span class="line">        weight_decay=0.01,             # L2 正则</span><br><span class="line">        lr_scheduler_type=&quot;linear&quot;,    # 线性衰减到 0</span><br><span class="line">        seed=3407,                     # 固定随机种子</span><br><span class="line">        report_to=&quot;swanlab&quot;,             # 把指标推送到 swanlab</span><br><span class="line">    ),</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="https://github.com/huggingface/trl">TRL</a> (Transformers
Reinforcement Learning，用强化学习训练Transformers模型)
是一个领先的Python库，旨在通过监督微调（SFT）、近端策略优化（PPO）和直接偏好优化（DPO）等先进技术，对基础模型进行训练后优化。TRL
建立在 🤗 Transformers
生态系统之上，支持多种模型架构和模态，并且能够在各种硬件配置上进行扩展。</p>
<p>其中<code>SFTTrainer</code>：一个专门为指令微调设计的训练器，封装了
Hugging Face 的
<code>Trainer</code>，而<code>SFTConfig</code>：配置训练参数的专用类，功能类似
<code>TrainingArguments</code>。而SFTConfig核心参数解释如下：</p>
<table>
<colgroup>
<col style="width: 34%">
<col style="width: 65%">
</colgroup>
<thead>
<tr class="header">
<th>参数名</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>dataset_text_field="text"</code></td>
<td>数据集中用于训练的字段名称，如 <code>text</code> 或
<code>prompt</code></td>
</tr>
<tr class="even">
<td><code>per_device_train_batch_size=2</code></td>
<td>每张 GPU 上的 batch size 是 2</td>
</tr>
<tr class="odd">
<td><code>gradient_accumulation_steps=4</code></td>
<td>梯度累计 4 次后才进行一次反向传播（等效于总 batch size = 2 × 4 =
8）</td>
</tr>
<tr class="even">
<td><code>warmup_steps=5</code></td>
<td>前 5 步进行 warmup（缓慢提升学习率）</td>
</tr>
<tr class="odd">
<td><code>max_steps=30</code></td>
<td>最多训练 30 步（适合调试或快速实验）</td>
</tr>
<tr class="even">
<td><code>learning_rate=2e-4</code></td>
<td>初始学习率（短训练可用较高值）</td>
</tr>
<tr class="odd">
<td><code>logging_steps=1</code></td>
<td>每训练 1 步就打印一次日志</td>
</tr>
<tr class="even">
<td><code>optim="adamw_8bit"</code></td>
<td>使用 8-bit AdamW 优化器（节省内存，Unsloth 支持）</td>
</tr>
<tr class="odd">
<td><code>weight_decay=0.01</code></td>
<td>权重衰减，用于防止过拟合</td>
</tr>
<tr class="even">
<td><code>lr_scheduler_type="linear"</code></td>
<td>线性学习率调度器（从高到低线性下降）</td>
</tr>
<tr class="odd">
<td><code>seed=3407</code></td>
<td>固定随机种子，确保结果可复现</td>
</tr>
<tr class="even">
<td><code>report_to="none"</code></td>
<td>不使用 WandB 或 TensorBoard 等日志平台（可改为
<code>"wandb"</code>）</td>
</tr>
</tbody>
</table>
<blockquote>
<ol type="1">
<li><p><strong>per_device_train_batch_size=2</strong>
<strong>每次前向只用了 2 条样本</strong> → 显存占用小，单卡就能跑。</p>
<p><strong>batch_size
决定「每一步真正喂给模型的样本数量」，越大训练越稳，但对显存要求越高。</strong></p></li>
<li><p><strong>gradient_accumulation_steps=4</strong> <strong>把这 2
条样本算出的梯度先攒起来，攒够 4 次再一次性做反向传播</strong> →
等效于一次性看了 <strong>2 × 4 = 8 条样本</strong>，但显存仍按 2
条算。</p></li>
</ol>
</blockquote>
<p>此时基本训练过程为： 1. 从 <code>combined_dataset</code>
中取出一批样本（2 条） 2. 重复上面过程 4
次（<code>gradient_accumulation_steps=4</code>） 3.
将累计的梯度用于更新模型一次参数（等效于一次大 batch 更新） 4.
重复上述过程，直到 <code>max_steps=30</code> 停止</p>
<h4 id="设置训练可视化swanlab">设置训练可视化swanlab</h4>
<p><a target="_blank" rel="noopener" href="https://docs.swanlab.cn/guide_cloud/integration/integration-huggingface-trl.html">🤗HuggingFace
Trl | SwanLab官方文档</a></p>
<p>只需要在你的训练代码中，找到HF的<code>Config</code>部分（比如<code>SFTConfig</code>、<code>GRPOConfig</code>等），添加<code>report_to="swanlab"</code>参数，即可完成集成。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from trl import SFTConfig, SFTTrainer</span><br><span class="line"></span><br><span class="line">args = SFTConfig(</span><br><span class="line">    ...,</span><br><span class="line">    report_to=&quot;swanlab&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer = Trainer(..., args=args)</span><br></pre></td></tr></table></figure>
<p>默认下，项目名会使用你运行代码的<code>目录名</code>。</p>
<p>如果你想自定义项目名，可以设置<code>SWANLAB_PROJECT</code>环境变量：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">os.environ[&quot;SWANLAB_PROJECT&quot;]=&quot;qwen2-sft&quot;</span><br></pre></td></tr></table></figure>
<h4 id="微调执行流程">微调执行流程</h4>
<p>一切准备就绪后，接下来即可开始进行微调。由于本次微调总共只运行30个step，整个过程并不会很长，实际执行过程如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trainer_stats = trainer.train()</span><br></pre></td></tr></table></figure>
<h4 id="保存模型">保存模型</h4>
<p><strong>1. 保存 LoRA Adapter</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 保存 LoRA adapter（仅几十 MB）</span><br><span class="line">save_path = &quot;./lora-adapter&quot;</span><br><span class="line">model.save_pretrained(save_path)          # LoRA 权重</span><br><span class="line">tokenizer.save_pretrained(save_path)      # 词表</span><br></pre></td></tr></table></figure>
<p>以后加载：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> unsloth <span class="keyword">import</span> FastLanguageModel</span><br><span class="line">model, tokenizer = FastLanguageModel.from_pretrained(</span><br><span class="line">    model_name = <span class="string">&quot;base-model-name-or-path&quot;</span>,</span><br><span class="line">    max_seq_length = <span class="number">2048</span>,</span><br><span class="line">    load_in_4bit = <span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line">model = FastLanguageModel.get_peft_model(model, ...)  <span class="comment"># 同训练时参数</span></span><br><span class="line">model.load_adapter(save_path)   <span class="comment"># 把 LoRA 权重挂回去</span></span><br></pre></td></tr></table></figure>
<p><strong>2.合并 LoRA → 完整模型</strong></p>
<p>如果你想把 <strong>LoRA 权重合并到基座</strong>
得到一个独立的大模型（方便推理、上传 Hub）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 合并权重</span></span><br><span class="line">merged_model = model.merge_and_unload()   <span class="comment"># 返回普通 transformers 模型</span></span><br><span class="line">merged_model.save_pretrained(<span class="string">&quot;./merged-model&quot;</span>)</span><br><span class="line">tokenizer.save_pretrained(<span class="string">&quot;./merged-model&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>合并后就是完整的大模型（GB 级），可直接用
<code>AutoModelForCausalLM.from_pretrained("./merged-model")</code>
加载，不依赖 Unsloth。</p>
<h3 id="微调结果">微调结果</h3>
<h4 id="可视化结果">可视化结果</h4>
<figure>
<img src="/2025/08/12/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/qwen3-8b%E5%BE%AE%E8%B0%83%E5%AE%9E%E6%88%98/image-20250813111238359.png" alt="image-20250813111238359">
<figcaption aria-hidden="true">image-20250813111238359</figcaption>
</figure>
<p><a target="_blank" rel="noopener" href="https://swanlab.cn/@zxj123/Fine-tune-Qwen-8B/runs/e2l6g6s3v7dlb7hmfircv/chart">图表
｜ Fine-tune-Qwen-8B/rat-2</a></p>
<table>
<colgroup>
<col style="width: 18%">
<col style="width: 22%">
<col style="width: 28%">
<col style="width: 30%">
</colgroup>
<thead>
<tr class="header">
<th>指标名称</th>
<th>含义</th>
<th>单位/范围提示</th>
<th>常见关注点</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>train/loss</td>
<td>训练损失（Training Loss）</td>
<td>标量，越小越好</td>
<td>是否持续下降、是否震荡、是否过拟合</td>
</tr>
<tr class="even">
<td>train/grad_norm</td>
<td>梯度范数（Gradient Norm）</td>
<td>标量，通常 0.01–1.0 为合理区间</td>
<td>是否爆炸（&gt;10）或消失（&lt;1e-4）</td>
</tr>
<tr class="odd">
<td>train/learning_rate</td>
<td>学习率（Learning Rate）</td>
<td>标量，如 1e-4、5e-4 等</td>
<td>是否过大导致震荡、过小导致收敛慢</td>
</tr>
<tr class="even">
<td>train/epoch</td>
<td>已训练的轮次（Epoch）</td>
<td>标量，1.0 表示完整遍历一次训练集</td>
<td>当前已训练多少轮、是否还需继续训练</td>
</tr>
<tr class="odd">
<td>train/global_step</td>
<td>全局步数（Global Step）</td>
<td>整数，每个 batch +1</td>
<td>与 epoch 对应，计算已见样本量</td>
</tr>
</tbody>
</table>
<h4 id="对话测试">对话测试</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">messages = [</span><br><span class="line">    &#123;&quot;role&quot; : &quot;user&quot;, &quot;content&quot; : &quot;解决(x + 2)^2 = 0.&quot;&#125;</span><br><span class="line">]</span><br><span class="line">text = tokenizer.apply_chat_template(</span><br><span class="line">    messages,</span><br><span class="line">    tokenize = False,</span><br><span class="line">    add_generation_prompt = True, # Must add for generation</span><br><span class="line">    enable_thinking = True, # Disable thinking</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">from transformers import TextStreamer</span><br><span class="line">_ = model.generate(</span><br><span class="line">    **tokenizer(text, return_tensors = &quot;pt&quot;).to(&quot;cuda&quot;),</span><br><span class="line">    max_new_tokens = 20488, # Increase for longer outputs!</span><br><span class="line">    temperature = 0.6, top_p = 0.95, top_k = 20, # For thinking</span><br><span class="line">    streamer = TextStreamer(tokenizer, skip_prompt = True),</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/08/11/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/LoRA%E5%85%B6%E4%BB%96%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E6%96%B9%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/08/11/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/LoRA%E5%85%B6%E4%BB%96%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E6%96%B9%E6%B3%95/" class="post-title-link" itemprop="url">LoRA其他的模型微调方法</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-08-11 00:00:00" itemprop="dateCreated datePublished" datetime="2025-08-11T00:00:00+08:00">2025-08-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-08-14 10:58:52" itemprop="dateModified" datetime="2025-08-14T10:58:52+08:00">2025-08-14</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%A8%A1%E5%9E%8B/" itemprop="url" rel="index"><span itemprop="name">模型</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%A8%A1%E5%9E%8B/%E5%BE%AE%E8%B0%83/" itemprop="url" rel="index"><span itemprop="name">微调</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <figure>
<img src="/2025/08/11/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/LoRA%E5%85%B6%E4%BB%96%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E6%96%B9%E6%B3%95/image-20250814105851771.png" alt="image-20250814105851771">
<figcaption aria-hidden="true">image-20250814105851771</figcaption>
</figure>
<p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzkzODI1NzQyNA==&amp;mid=2247494667&amp;idx=1&amp;sn=c3af7d2472de61752ef8b8df28746f2e&amp;poc_token=HCyNmWijps0ViWD6wPgqFiDYUZVRSs7xUDRfowWE">大模型微调技巧：LoRA
与 QLoRA讲解</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/javatiange/article/details/149964743?fromshare=blogdetail&amp;sharetype=blogdetail&amp;sharerId=149964743&amp;sharerefer=PC&amp;sharesource=2501_91530961&amp;sharefrom=from_link">一文详解：8种常见的大模型微调方法，看这篇就够了！-CSDN博客</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/682082440">大模型微调技术 -
知乎</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/08/11/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/08/11/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B/" class="post-title-link" itemprop="url">大模型训练流程</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2025-08-11 00:00:00 / 修改时间：09:55:54" itemprop="dateCreated datePublished" datetime="2025-08-11T00:00:00+08:00">2025-08-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%A8%A1%E5%9E%8B/" itemprop="url" rel="index"><span itemprop="name">模型</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%A8%A1%E5%9E%8B/%E5%BE%AE%E8%B0%83/" itemprop="url" rel="index"><span itemprop="name">微调</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="什么是大模型">什么是大模型</h3>
<p>随着2022年底 ChatGPT 再一次刷新 NLP
的能力上限，大<strong>语言模型（Large Language
Model，LLM）开始接替传统的预训练语言模型（Pre-trained Language
Model，PLM）</strong> 成为 NLP 的主流方向，基于 LLM
的全新研究范式也正在刷新被 BERT
发扬光大的<strong>预训练-微调范式</strong>，NLP
由此迎来又一次翻天覆地的变化。</p>
<p>LLM，即 Large Language
Model，中文名为大语言模型或大型语言模型，是一种相<strong>较传统语言模型参数量更多、在更大规模语料上进行预训练的语言模型</strong>。</p>
<p>一般来说，LLM
指包含<strong>数百亿（或更多）参数的语言模型</strong>，它们往往在<strong>数
T token
语料上</strong>通过多卡分布式集群进行预训练，具备远超出传统预训练模型的文本理解与生成能力。不过，随着
LLM 研究的不断深入，多种参数尺寸的 LLM 逐渐丰富，广义的 LLM
一般覆盖了从<strong>十亿参数</strong>（如
Qwen-1.5B）到<strong>千亿参数</strong>（如
Grok-314B）的所有大型语言模型。只要模型展现出<strong>涌现能力</strong>，即在一系列复杂任务上表现出远超传统预训练模型（如
BERT、T5）的能力与潜力，都可以称之为 LLM。</p>
<p>一般认为，GPT-3（1750亿参数）是 LLM 的开端，基于 GPT-3 通过
<strong>预训练（Pretraining）、监督微调（Supervised
Fine-Tuning，SFT）、强化学习与人类反馈（Reinforcement Learning with
Human Feedback，RLHF）</strong>三阶段训练得到的 ChatGPT 更是主导了 LLM
时代的到来。</p>
<blockquote>
<p>区分 LLM 与传统 PLM 最显著的特征即是 LLM 具备 <code>涌现能力</code>
。涌现能力是指同样的模型架构与预训练任务下，某些能力在小型模型中不明显，但在大型模型中特别突出。</p>
</blockquote>
<h3 id="训练流程">训练流程</h3>
<figure>
<img src="/2025/08/11/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B/image-20250811092459843.png" alt="image-20250811092459843">
<figcaption aria-hidden="true">image-20250811092459843</figcaption>
</figure>
<p>一般而言，训练一个完整的 LLM 需要经过图1中的三个阶段——Pretrain、SFT
和 RLHF。</p>
<h3 id="pretrain">Pretrain</h3>
<p>Pretrain，即预训练，是训练 LLM 最核心也是工程量最大的第一步。</p>
<h4 id="参数">参数</h4>
<table style="width:100%;">
<colgroup>
<col style="width: 16%">
<col style="width: 21%">
<col style="width: 18%">
<col style="width: 8%">
<col style="width: 16%">
<col style="width: 19%">
</colgroup>
<thead>
<tr class="header">
<th>模型</th>
<th>hidden_layers</th>
<th>hidden_size</th>
<th>heads</th>
<th>整体参数量</th>
<th>预训练数据量</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>BERT-base</td>
<td>12</td>
<td>768</td>
<td>12</td>
<td>0.1B</td>
<td>3B</td>
</tr>
<tr class="even">
<td>BERT-large</td>
<td>24</td>
<td>1024</td>
<td>16</td>
<td>0.3B</td>
<td>3B</td>
</tr>
<tr class="odd">
<td>Qwen-1.8B</td>
<td>24</td>
<td>2048</td>
<td>16</td>
<td>1.8B</td>
<td>2.2T</td>
</tr>
<tr class="even">
<td>LLaMA-7B</td>
<td>32</td>
<td>4096</td>
<td>32</td>
<td>7B</td>
<td>1T</td>
</tr>
<tr class="odd">
<td>GPT-3</td>
<td>96</td>
<td>12288</td>
<td>96</td>
<td>175B</td>
<td>300B</td>
</tr>
</tbody>
</table>
<p>根据定义，LLM
的核心特点即在于其具有<strong>远超传统预训练模型的参数量</strong>，<strong>同时在更海量的语料上进行预训练</strong>。传统预训练模型如
BERT，有 base 和 large 两个版本。BERT-base 模型由 12个 Encoder
层组成，其 hidden_size 为 768，使用 12个头作为多头注意力层，整体参数量为
1亿（110M）；而 BERT-large 模型由 24个 Encoder 层组成，hidden_size 为
1024，有 16个头，整体参数量为 3亿（340M）。同时，BERT 预训练使用了
33亿（3B）token 的语料，在 64块 TPU 上训练了
4天。事实上，相对于传统的深度学习模型，3亿参数量、33亿训练数据的 BERT
已经是一个能力超群、资源消耗巨大的庞然大物。</p>
<p>但是，前面我们提到，<strong>一般而言的 LLM
通常具有数百亿甚至上千亿参数</strong>，即使是广义上最小的
LLM，一般也有十亿（1B）以上的参数量。例如以开山之作 GPT-3 为例，其有
96个 Decoder 层，12288 的 hidden_size 和 96个头，<strong>共有
1750亿（175B）参数，比 BERT 大出快
3个数量级</strong>。即使是目前流行的小型 LLM 如 Qwen-1.8B，其也有 24个
Decoder 层、2048的 hidden_size 和 16个注意力头，整体参数量达到
18亿（1.8B）。</p>
<h4 id="分布式训练">分布式训练</h4>
<p>也正因如此，<strong>分布式训练框架也成为 LLM
训练必不可少的组成部分</strong>。分布式训练框架的核心思路是<strong>数据并行和模型并行</strong>。所谓数据并行，是指训练模型的尺寸可以被单个
GPU 内存容纳，但是由于增大训练的 batch_size
会增大显存开销，无法使用较大的 batch_size
进行训练；同时，训练数据量非常大，使用单张 GPU 训练时长难以接受。</p>
<h4 id="数据集">数据集</h4>
<p><strong>训练数据本身也是预训练 LLM 的一个重大挑战</strong>。训练一个
LLM，至少需要数百 B 甚至上 T 的预训练语料。根据研究，LLM
所掌握的知识绝大部分都是在预训练过程中学会的，因此，为了使训练出的 LLM
能够覆盖尽可能广的知识面，预训练语料需要组织多种来源的数据，并以一定比例进行混合。目前，主要的开源预训练语料包括
CommonCrawl、C4、Github、Wikipedia 等。<strong>不同的 LLM
往往会在开源预训练语料基础上，加入部分私有高质量语料，再基于自己实验得到的最佳配比来构造预训练数据集</strong>。事实上，<strong>数据配比</strong>向来是预训练
LLM
的“核心秘籍”，不同的配比往往会相当大程度影响最终模型训练出来的性能。</p>
<p>训练一个中文
LLM，训练数据的难度会更大。目前，高质量语料还是大部分集中在英文范畴，例如上表的
Wikipedia、Arxiv 等，均是英文数据集；而 C4
等多语言数据集中，英文语料也占据主要地位。目前开源的中文 LLM 如
ChatGLM、Baichuan
等模型均未开放其预训练数据集，开源的中文预训练数据集目前仅有昆仑天工开源的<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/Skywork/SkyPile-150B">SkyPile</a>（150B）、中科闻歌开源的<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/wenge-research/yayi2_pretrain_data">yayi2</a>（100B）等，相较于英文开源数据集有明显差距。</p>
<h4 id="数据清洗">数据清洗</h4>
<p><strong>预训练数据的处理与清洗</strong>也是 LLM
预训练的一个重要环节。诸多研究证明，预训练数据的质量往往比体量更加重要。预训练数据处理一般包括以下流程：</p>
<ol type="1">
<li>文档准备。由于海量预训练语料往往是从互联网上获得，一般需要从爬取的网站来获得自然语言文档。文档准备主要包括
URL 过滤（根据网页 URL 过滤掉有害内容）、文档提取（从 HTML
中提取纯文本）、语言选择（确定提取的文本的语种）等。</li>
<li>语料过滤。语料过滤的核心目的是去除低质量、无意义、有毒有害的内容，例如乱码、广告等。语料过滤一般有两种方法：基于模型的方法，即通过高质量语料库训练一个文本分类器进行过滤；基于启发式的方法，一般通过人工定义
web 内容的质量指标，计算语料的指标值来进行过滤。</li>
<li>语料去重。实验表示，大量重复文本会显著影响模型的泛化能力，因此，语料去重即删除训练语料中相似度非常高的文档，也是必不可少的一个步骤。去重一般基于
hash
算法计算数据集内部或跨数据集的文档相似性，将相似性大于指定阈值的文档去除；也可以基于子串在序列级进行精确匹配去重。</li>
</ol>
<h3 id="sft-指令微调">SFT 指令微调</h3>
<p>预训练赋予了 LLM 能力，却还需要第二步将其激发出来。经过预训练的 LLM
好像一个博览群书但又不求甚解的书生，对什么样的偏怪问题，都可以流畅地接出下文，但他偏偏又<strong>不知道问题本身的含义</strong>，只会“死板背书”。这一现象的本质是因为，LLM
的预训练任务就是经典的
<strong>CLM</strong>，也就是训<strong>练其预测下一个 token
的能力</strong>，在没有进一步微调之前，其无法与其他下游任务或是用户指令适配。</p>
<p>因此，我们还需要第二步来教这个博览群书的学生如何去使用它的知识，也就是
<strong>SFT（Supervised Fine-Tuning，有监督微调）</strong>。</p>
<p>面对能力强大的
LLM，我们往往不再是在指定下游任务上构造有监督数据进行微调，而是选择训练模型的“通用指令遵循能力”，也就是一般<strong>通过<code>指令微调</code>的方式来进行
SFT</strong>。</p>
<p>所谓指令微调，即我们训练的输入是各种类型的用户指令，而需要模型拟合的输出则是我们希望模型在收到该指令后做出的回复。例如，我们的一条训练样本可以是：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">input:告诉我今天的天气预报？</span><br><span class="line">output:根据天气预报，今天天气是晴转多云，最高温度26摄氏度，最低温度9摄氏度，昼夜温差大，请注意保暖哦</span><br></pre></td></tr></table></figure>
<p>也就是说，SFT
的主要目标是让模型从多种类型、多种风格的指令中获得泛化的指令遵循能力，也就是能够理解并回复用户的指令。</p>
<h3 id="rlhf">RLHF</h3>
<p>RLHF，全称是 <strong>Reinforcement Learning from Human
Feedback，即人类反馈强化学习</strong>，是利用强化学习来训练 LLM
的关键步骤。相较于在 GPT-3 就已经初见雏形的 SFT，RLHF 往往被认为是
ChatGPT 相较于 GPT-3 的最核心突破。事实上，从功能上出发，我们可以将 LLM
的训练过程分成<strong>预训练与对齐（alignment）两个阶段</strong>。预训练的核心作用是赋予模型海量的知识，而所谓对齐，其实就是让模型与人类价值观一致，从而输出人类希望其输出的内容。在这个过程中，SFT
是让 LLM 和人类的指令对齐，从而具有指令遵循能力；而 RLHF
则是从更深层次令 LLM
和人类价值观对齐，令其达到安全、有用、无害的核心标准。</p>
<p>RLHF 分为两个步骤：<strong>训练 RM 和 PPO 训练</strong>。</p>
<p><strong>RM，Reward Model，即奖励模型</strong>。RM
是用于拟合人类偏好，来给 LLM 做出反馈的。在强化学习的训练中，对于 LLM
的每一个回复，RM
会进行打分，这个打分反映了生成回复符合人类偏好的程度。然后 LLM
会根据强化学习的原理，基于 RM 的打分来进行优化训练。</p>
<p>在完成 RM 训练之后，就可以使用 PPO
算法来进行强化学习训练。<strong>PPO，Proximal Policy
Optimization，近端策略优化算法</strong>，是一种经典的 RL
算法。事实上，强化学习训练时也可以使用其他的强化学习算法，但目前 PPO
算法因为成熟、成本较低，还是最适合 RLHF 的算法。</p>
<h3 id="参考资料">参考资料</h3>
<p><a target="_blank" rel="noopener" href="https://datawhalechina.github.io/happy-llm/#/./chapter4/第四章%20大语言模型">第四章
大语言模型</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/08/11/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E2%80%94%E2%80%94LoRA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/08/11/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E2%80%94%E2%80%94LoRA/" class="post-title-link" itemprop="url">模型微调——LoRA</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-08-11 00:00:00" itemprop="dateCreated datePublished" datetime="2025-08-11T00:00:00+08:00">2025-08-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-08-12 19:15:14" itemprop="dateModified" datetime="2025-08-12T19:15:14+08:00">2025-08-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%A8%A1%E5%9E%8B/" itemprop="url" rel="index"><span itemprop="name">模型</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%A8%A1%E5%9E%8B/%E5%BE%AE%E8%B0%83/" itemprop="url" rel="index"><span itemprop="name">微调</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="为什么要微调">为什么要微调</h3>
<p>预训练大模型在海量通用语料上学到的知识，在垂直场景（医疗、法律、零售客服等）里往往“泛而浅”。</p>
<p>从零训练一个同等规模的大模型成本极高（千卡周级别），而微调只需在已有权重上做小步调整，算力/数据量都指数级下降。</p>
<h3 id="什么是全量微调">什么是全量微调</h3>
<figure>
<img src="/2025/08/11/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E2%80%94%E2%80%94LoRA/image-20250811104846104.png" alt="image-20250811104846104">
<figcaption aria-hidden="true">image-20250811104846104</figcaption>
</figure>
<p>全量微调（full
fine-tuning）通俗来说，对于参数的每一个权重，都要学习一个新的值（或者偏移量），更新所有
Transformer 层里的权重矩阵（包括
embedding、attention、FFN），这样的开销是很大的。</p>
<h3 id="什么是lora">什么是LoRA</h3>
<p>LoRA（Low-Rank
Adaptation，低秩适配）是一种<strong>参数高效微调（PEFT）</strong>技术，核心目的：
<strong>“冻结大模型 99 %
以上原始权重，只额外训练极少量低秩矩阵，就能让模型在下游任务上达到近似全量微调的效果。”</strong></p>
<figure>
<img src="/2025/08/11/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E2%80%94%E2%80%94LoRA/image-20250811105145633.png" alt="image-20250811105145633">
<figcaption aria-hidden="true">image-20250811105145633</figcaption>
</figure>
<p>通俗来说，通过学习两个低秩的矩阵，来近似于完整的矩阵，如图，W=A*B，矩阵相乘</p>
<p>在实际应用中，<strong>LoRA可以直接和transformer的FFN层（线性层）对齐</strong></p>
<p>Transformer 模型的核心是注意力机制，其中涉及到 Query, Key, Value
的计算，这些都是线性变换。</p>
<p>在标准的注意力机制中，计算公式为：</p>
<p><span class="math display">$$
\text{Attention}(Q, K, V) =
\text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$</span></p>
<p>其中 <span class="math inline"><em>Q</em></span>, <span class="math inline"><em>K</em></span>, <span class="math inline"><em>V</em></span> 的计算为：</p>
<p><span class="math display"><em>Q</em> = <em>X</em><sub><em>Q</em></sub><em>W</em><sub><em>Q</em></sub>,  <em>K</em> = <em>X</em><sub><em>K</em></sub><em>W</em><sub><em>K</em></sub>,  <em>V</em> = <em>X</em><sub><em>V</em></sub><em>W</em><sub><em>V</em></sub></span></p>
<p><span class="math inline"><em>X</em><sub><em>Q</em></sub></span>,
<span class="math inline"><em>X</em><sub><em>K</em></sub></span>, <span class="math inline"><em>X</em><sub><em>V</em></sub></span>
的输入可以相同，也可以不同。例如，在 Cross-Attention
中，解码器的隐藏状态作为 <span class="math inline"><em>X</em><sub><em>Q</em></sub></span>，编码器的输出作为
<span class="math inline"><em>X</em><sub><em>K</em></sub></span> 和
<span class="math inline"><em>X</em><sub><em>V</em></sub></span>。</p>
<p><strong>LoRA 可以应用到 <span class="math inline"><em>W</em><sub><em>Q</em></sub></span>, <span class="math inline"><em>W</em><sub><em>K</em></sub></span>, <span class="math inline"><em>W</em><sub><em>V</em></sub></span>
上，采用与线性层类似的方式</strong>。</p>
<h3 id="为什么要用lora">为什么要用lora</h3>
<p>首先要理解低秩：秩可以理解成一个矩阵所代表的信息，低秩矩阵，便是带有少量信息的矩阵，当然这样的矩阵计算效率是更高的，</p>
<p>在全量微调中，由于训练一个完整的矩阵开销是非常大的；在lora中就通过训练低秩矩阵，来近似<strong>模型权重更新</strong>的效果</p>
<blockquote>
<p>若模型参数比较小，使用冻结部分参数或全量微调的方式，往往更好</p>
</blockquote>
<p>初学者不禁会思考，这样难道不会损失信息导致大模型的性能变差吗？但是，实验下来效果还是不错的，通过牺牲一点性能，来换取开销的大幅度减少</p>
<blockquote>
<p>LoRA 原文实验 在 GPT-3 175 B 上，仅用 rank 4 的 LoRA 就能在全量微调
99 % 参数量的情况下，保持 97 % 的下游指标。</p>
</blockquote>
<h3 id="什么是qlora">什么是QLoRA</h3>
<p>QLoRA（Quantized Low-Rank Adaptation，量化低秩适应）是 <strong>LoRA
的“极致省内存”版本</strong>。它把 LoRA
的“低秩增量”思路再往前推一步：<strong>先把整个底座模型权重压到
4-bit，再在上面做 LoRA 微调</strong>。</p>
<p>QLoRA 是另一个热门术语，它与 LoRA
之间的唯一区别在于首字母“Q”，代表“量化（quantized）”。“量化”一词指的是用来减少存储神经元权重的比特数。</p>
<p>例如，神经网络的权重通常以浮点数表示，每个权重需要 32
位。量化的思想是将神经网络的权重压缩为更低的精度，而不会显著损失模型性能或产生重大影响。因此，不再使用
32 位，而是可以舍弃部分比特，例如只用 16 位。</p>
<figure>
<img src="/2025/08/11/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E2%80%94%E2%80%94LoRA/image-20250811142432782.png" alt="image-20250811142432782">
<figcaption aria-hidden="true">image-20250811142432782</figcaption>
</figure>
<h3 id="微调工具的介绍">微调工具的介绍</h3>
<h4 id="unsloth">unsloth</h4>
<p><a target="_blank" rel="noopener" href="https://github.com/unslothai/unsloth?tab=readme-ov-file">unslothai/unsloth:
Fine-tuning &amp; Reinforcement Learning for LLMs. 🦥 Train OpenAI
gpt-oss, Qwen3, Llama 4, DeepSeek-R1, Gemma 3, TTS 2x faster with 70%
less VRAM.</a></p>
<p>unsloth是一个专为大型语言模型（LLM）设计的动态量化与微调框架，旨在提高微调效率并减少显存占用，因此主要用于单机单卡的模型微调。</p>
<p>值得一提的是，Unsloth动态量化模型：https://unsloth.ai/blog/dynamic-v2</p>
<p>Unsloth的动态量化方法，特别是其最新的Dynamic
2.0版本，旨在在尽量减少性能损失的同时显著压缩大型语言模型（LLMs）的体积。对于Qwen3模型，尤其是4-bit动态量化版本，现有的评测显示其性能下降非常有限，甚至在某些任务上与原始模型相当。</p>
<blockquote>
<p>Unsloth 的「动态量化」可以一句话概括为：
<strong>“按层、按敏感度自动决定每块权重到底用 2.5 / 3.5 / 4 / 6 / 8 / 32
bit 的精细化量化策略，而不是一股脑全量化到 4 bit。”</strong></p>
</blockquote>
<p>这也使得Unsloth的动态量化模型成为<strong>个人配置</strong>下的最佳微调工具。</p>
<p>不过需要注意的是，动态量化由利也有弊，其<strong>好处在于可以极大程度压缩模型运行所需占用的显存大小，同时几乎不损失性能</strong>，但问题在于动态量化的模型，无论是推理还是微调，<strong>只能单卡运行</strong>，这就使得其吞吐量有限，无法在一台物理机上实现多GPU并行从而扩大吞吐量。</p>
<h4 id="llama-factory"><strong>LLaMA Factory</strong></h4>
<p><a target="_blank" rel="noopener" href="https://github.com/hiyouga/LLaMA-Factory/tree/main">hiyouga/LLaMA-Factory:
Unified Efficient Fine-Tuning of 100+ LLMs &amp; VLMs (ACL 2024)</a></p>
<p>LLaMA Factory
是一个简单易用且高效的大型语言模型训练与微调平台。通过它，用户可以在无需编写任何代码的前提下，在本地完成上百种预训练模型的微调。</p>
<p>LLaMA Factory 提供了API Server 和一站式 WebUI
Board，方便企业进行模型的管理和部署。适合不会写代码或代码基础比较弱的同学快速上手进行微调。</p>
<h4 id="其他">其他</h4>
<p>ms-SWIFT GitHub项目主页：https://github.com/modelscope/swift</p>
<p>ColossalAI
GitHub项目主页：https://github.com/hpcaitech/ColossalAI</p>
<p>除此之外，也可以借助更加底层的库，如peft、LoRA、transformer等实现高效微调。</p>
<h3 id="模型性能评估框架">模型性能评估框架</h3>
<h4 id="evalscope">EvalScope</h4>
<p>项目地址： https://github.com/modelscope/evalscope</p>
<p>EvalScope
是由阿里巴巴魔搭社区（ModelScope）推出的一款开源模型评估框架，旨在为大语言
模型（LLM）和多模态模型提供统一、系统化的性能评估方案。该框架具备高度的自动化和可扩展性，
适用于研究机构、工业界以及模型开发者在模型验证与性能对比场景中的广泛需求。</p>
<h3 id="可视化框架">可视化框架</h3>
<h4 id="wandb">wandb</h4>
<p><strong>Weights &amp; Biases（简称 wandb）</strong>
是一个专为机器学习 / 深度学习设计的
<strong>云端实验管理、可视化与协作平台</strong>。它帮你把“训练过程中发生了什么”全部自动化地记录下来，并以网页仪表盘的形式实时展示，省去你手动保存日志、画图、整理表格的麻烦。</p>
<p>wandb官网： https://wandb.ai/site</p>
<h4 id="swanlab">swanlab</h4>
<p>SwanLab 是一款<strong>开源、轻量</strong>的 AI
模型训练跟踪与可视化工具，提供了一个<strong>跟踪、记录、比较、和协作实验</strong>的平台。</p>
<p>SwanLab 面向人工智能研究者，设计了友好的Python API
和漂亮的UI界面，并提供<strong>训练可视化、自动日志记录、超参数记录、实验对比、多人协同等功能</strong>。在SwanLab上，研究者能基于直观的可视化图表发现训练问题，对比多个实验找到研究灵感，并通过<strong>在线网页</strong>的分享与基于组织的<strong>多人协同训练</strong>，打破团队沟通的壁垒，提高组织训练效率。</p>
<p><a target="_blank" rel="noopener" href="https://docs.swanlab.cn/">SwanLab官方文档 |
先进的AI团队协作与模型创新引擎</a></p>
<h3 id="构造微调数据集">构造微调数据集</h3>
<h4 id="为什么要构造微调数据集">为什么要构造微调数据集</h4>
<figure>
<img src="/2025/08/11/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E2%80%94%E2%80%94LoRA/image-20250811162229104.png" alt="image-20250811162229104">
<figcaption aria-hidden="true">image-20250811162229104</figcaption>
</figure>
<p>其中 &lt;∣im_start∣&gt;
代表文本开始,而user则代表消息身份,用于构建多轮对话,而<lim_end>则代表文本结束,即用户输入结束,而<lim_start>代表新一段文本开始,assistant代表接下来由模型创建消息,而<lim_end>同样代表模型创建消息的结束。</lim_end></lim_start></lim_end></p>
<p>而模型其实是通过这样一组<strong>特殊字符标记</strong>来规范自己的行为,<strong>判断当前消息类型,以及通过输出特殊标记来确定停止时间</strong>。对于绝大多数模型,我们可以在模型的<strong>tokenizer_config.json中看到完整的特殊标记符</strong>(以及系统提示词模板):</p>
<figure>
<img src="/2025/08/11/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E2%80%94%E2%80%94LoRA/image-20250811163120092.png" alt="image-20250811163120092">
<figcaption aria-hidden="true">image-20250811163120092</figcaption>
</figure>
<p>而在实际微调过程中,我们都知道需要<strong>有监督的数据集</strong>、也就是需要输入QA对来进行微调。以著名的<strong>alpaca_zh中文微调数据集</strong>来说,其基本格式如下:</p>
<figure>
<img src="/2025/08/11/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E2%80%94%E2%80%94LoRA/image-20250811163232521.png" alt="image-20250811163232521">
<figcaption aria-hidden="true">image-20250811163232521</figcaption>
</figure>
<p>就可以表示为下列json格式数据集:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">json&#123;  &quot;instruction&quot;: &quot;&quot;,  &quot;input&quot;: &quot;输入:你好。&quot;,  &quot;output&quot;: &quot;输出:你好,有什么可以帮到你的?&quot;&#125;</span><br></pre></td></tr></table></figure>
<p>而在真实的微调过程中,如果是针对Qwen3进行微调,微调脚本会将这条数据集(无论什么格式)转化为如下格式:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xml&lt;im_start|&gt;user\n你好&lt;im_end|&gt;\n&lt;im_start|&gt;assistant\n你好,有什么可以帮到你的?&lt;im_end|&gt;</span><br></pre></td></tr></table></figure>
<p>而在实际训练过程中,模型就会根据assistant前的内容,学习assistant后面的输出内容。</p>
<p><strong>因此我们要在下载数据集后，进行微调前，对数据集进行预处理</strong>，接下来引出构造数据集的几种场景</p>
<h4 id="带有系统提示微调数据集格式">带有系统提示微调数据集格式</h4>
<p>在很多场景下,我们还会发现一些<strong>带有instruction字段的微调数据集</strong>,那instruction字段是如何带入到微调过程中的呢?</p>
<figure>
<img src="/2025/08/11/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E2%80%94%E2%80%94LoRA/image-20250811163232521.png" alt="image-20250811163232521">
<figcaption aria-hidden="true">image-20250811163232521</figcaption>
</figure>
<p>答案非常简单,还是依靠特殊字符。例如有一个对话内容如下:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">- 系统提示词(instruction):你是一名助人为乐的助手。</span><br><span class="line">- 用户输入(input):你好,好久不见。</span><br><span class="line">- 助手回复(output):是的呀,好久不见,最近有什么有趣的事情要和我分享么?</span><br></pre></td></tr></table></figure>
<p>此时模型的输入和输出如下:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;lim_start|&gt;system你是一名助人为乐的助手。&lt;/im_end&gt;</span><br><span class="line">&lt;lim_start|&gt;user 你好,好久不见。&lt;/lim_end&gt;</span><br><span class="line">&lt;lim_start|&gt;assistant 是的呀,好久不见,最近有什么有趣的事情要和我分享么?&lt;/lim_end&gt;</span><br></pre></td></tr></table></figure>
<p>即会通过&lt;lim_start|&gt;system…&lt;lim_end|&gt;来标记系统提示词。实际进行微调时,模型会根据assistant为界,学习assistant之前的文本输入情况下应该如何输出。</p>
<h4 id="带function-calling微调数据集格式">带Function
calling微调数据集格式</h4>
<p>更进一步的,如果对话过程中带入了<strong>Function
calling</strong>,此时首先模型会读取提前准备好的tool
schema(也可能是自动生成的,例如MCP即可自动创建tool schema):</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;tool_schema&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;name&quot;: &quot;get_weather&quot;,</span><br><span class="line">      &quot;description&quot;: &quot;查询指定城市的天气信息&quot;,</span><br><span class="line">      &quot;parameters&quot;: &#123;</span><br><span class="line">        &quot;type&quot;: &quot;object&quot;,</span><br><span class="line">        &quot;properties&quot;: &#123;</span><br><span class="line">          &quot;location&quot;: &#123;</span><br><span class="line">            &quot;type&quot;: &quot;string&quot;,</span><br><span class="line">            &quot;description&quot;: &quot;要查询天气的城市名称&quot;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;required&quot;: [&quot;location&quot;]</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>而假设我们的对话内容如下:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">- 系统提示词(instruction):你是一名助人为乐的助手。当用户查询天气的时候,请调用get_weather函数进行天气信息查询。</span><br><span class="line">- 用户输入(input):你好,请帮我查询下北京天气。</span><br><span class="line">- 助手回复(output):&#123;&quot;name&quot;: &quot;get_weather&quot;, &quot;arguments&quot;: &#123;&quot;location&quot;: &quot;北京&quot;&#125;&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>此时回复内容就是一条Function call message</p>
</blockquote>
<p>而此时模型真实的输入和输出内容如下:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&lt;|im_start|&gt;system</span><br><span class="line">你是天气助手，当用户查询天气时请调用 get_weather 函数。</span><br><span class="line"># Tools</span><br><span class="line">You may call one or more functions to assist with the user query.</span><br><span class="line">You are provided with function signatures within &lt;tools&gt;&lt;/tools&gt; XML tags:</span><br><span class="line">&lt;tools&gt;</span><br><span class="line">[&#123;&quot;name&quot;:&quot;get_weather&quot;,&quot;description&quot;:&quot;查询指定城市的天气信息&quot;,&quot;parameters&quot;:&#123;&quot;type&quot;:&quot;object&quot;,&quot;properties&quot;:&#123;&quot;location&quot;:&#123;&quot;type&quot;:&quot;string&quot;,&quot;description&quot;:&quot;要查询天气的城市名称&quot;&#125;&#125;,&quot;required&quot;:[&quot;location&quot;]&#125;&#125;]</span><br><span class="line">&lt;/tools&gt;</span><br><span class="line">&lt;tool_call&gt;</span><br><span class="line"> &#123;&quot;name&quot;: &lt;function-name&gt;, &quot;arguments&quot;: &lt;args-json-object&gt;&#125;</span><br><span class="line">&lt;/tool_call&gt;.</span><br><span class="line">&lt;|im_end|&gt;</span><br><span class="line">&lt;|im_start|&gt;user</span><br><span class="line">北京天气如何？</span><br><span class="line">&lt;|im_end|&gt;</span><br><span class="line">&lt;|im_start|&gt;assistant</span><br><span class="line">&lt;tool_call&gt;&#123;&quot;name&quot;:&quot;get_weather&quot;,&quot;arguments&quot;:&#123;&quot;location&quot;:&quot;北京&quot;&#125;&#125;&lt;/tool_call&gt;</span><br><span class="line">&lt;|im_end|&gt;</span><br></pre></td></tr></table></figure>
<p>接下来在进行训练时,模型同样根据assistant前的内容,学习assistant后面的输出内容。不过需要注意的是,由于高效微调调整的参数量较少,因此只能优化模型的Function
calling能力,并不能从无到有让模型学会Function calling。</p>
<h4 id="带有思考过程的微调数据集结构">带有思考过程的微调数据集结构</h4>
<p>而如果是带有思考链,则一个简单的问答数据如下:</p>
<figure>
<img src="/2025/08/11/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E2%80%94%E2%80%94LoRA/image-20250811165802090.png" alt="image-20250811165802090">
<figcaption aria-hidden="true">image-20250811165802090</figcaption>
</figure>
<ul>
<li>系统提示词(instruction):你是一名助人为乐的助手。</li>
<li>用户输入(input):你好,好久不见。</li>
<li>助手回复(output):好的,用户发来“你好,好久不见!”,我需要回应。首先,用户可能希望得到亲切的回应,所以应该用友好的语气。/n是的呀,好久不见,最近有什么有趣的事情要和我分享么?</li>
</ul>
<p>此时模型真实的内部输入和输出结果如下:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;lim_start|&gt;system</span><br><span class="line">你是一名助人为乐的助手。&lt;lim_end|&gt;</span><br><span class="line">&lt;lim_start|&gt;user</span><br><span class="line">你好,好久不见。&lt;lim_end|&gt;</span><br><span class="line">&lt;lim_start|&gt;assistant</span><br><span class="line"></span><br><span class="line">&lt;think&gt;  好的,用户发来“你好,好久不见!”,我需要回应。首先,用户可能希望得到亲切的回应,所以应该用友好的语气。&lt;/think&gt;</span><br><span class="line"></span><br><span class="line">是的呀,好久不见,最近有什么有趣的事情要和我分享么?&lt;/lim_end|&gt;</span><br></pre></td></tr></table></figure>
<p>模型同样根据assistant前的内容,学习assistant后面的输出内容。也就是说,所谓的思考过程,本质上其实是一种文本响应格式,通过模型训练而来。</p>
<h4 id="混合推理模型构造微调数据集基本方法">混合推理模型构造微调数据集基本方法</h4>
<p>在了解了微调数据集结构背后的基本原理后,接下来的问题是应该如何构造微调数据集呢?</p>
<p>一般来说我们可以在huggingface、ModelScope或llama-
factory中挑选合适的数据集,并根据实际情况进行组装。</p>
<p>例如围绕Qwen3模型的高效微调,为了确保其仍然<strong>保留混合推理能力,</strong>我们可以考虑在微调数据集中加入如普<strong>通对话数据集</strong><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/mlabonne/FineTome-100k">FineTome</a>,以及<strong>带有推理字段的数学类数据集</strong><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/nvidia/OpenMathReasoning">OpenMathReasoning</a>,<strong>并围绕这两个数据集进行拼接</strong>,从而在确保能提升模型的数学能力的同时,保留非推理的功能。</p>
<p>同时还需要在持续微调训练过程中<strong>不断调整COT数学数据集和普通文本问答数据集之间的配比</strong>,以确保模型能够在提升数学能力的同时,保留混合推理的性能。</p>
<blockquote>
<p>Qwen3 的「混合推理能力」=
<strong>在同一个模型里内置两套“大脑”</strong>： •
<strong>快思考（非思考模式）</strong>：轻量算力、秒级响应，适合简单问答；
•
<strong>慢思考（思考模式）</strong>：多步链式推理、深度推敲，适合复杂逻辑、数学、代码。
系统会自动或按用户指令在两种模式之间 <strong>动态切换</strong>，从而
<strong>既省算力又保证难题精度</strong>。</p>
</blockquote>
<h3 id="微调的基本流程">微调的基本流程</h3>
<figure>
<img src="/2025/08/11/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E2%80%94%E2%80%94LoRA/image-20250812105535330.png" alt="image-20250812105535330">
<figcaption aria-hidden="true">image-20250812105535330</figcaption>
</figure>
<h3 id="环境配置">环境配置</h3>
<p><strong>安装Unsloth</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install --upgrade --force-reinstall --no-cache-dir unsloth unsloth_zoo</span><br></pre></td></tr></table></figure>
<p><strong>安装Qwen3-8B-unsloth-bnb-4bit</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">modelscope download --model unsloth/Qwen3-8B-unsloth-bnb-4bit --local_dir /workspace/qwen3-8b</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#模型下载</span><br><span class="line">from modelscope import snapshot_download</span><br><span class="line">model_dir = snapshot_download(&#x27;unsloth/Qwen3-8B-unsloth-bnb-4bit&#x27;)</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>unsloth/Qwen3-8B-unsloth-bnb-4bit</strong> 这个模型它是
<strong>专门为Unsloth微调框架优化过的4bit量化版本</strong></p>
<p>原始 Qwen3-8B（FP16）需要约 <strong>22GB 显存</strong>，而 4bit
量化后仅需 <strong>6GB 左右</strong></p>
<p><strong>只要显存允许，原始 FP16/BF16 模型也可以用 Unsloth 做 4-bit
LoRA（即 QLoRA）微调；官方预量化 4-bit
模型只是帮你把“量化”这一步提前做完了，二者本质相同。</strong></p>
<p><strong>Unsloth 的两种用法示例</strong></p>
<table>
<colgroup>
<col style="width: 30%">
<col style="width: 42%">
<col style="width: 27%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">场景</th>
<th style="text-align: left;">代码片段</th>
<th style="text-align: left;">备注</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">A. 用官方已量化好的 4-bit 权重</td>
<td style="text-align: left;"><code>model_name="unsloth/Qwen3-8B-bnb-4bit"</code></td>
<td style="text-align: left;">显卡 6 GB 就能跑，省去自己量化</td>
</tr>
<tr class="even">
<td style="text-align: left;">B. 用原始 FP16 权重并现场 4-bit 量化</td>
<td style="text-align: left;"><code>model_name="Qwen/Qwen3-8B"</code> +
<code>load_in_4bit=True</code></td>
<td style="text-align: left;">显卡仍需 6 GB，显存占用与 A 相同</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> unsloth <span class="keyword">import</span> FastLanguageModel</span><br><span class="line"></span><br><span class="line"><span class="comment"># 两种写法效果等价</span></span><br><span class="line">model, tokenizer = FastLanguageModel.from_pretrained(</span><br><span class="line">    model_name=<span class="string">&quot;Qwen/Qwen3-8B&quot;</span>,   <span class="comment"># 原始权重</span></span><br><span class="line">    load_in_4bit=<span class="literal">True</span>,            <span class="comment"># 现场量化到 4-bit</span></span><br><span class="line">    max_seq_length=<span class="number">2048</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
</blockquote>
<p><strong>安装EvalScope</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">pip install evalscope                </span><br><span class="line"># 安装 Native backend (默认)</span><br><span class="line"> # 额外选项</span><br><span class="line">pip install &#x27;evalscope[opencompass]&#x27;   # 安装 OpenCompass backend</span><br><span class="line"> pip install &#x27;evalscope[vlmeval]&#x27;       </span><br><span class="line"># 安装 VLMEvalKit backend</span><br><span class="line"> pip install &#x27;evalscope[rag]&#x27;           </span><br><span class="line">pip install &#x27;evalscope[perf]&#x27;          </span><br><span class="line">pip install &#x27;evalscope[app]&#x27;           </span><br><span class="line"># 或可以直接输入all，安装全部模块</span><br><span class="line"># pip install &#x27;evalscope[all]&#x27;           </span><br><span class="line"># 安装 RAGEval backend</span><br><span class="line"> # 安装 模型压测模块 依赖</span><br><span class="line"># 安装 可视化 相关依赖</span><br><span class="line"># 安装所有 backends (Native, OpenCompass, </span><br><span class="line">VLMEvalKit, RAGEval)</span><br></pre></td></tr></table></figure>
<p><strong>安装wandb</strong></p>
<p>wandb官网： https://wandb.ai/site</p>
<p>安装wandb：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install wandb</span><br></pre></td></tr></table></figure>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://github.com/SwanHubX/SwanLab?tab=readme-ov-file#-快速开始">SwanHubX/SwanLab:
⚡️SwanLab - an open-source, modern-design AI training tracking and
visualization tool. Supports Cloud / Self-hosted use. Integrated with
PyTorch / Transformers / LLaMA Factory / veRL/ Swift / Ultralytics /
MMEngine / Keras etc.</a></p>
<p>与其类似，一个开源、现代化设计的深度学习训练跟踪与可视化工具</p>
</blockquote>
<h3 id="参考资料">参考资料</h3>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV13BKozLEXE/?spm_id_from=333.337.search-card.all.click&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">DIY你的AI梦中情人？Qwen3微调手把手教你！_哔哩哔哩_bilibili</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1tthPeFEWb/?spm_id_from=333.337.search-card.all.click&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">通俗易懂理解全量微调和LoRA微调_哔哩哔哩_bilibili</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1DT421r7Et?spm_id_from=333.788.videopod.sections&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">通俗易懂理解大模型预训练和微调_哔哩哔哩_bilibili</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1YLE1zyEvX?spm_id_from=333.788.videopod.episodes&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11&amp;p=3">3.四大微调框架及微调硬件环境介绍_哔哩哔哩_bilibili</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1s2AUe2EBq/?spm_id_from=333.788.videopod.sections&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">如何把你的
DeePseek-R1 微调为某个领域的专家？（实战篇）_哔哩哔哩_bilibili</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/javatiange/article/details/149964743?fromshare=blogdetail&amp;sharetype=blogdetail&amp;sharerId=149964743&amp;sharerefer=PC&amp;sharesource=2501_91530961&amp;sharefrom=from_link">一文详解：8种常见的大模型微调方法，看这篇就够了！-CSDN博客</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/08/08/%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AE%97%E6%B3%95/tokenizer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/08/08/%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AE%97%E6%B3%95/tokenizer/" class="post-title-link" itemprop="url">tokenizer</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2025-08-08 00:00:00 / 修改时间：16:21:18" itemprop="dateCreated datePublished" datetime="2025-08-08T00:00:00+08:00">2025-08-08</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">大模型算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AE%97%E6%B3%95/tokenizer/" itemprop="url" rel="index"><span itemprop="name">tokenizer</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="什么是-tokenizer">什么是 Tokenizer？</h3>
<p><strong>Tokenizer</strong>（分词器）可以将原始文本（raw
text）转换为模型能够理解的数字序列，在模型输入和输出的两个主要阶段中发挥重要作用：</p>
<h4 id="模型输入编码-encode阶段">模型输入（编码 Encode）阶段</h4>
<ol type="1">
<li><p><strong>分词（Tokenize）</strong></p>
<p>将文本拆分为词元（Token），常见的分词方式包括字级、词级、子词级（如
BPE、WordPiece）、空格分词等。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">输入: &quot;你好&quot;</span><br><span class="line">分词: [&quot;你&quot;, &quot;好&quot;]</span><br></pre></td></tr></table></figure></li>
<li><p><strong>映射（Mapping）</strong></p>
<p>将每个词元映射为词汇表中的唯一 ID，生成的数字序列即为模型的输入。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">分词: [&quot;你&quot;, &quot;好&quot;]</span><br><span class="line">映射: [<span class="number">1001</span>, <span class="number">1002</span>]</span><br></pre></td></tr></table></figure></li>
</ol>
<h4 id="模型输出解码-decode阶段">模型输出（解码 Decode）阶段</h4>
<ol type="1">
<li><p><strong>反映射（De-mapping）</strong></p>
<p>模型输出的数字序列通过词汇表映射回对应的词元，二者是一一对应的关系。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">输出: [<span class="number">1001</span>, <span class="number">1002</span>]</span><br><span class="line">反映射: [&quot;你&quot;, &quot;好&quot;]</span><br></pre></td></tr></table></figure></li>
<li><p><strong>文本重组</strong></p>
<p>将解码后的词元以某种规则重新拼接为完整文本。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">反映射: [&quot;你&quot;, &quot;好&quot;]</span><br><span class="line">重组: &quot;你好&quot;</span><br></pre></td></tr></table></figure></li>
</ol>
<h4 id="直观感受">直观感受</h4>
<p>访问 <a target="_blank" rel="noopener" href="https://tiktokenizer.vercel.app">Tiktokenizer</a>，通过右上角选取不同的
Tokenizer 进行尝试</p>
<h3 id="词汇表">词汇表</h3>
<p>两种常见的构建词汇表的方法：</p>
<ul>
<li><strong>BPE（Byte-Pair Encoding）</strong>：用于
GPT、GPT-2、RoBERTa、BART 和 DeBERTa 等模型。</li>
<li><strong>WordPiece</strong>：用于 DistilBERT、MobileBERT、Funnel
Transformers 和 MPNET 等模型。</li>
</ul>
<h4 id="bpe">BPE</h4>
<p>BPE（Byte Pair Encoding，字节对编码）在 NLP
里是一种<strong>贪心式的子词（subword）分词算法</strong>。
理解：从“字符”开始，反复把<strong>出现次数最多的相邻字符对</strong>合并成新的符号，并加入词汇表，直到达到预设的词汇表大小。</p>
<blockquote>
<p>为什么可以处理 OOV（Out-Of-Vocabulary）情况</p>
<p>因为所有词汇都是由字符或词根组成的，通过对单个字符的学习，可以组成oov的词汇</p>
<p>为什么需要词汇表</p>
<p>编码时，从文本到模型：需要将文本分词为 Tokens，再通过词汇表将 Tokens
转换为 Token IDs，再传给transformer</p>
<p>解码时，从模型到文本：需要通过词汇表Token IDs 转换为
Tokens，再把Tokens 拼接为文本</p>
</blockquote>
<h5 id="步骤">步骤</h5>
<ol type="1">
<li><strong>初始化词汇表 <span class="math inline"><em>V</em></span></strong>：
<ul>
<li><span class="math inline"><em>V</em></span>
包含语料库中的所有唯一字符，即单词字符的集合。</li>
</ul></li>
<li><strong>统计字符对的频次</strong>：
<ul>
<li>对于每个单词的字符序列，统计相邻字符对的出现频次。</li>
</ul></li>
<li><strong>找到频次（Score）最高的字符对并合并</strong>：
<ul>
<li>选择出现频率最高的字符对 <span class="math inline">(<em>x</em>,<em>y</em>)</span>，将其合并为新符号
<span class="math inline"><em>x</em><em>y</em></span>。</li>
</ul></li>
<li><strong>更新词汇表并重复步骤 2 到 4</strong>：
<ul>
<li>将新符号添加到词汇表 <span class="math inline"><em>V</em> = <em>V</em> ∪ {<em>x</em><em>y</em>}</span>。</li>
<li>更新语料库中的单词表示，重复统计和合并过程，直到满足停止条件（例如，词汇表达到预定大小）。</li>
</ul></li>
</ol>
<p><strong>示例</strong></p>
<p>我们需要将语料库（corpus）的文本拆分为单词，假设当前语料库包含的单词和对应频次如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(&quot;low&quot;, 5), (&quot;lower&quot;, 2), (&quot;newest&quot;, 6), (&quot;widest&quot;, 3)</span><br></pre></td></tr></table></figure>
<p><strong>步骤 1：初始化词汇表</strong></p>
<p><strong>将单词拆分为字符序列</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(&quot;l&quot;, &quot;o&quot;, &quot;w&quot;), 5  </span><br><span class="line">(&quot;l&quot;, &quot;o&quot;, &quot;w&quot;, &quot;e&quot;, &quot;r&quot;), 2  </span><br><span class="line">(&quot;n&quot;, &quot;e&quot;, &quot;w&quot;, &quot;e&quot;, &quot;s&quot;, &quot;t&quot;), 6  </span><br><span class="line">(&quot;w&quot;, &quot;i&quot;, &quot;d&quot;, &quot;e&quot;, &quot;s&quot;, &quot;t&quot;), 3</span><br></pre></td></tr></table></figure>
<p><strong>词汇表 V</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;l&#x27;, &#x27;o&#x27;, &#x27;w&#x27;, &#x27;e&#x27;, &#x27;r&#x27;, &#x27;n&#x27;, &#x27;s&#x27;, &#x27;t&#x27;, &#x27;i&#x27;, &#x27;d&#x27;&#125;</span><br></pre></td></tr></table></figure>
<p><strong>步骤 2：统计字符对的频次</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">字符对频次统计结果:</span><br><span class="line">(&#x27;l&#x27;, &#x27;o&#x27;): 7        # 5 (low) + 2 (lower)</span><br><span class="line">(&#x27;o&#x27;, &#x27;w&#x27;): 7        # 5 (low) + 2 (lower)</span><br><span class="line">(&#x27;w&#x27;, &#x27;e&#x27;): 8        # 2 (lower) + 6 (newest)</span><br><span class="line">(&#x27;e&#x27;, &#x27;r&#x27;): 2</span><br><span class="line">(&#x27;n&#x27;, &#x27;e&#x27;): 6</span><br><span class="line">(&#x27;e&#x27;, &#x27;w&#x27;): 6</span><br><span class="line">(&#x27;e&#x27;, &#x27;s&#x27;): 9        # 6 (newest) + 3 (widest)</span><br><span class="line">(&#x27;s&#x27;, &#x27;t&#x27;): 9        # 6 (newest) + 3 (widest)</span><br><span class="line">(&#x27;w&#x27;, &#x27;i&#x27;): 3</span><br><span class="line">(&#x27;i&#x27;, &#x27;d&#x27;): 3</span><br><span class="line">(&#x27;d&#x27;, &#x27;e&#x27;): 3</span><br></pre></td></tr></table></figure>
<p><strong>步骤 3：找到频次最高的字符对并合并</strong></p>
<p><strong>选择频次最高的字符对</strong>：</p>
<ul>
<li><code>("e", "s")</code> 和 <code>("s", "t")</code>，频次均为
9。可以任选其一进行合并，假设选择排序第一的：
<code>("e", "s")</code>。</li>
</ul>
<p><strong>合并 <code>("e", "s")</code> 为新符号
<code>es</code></strong>。</p>
<p><strong>记录合并操作</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Merge 1: (&quot;e&quot;, &quot;s&quot;) -&gt; &quot;es&quot;</span><br></pre></td></tr></table></figure>
<p><strong>步骤 4：更新词汇表并重复</strong></p>
<p><strong>更新单词序列</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(&quot;l&quot;, &quot;o&quot;, &quot;w&quot;), 5  </span><br><span class="line">(&quot;l&quot;, &quot;o&quot;, &quot;w&quot;, &quot;e&quot;, &quot;r&quot;), 2  </span><br><span class="line">(&quot;n&quot;, &quot;e&quot;, &quot;w&quot;, &quot;es&quot;, &quot;t&quot;), 6  </span><br><span class="line">(&quot;w&quot;, &quot;i&quot;, &quot;d&quot;, &quot;es&quot;, &quot;t&quot;), 3</span><br></pre></td></tr></table></figure>
<p><strong>更新词汇表 V</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;l&#x27;, &#x27;o&#x27;, &#x27;w&#x27;, &#x27;e&#x27;, &#x27;r&#x27;, &#x27;n&#x27;, &#x27;s&#x27;, &#x27;t&#x27;, &#x27;i&#x27;, &#x27;d&#x27;, &#x27;es&#x27;&#125;</span><br></pre></td></tr></table></figure>
<p><strong>重复步骤 2 到 4，直到达到预定的词汇表大小</strong>。</p>
<h4 id="wordpiece">WordPiece</h4>
<p>WordPiece 是 Google 在 2016 年为语音识别与 BERT
提出的<strong>子词（subword）分词算法</strong>，可看作 BPE
的“似然改进版”。理解：“<strong>用概率贪心而不是频次贪心，从字符开始逐步合并子词</strong>。”</p>
<p>与 BPE 不同，WordPiece 的 Score
由字符对频次与其组成部分频次的比值决定，定义 Score：</p>
<p><span class="math display">$$
\text{Score}_{\text{WordPiece}}(x, y) =
\frac{\text{freq}(xy)}{\text{freq}(x) \times \text{freq}(y)}
$$</span></p>
<p>其中, <span class="math inline">freq(<em>x</em>)</span>, <span class="math inline">freq(<em>y</em>)</span> 和 <span class="math inline">freq(<em>x</em><em>y</em>)</span> 分别表示符号 <span class="math inline"><em>x</em></span>, <span class="math inline"><em>y</em></span> 和它们合并后的符号 <span class="math inline"><em>x</em><em>y</em></span> 的频次。</p>
<h5 id="步骤-1">步骤</h5>
<ol type="1">
<li><strong>初始化词汇表 <span class="math inline"><em>V</em></span></strong>：
<ul>
<li>与 BPE 相同, <span class="math inline"><em>V</em></span>
包含语料库中的所有唯一字符，但处理方式略有不同：<strong>对于每个单词，除了首个字符外，其他字符前都加上
<code>##</code> 前缀。</strong></li>
</ul></li>
<li><strong>统计字符对的频次及 Score</strong>：
<ul>
<li>对于每个可能的字符对 <span class="math inline">(<em>x</em>,<em>y</em>)</span>，计算 <span class="math inline">freq(<em>x</em>)</span>, <span class="math inline">freq(<em>y</em>)</span>, <span class="math inline">freq(<em>x</em><em>y</em>)</span>，并计算
Score。</li>
</ul></li>
<li><strong>找到 Score 最高的字符对并合并</strong>：
<ul>
<li>选择 Score 最高的字符对 <span class="math inline">(<em>x</em>,<em>y</em>)</span>，将其合并为新符号
<span class="math inline"><em>x</em><em>y</em></span>，注意：
<ul>
<li>如果第二个符号以 <code>##</code> 开头，合并时去掉 <code>##</code>
前缀再进行连接。</li>
<li>新符号是否以 <code>##</code> 开头，取决于第一个符号是否以
<code>##</code> 开头。</li>
</ul></li>
</ul></li>
<li><strong>更新词汇表并重复步骤 2 到 4</strong>：
<ul>
<li>将新符号添加到词汇表 <span class="math inline"><em>V</em> = <em>V</em> ∪ {<em>x</em><em>y</em>}</span>。</li>
<li>更新语料库中的单词表示，重复统计和合并过程，直到满足停止条件。</li>
</ul></li>
</ol>
<h3 id="映射mapping">映射（Mapping）</h3>
<p>以 BPE 为例，最终词汇表 <span class="math inline"><em>V</em></span>
中的 Token 和对应的频次分别为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">vocab = &#123;</span><br><span class="line">    &#x27;lo&#x27;: 7,</span><br><span class="line">    &#x27;w&#x27;: 16,</span><br><span class="line">    &#x27;e&#x27;: 8,</span><br><span class="line">    &#x27;r&#x27;: 2,</span><br><span class="line">    &#x27;n&#x27;: 6,</span><br><span class="line">    &#x27;est&#x27;: 9,</span><br><span class="line">    &#x27;i&#x27;: 3,</span><br><span class="line">    &#x27;d&#x27;: 3</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>输出</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Token to ID: &#123;&#x27;lo&#x27;: 0, &#x27;w&#x27;: 1, &#x27;e&#x27;: 2, &#x27;r&#x27;: 3, &#x27;n&#x27;: 4, &#x27;est&#x27;: 5, &#x27;i&#x27;: 6, &#x27;d&#x27;: 7&#125;</span><br><span class="line">ID to Token: &#123;0: &#x27;lo&#x27;, 1: &#x27;w&#x27;, 2: &#x27;e&#x27;, 3: &#x27;r&#x27;, 4: &#x27;n&#x27;, 5: &#x27;est&#x27;, 6: &#x27;i&#x27;, 7: &#x27;d&#x27;&#125;</span><br></pre></td></tr></table></figure>
<p>当然，也可以根据频次或者其他规则进行特殊处理。</p>
<p>以上是编码部分的概述，实际上在文本预处理的时候还会增加特殊标记，但这些以及后续的解码部分大多是一些文本处理的规则，这里就不过多赘述了，Tokenizer
之间的核心差异在于使用的分割方法和词汇表的构建策略。</p>
<h3 id="transformer中的分词">transformer中的分词</h3>
<p>在 Transformers 中，<strong>分词（tokenization）</strong>
实际上包含以下几个步骤：</p>
<ol type="1">
<li><strong>标准化（Normalization）</strong>：对文本进行必要的清理操作，例如删除多余空格或重音符号、进行
Unicode 标准化等。</li>
<li><strong>预分词（Pre-tokenization）</strong>：将输入拆分为单词。</li>
<li><strong>通过模型处理输入（Running the input through the
model）</strong>：使用预分词后的单词生成一系列词元（tokens）。</li>
<li><strong>后处理（Post-processing）</strong>：添加分词器的特殊标记，生成注意力掩码（attention
mask）和词元类型 ID（token type IDs）。</li>
</ol>
<p>流程图如下</p>
<figure>
<img src="/2025/08/08/%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AE%97%E6%B3%95/tokenizer/image-20250808100051423.png" alt="image-20250808100051423">
<figcaption aria-hidden="true">image-20250808100051423</figcaption>
</figure>
<h4 id="注意力掩码attention-mask和词元类型-id-token-type-ids是什么">注意力掩码（Attention
Mask）和词元类型 ID （Token Type IDs）是什么？</h4>
<figure>
<img src="/2025/08/08/%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AE%97%E6%B3%95/tokenizer/image-20250808100813881.png" alt="image-20250808100813881">
<figcaption aria-hidden="true">image-20250808100813881</figcaption>
</figure>
<p>1️⃣ 注意力掩码（Attention Mask） •
目的：告诉模型“哪些位置可以被看到”，其余位置直接屏蔽。 • 典型场景： –
<strong>自注意力里做 padding 掩码</strong>：把 <code>&lt;pad&gt;</code>
对应的位置设为 −∞，softmax 后权重=0。 –
<strong>解码器自回归掩码</strong>：生成任务用下三角掩码，避免第 i 个
token 看到未来 token。</p>
<p>2️⃣ 词元类型 ID（Token Type IDs，也叫 Segment IDs） •
目的：区分<strong>同一次输入里不同句子或段落</strong>，让模型知道“这段属于
A，那段属于 B”。 • 典型场景： – BERT
做句子对分类（NSP）：<code>[CLS] 句子A [SEP] 句子B [SEP]</code> → TypeID
= 0 0 0 0 1 1 1。 – RoBERTa、GPT 等单句模型则<strong>不需要</strong>
Token Type IDs。</p>
<p><strong>注意力掩码</strong>确保模型只关注实际的词元，忽略填充部分，从而避免无效的计算：</p>
<ul>
<li><strong>1</strong>：表示模型应关注的词元（Tokens）</li>
<li><strong>0</strong>：表示模型应忽略的词元（通常是填充
<code>padding</code> 的部分）。</li>
</ul>
<p><strong>词元类型 ID</strong> 用于区分输入中的不同句子或段落：</p>
<ul>
<li><strong>0</strong>：表示第一个句子的词元。</li>
<li><strong>1</strong>：表示第二个句子的词元。</li>
</ul>
<blockquote>
<p>CLS，SEP，PAD都是什么意思</p>
<p><code>[CLS]</code>（Classification），作用：对应位置的隐藏状态被当作<strong>整句/句对的“整体表示”</strong>，用来接分类头做句子级任务（情感分类、NLI
等）。</p>
<p><code>[SEP]</code>（Separator），作用：让模型知道<strong>分段 /
句子边界</strong>，配合 Token Type IDs 区分句子 A 和句子 B。</p>
<p><code>[PAD]</code>（padding token）的作用是
<strong>批量训练时把不同长度的序列补齐到同一长度</strong>，让张量可以堆叠成规整的矩阵；模型在计算注意力时通过
Attention Mask 把 <code>[PAD]</code> 对应的位置屏蔽掉，不让它们影响有效
token 的表示。</p>
</blockquote>
<h3 id="参考资料">参考资料</h3>
<p><a target="_blank" rel="noopener" href="https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/Guide/21.%20BPE%20vs%20WordPiece：理解%20Tokenizer%20的工作原理与子词分割方法.md">AI-Guide-and-Demos-zh_CN/Guide/21.
BPE vs WordPiece：理解 Tokenizer 的工作原理与子词分割方法.md at master ·
Hoper-J/AI-Guide-and-Demos-zh_CN</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/08/07/college/%E5%A4%A7%E4%B8%89%E4%B8%8A/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/08/07/college/%E5%A4%A7%E4%B8%89%E4%B8%8A/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-title-link" itemprop="url">未命名</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2025-08-07 10:55:13 / 修改时间：10:58:21" itemprop="dateCreated datePublished" datetime="2025-08-07T10:55:13+08:00">2025-08-07</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><a target="_blank" rel="noopener" href="https://zh.d2l.ai/index.html">《动手学深度学习》 —
动手学深度学习 2.0.0 documentation</a></p>
<p><a target="_blank" rel="noopener" href="https://courses.d2l.ai/zh-v2/">课程安排 -
动手学深度学习课程</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/08/06/%E5%AD%A6%E4%B9%A0/agent%E5%AE%9E%E6%88%98/redis%E5%AD%98%E5%82%A8%E7%8A%B6%E6%80%81/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/08/06/%E5%AD%A6%E4%B9%A0/agent%E5%AE%9E%E6%88%98/redis%E5%AD%98%E5%82%A8%E7%8A%B6%E6%80%81/" class="post-title-link" itemprop="url">redis存储状态</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-08-06 00:00:00" itemprop="dateCreated datePublished" datetime="2025-08-06T00:00:00+08:00">2025-08-06</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-08-10 00:00:41" itemprop="dateModified" datetime="2025-08-10T00:00:41+08:00">2025-08-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/agent%E5%AE%9E%E6%88%98/" itemprop="url" rel="index"><span itemprop="name">agent实战</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="为什么用redis">为什么用redis</h3>
<p>Redis通过 RedisSessionManager 类来管理用户会话，存储结构如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">session:&#123;user_id&#125; -&gt; &#123;</span><br><span class="line">  &quot;session_id&quot;: &quot;会话ID&quot;,</span><br><span class="line">  &quot;status&quot;: &quot;idle|running|interrupted|completed|error&quot;,</span><br><span class="line">  &quot;last_response&quot;: &quot;上次智能体响应&quot;,</span><br><span class="line">  &quot;last_query&quot;: &quot;用户上次查询&quot;,</span><br><span class="line">  &quot;last_updated&quot;: &quot;最后更新时间戳&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure>
<img src="/2025/08/06/%E5%AD%A6%E4%B9%A0/agent%E5%AE%9E%E6%88%98/redis%E5%AD%98%E5%82%A8%E7%8A%B6%E6%80%81/image-20250809232853613.png" alt="image-20250809232853613">
<figcaption aria-hidden="true">image-20250809232853613</figcaption>
</figure>
<p>主要功能</p>
<ul>
<li>会话创建与维护 ：为每个用户创建唯一会话，支持会话超时自动清理</li>
<li>状态跟踪
：实时跟踪智能体执行状态（空闲、运行中、中断、完成、错误）</li>
<li>中断恢复支持
：当智能体需要人工干预时，Redis保存中断状态，支持后续恢复执行</li>
<li>用户管理 ：统计活跃用户数量，管理多用户并发访问</li>
</ul>
<p>与PostgreSQL的分工</p>
<ul>
<li>Redis ：负责临时会话状态和实时数据（快速读写）</li>
<li>PostgreSQL
：负责智能体的长期记忆存储（通过LangGraph的checkpointer）</li>
</ul>
<blockquote>
<p>为什么不使用pgsql完成对状态的存储</p>
<p>频繁读写
：会话状态需要频繁更新（每次请求都要更新状态），PostgreSQL的磁盘I/O比Redis内存操作慢很多4</p>
<p>短期记忆（PostgreSQL + LangGraph Checkpointer）</p>
<p>临时状态记忆（Redis）</p>
</blockquote>
<h3 id="redis实现状态存储业务逻辑总览图">redis实现状态存储业务逻辑总览图</h3>
<figure>
<img src="/2025/08/06/%E5%AD%A6%E4%B9%A0/agent%E5%AE%9E%E6%88%98/redis%E5%AD%98%E5%82%A8%E7%8A%B6%E6%80%81/image-20250806164348663.png" alt="image-20250806164348663">
<figcaption aria-hidden="true">image-20250806164348663</figcaption>
</figure>
<p>使用redis的根本逻辑：存储对话的状态，当出现由工具调用或者客户端崩溃导致的中断时，可以存储状态在redis，在开始对话时，通过session_id获取redis的状态，并根据状态判断是要恢复中断还是正常对话</p>
<p>存储的redis（调用invoke_agent接口）：开始（创建）对话时要根据会话user_id获取或创建redis；再调用agent后，根据响应是否存在<strong>status</strong>字段是否是”<strong>interrupt</strong>”，判断是否有终端，最后更新redis状态</p>
<p>恢复的redis（调用resume_agent接口）：获取redis状态，并根据请求的恢复内容，使用Command命令恢复agent，最后更新redis</p>
<figure>
<img src="/2025/08/06/%E5%AD%A6%E4%B9%A0/agent%E5%AE%9E%E6%88%98/redis%E5%AD%98%E5%82%A8%E7%8A%B6%E6%80%81/image-20250809233037563.png" alt="image-20250809233037563">
<figcaption aria-hidden="true">image-20250809233037563</figcaption>
</figure>
<h3 id="redis类">redis类</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># 初始化异步 Redis 连接和会话配置</span><br><span class="line">def __init__(self, redis_host, redis_port, redis_db, session_timeout):</span><br><span class="line">    self.redis_client = redis.Redis(</span><br><span class="line">        host=redis_host,</span><br><span class="line">        port=redis_port,</span><br><span class="line">        db=redis_db,</span><br><span class="line">        decode_responses=True</span><br><span class="line">    )</span><br><span class="line">    self.session_timeout = session_timeout  # 会话过期时间（秒）</span><br><span class="line"></span><br><span class="line"># 关闭 Redis 连接</span><br><span class="line">async def close(self):</span><br><span class="line">    await self.redis_client.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<table>
<colgroup>
<col style="width: 8%">
<col style="width: 16%">
<col style="width: 27%">
<col style="width: 19%">
<col style="width: 27%">
</colgroup>
<thead>
<tr class="header">
<th>方法名</th>
<th>作用</th>
<th>输入参数</th>
<th>返回值</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>__init__</code></td>
<td>建立与 Redis 的异步连接并设置会话超时</td>
<td><code>redis_host</code>, <code>redis_port</code>,
<code>redis_db</code>, <code>session_timeout</code></td>
<td>-</td>
<td><code>decode_responses=True</code> 使 Redis 返回字符串而非字节</td>
</tr>
<tr class="even">
<td><code>close</code></td>
<td>优雅关闭 Redis 连接</td>
<td>-</td>
<td>-</td>
<td>异步方法，需 <code>await</code></td>
</tr>
<tr class="odd">
<td><code>create_session</code></td>
<td>为指定用户新建（或覆盖）会话记录</td>
<td><code>user_id</code>, 可选 <code>session_id</code>,
<code>status</code>, <code>last_query</code>,
<code>last_response</code>, <code>last_updated</code></td>
<td><code>str</code>：生成的 <code>session_id</code></td>
<td>会话键格式：<code>session:&#123;user_id&#125;</code>；过期时间为
<code>session_timeout</code></td>
</tr>
<tr class="even">
<td><code>get_session</code></td>
<td>读取指定用户的完整会话字典</td>
<td><code>user_id</code></td>
<td><code>dict</code> 或 <code>None</code></td>
<td>自动将 JSON 里的 <code>last_response</code> 反序列化为
<code>AgentResponse</code> 对象</td>
</tr>
<tr class="odd">
<td><code>update_session</code></td>
<td>增量更新已有会话的字段</td>
<td><code>user_id</code>, 可选 <code>status</code>,
<code>last_query</code>, <code>last_response</code>,
<code>last_updated</code></td>
<td><code>bool</code>：<code>True</code> 更新成功，<code>False</code>
用户不存在</td>
<td>更新后刷新过期时间</td>
</tr>
<tr class="even">
<td><code>delete_session</code></td>
<td>删除单个用户的会话</td>
<td><code>user_id</code></td>
<td><code>bool</code>：<code>True</code> 删除成功</td>
<td>直接删除 <code>session:&#123;user_id&#125;</code></td>
</tr>
<tr class="odd">
<td><code>get_session_count</code></td>
<td>计算当前活跃会话总数</td>
<td>-</td>
<td><code>int</code></td>
<td>使用异步扫描 <code>session:*</code> 键空间</td>
</tr>
<tr class="even">
<td><code>get_all_user_ids</code></td>
<td>取出所有已创建会话的 <code>user_id</code></td>
<td>-</td>
<td><code>List[str]</code></td>
<td>同样基于 <code>session:*</code> 扫描</td>
</tr>
<tr class="odd">
<td><code>user_id_exists</code></td>
<td>快速判断某用户是否已有会话</td>
<td><code>user_id</code></td>
<td><code>bool</code></td>
<td>利用 <code>EXISTS</code> 命令</td>
</tr>
</tbody>
</table>
<h3 id="安装redis">安装redis</h3>
<h4 id="linux系统">linux系统</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sudo apt update</span><br><span class="line">sudo apt install -y redis-server</span><br><span class="line"># 启动 Redis 服务</span><br><span class="line">sudo service redis-server start</span><br><span class="line"># 检查 Redis 服务状态</span><br><span class="line">sudo service redis-server status</span><br></pre></td></tr></table></figure>
<h4 id="docker">docker</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"># Docker Compose 配置文件，用于启动 Redis 服务</span><br><span class="line"># 该配置为 FastAPI 应用提供 Redis 后端，支持分布式会话管理</span><br><span class="line">version: &#x27;3.8&#x27;</span><br><span class="line"></span><br><span class="line">services:</span><br><span class="line">  redis:</span><br><span class="line">    # 使用官方 Redis 镜像</span><br><span class="line">    image: redis:latest</span><br><span class="line">    # 服务名称</span><br><span class="line">    container_name: redis</span><br><span class="line">    # 映射 Redis 默认端口到主机</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;6379:6379&quot;</span><br><span class="line">    # 持久化存储配置（可选）</span><br><span class="line">    volumes:</span><br><span class="line">      - redis-data:/data</span><br><span class="line">    # 确保容器在重启时自动启动</span><br><span class="line">    restart: unless-stopped</span><br><span class="line">    # 健康检查：验证 Redis 服务是否正常运行</span><br><span class="line">    healthcheck:</span><br><span class="line">      test: [&quot;CMD&quot;, &quot;redis-cli&quot;, &quot;ping&quot;]</span><br><span class="line">      interval: 30s</span><br><span class="line">      timeout: 10s</span><br><span class="line">      retries: 3</span><br><span class="line">      start_period: 10s</span><br><span class="line">    # 网络配置</span><br><span class="line">    networks:</span><br><span class="line">      - app-network</span><br><span class="line"></span><br><span class="line"># 定义持久化存储卷</span><br><span class="line">volumes:</span><br><span class="line">  redis-data:</span><br><span class="line">    name: redis-data</span><br><span class="line"></span><br><span class="line"># 定义网络</span><br><span class="line">networks:</span><br><span class="line">  app-network:</span><br><span class="line">    driver: bridge</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/13/">13</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">张熙浚</span>
  </div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="本站访问数 fa fa-user 次"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="本站总访问量 fa fa-eye 次"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script size="400" alpha="0.6" zIndex="-1" src="https://cdnjs.cloudflare.com/ajax/libs/ribbon.js/1.0.2/ribbon.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.1.0/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script src="/js/pjax.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  <script src="/js/third-party/pace.js"></script>


  
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
