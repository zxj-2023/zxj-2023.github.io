<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha256-XOqroi11tY4EFQMR9ZYwZWKj5ZXiftSx36RRuC3anlA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-bounce.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.20.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="zxj Blogs">
<meta property="og:type" content="website">
<meta property="og:title" content="Zhang XiJun">
<meta property="og:url" content="http://example.com/page/3/index.html">
<meta property="og:site_name" content="Zhang XiJun">
<meta property="og:description" content="zxj Blogs">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="张熙浚">
<meta property="article:tag" content="zxj">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/page/3/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"page/3/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Zhang XiJun</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Zhang XiJun</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">BLOGS</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="张熙浚"
      src="/images/zxjavatar.gif">
  <p class="site-author-name" itemprop="name">张熙浚</p>
  <div class="site-description" itemprop="description">zxj Blogs</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">112</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">40</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">49</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/zxj-2023" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zxj-2023" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="http://wpa.qq.com/msgrd?v=3&uin=2902065320&site=qq&menu=yes" title="QQ → http:&#x2F;&#x2F;wpa.qq.com&#x2F;msgrd?v&#x3D;3&amp;uin&#x3D;2902065320&amp;site&#x3D;qq&amp;menu&#x3D;yes" rel="noopener me" target="_blank"><i class="fab fa-qq fa-fw"></i>QQ</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
    <div class="sidebar-inner sidebar-blogroll">
      <div class="links-of-blogroll animated">
        <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
          链接
        </div>
        <ul class="links-of-blogroll-list">
            <li class="links-of-blogroll-item">
              <a href="https://zxj-2023.github.io/" title="https:&#x2F;&#x2F;zxj-2023.github.io" rel="noopener" target="_blank">Zhang XiJun</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://theme-next.js.org/" title="https:&#x2F;&#x2F;theme-next.js.org" rel="noopener" target="_blank">NexT</a>
            </li>
        </ul>
      </div>
    </div>
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/docker/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/docker/" class="post-title-link" itemprop="url">docker</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-07-08 00:00:00" itemprop="dateCreated datePublished" datetime="2025-07-08T00:00:00+08:00">2025-07-08</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-07-24 16:00:56" itemprop="dateModified" datetime="2025-07-24T16:00:56+08:00">2025-07-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AE%9E%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">实习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/" itemprop="url" rel="index"><span itemprop="name">晨晟智控</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="镜像（Image）"><a href="#镜像（Image）" class="headerlink" title="镜像（Image）"></a>镜像（Image）</h2><p>镜像可以被看作是一个轻量级、可执行的独立软件包，包含了运行某个应用所需的所有代码、库、环境变量和配置文件。它是容器的<strong>静态模板</strong>，在创建容器时用作基础。</p>
<p><strong>只读</strong>：镜像本身是只读的，无法修改。</p>
<p><strong>可重用</strong>：镜像是可以多次重用的，你可以基于相同的镜像创建多个容器。</p>
<h2 id="容器（Container）"><a href="#容器（Container）" class="headerlink" title="容器（Container）"></a>容器（Container）</h2><p>与虚拟机通过操作系统实现隔离不同，容器技术<strong>只隔离应用程序的运行时环境但容器之间可以共享同一个操作系统</strong>，这里的运行时环境指的是程序运行依赖的各种库以及配置。</p>
<p>容器更加的<strong>轻量级且占用的资源更少</strong>，与操作系统动辄几G的内存占用相比，容器技术只需数M空间，因此我们可以在同样规格的硬件上<strong>大量部署容器</strong>，这是虚拟机所不能比拟的，而且不同于操作系统数分钟的启动时间容器几乎瞬时启动，容器技术为<strong>打包服务栈</strong>提供了一种更加高效的方式</p>
<h2 id="镜像与容器的关系"><a href="#镜像与容器的关系" class="headerlink" title="镜像与容器的关系"></a>镜像与容器的关系</h2><p><strong>镜像是静态的</strong>：它只包含应用和运行环境，不能进行任何运行时的操作。你可以把它看作是软件的<strong>安装包</strong>。</p>
<p><strong>容器是动态的</strong>：它是在镜像的基础上创建的，可以运行、执行代码、修改文件系统等。你可以把它看作是镜像的<strong>运行实例</strong>。</p>
<h2 id="docker"><a href="#docker" class="headerlink" title="docker"></a>docker</h2><p>docker将程序以及程序所有的依赖都打包到<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=129800958&amp;content_type=Article&amp;match_order=1&amp;q=docker+container&amp;zhida_source=entity">docker container</a>，这样你的程序可以在任何环境都会有一致的表现</p>
<p>此外docker的另一个好处就是<strong>快速部署</strong>，这是当前互联网公司最常见的一个应用场景，一个原因在于容器启动速度非常快，另一个原因在于只要确保一个容器中的程序正确运行，那么你就能确信无论在生产环境部署多少都能正确运行。</p>
<p>每一种容器都是一个完整的运行环境，容器之间互相隔离。</p>
<p>简单来说，docker将程序打包部署，方便了软件的部署，避免了环境冲突等问题</p>
<h2 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h2><p>查看所有容器（包括停止的容器）：<code>docker ps -a</code></p>
<p>在Docker中运行容器：<code>docker run [OPTIONS] IMAGE [COMMAND] [ARG...]</code></p>
<ul>
<li>• <code>[OPTIONS]</code>：可选参数，用于配置容器的各种选项，如端口映射、容器名称等。</li>
<li>• <code>IMAGE</code>：要运行的镜像名称或ID。</li>
<li>• <code>[COMMAND] [ARG...]</code>：可选的命令和参数，用于在容器内执行特定的命令。</li>
</ul>
<p>停止正在运行的容器：<code>docker stop [OPTIONS] CONTAINER [CONTAINER...]</code></p>
<p>启动已停止的容器：<code>docker start [OPTIONS] CONTAINER [CONTAINER...]</code></p>
<p>删除已停止的容器或镜像：<code>docker rm [OPTIONS] CONTAINER [CONTAINER...]   docker rmi [OPTIONS] IMAGE [IMAGE...]</code></p>
<ul>
<li>• <code>docker rm</code>：删除容器的命令。</li>
<li>• <code>docker rmi</code>：删除镜像的命令。</li>
</ul>
<p>从Docker仓库中拉取现有的镜像：<code>docker pull [OPTIONS] NAME[:TAG|@DIGEST]</code></p>
<ul>
<li>• <code>docker pull</code>：拉取镜像的命令。</li>
<li>• <code>[OPTIONS]</code>：可选参数，用于配置拉取过程，如认证信息等。</li>
<li>• <code>NAME[:TAG|@DIGEST]</code>：要拉取的镜像名称、标签或摘要。</li>
</ul>
<h2 id="docker部分指令"><a href="#docker部分指令" class="headerlink" title="docker部分指令"></a>docker部分指令</h2><p><strong>linux安装docker</strong>：<code>sudo apt-get update &amp;&amp; sudo apt-get install docker.io</code></p>
<p><strong>查看 Docker 版本信息</strong>：<code>docker version</code></p>
<p><strong>查看镜像</strong>：<code>docker images</code></p>
<p><strong>查看所有的容器</strong>：<code>docker ps -a</code> </p>
<blockquote>
<p><code>systemctl</code> 是 <strong>systemd</strong> 系统和服务管理器的核心工具，用于管理系统和服务的状态及配置。</p>
</blockquote>
<p><code>mysql-client</code> 是 MySQL 数据库的命令行客户端工具。它允许你通过命令行连接和操作 MySQL 数据库服务器，比如执行 SQL 查询、管理数据库和用户等。</p>
<p>常用命令格式如下：<code>mysql -h 主机地址 -P 端口号 -u 用户名 -p</code></p>
<p>你可以在终端输入以下命令来检查是否已安装 <code>mysql-client</code>：<code>mysql --version</code></p>
<p>可以使用以下命令安装：<code>sudo apt-get update  sudo apt-get install mysql-client</code></p>
<blockquote>
<p><code>sudo apt-get update</code> 这个命令的作用是<strong>更新本地软件包列表</strong>。</p>
</blockquote>
<p><strong>停止并删除容器</strong>：<code>docker stop fastapi  docker rm fastapi</code></p>
<p><strong>Linux修改镜像源</strong>：<a target="_blank" rel="noopener" href="https://blog.csdn.net/couragehope/article/details/137777158">如何查看docker配置的镜像仓库_查看docker镜像地址-CSDN博客</a></p>
<h2 id="常见参数"><a href="#常见参数" class="headerlink" title="常见参数"></a>常见参数</h2><p>基础参数：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><code>-d</code>或<code>--detach</code></th>
<th>后台运行容器（detached mode）</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>--name &lt;name&gt;</code></td>
<td>为容器指定名称（如<code>--name my_container</code>）</td>
</tr>
<tr>
<td><code>--rm</code></td>
<td>容器停止后自动删除（适用于临时容器）</td>
</tr>
</tbody>
</table>
</div>
<p>端口映射：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><code>-p &lt;主机端口&gt;:&lt;容器端口&gt;</code></th>
<th>映射主机端口到容器端口（如<code>-p 80:80</code>）</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>-p &lt;主机IP&gt;:&lt;主机端口&gt;:&lt;容器端口&gt;</code></td>
<td>指定主机IP绑定（如<code>-p 127.0.0.1:8080:80</code>）</td>
</tr>
<tr>
<td><code>-P</code>或<code>--publish-all</code></td>
<td>自动映射所有暴露的端口（随机分配主机端口）</td>
</tr>
</tbody>
</table>
</div>
<p>卷挂载：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><code>-v &lt;主机路径&gt;:&lt;容器路径&gt;</code></th>
<th>挂载主机目录到容器（如<code>-v //app</code>）</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>-v &lt;卷名&gt;:&lt;容器路径&gt;</code></td>
<td>使用命名卷（如<code>-v my_volume:/data</code>）</td>
</tr>
</tbody>
</table>
</div>
<p>环境变量：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><code>-e &lt;KEY=VALUE&gt;</code></th>
<th>设置环境变量（如<code>-e DEBUG=true</code>）</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>--env-file &lt;文件名&gt;</code></td>
<td>从文件加载环境变量（每行<code>KEY=VALUE</code>）</td>
</tr>
</tbody>
</table>
</div>
<p>网络配置：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><code>--network &lt;网络名&gt;</code></th>
<th>指定容器使用的网络（如<code>--network bridge</code>或自定义网络）</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>--network host</code></td>
<td>使用主机网络（共享主机网络命名空间）</td>
</tr>
</tbody>
</table>
</div>
<h2 id="拯救被wsl占用的内存"><a href="#拯救被wsl占用的内存" class="headerlink" title="拯救被wsl占用的内存"></a>拯救被wsl占用的内存</h2><p>以笔者的情况来说，我的wsl中只有一些必备的开发环境，项目源代码 和 docker。前两者显然没啥可操作的空间，所以只有一个靶子 —— docker。</p>
<p>首先，我们可以进入wsl，通过以下命令，看看 Docker 的磁盘使用情况和资源总量。</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker system df </span><br></pre></td></tr></table></figure>
<p>大家都知道，docker运行一段时间后，可能会产生一些无用的镜像文件。要清理无用的 Docker 镜像，则可以运行以下命令：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker image prune </span><br></pre></td></tr></table></figure>
<p>该命令可以删除所有未被任何容器使用的镜像。如果想清理所有已停止的容器和未使用的镜像：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker system prune -a</span><br></pre></td></tr></table></figure>
<p>执行完后咱们可以再运行第一个命令查看磁盘使用情况，大概率能看到释放了一部分磁盘空间。如果确实长时间为清理过，很大可能可释放几十G。</p>
<p>然而这时候我们退出wsl回到win10, 你可能会看到磁盘空间几乎没啥变化。这是因为wsl还需要我们手动释放这部分空间，即压缩磁盘。</p>
<h2 id="修改docker存储镜像位置"><a href="#修改docker存储镜像位置" class="headerlink" title="修改docker存储镜像位置"></a>修改docker存储镜像位置</h2><p>windows</p>
<p><img src="/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/docker/image-20250709095041821.png" alt="image-20250709095041821"></p>
<p><img src="/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/docker/image-20250709092539404.png" alt="image-20250709092539404"></p>
<p><img src="/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/docker/image-20250709092605183.png" alt="image-20250709092605183"></p>
<p>Linux</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_43412762/article/details/134571411">修改Docker默认镜像和容器存储位置（超详细！！！）_docker更改存储位置-CSDN博客</a></p>
<h2 id="修改镜像源"><a href="#修改镜像源" class="headerlink" title="修改镜像源"></a>修改镜像源</h2><p>查看可用的镜像源<a target="_blank" rel="noopener" href="https://tools.opsnote.top/registry-mirrors/">DockerHub加速器可用性监控</a></p>
<h3 id="如何使用vscode进入远程服务器的docker容器内部调试代码"><a href="#如何使用vscode进入远程服务器的docker容器内部调试代码" class="headerlink" title="如何使用vscode进入远程服务器的docker容器内部调试代码"></a>如何使用vscode进入远程服务器的docker容器内部调试代码</h3><p>安装一下插件</p>
<p><img src="/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/docker/9a3be11a07b5a0866d09d1bcbbaae4dc.png" alt="9a3be11a07b5a0866d09d1bcbbaae4dc"></p>
<p><img src="/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/docker/image-20250716165555700.png" alt="image-20250716165555700"></p>
<h2 id="build，pull与run"><a href="#build，pull与run" class="headerlink" title="build，pull与run"></a>build，pull与run</h2><p><strong><code>docker build</code>：从源代码构建镜像</strong></p>
<ul>
<li><strong>作用</strong>：根据你提供的 <code>Dockerfile</code>（一个包含构建镜像所需指令的文本文件）以及上下文（通常是包含 <code>Dockerfile</code> 的目录及其子目录），<strong>创建</strong>一个新的 Docker 镜像。</li>
</ul>
<p><strong><code>docker pull</code>：从注册中心下载镜像</strong></p>
<ul>
<li><strong>作用</strong>：从 Docker 注册中心（默认是 Docker Hub，也可以是私有注册中心如 Harbor, GitLab Registry, AWS ECR 等）<strong>下载</strong>一个已经构建好的 Docker 镜像到你的本地机器。</li>
</ul>
<p><strong><code>docker run</code>：创建并启动容器</strong></p>
<ul>
<li><strong>作用</strong>：基于一个<strong>本地已有的镜像</strong>（无论这个镜像是你刚 <code>build</code> 出来的，还是 <code>pull</code> 下来的，或是之前就存在的），<strong>创建</strong>一个新的容器实例，并按照指定的命令（或镜像默认的命令）<strong>启动</strong>它。</li>
</ul>
<h2 id="docker的自定义网络"><a href="#docker的自定义网络" class="headerlink" title="docker的自定义网络"></a>docker的自定义网络</h2><p>创建自定义桥接网络</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker network create mynet</span><br></pre></td></tr></table></figure>
<p>启动服务容器（不映射宿主机端口也能被同网络容器访问）</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --name api --network demo-net fastapi-svc</span><br></pre></td></tr></table></figure>
<p>列出所有网络（包括自定义网络）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker network ls</span><br><span class="line"></span><br><span class="line">#只列出自定义网络（过滤掉默认网络）</span><br><span class="line">docker network ls --filter type=custom</span><br></pre></td></tr></table></figure>
<p>查看某个自定义网络的详细信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker network inspect network_test </span><br></pre></td></tr></table></figure>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1ai421S7zj/?spm_id_from=333.337.search-card.all.click&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">改变软件行业的技术！程序员、软件爱好者必须掌握的Docker，到底是什么？_哔哩哔哩_bilibili</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/187505981">什么是Docker？看这一篇干货文章就够了！ - 知乎</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/Python_0011/article/details/140313812">Docker常用命令大全（非常详细）零基础入门到精通，收藏这一篇就够了-CSDN博客</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1THKyzBER6/?spm_id_from=333.337.search-card.all.click&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">40分钟的Docker实战攻略，一期视频精通Docker_哔哩哔哩_bilibili</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/mineru/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/mineru/" class="post-title-link" itemprop="url">MinerU</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-07-08 00:00:00" itemprop="dateCreated datePublished" datetime="2025-07-08T00:00:00+08:00">2025-07-08</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-07-30 10:43:13" itemprop="dateModified" datetime="2025-07-30T10:43:13+08:00">2025-07-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AE%9E%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">实习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/" itemprop="url" rel="index"><span itemprop="name">晨晟智控</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="docker部署"><a href="#docker部署" class="headerlink" title="docker部署"></a>docker部署</h3><p>使用dockerfile构建镜像：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget https://gcore.jsdelivr.net/gh/opendatalab/MinerU@master/docker/china/Dockerfile</span><br><span class="line">docker build -t mineru-sglang:latest -f Dockerfile .</span><br></pre></td></tr></table></figure>
<p>使用<code>wget https://gcore.jsdelivr.net/gh/opendatalab/MinerU @master/docker/china/Dockerfile -O Dockerfile</code>将指定的 Dockerfile 下载到本地</p>
<p>Dockerfile：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># 使用官方的 sglang 镜像作为基础镜像</span><br><span class="line">FROM lmsysorg/sglang:v0.4.9-cu126</span><br><span class="line"></span><br><span class="line"># 安装 OpenCV 依赖库</span><br><span class="line">RUN apt-get update &amp;&amp; apt-get install -y libgl1 &amp;&amp; apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/*</span><br><span class="line"></span><br><span class="line"># 安装 mineru Python 包</span><br><span class="line">RUN python3 -m pip install -U &#x27;mineru[core]&#x27; -i https://mirrors.aliyun.com/pypi/simple --break-system-packages</span><br><span class="line"></span><br><span class="line"># 下载模型并配置</span><br><span class="line">RUN /bin/bash -c &quot;mineru-models-download -s modelscope -m all&quot;</span><br><span class="line"></span><br><span class="line"># 设置容器入口命令</span><br><span class="line">ENTRYPOINT [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;export MINERU_MODEL_SOURCE=local &amp;&amp; exec \&quot;$@\&quot;&quot;, &quot;--&quot;]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>SGLang（全称可能为 <strong>Serving Large Language Models with Golang</strong> ）是由斯坦福大学研究团队开发的一个<strong>高效的大语言模型（LLM）推理服务框架</strong> ，旨在通过优化模型推理过程，显著提升生成式AI服务的吞吐量和响应速度。</p>
<ul>
<li><strong>SGlang 版本</strong> ：<code>v0.4.8.post1</code>（SGlang 是一个用于大语言模型（LLM）推理和服务的高性能框架）。</li>
<li><strong>CUDA 版本</strong> ：<code>cu126</code> 表示使用 <strong>CUDA 12.6</strong> ，适用于 <strong>Turing/Ampere/Ada Lovelace/Hopper 架构的 GPU</strong> （如 RTX 30/40 系列、A100、H100）。</li>
</ul>
</blockquote>
<h4 id="报错排查"><a href="#报错排查" class="headerlink" title="报错排查"></a>报错排查</h4><p>之前由于默认dockerfile内容为<code>FROM lmsysorg/sglang:v0.4.8.post1-cu126</code>报错</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lmsysorg/sglang:v0.4.8.post1-cu126: failed to resolve source metadata for docker.io/lmsysorg/sglang:v0.4.8.post1-cu126: unexpected status from HEAD request to https://yaj2teeh.mirror.aliyuncs.com/v2/lmsysorg/sglang/manifests/v0.4.8.post1-cu126?ns=docker.io: 403 Forbidden</span><br></pre></td></tr></table></figure>
<p>之前以为是sglang版本问题，然后去dockerhub上查找，并通过<code>docker pull sglang:v0.4.8.post1-cu126</code>测试，是可以拉取的，最后认为原因还是网络问题</p>
<p>解决方法，更换了镜像源</p>
<h4 id="镜像源配置"><a href="#镜像源配置" class="headerlink" title="镜像源配置"></a>镜像源配置</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&quot;registry-mirrors&quot;: [</span><br><span class="line">  &quot;https://registry.docker-cn.com&quot;,</span><br><span class="line">  &quot;http://hub-mirror.c.163.com&quot;,</span><br><span class="line">  &quot;https://dockerhub.azk8s.cn&quot;,</span><br><span class="line">  &quot;https://mirror.ccs.tencentyun.com&quot;,</span><br><span class="line">  &quot;https://registry.cn-hangzhou.aliyuncs.com&quot;,</span><br><span class="line">  &quot;https://docker.mirrors.ustc.edu.cn&quot;,</span><br><span class="line">  &quot;https://docker.m.daocloud.io&quot;,  </span><br><span class="line">  &quot;https://noohub.ru&quot;, </span><br><span class="line">  &quot;https://huecker.io&quot;,</span><br><span class="line">  &quot;https://dockerhub.timeweb.cloud&quot; </span><br><span class="line"> ]</span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1xHA3euEcn/?spm_id_from=333.337.search-card.all.click&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">保姆级Docker安装+镜像加速 计算机系必备技能_哔哩哔哩_bilibili</a></p>
<h4 id="为什么要指定基础镜像"><a href="#为什么要指定基础镜像" class="headerlink" title="为什么要指定基础镜像"></a>为什么要指定基础镜像</h4><ul>
<li><strong>提供操作系统和依赖</strong><br>基础镜像包含操作系统（如 Ubuntu、Alpine）、运行时环境（如 Python、Node.js）或框架（如 TensorFlow、PyTorch）等核心组件，后续所有操作（如安装依赖、拷贝文件）都基于此环境。<ul>
<li>例如：<code>FROM python:3.9</code> 提供了 Python 3.9 的运行环境，后续可以直接用 <code>pip install</code> 安装 Python 包。</li>
</ul>
</li>
<li><strong>避免重复造轮子</strong><br>如果直接从空镜像（<code>scratch</code>）开始，需要手动安装所有依赖，效率低下且容易出错。使用现有基础镜像可以复用已验证的环境配置。</li>
</ul>
<h4 id="确认支持的cuda版本"><a href="#确认支持的cuda版本" class="headerlink" title="确认支持的cuda版本"></a>确认支持的cuda版本</h4><p>命令<code>nvidia-smi</code></p>
<p><img src="/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/mineru/image-20250708160948192.png" alt="image-20250708160948192"></p>
<p><strong>CUDA Version</strong> 显示当前驱动支持的最高 CUDA 版本</p>
<h4 id="问题：使用dockerfile直接部署，始终出现网络问题"><a href="#问题：使用dockerfile直接部署，始终出现网络问题" class="headerlink" title="问题：使用dockerfile直接部署，始终出现网络问题"></a>问题：使用dockerfile直接部署，始终出现网络问题</h4><p>解决方案</p>
<p>先修改了一下docker储存镜像的位置，太大了</p>
<p>先拉取基础镜像<code>docker pull lmsysorg/sglang:v0.4.8.post1-cu126</code></p>
<p>再使用<code>docker build -t mineru-sglang:latest -f Dockerfile .</code>，可以直接跳过基础镜像的拉取</p>
<h3 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h3><p>官方启动命令</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">docker run -d \</span><br><span class="line">  --name sglang-server \          # 容器命名（便于管理）</span><br><span class="line">  --gpus all \                   # 启用所有GPU</span><br><span class="line">  --shm-size 32g \               # 共享内存大小</span><br><span class="line">  -p 30000:30000 \               # 端口映射（主机端口:容器端口）</span><br><span class="line">  --ipc=host \                   # 共享主机IPC命名空间</span><br><span class="line">  mineru-sglang:latest \</span><br><span class="line">  mineru-sglang-server --host 0.0.0.0 --port 30000</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --name sglang-server --gpus all --shm-size 32g -p 30000:30000 --ipc=host mineru-sglang:latest mineru-sglang-server --host 0.0.0.0 --port 30000</span><br></pre></td></tr></table></figure>
<blockquote>
<p>将mineru-sglang-server暴露到30000端口的作用</p>
<p>为了支持 vlm-sglang-client 后端模式，使得MinerU客户端可以通过网络连接到这个服务器，实现多个客户端可以同时连接到同一个服务器</p>
</blockquote>
<p>使用<code>docker exec -it sglang-server bash</code>命令进入容器</p>
<p>或</p>
<p>使用docker desk</p>
<p><img src="/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/mineru/image-20250709152627120.png" alt="image-20250709152627120"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --name mineru-server --gpus all --shm-size 32g -p 30000:30000 -p 7860:7860 -p 8000:8000 --ipc=host \</span><br><span class="line">-v &quot;F:/project python/实习/mineru/demo/pdfs:/pdfs&quot; \</span><br><span class="line">-v &quot;F:/project python/实习/mineru/output:/output&quot; \</span><br><span class="line">mineru-sglang:latest \</span><br><span class="line">mineru-sglang-server --host 0.0.0.0 --port 30000</span><br></pre></td></tr></table></figure>
<p>使用挂载卷启动</p>
<ul>
<li>将本地的PDF文件目录挂载到容器内的 /pdfs 目录</li>
<li>将本地的输出目录挂载到容器内的 /output 目录</li>
<li>把8000，和7860端口暴露，方便调用fastapi与gradio webui 可视化</li>
</ul>
<p><img src="/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/mineru/image-20250710100047464.png" alt="image-20250710100047464"></p>
<p><img src="/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/mineru/image-20250710103505453.png" alt="image-20250710103505453"></p>
<h3 id="调用"><a href="#调用" class="headerlink" title="调用"></a>调用</h3><h4 id="命令行调用sglang-server-client-模式"><a href="#命令行调用sglang-server-client-模式" class="headerlink" title="命令行调用sglang-server/client 模式"></a>命令行调用sglang-server/client 模式</h4><p><code>docker exec mineru-server mineru -p /pdfs/demo1.pdf -o /output -b vlm-sglang-client -u http://localhost:30000</code></p>
<p>这条命令在名为 mineru-server 的容器内执行 mineru 工具，处理 /pdfs 目录下的 demo1.pdf 文件，输出结果到 /output 目录，使用 vlm-sglang-client 后端，并连接到 <a target="_blank" rel="noopener" href="http://localhost:30000">http://localhost:30000</a> 的SGLang服务器。</p>
<p><img src="/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/mineru/image-20250710103615272.png" alt="image-20250710103615272"></p>
<h4 id="fastapi调用与gradio-webui-可视化"><a href="#fastapi调用与gradio-webui-可视化" class="headerlink" title="fastapi调用与gradio webui 可视化"></a>fastapi调用与gradio webui 可视化</h4><p>在完成docker的端口映射之后，运行<code>mineru-api --host 0.0.0.0 --port 8080</code>启动fastapi服务，</p>
<blockquote>
<p>FastAPI服务的使用场景</p>
<p>FastAPI服务提供了一个<code>/file_parse</code>端点，用于处理PDF和图像文件的解析请求</p>
<p>微服务架构部署，FastAPI服务可以独立部署</p>
<p>服务提供了标准的HTTP API接口，允许客户端通过网络请求进行文档解析</p>
</blockquote>
<p>运行<code>mineru-gradio --server-name 0.0.0.0 --server-port 7860</code>启动gradio webui服务</p>
<p>或<code>mineru-gradio --server-name 0.0.0.0 --server-port 7860 --enable-sglang-engine true</code></p>
<blockquote>
<p>注意，模型下载需要配置环境变量</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 在容器内设置环境变量</span><br><span class="line">export MINERU_MODEL_SOURCE=local</span><br><span class="line"># 验证环境变量是否设置成功</span><br><span class="line">echo $MINERU_MODEL_SOURCE</span><br></pre></td></tr></table></figure>
</blockquote>
<p><img src="/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/mineru/115678648318f55f1fe3a5baaeac2aaf.png" alt="115678648318f55f1fe3a5baaeac2aaf"></p>
<p><strong>在调用过程中关于端口的问题与思考</strong></p>
<p>调用过程中发现，在容器中使用<code>mineru-api --host 127.0.0.1 --port 8000</code>，宿主机无法访问<code>http://127.0.0.1:8000/docs/</code>，经过查询ai，命令改为<code>mineru-api --host 0.0.0.0 --port 8000</code>就可以正常访问，那么关键在于对这两个地址的理解</p>
<blockquote>
<p>查看端口<code>netstat -ano | findstr LISTENING</code></p>
</blockquote>
<p>127.0.0.1与0.0.0.0</p>
<ul>
<li>127.0.0.1 (localhost) ：仅表示本机回环地址，只能在 同一设备内 访问</li>
<li>0.0.0.0 ：表示监听所有可用的网络接口，允许 来自任何地址 的连接</li>
</ul>
<p>当您在Docker容器内运行服务时：</p>
<ol>
<li><p>使用127.0.0.1作为绑定地址 ：</p>
<ul>
<li>服务只接受来自容器内部的连接</li>
<li>即使您映射了端口，宿主机也无法访问该服务</li>
<li>只有容器内的应用程序可以通过 127.0.0.1:端口 访问</li>
</ul>
</li>
<li><p>使用0.0.0.0作为绑定地址 ：</p>
<ul>
<li>服务接受来自任何网络接口的连接请求</li>
<li>允许从容器外部（包括宿主机）访问该服务</li>
<li>当您映射端口时（如 -p 8000:8000 ），宿主机可以通过 localhost:8000 或 127.0.0.1:8000 访问</li>
</ul>
</li>
</ol>
<p><strong>为什么需要在容器内使用0.0.0.0</strong></p>
<p>在Docker环境中，容器有自己独立的网络命名空间，这意味着容器内的 127.0.0.1 与宿主机的 127.0.0.1 是完全不同的两个环境。因此：</p>
<ul>
<li>当您在容器内使用 —host 0.0.0.0 启动服务时，该服务会监听容器的所有网络接口</li>
<li>当您在宿主机上访问 127.0.0.1:映射端口 时，Docker会将请求转发到容器内监听在 0.0.0.0:容器端口 的服务</li>
</ul>
<h3 id="mineru相关知识"><a href="#mineru相关知识" class="headerlink" title="mineru相关知识"></a>mineru相关知识</h3><h4 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Usage: mineru [OPTIONS]</span><br><span class="line"></span><br><span class="line">Options:</span><br><span class="line">  -v, --version                   显示版本并退出</span><br><span class="line">  -p, --path PATH                 输入文件路径或目录（必填）</span><br><span class="line">  -o, --output PATH               输出目录（必填）</span><br><span class="line">  -m, --method [auto|txt|ocr]     解析方法：auto（默认）、txt、ocr（仅用于 pipeline 后端）</span><br><span class="line">  -b, --backend [pipeline|vlm-transformers|vlm-sglang-engine|vlm-sglang-client]</span><br><span class="line">                                  解析后端（默认为 pipeline）</span><br><span class="line">  -l, --lang [ch|ch_server|ch_lite|en|korean|japan|chinese_cht|ta|te|ka|latin|arabic|east_slavic|cyrillic|devanagari]</span><br><span class="line">                                  指定文档语言（可提升 OCR 准确率，仅用于 pipeline 后端）</span><br><span class="line">  -u, --url TEXT                  当使用 sglang-client 时，需指定服务地址</span><br><span class="line">  -s, --start INTEGER             开始解析的页码（从 0 开始）</span><br><span class="line">  -e, --end INTEGER               结束解析的页码（从 0 开始）</span><br><span class="line">  -f, --formula BOOLEAN           是否启用公式解析（默认开启）</span><br><span class="line">  -t, --table BOOLEAN             是否启用表格解析（默认开启）</span><br><span class="line">  -d, --device TEXT               推理设备（如 cpu/cuda/cuda:0/npu/mps，仅 pipeline 后端）</span><br><span class="line">  --vram INTEGER                  单进程最大 GPU 显存占用(GB)（仅 pipeline 后端）</span><br><span class="line">  --source [huggingface|modelscope|local]</span><br><span class="line">                                  模型来源，默认 huggingface</span><br><span class="line">  --help                          显示帮助信息</span><br></pre></td></tr></table></figure>
<h4 id="后端的区别"><a href="#后端的区别" class="headerlink" title="后端的区别"></a>后端的区别</h4><p>pipeline (默认后端) :</p>
<ul>
<li>含义 : 这是 MinerU 的默认后端，它使用本地安装的 mineru 库来执行文档解析任务。它通常不依赖于外部的 VLM（视觉语言模型）服务，而是直接在本地处理 PDF 文件。</li>
</ul>
<p><img src="/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/mineru/10d6d1a7f702c5806e29ee7b1c51283.png" alt="10d6d1a7f702c5806e29ee7b1c51283"></p>
<p>vlm-transformers :</p>
<ul>
<li>含义 : 这个后端利用 Hugging Face transformers 库中提供的 VLM 模型进行文档分析。它会在本地加载并运行一个基于 transformers 的 VLM 模型来处理 PDF 中的视觉信息和文本内容。</li>
</ul>
<blockquote>
<p>VLM（Vision-Language Model，视觉语言模型）是一种结合计算机视觉和自然语言处理能力的多模态人工智能模型。</p>
<p>OCR 是 Optical Character Recognition（光学字符识别）的缩写。它是一种技术，用于将图像中的手写、打印或打字文本转换为机器编码的文本，使其可以被计算机编辑、搜索、存储和处理。</p>
</blockquote>
<p>vlm-sglang-engine :</p>
<ul>
<li>含义 : 这个后端表示 MinerU 将直接集成并使用 SGLang 引擎进行 VLM 推理。SGLang 是一个高性能的推理引擎，旨在优化大型语言模型（LLM）和 VLM 的推理速度和效率。在这种模式下，SGLang 引擎作为 MinerU 进程的一部分运行。</li>
</ul>
<p>vlm-sglang-client :</p>
<ul>
<li>含义 : 这个后端表示 MinerU 作为 SGLang 服务器的客户端。在这种模式下，MinerU 不会直接运行 VLM 模型，而是将 PDF 处理请求发送到一个独立的 SGLang 服务器（通过 -u 参数指定的 URL，例如 <a target="_blank" rel="noopener" href="http://localhost:30000">http://localhost:30000</a> ）。SGLang 服务器负责执行实际的 VLM 推理，并将结果返回给 MinerU 客户端。</li>
</ul>
<blockquote>
<p>用场景 : 这是我们之前讨论的 Docker 容器部署场景中推荐的模式。它非常适合以下情况：</p>
<ul>
<li>资源隔离 : 将 VLM 推理的计算密集型任务从 MinerU 主进程中分离出来，允许独立扩展和管理 SGLang 服务器。</li>
<li>集中管理 : 可以在一个或多个 SGLang 服务器上集中管理 VLM 模型，供多个 MinerU 客户端共享使用。</li>
<li>性能优化 : SGLang 服务器可以针对 VLM 推理进行专门优化，提供更好的吞吐量和延迟。</li>
<li>灵活部署 : SGLang 服务器可以部署在不同的机器上，甚至作为微服务运行，提供更大的部署灵活性。</li>
</ul>
</blockquote>
<h4 id="关于吞吐量的相关知识"><a href="#关于吞吐量的相关知识" class="headerlink" title="关于吞吐量的相关知识"></a>关于吞吐量的相关知识</h4><p><strong>“吞吐量”（throughput）\</strong>指的是系统在单位时间内能处理的 **Token 数量**，单位通常是 **tokens/秒**。这个指标衡量的是整体系统*<em>处理并发请求的能力*</em>，而不仅仅是单个请求的速度。</p>
<p>SGLang 支持两种主要并行方式来提升吞吐量：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>并行类型</th>
<th>作用</th>
<th>对吞吐量的影响</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>张量并行（TP）</strong></td>
<td>把模型权重切分到多张卡上，<strong>减少单卡负载</strong></td>
<td>提升单请求处理能力，<strong>但通信开销大</strong></td>
</tr>
<tr>
<td><strong>数据并行（DP）</strong></td>
<td>把不同请求分发到不同卡上，<strong>并行处理多个请求</strong></td>
<td>直接提升并发吞吐量，<strong>尤其适合高并发场景</strong></td>
</tr>
</tbody>
</table>
</div>
<h3 id="部署服务器并运行"><a href="#部署服务器并运行" class="headerlink" title="部署服务器并运行"></a>部署服务器并运行</h3><p>load镜像</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker load -i mineru-sglang-latest.tar</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">free -h</span><br><span class="line">nvidia-smi</span><br></pre></td></tr></table></figure>
<p><img src="/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/mineru/image-20250716092152823.png" alt="image-20250716092152823"></p>
<p>每秒刷新<code>watch -n1 nvidia-smi          # 每秒刷新</code></p>
<p>查看Linux路径</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pwd </span><br></pre></td></tr></table></figure>
<p>启动容器</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --name mineru-server --gpus all --shm-size 32g -p 30000:30000 -p 7860:7860 -p 8000:8000 --ipc=host \</span><br><span class="line">-v &quot;/aisys/repo_dev/xizhang/pdfs:/pdfs&quot; \</span><br><span class="line">-v &quot;/aisys/repo_dev/xizhang/outputs:/output&quot; \</span><br><span class="line">mineru-sglang:latest \</span><br><span class="line">mineru-sglang-server --host 0.0.0.0 --port 30000</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --name mineru-server --gpus all --shm-size 32g -p 30000:30000 --ipc=host -v &quot;/aisys/repo_dev/xizhang/pdfs:/pdfs&quot; -v &quot;/aisys/repo_dev/xizhang/outputs:/output&quot; mineru-sglang:latest mineru-sglang-server --host 0.0.0.0 --port 30000</span><br></pre></td></tr></table></figure>
<p>调用</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker exec mineru-server mineru -p /pdfs/demo1.pdf -o /output -b vlm-sglang-client -u http://localhost:30000`</span><br></pre></td></tr></table></figure>
<p>进入容器</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker exec -it mineru-server /bin/bash</span><br></pre></td></tr></table></figure>
<p>使用pipline解析后端模式</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mineru -p /pdfs/demo1.pdf -o /output --source local</span><br></pre></td></tr></table></figure>
<p>使用sglang加速推理</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=1,2,3 mineru -p /pdfs/small_ocr.pdf -o /output -b vlm-sglang-engine --source local</span><br></pre></td></tr></table></figure>
<blockquote>
<p>vlm模式同样可以处理扫描件</p>
</blockquote>
<p>使用ocr解析扫描件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mineru -p /pdfs/small_ocr.pdf -o /output --source local -m ocr</span><br></pre></td></tr></table></figure>
<p>增加推理设备</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mineru -p /pdfs/small_ocr.pdf -o /output --source local -m ocr -d cuda</span><br></pre></td></tr></table></figure>
<p>通过在命令行的开头添加<code>CUDA_VISIBLE_DEVICES</code> 环境变量来指定可见的 GPU 设备。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=1,2,3 mineru -p /pdfs/small_ocr.pdf -o /output --source local -m ocr</span><br></pre></td></tr></table></figure>
<p>使用sglang加速模式的多GPU并行</p>
<p>数据并行（dp-size）和张量并行（tp-size）</p>
<p>MinerU支持通过sglang的多GPU并行模式来提升推理速度。您可以使用以下参数：</p>
<ul>
<li><code>--dp-size</code>: 数据并行，通过多卡同时处理多个输入来增加吞吐量</li>
<li><code>--tp-size</code>: 张量并行，将模型分布到多张GPU上以扩展可用显存</li>
</ul>
<blockquote>
<p>如果您已经可以正常使用sglang对vlm模型进行加速推理，但仍然希望进一步提升推理速度，可以尝试以下参数：</p>
<ul>
<li>如果您有超过多张显卡，可以使用sglang的多卡并行模式来增加吞吐量：<code>--dp-size 2</code></li>
<li>同时您可以启用<code>torch.compile</code>来将推理速度加速约15%：<code>--enable-torch-compile</code></li>
</ul>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=1,2,3 mineru -p /pdfs -o /output -b vlm-sglang-engine --source local --dp-size 3 --enable-torch-compile</span><br></pre></td></tr></table></figure>
<p>将python文件上传docker并运行</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker cp demo.py mineru-server:/demo.py</span><br><span class="line"></span><br><span class="line">docker exec mineru-server python /demo.py</span><br><span class="line"></span><br><span class="line">#删除</span><br><span class="line">rm -i demo.py</span><br></pre></td></tr></table></figure>
<p>添加自定义网络，修改挂载卷</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --name mineru-server --gpus all --shm-size 32g -p 30000:30000 --ipc=host -v /aisys/:/aisys/ --network network_test mineru-sglang:latest mineru-sglang-server --host 0.0.0.0 --port 30000</span><br></pre></td></tr></table></figure>
<p>后面才知道，上面这个命令会自动启动sglang-server服务</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --name mineru-server --gpus all --shm-size 32g -p 30000:30000 --ipc=host -v /aisys/:/aisys/ --network network_test mineru-sglang:latest tail -f /dev/null</span><br><span class="line">docker start mineru-server</span><br></pre></td></tr></table></figure>
<p>使用上面这个命令启动容器，但不启动sglang-server服务，使用下面指令手动启动</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker exec -it mineru-server /bin/bash</span><br><span class="line">MINERU_MODEL_SOURCE=local CUDA_VISIBLE_DEVICES=1,2,3 mineru-sglang-server --port 30000 --dp-size 3 --enable-torch-compile</span><br></pre></td></tr></table></figure>
<p>启动服务后在另一个容器尝试访问</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl http://mineru-server:30000/get_model_info</span><br></pre></td></tr></table></figure>
<p>在另一个容器使用服务</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mineru -p /test -o / -b vlm-sglang-client -u http://mineru-server:30000</span><br></pre></td></tr></table></figure>
<hr>
<p>启动fastapi服务</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MINERU_MODEL_SOURCE=local CUDA_VISIBLE_DEVICES=1,2,3 mineru-api --host 0.0.0.0 --port 30000 --dp-size 3 --enable-torch-compile</span><br></pre></td></tr></table></figure>
<p>在另一个容器验证</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl http://mineru-server:30000/openapi.json</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 在容器内设置环境变量</span><br><span class="line">export CUDA_VISIBLE_DEVICES=1,2,3</span><br><span class="line"># 验证环境变量是否设置成功</span><br><span class="line">echo $CUDA_VISIBLE_DEVICES</span><br><span class="line"></span><br><span class="line"># 在容器内设置环境变量</span><br><span class="line">export MINERU_MODEL_SOURCE=local</span><br><span class="line"># 验证环境变量是否设置成功</span><br><span class="line">echo $MINERU_MODEL_SOURCE</span><br></pre></td></tr></table></figure>
<h3 id="资料"><a href="#资料" class="headerlink" title="资料"></a>资料</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/liuzhenghua66/article/details/148980203">MinerU 2.0部署-CSDN博客</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/opendatalab/MinerU?tab=readme-ov-file#local-deployment">https://github.com/opendatalab/MinerU?tab=readme-ov-file#local-deployment</a> </p>
<p><a target="_blank" rel="noopener" href="https://deepwiki.com/opendatalab/MinerU">https://deepwiki.com/opendatalab/MinerU</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/%E6%97%A5%E5%BF%97/7.8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/%E6%97%A5%E5%BF%97/7.8/" class="post-title-link" itemprop="url">实习日志</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-07-08 00:00:00" itemprop="dateCreated datePublished" datetime="2025-07-08T00:00:00+08:00">2025-07-08</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-07-30 09:11:25" itemprop="dateModified" datetime="2025-07-30T09:11:25+08:00">2025-07-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AE%9E%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">实习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/" itemprop="url" rel="index"><span itemprop="name">晨晟智控</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="日志"><a href="#日志" class="headerlink" title="日志"></a>日志</h3><p>7.16 10：46</p>
<p>已经将mineru部署至服务器，完成pdf和扫描件的提取测试，但是mineru是没办法直接提取doc与docx的，能不能直接对doc与docx进行文本分块，或者先转换成pdf再提取（官方给出的解决方案：通过独立部署的LibreOffice服务先行转换为PDF格式，再进行后续解析操作。）</p>
<p>7.17 11：04</p>
<p>完成mineru脚本的编写，仅输出提取的md与image；实现调用多块gpu；实现数据并行，通过多卡同时处理多个输入来增加吞吐量；使用sglang框架</p>
<p>7.18 14：37</p>
<p>发现mineru提取的markdown文档是不带多级标题的，只有一级标题，所以不考虑文档结构分块；语义分块要调用embedding模型，而且文档里有很多表格，我感觉效果不一定会好；后续我想法是使用递归分块，表格在md文档中以html表格格式存储的，大量冗余信息，我想先对其进行预处理，转换成md表格的形式吧（用“ |”存储），然后再分块，或许效果会好一些，正在进行</p>
<p>还有一个就是libreoffice是部署在哪里，我没有找到诶</p>
<p>7.19 10：26</p>
<p>完成表格预处理的脚本编写，完成对md文档的预处理；完成递归分块，后续完成存入es数据库</p>
<p>7.21</p>
<p>完成简单地将分块结果存入elasticsearch（mapping只有content字段，使用http请求存的，没有用langchain-elasticsearch）后面要试试langchain-elasticsearch，去看看字段的处理</p>
<p>7.22</p>
<p>完成langchain-elasticsearch的bm25的检索测试，接下来尝试使用服务器的embedding模型进行测试，阅读项目文件，理清思路，具体内容见<a target="_blank" rel="noopener" href="https://icnrn6ghqrgx.feishu.cn/wiki/MWZ9wVYRxixABqkMhjQcQPqTnfe">多模态 - PDF表格图片&amp;扫描件</a>，存入es与文件处理字段处理部分已经理清了，成功存入服务器的es</p>
<p>7.23</p>
<p>今天在把之前做的所有工作进行整合，编写一个完整的代码，实现生产的流程，遇到的主要问题有两个：1.我还是没有很看懂之前对字段的存储，和字段的结构2.我没有太理解pdf分块的部分在哪里，是没有吗，我看doc的rewrite_word是有文本分块的</p>
<p>感觉需要你给我讲一下，不然我后面不大知道该怎么处理，那我明天去把语义分块和其他的检索方案试一下吧，整合代码先稍微延后</p>
<p>7.24</p>
<p>使用docker的自定义网络，实现容器之间的通信，从而可以在repo容器中调用mineru，其他容器都在network_test下，我把mineru也启动到里面</p>
<p>成功在repo容器访问到mineru容器，完成pdf分块数据的批量写入elasticsearch并可以成功检索，后续完成调用mineru的fastapi接口（还没编完），继续完成代码整合（已经完成大部分，主要差mineru的部分，和一些衔接的代码）</p>
<p>7.25</p>
<p>完成通过接口使用mineru批量处理pdf文件</p>
<p>7.28</p>
<p>完成了代码整合，可以完成pdf整个流程的处理</p>
<p>7.29</p>
<p>补充了libreoffice的代码，实现了将doc，docx转换成pdf，统一处理流程；完成了数据标记web；后续计划：根据问题检索url，处理后存入zxj_test es数据库，再进行进一步检索产生数据集</p>
<p>7.30</p>
<p>完成扫描件的测试，文字公式图片皆可正常识别</p>
<h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><h4 id="vscode连接远程服务器"><a href="#vscode连接远程服务器" class="headerlink" title="vscode连接远程服务器"></a>vscode连接远程服务器</h4><ol>
<li>输入 ssh root@10.117.128.50</li>
<li>输入密码 think123@</li>
</ol>
<h5 id="使用旧版remotessh"><a href="#使用旧版remotessh" class="headerlink" title="使用旧版remotessh"></a>使用旧版remotessh</h5><p><img src="/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/%E6%97%A5%E5%BF%97/7.8/image-20250708104452761.png" alt="image-20250708104452761"></p>
<blockquote>
<p>原因：可能因为内网，服务器那边没有进行更新，所以新版的remotessh无法连接</p>
<p>其他问题：可能由于上述原因，trae也无法连接，并且由于trae的远程连接插件无法更改版本，因此无法使用</p>
</blockquote>
<h5 id="虚拟环境创建"><a href="#虚拟环境创建" class="headerlink" title="虚拟环境创建"></a>虚拟环境创建</h5><p><img src="/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/%E6%97%A5%E5%BF%97/7.8/image-20250708104254347.png" alt="image-20250708104254347"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">python3 -m venv .venv</span><br><span class="line"></span><br><span class="line">which python</span><br><span class="line"></span><br><span class="line">#激活虚拟环境</span><br><span class="line"> .venv\Scripts\activate</span><br><span class="line"> source .venv/bin/activate</span><br><span class="line"> </span><br><span class="line"> #停掉虚拟环境</span><br><span class="line"> deactivate</span><br></pre></td></tr></table></figure>
<h4 id="git连接远程仓库"><a href="#git连接远程仓库" class="headerlink" title="git连接远程仓库"></a>git连接远程仓库</h4><p>初始化仓库：<code>git init</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 添加所有文件到暂存区</span><br><span class="line">git add .</span><br><span class="line"></span><br><span class="line"># 提交更改</span><br><span class="line">git commit -m &quot;Initial commit&quot;</span><br><span class="line"></span><br><span class="line">#上传远程库</span><br><span class="line">git push -u origin main</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th>目标</th>
<th>命令</th>
</tr>
</thead>
<tbody>
<tr>
<td>查看远程仓库</td>
<td><code>git remote -v</code></td>
</tr>
<tr>
<td>修改远程仓库地址</td>
<td><code>git remote set-url origin &lt;新地址&gt;</code></td>
</tr>
<tr>
<td>添加新远程仓库（不同名）</td>
<td><code>git remote add upstream &lt;新地址&gt;</code></td>
</tr>
<tr>
<td>删除远程仓库</td>
<td><code>git remote remove origin</code></td>
</tr>
<tr>
<td>添加第一个远程仓库</td>
<td><code>git remote add origin</code></td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p><img src="/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/%E6%97%A5%E5%BF%97/7.8/image-20250708151320868.png" alt="image-20250708151320868"></p>
<p>当前存在连接超时问题，可能是服务器连接的原因</p>
</blockquote>
<h4 id="mineru部署情况"><a href="#mineru部署情况" class="headerlink" title="mineru部署情况"></a>mineru部署情况</h4><p><img src="/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/%E6%97%A5%E5%BF%97/7.8/1a9954d6-31d7-404a-9419-cd9a87c9ee09.png" alt="1a9954d6-31d7-404a-9419-cd9a87c9ee09"></p>
<p>通过调整docker镜像源，可以拉取基础镜像了，但是遇到<code>RUN apt-get update &amp;&amp; apt-get install -y libgl1 &amp;&amp; apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/*</code>第二部命令再次出现网络问题</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/06/24/diary/%E5%A4%A9%E4%B8%8B%E8%8B%B1%E9%9B%84%E5%A6%82%E8%BF%87%E6%B1%9F%E4%B9%8B%E9%B2%AB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/06/24/diary/%E5%A4%A9%E4%B8%8B%E8%8B%B1%E9%9B%84%E5%A6%82%E8%BF%87%E6%B1%9F%E4%B9%8B%E9%B2%AB/" class="post-title-link" itemprop="url">天下英雄如过江之鲫</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2025-06-24 00:00:00 / 修改时间：11:37:06" itemprop="dateCreated datePublished" datetime="2025-06-24T00:00:00+08:00">2025-06-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E9%9A%8F%E7%AC%94/" itemprop="url" rel="index"><span itemprop="name">随笔</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>当你来到双非，你会艳羡211的牌子，当你拼死考上211，你会发现985的头衔会处处卡死你。当你侥幸考上末流
985，你就会发现华五c9的光芒压的你喘不过气。若你真问鼎华五，抬头看，京城中双日凌空。或许你是真正的天才，清北中的佼佼者，他们告诉你，世界不止中国。当你最终成为这一世最不折不扣的天才，你会发现欧拉，黎曼，还有7岁因为想快点放学而创造求和公式的高斯，
早已在山顶等候多时。</p>
<p>有时候想想，这学上到多高才算高啊，大专上面有本科，本科上面有硕博，好不容易毕业了吧，副高，正高，青基，博导，在上面还有杰青，院士。唉，天下英雄，如过江之鲫，无穷尽也。之前没有感觉，自从上了大学，这种感受就像一团乌云一直萦绕在我心头，不禁反思人这一生究竟在追求什么。</p>
<p>大多数人追求的东西，无非三者：权钱学。</p>
<p>有的人梦想升官，但官外有官，权外有权，科级处级厅级，省部级已经算是人中龙凤，但上面还有副国级正国级，大多数人忙忙碌碌一生也就当个副处级，他们真的甘心吗，那种拼劲全力也无法跨过的鸿沟，最后只剩下无奈，遗憾和释然。</p>
<p>有人的渴望财富，赚到了十万就想赚百万，赚到了百万又开始想办法，想赚千万，亿，觉得自己有能力了，开始创业投资，拿着钱去炒股炒币最后赔了个精光，更有甚者权钱勾结，做些不法勾当，不都是为了满足自己的贪婪，但欲望无穷无尽，何时才能填满这个无底洞，更可怕的是，多少人的欲望和他的能力并不符合，自身没有那么大的能力却渴望一切，最后只会反噬，自食其果。</p>
<p>有的人钻研学识，中国的大多数人都是通过高考这一途径踏进学识的殿堂，那些在高中自命不凡的天才们，进入了高校才发现，自己只是芸芸众生的普通一员，以前的光辉也变的暗淡无光，即便是清北级别，已经是很多人可望而不可即的存在，在面对越来越难的知识，在面对更聪明的身边人，在发现自己再努力也无法达成目标时，也会学习释然这一门课。</p>
<p>我想起来看过的一个清华物理系同学的采访，有一句话我印象很深刻，古人会说少壮不努力，老大徒伤悲，但不会说少壮不成功，老大徒伤悲，或许我上面说的这些，都太注重结果了，以结果的好坏判定了过程的意义，这是不对的，在追求这些目标的过程中，我们努力了，拼搏了，奋斗了，其实那就足够了，不应该把结果失利的压力强加在自己身上。</p>
<p>唉，这些说起来容易，真正能做到不为结果所动哪里容易呢，只跟自己比较，不与他人攀比，处之泰然地面对任何困难与挑战。这就是我所追求的心境吧，我什么时候能做到这种地步，可能才是真正的长大了吧。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/06/18/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%80%E4%BC%98%E5%8C%96/%E6%9C%80%E4%BC%98%E5%8C%96%E6%9C%9F%E6%9C%AB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/06/18/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%80%E4%BC%98%E5%8C%96/%E6%9C%80%E4%BC%98%E5%8C%96%E6%9C%9F%E6%9C%AB/" class="post-title-link" itemprop="url">最优化方法——期末复习</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2025-06-18 00:00:00 / 修改时间：17:59:42" itemprop="dateCreated datePublished" datetime="2025-06-18T00:00:00+08:00">2025-06-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%A4%A7%E4%BA%8C%E4%B8%8B/" itemprop="url" rel="index"><span itemprop="name">大二下</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%80%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">最优化方法</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1uP411K7Hf?spm_id_from=333.788.videopod.sections&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">最优化理论与方法-对偶线性规划（例题分析）_哔哩哔哩_bilibili</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/06/14/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8A%EF%BC%89/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/06/14/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8A%EF%BC%89/" class="post-title-link" itemprop="url">机器学习——期末复习（上）</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-06-14 00:00:00" itemprop="dateCreated datePublished" datetime="2025-06-14T00:00:00+08:00">2025-06-14</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-06-16 07:27:17" itemprop="dateModified" datetime="2025-06-16T07:27:17+08:00">2025-06-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%A4%A7%E4%BA%8C%E4%B8%8B/" itemprop="url" rel="index"><span itemprop="name">大二下</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="svm支持向量机">SVM支持向量机</h3>
<h4 id="作业">作业</h4>
<h5 id="section">1</h5>
<p>关于核化软间隔支持向量机，推导目标函数的原始问题转换为对偶问题的过程、KKT条件、预测函数。</p>
<p><strong>原始问题</strong></p>
<p>软间隔SVM的目标函数为： <span class="math display">$$
\min_{\mathbf{w}, b, \xi} \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^n
\xi_i
$$</span> 约束条件： <span class="math display"><em>y</em><sub><em>i</em></sub>(<strong>w</strong><sup><em>T</em></sup><em>ϕ</em>(<strong>x</strong><sub><em>i</em></sub>)+<em>b</em>) ≥ 1 − <em>ξ</em><sub><em>i</em></sub>,  <em>ξ</em><sub><em>i</em></sub> ≥ 0,  <em>i</em> = 1, …, <em>n</em></span>
其中 <span class="math inline"><em>C</em> &gt; 0</span>
是惩罚参数，<span class="math inline"><em>ξ</em><sub><em>i</em></sub></span>
是松弛变量，<span class="math inline"><em>ϕ</em>(⋅)</span>
是特征映射。</p>
<p><strong>转化为对偶问题</strong></p>
<ol type="1">
<li><p><strong>构造拉格朗日函数</strong>： <span class="math display">$$
\mathcal{L}(\mathbf{w}, b, \xi, \boldsymbol{\alpha}, \boldsymbol{\beta})
= \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^n \xi_i - \sum_{i=1}^n
\alpha_i \left[y_i (\mathbf{w}^T \phi(\mathbf{x}_i) + b) - 1 +
\xi_i\right] - \sum_{i=1}^n \beta_i \xi_i
$$</span> 其中 <span class="math inline"><em>α</em><sub><em>i</em></sub> ≥ 0, <em>β</em><sub><em>i</em></sub> ≥ 0</span>
是拉格朗日乘子。</p></li>
<li><p><strong>对原始变量求偏导并令其为零</strong>：</p></li>
</ol>
<ul>
<li>对 <span class="math inline"><strong>w</strong></span> 求导： <span class="math display">$$
\frac{\partial \mathcal{L}}{\partial \mathbf{w}} = \mathbf{w} -
\sum_{i=1}^n \alpha_i y_i \phi(\mathbf{x}_i) = 0 \quad \Rightarrow
\mathbf{w} = \sum_{i=1}^n \alpha_i y_i \phi(\mathbf{x}_i)
$$</span></li>
<li>对 <span class="math inline"><em>b</em></span> 求导： <span class="math display">$$
\frac{\partial \mathcal{L}}{\partial b} = -\sum_{i=1}^n \alpha_i y_i = 0
\quad \Rightarrow \sum_{i=1}^n \alpha_i y_i = 0
$$</span></li>
<li>对 <span class="math inline"><em>ξ</em><sub><em>i</em></sub></span>
求导： <span class="math display">$$
\frac{\partial \mathcal{L}}{\partial \xi_i} = C - \alpha_i - \beta_i = 0
\quad \Rightarrow \beta_i = C - \alpha_i
$$</span></li>
</ul>
<ol start="3" type="1">
<li><p><strong>代入拉格朗日函数消去原始变量</strong>： 将 <span class="math inline"><strong>w</strong></span> 和 <span class="math inline"><em>β</em><sub><em>i</em></sub></span> 代入 <span class="math inline">ℒ</span>，得到对偶目标函数： <span class="math display">$$
\max_{\boldsymbol{\alpha}} \sum_{i=1}^n \alpha_i - \frac{1}{2}
\sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j \phi(\mathbf{x}_i)^T
\phi(\mathbf{x}_j)
$$</span> 约束条件： <span class="math display">$$
0 \leq \alpha_i \leq C, \quad \sum_{i=1}^n \alpha_i y_i = 0
$$</span></p></li>
<li><p><strong>引入核函数</strong>： 用核函数 <span class="math inline"><em>K</em>(<strong>x</strong><sub><em>i</em></sub>,<strong>x</strong><sub><em>j</em></sub>) = <em>ϕ</em>(<strong>x</strong><sub><em>i</em></sub>)<sup><em>T</em></sup><em>ϕ</em>(<strong>x</strong><sub><em>j</em></sub>)</span>
替换内积，得到最终对偶问题： <span class="math display">$$
\max_{\boldsymbol{\alpha}} \sum_{i=1}^n \alpha_i - \frac{1}{2}
\sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j K(\mathbf{x}_i,
\mathbf{x}_j)
$$</span> 约束条件不变。</p></li>
</ol>
<p><strong>KKT条件</strong></p>
<ul>
<li><strong>原始可行性</strong>：<span class="math inline"><em>y</em><sub><em>i</em></sub>(<strong>w</strong><sup><em>T</em></sup><em>ϕ</em>(<strong>x</strong><sub><em>i</em></sub>)+<em>b</em>) ≥ 1 − <em>ξ</em><sub><em>i</em></sub>,  <em>ξ</em><sub><em>i</em></sub> ≥ 0</span></li>
<li><strong>对偶可行性</strong>：<span class="math inline"><em>α</em><sub><em>i</em></sub> ≥ 0,  <em>β</em><sub><em>i</em></sub> = <em>C</em> − <em>α</em><sub><em>i</em></sub> ≥ 0</span></li>
<li><strong>互补松弛性</strong>：<span class="math inline"><em>α</em><sub><em>i</em></sub>[<em>y</em><sub><em>i</em></sub>(<strong>w</strong><sup><em>T</em></sup><em>ϕ</em>(<strong>x</strong><sub><em>i</em></sub>)+<em>b</em>)−1+<em>ξ</em><sub><em>i</em></sub>] = 0,  <em>β</em><sub><em>i</em></sub><em>ξ</em><sub><em>i</em></sub> = 0</span></li>
<li><strong>梯度为零条件</strong>：已通过偏导数消去原始变量。</li>
</ul>
<p><strong>预测函数</strong></p>
<p>测试样本 <span class="math inline"><strong>x</strong></span>
的预测函数为： <span class="math display">$$
f(\mathbf{x}) = \text{sign} \left( \sum_{i=1}^n \alpha_i y_i
K(\mathbf{x}_i, \mathbf{x}) + b \right)
$$</span> 其中 <span class="math inline"><em>b</em></span>
可通过任一支持向量（满足 <span class="math inline">0 &lt; <em>α</em><sub><em>i</em></sub> &lt; <em>C</em></span>）计算：
<span class="math display">$$
b = y_i - \sum_{j=1}^n \alpha_j y_j K(\mathbf{x}_j, \mathbf{x}_i)
$$</span></p>
<h5 id="section-1">2</h5>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/06/12/%E7%AE%97%E6%B3%95/%E7%AE%97%E6%B3%95%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0/%E7%AE%97%E6%B3%95%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/06/12/%E7%AE%97%E6%B3%95/%E7%AE%97%E6%B3%95%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0/%E7%AE%97%E6%B3%95%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0/" class="post-title-link" itemprop="url">算法期末复习</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-06-12 00:00:00" itemprop="dateCreated datePublished" datetime="2025-06-12T00:00:00+08:00">2025-06-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-06-14 22:26:13" itemprop="dateModified" datetime="2025-06-14T22:26:13+08:00">2025-06-14</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="贪心问题">贪心问题</h3>
<h4 id="找零钱">找零钱</h4>
<p>用最少数量的钱币凑出目标金额 m 元。</p>
<p><strong>核心思想</strong> ：
每次选择<strong>不超过剩余金额的最大面值</strong>
，直到凑够目标金额。</p>
<p><strong>步骤：</strong></p>
<ol type="1">
<li>将钱币面值按从大到小排序。</li>
<li>对于当前剩余金额，不断减去最大可用面值，直到金额为 0。</li>
</ol>
<p><strong>贪心策略的适用性</strong></p>
<p><strong>仅当钱币面值满足以下条件时有效</strong> ：</p>
<ul>
<li>面值序列中每个元素都是前一个元素的因数（如
<code>1, 2, 5, 10</code>）。</li>
<li>否则，贪心可能失败（例如面值 <code>[1, 3, 4]</code>，目标
<code>6</code>：贪心选 <code>4+1+1</code> 需 3 枚，而最优解是
<code>3+3</code> 需 2 枚）。</li>
</ul>
<p><strong>代码问题分析</strong></p>
<p>用户提供的代码是一个基于贪心策略的找零钱实现，但在<strong>硬币面值不满足贪心条件</strong>时可能无法得到最优解。以下是具体分析：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">findMinCoins</span>(<span class="params">coins, amount</span>):</span><br><span class="line">    coins.sort(reverse=<span class="literal">True</span>)  <span class="comment"># 降序排序</span></span><br><span class="line">    res = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(coins)):</span><br><span class="line">        <span class="keyword">while</span> amount &gt;= coins[i]:</span><br><span class="line">            res.append(coins[i])</span><br><span class="line">            amount -= coins[i]</span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line">coins = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">50</span>, <span class="number">100</span>]</span><br><span class="line">amount = <span class="number">1136</span></span><br><span class="line">out = findMinCoins(coins, amount)</span><br><span class="line"><span class="built_in">print</span>(out)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;钱币数量为<span class="subst">&#123;<span class="built_in">len</span>(out)&#125;</span>.&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>问题点：</strong></p>
<ol type="1">
<li><p><strong>贪心策略的局限性</strong><br>
仅当硬币面值满足 <strong>每种面值是前一种面值的因数</strong>（如
<code>[1, 2, 5, 10, 50, 100]</code>）时，贪心算法才能保证最优解。若面值不满足此条件（如
<code>[1, 3, 4]</code>），则可能失败。</p></li>
<li><p><strong>未处理特殊情况</strong></p>
<ul>
<li>若 <code>amount</code> 无法被硬币组合凑出（如硬币为
<code>[2, 5]</code>，目标
<code>3</code>），代码会返回非最优解或死循环。</li>
</ul></li>
</ol>
<p><strong>改进方案</strong></p>
<p>适用于任意硬币面值，确保最优解： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">min_coins_dp</span>(<span class="params">coins, amount</span>):</span><br><span class="line">    dp = [<span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>)] * (amount + <span class="number">1</span>)</span><br><span class="line">    dp[<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> a <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, amount + <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> coin <span class="keyword">in</span> coins:</span><br><span class="line">            <span class="keyword">if</span> a &gt;= coin <span class="keyword">and</span> dp[a - coin] + <span class="number">1</span> &lt; dp[a]:</span><br><span class="line">                dp[a] = dp[a - coin] + <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dp[amount] <span class="keyword">if</span> dp[amount] != <span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>) <span class="keyword">else</span> -<span class="number">1</span></span><br><span class="line"></span><br><span class="line">coins = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">50</span>, <span class="number">100</span>]</span><br><span class="line">amount = <span class="number">1136</span></span><br><span class="line"><span class="built_in">print</span>(min_coins_dp(coins, amount))  <span class="comment"># 输出 16</span></span><br></pre></td></tr></table></figure></p>
<h4 id="最优装载问题">最优装载问题</h4>
<p>🧮 问题描述</p>
<p>给定一个集装箱重量列表 <code>weights</code> 和轮船的最大载重
<code>W</code>，目标是
<strong>尽可能多地装载集装箱</strong>（不考虑体积限制）。</p>
<p>✅ 算法思路</p>
<ol type="1">
<li><strong>排序</strong>：将所有集装箱按重量从小到大排序。</li>
<li><strong>贪心装载</strong>：依次尝试装载每个集装箱，若当前总重量加上该集装箱的重量不超过
<code>W</code>，则装载；否则停止。</li>
<li><strong>返回结果</strong>：返回成功装载的集装箱数量。</li>
</ol>
<p>🧾 Python 实现</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">max_loaded_containers</span>(<span class="params">weights, W</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    返回在总载重 W 下，最多可以装载的集装箱数量。</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    weights (list of int/float): 集装箱重量列表</span></span><br><span class="line"><span class="string">    W (int/float): 轮船的最大载重</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    返回:</span></span><br><span class="line"><span class="string">    int: 最多可以装载的集装箱数量</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 按重量从小到大排序</span></span><br><span class="line">    weights.sort()</span><br><span class="line">    </span><br><span class="line">    total_weight = <span class="number">0</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> weight <span class="keyword">in</span> weights:</span><br><span class="line">        <span class="keyword">if</span> total_weight + weight &lt;= W:</span><br><span class="line">            total_weight += weight</span><br><span class="line">            count += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> count</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例测试</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    weights = [<span class="number">3</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line">    W = <span class="number">10</span></span><br><span class="line">    result = max_loaded_containers(weights, W)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;最多可以装载 <span class="subst">&#123;result&#125;</span> 个集装箱&quot;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="活动选择问题最大相容活动子集">活动选择问题（最大相容活动子集）</h4>
<p>📌 问题描述</p>
<p>给定 $ n $ 个活动的集合 $ C = {1, 2, …, n} $，每个活动 $ i $
都有起始时间 $ s_i $ 和结束时间 $ f_i $（满足 $ s_i &lt; f_i
$）。要求选择一个<strong>最大相容活动子集</strong>，使得被选中的活动之间<strong>时间互不重叠</strong>。</p>
<p>两个活动 $ i $ 和 $ j $ 相容的条件为： <span class="math display"><em>s</em><sub><em>i</em></sub> ≥ <em>f</em><sub><em>j</em></sub>  或  <em>s</em><sub><em>j</em></sub> ≥ <em>f</em><sub><em>i</em></sub></span></p>
<p>✅ 贪心策略与正确性</p>
<p><strong>贪心策略</strong>：<br>
1. <strong>按活动结束时间 $ f_i $ 从小到大排序</strong>。 2.
<strong>依次选择结束最早的活动</strong>，并跳过与其冲突的所有活动。</p>
<p><strong>正确性证明（归纳法）</strong>：</p>
<ul>
<li><strong>基础情况</strong>：当只有一项活动时，显然选择它是最优的。</li>
<li><strong>归纳假设</strong>：对于前 $ k $
个活动，该策略能得到最大相容子集。</li>
<li><strong>归纳步骤</strong>：考虑第 $ k+1 $
个活动。若选择结束最早的活动 $ A $，则剩下的可用时间区间为 $ [f_A, +)
$，此时在该区间内继续应用该策略，仍能得到最大子集。若不选 $ A $
而选其他活动，则剩余时间更少，无法容纳更多活动。</li>
</ul>
<p>🧾 Python 实现</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">max_compatible_activities</span>(<span class="params">activities</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    返回最大相容活动子集的数量及具体活动列表。</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    activities (list of tuples): 每个元素为 (s_i, f_i)，表示活动的起始和结束时间</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    返回:</span></span><br><span class="line"><span class="string">    tuple: (最大活动数量, 相容活动列表)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 按结束时间从小到大排序</span></span><br><span class="line">    sorted_activities = <span class="built_in">sorted</span>(activities, key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>])</span><br><span class="line">    </span><br><span class="line">    selected = []</span><br><span class="line">    last_end = -<span class="number">1</span>  <span class="comment"># 上一个选中的活动的结束时间</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> activity <span class="keyword">in</span> sorted_activities:</span><br><span class="line">        s, f = activity</span><br><span class="line">        <span class="keyword">if</span> s &gt;= last_end:</span><br><span class="line">            selected.append(activity)</span><br><span class="line">            last_end = f</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">len</span>(selected), selected</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例测试</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    activities = [</span><br><span class="line">        (<span class="number">1</span>, <span class="number">4</span>), (<span class="number">3</span>, <span class="number">5</span>), (<span class="number">0</span>, <span class="number">6</span>), (<span class="number">5</span>, <span class="number">7</span>), </span><br><span class="line">        (<span class="number">3</span>, <span class="number">8</span>), (<span class="number">5</span>, <span class="number">9</span>), (<span class="number">6</span>, <span class="number">10</span>), (<span class="number">8</span>, <span class="number">11</span>)</span><br><span class="line">    ]</span><br><span class="line">    count, selected = max_compatible_activities(activities)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;最大相容活动数: <span class="subst">&#123;count&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;所选活动:&quot;</span>, selected)</span><br></pre></td></tr></table></figure>
<h4 id="使用堆优化的-dijkstra-算法python-实现">使用堆优化的 Dijkstra
算法（Python 实现）</h4>
<p>🧠 <strong>核心思想</strong></p>
<ul>
<li>使用<strong>最小堆</strong>（优先队列）高效选择当前距离最小的节点，避免暴力遍历。</li>
<li>每次从堆中取出当前最短路径的节点，进行<strong>松弛操作</strong>（Relaxation）。</li>
<li>若发现堆中存在过时的路径记录，则跳过（因为已找到更优路径）。</li>
</ul>
<p>📦 <strong>图的表示</strong></p>
<p>使用邻接表（字典嵌套列表）： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">graph = &#123;</span><br><span class="line">    <span class="string">&#x27;A&#x27;</span>: [(<span class="string">&#x27;B&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;C&#x27;</span>, <span class="number">4</span>)],</span><br><span class="line">    <span class="string">&#x27;B&#x27;</span>: [(<span class="string">&#x27;A&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;C&#x27;</span>, <span class="number">2</span>), (<span class="string">&#x27;D&#x27;</span>, <span class="number">5</span>)],</span><br><span class="line">    <span class="string">&#x27;C&#x27;</span>: [(<span class="string">&#x27;A&#x27;</span>, <span class="number">4</span>), (<span class="string">&#x27;B&#x27;</span>, <span class="number">2</span>), (<span class="string">&#x27;D&#x27;</span>, <span class="number">1</span>)],</span><br><span class="line">    <span class="string">&#x27;D&#x27;</span>: [(<span class="string">&#x27;B&#x27;</span>, <span class="number">5</span>), (<span class="string">&#x27;C&#x27;</span>, <span class="number">1</span>)]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>🧾 <strong>Python 代码实现</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> heapq</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dijkstra_with_heap</span>(<span class="params">graph, start</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    使用堆优化的 Dijkstra 算法求单源最短路径。</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">    graph (dict): 邻接表形式的图，格式为 &#123;节点: [(邻接节点, 权重), ...]&#125;</span></span><br><span class="line"><span class="string">    start (str/int): 起始节点</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    返回:</span></span><br><span class="line"><span class="string">    dict: 从起始节点到所有节点的最短路径长度</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 初始化距离字典，所有节点初始距离为无穷大</span></span><br><span class="line">    distances = &#123;node: <span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>) <span class="keyword">for</span> node <span class="keyword">in</span> graph&#125;</span><br><span class="line">    distances[start] = <span class="number">0</span>  <span class="comment"># 起始节点到自身的距离为 0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 优先队列（最小堆），存储 (距离, 节点)</span></span><br><span class="line">    heap = [(<span class="number">0</span>, start)]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> heap:</span><br><span class="line">        current_distance, current_node = heapq.heappop(heap)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 如果当前弹出的距离大于记录的距离，说明该节点已被处理过，跳过</span></span><br><span class="line">        <span class="keyword">if</span> current_distance &gt; distances[current_node]:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 遍历当前节点的所有邻接边</span></span><br><span class="line">        <span class="keyword">for</span> neighbor, weight <span class="keyword">in</span> graph[current_node]:</span><br><span class="line">            distance = current_distance + weight</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 如果找到更短路径，更新距离并推入堆</span></span><br><span class="line">            <span class="keyword">if</span> distance &lt; distances[neighbor]:</span><br><span class="line">                distances[neighbor] = distance</span><br><span class="line">                heapq.heappush(heap, (distance, neighbor))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> distances</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例测试</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    graph = &#123;</span><br><span class="line">        <span class="string">&#x27;A&#x27;</span>: [(<span class="string">&#x27;B&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;C&#x27;</span>, <span class="number">4</span>)],</span><br><span class="line">        <span class="string">&#x27;B&#x27;</span>: [(<span class="string">&#x27;A&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;C&#x27;</span>, <span class="number">2</span>), (<span class="string">&#x27;D&#x27;</span>, <span class="number">5</span>)],</span><br><span class="line">        <span class="string">&#x27;C&#x27;</span>: [(<span class="string">&#x27;A&#x27;</span>, <span class="number">4</span>), (<span class="string">&#x27;B&#x27;</span>, <span class="number">2</span>), (<span class="string">&#x27;D&#x27;</span>, <span class="number">1</span>)],</span><br><span class="line">        <span class="string">&#x27;D&#x27;</span>: [(<span class="string">&#x27;B&#x27;</span>, <span class="number">5</span>), (<span class="string">&#x27;C&#x27;</span>, <span class="number">1</span>)]</span><br><span class="line">    &#125;</span><br><span class="line">    start_node = <span class="string">&#x27;A&#x27;</span></span><br><span class="line">    shortest_paths = dijkstra_with_heap(graph, start_node)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;从节点 <span class="subst">&#123;start_node&#125;</span> 出发的最短路径：&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> node, dist <span class="keyword">in</span> shortest_paths.items():</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;start_node&#125;</span> → <span class="subst">&#123;node&#125;</span> : <span class="subst">&#123;dist&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="哈夫曼编码"><strong>哈夫曼编码</strong></h4>
<p><strong>2. 构建哈夫曼树的步骤</strong></p>
<p><strong>步骤 1：统计字符频率</strong></p>
<p>假设输入字符串为
<code>"BCCABBDDAECCBAAAEC"</code>，统计每个字符的出现次数：
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A: 6, B: 4, C: 5, D: 2, E: 1</span><br></pre></td></tr></table></figure></p>
<p><strong>步骤 2：创建最小堆（优先队列）</strong></p>
<ul>
<li>将每个字符及其频率构建成节点，并按频率升序排列。</li>
<li>初始堆：<code>[E(1), D(2), B(4), C(5), A(6)]</code></li>
</ul>
<p><strong>步骤 3：合并节点，构建哈夫曼树</strong></p>
<ol type="1">
<li>取出两个频率最小的节点 <code>E(1)</code> 和
<code>D(2)</code>，合并为新节点 <code>ED(3)</code>。</li>
<li>将新节点插入堆：<code>[B(4), C(5), ED(3), A(6)]</code> → 重新排序为
<code>[ED(3), B(4), C(5), A(6)]</code></li>
<li>重复上述步骤，直到堆中只剩一个根节点（哈夫曼树）。</li>
</ol>
<p>最终树结构示意图（频率越小越靠近叶子）： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">        (18)</span><br><span class="line">       /    \</span><br><span class="line">     (8)    A(6)</span><br><span class="line">    /   \</span><br><span class="line"> (4)   (4)</span><br><span class="line">B     C(5)</span><br></pre></td></tr></table></figure></p>
<p><strong>步骤 4：生成哈夫曼编码表</strong></p>
<p>从根节点出发，左子树标记为 <code>0</code>，右子树标记为
<code>1</code>： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">A: 11</span><br><span class="line">B: 00</span><br><span class="line">C: 01</span><br><span class="line">D: 100</span><br><span class="line">E: 101</span><br></pre></td></tr></table></figure></p>
<p><strong>4. Python 实现哈夫曼编码</strong></p>
<p>以下代码展示如何用 Python 构建哈夫曼树并生成编码表：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> heapq</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">HuffmanNode</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, char, freq</span>):</span><br><span class="line">        <span class="variable language_">self</span>.char = char</span><br><span class="line">        <span class="variable language_">self</span>.freq = freq</span><br><span class="line">        <span class="variable language_">self</span>.left = <span class="literal">None</span></span><br><span class="line">        <span class="variable language_">self</span>.right = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__lt__</span>(<span class="params">self, other</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.freq &lt; other.freq</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_huffman_tree</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="comment"># 统计频率</span></span><br><span class="line">    frequency = Counter(text)</span><br><span class="line">    <span class="comment"># 创建最小堆</span></span><br><span class="line">    heap = [HuffmanNode(char, freq) <span class="keyword">for</span> char, freq <span class="keyword">in</span> frequency.items()]</span><br><span class="line">    heapq.heapify(heap)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 合并节点</span></span><br><span class="line">    <span class="keyword">while</span> <span class="built_in">len</span>(heap) &gt; <span class="number">1</span>:</span><br><span class="line">        left = heapq.heappop(heap)</span><br><span class="line">        right = heapq.heappop(heap)</span><br><span class="line">        merged = HuffmanNode(<span class="literal">None</span>, left.freq + right.freq)</span><br><span class="line">        merged.left = left</span><br><span class="line">        merged.right = right</span><br><span class="line">        heapq.heappush(heap, merged)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> heapq.heappop(heap) <span class="keyword">if</span> heap <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_huffman_codes</span>(<span class="params">root</span>):</span><br><span class="line">    codes = &#123;&#125;</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">node, current_code</span>):</span><br><span class="line">        <span class="keyword">if</span> node:</span><br><span class="line">            <span class="keyword">if</span> node.char <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                codes[node.char] = current_code</span><br><span class="line">            dfs(node.left, current_code + <span class="string">&quot;0&quot;</span>)</span><br><span class="line">            dfs(node.right, current_code + <span class="string">&quot;1&quot;</span>)</span><br><span class="line">    dfs(root, <span class="string">&quot;&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> codes</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例</span></span><br><span class="line">text = <span class="string">&quot;BCCABBDDAECCBAAAEC&quot;</span></span><br><span class="line">root = build_huffman_tree(text)</span><br><span class="line">codes = build_huffman_codes(root)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;哈夫曼编码表:&quot;</span>, codes)</span><br></pre></td></tr></table></figure>
<p><strong>输出示例：</strong> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">哈夫曼编码表: &#123;&#x27;B&#x27;: &#x27;0&#x27;, &#x27;C&#x27;: &#x27;10&#x27;, &#x27;A&#x27;: &#x27;11&#x27;, &#x27;D&#x27;: &#x27;110&#x27;, &#x27;E&#x27;: &#x27;111&#x27;&#125;</span><br></pre></td></tr></table></figure></p>
<h4 id="prim">prim</h4>
<p>以下是 <strong>朴素 Prim 算法</strong>
的实现与详解，适用于稠密图（如邻接矩阵存储的图）：</p>
<p><strong>Prim 算法核心思想</strong></p>
<ol type="1">
<li>从任意顶点开始（如 <code>start=0</code>）。</li>
<li>维护一个集合 <code>selected</code>，记录已加入生成树的顶点。</li>
<li>每次从未选顶点中选择到当前生成树的最小权重边的顶点。</li>
<li>重复步骤 3，直到所有顶点加入生成树。</li>
</ol>
<p>时间复杂度：<strong>O(V²)</strong>，其中 V 是顶点数。</p>
<p><strong>Python 实现</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">prim</span>(<span class="params">graph, start=<span class="number">0</span></span>):</span><br><span class="line">    V = <span class="built_in">len</span>(graph)  <span class="comment"># 顶点数量</span></span><br><span class="line">    selected = [<span class="literal">False</span>] * V  <span class="comment"># 标记顶点是否已加入生成树</span></span><br><span class="line">    key = [sys.maxsize] * V  <span class="comment"># 记录各顶点到生成树的最小权重</span></span><br><span class="line">    parent = [-<span class="number">1</span>] * V         <span class="comment"># 记录最小生成树的父节点</span></span><br><span class="line"></span><br><span class="line">    key[start] = <span class="number">0</span>  <span class="comment"># 起始顶点的权值设为0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(V):</span><br><span class="line">        <span class="comment"># 找到当前未选顶点中 key 最小的顶点 u</span></span><br><span class="line">        min_key = sys.maxsize</span><br><span class="line">        u = -<span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> v <span class="keyword">in</span> <span class="built_in">range</span>(V):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> selected[v] <span class="keyword">and</span> key[v] &lt; min_key:</span><br><span class="line">                min_key = key[v]</span><br><span class="line">                u = v</span><br><span class="line">        <span class="keyword">if</span> u == -<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">break</span>  <span class="comment"># 无连通顶点，生成树结束</span></span><br><span class="line">        </span><br><span class="line">        selected[u] = <span class="literal">True</span>  <span class="comment"># 将 u 加入生成树</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 更新 u 的所有邻接顶点的 key 值</span></span><br><span class="line">        <span class="keyword">for</span> v <span class="keyword">in</span> <span class="built_in">range</span>(V):</span><br><span class="line">            <span class="keyword">if</span> graph[u][v] &gt; <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> selected[v] <span class="keyword">and</span> graph[u][v] &lt; key[v]:</span><br><span class="line">                key[v] = graph[u][v]</span><br><span class="line">                parent[v] = u  <span class="comment"># 记录 v 的父节点为 u</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> key, parent</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例：邻接矩阵表示的图</span></span><br><span class="line">graph = [</span><br><span class="line">    [<span class="number">0</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">6</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">8</span>, <span class="number">5</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">7</span>],</span><br><span class="line">    [<span class="number">6</span>, <span class="number">8</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">9</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>, <span class="number">0</span>]</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">key, parent = prim(graph)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;最小生成树的总权重:&quot;</span>, <span class="built_in">sum</span>(key))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;父节点数组:&quot;</span>, parent)</span><br></pre></td></tr></table></figure>
<h4 id="kruskal">kruskal</h4>
<p><strong>2. Kruskal 算法的核心思想</strong></p>
<ol type="1">
<li><strong>按权重从小到大排序所有边</strong>。</li>
<li><strong>依次选择边</strong>：
<ul>
<li>如果这条边的两个顶点不在同一个连通分量中（即不形成环），则将这条边加入生成树。</li>
<li>否则跳过这条边。</li>
</ul></li>
<li><strong>重复步骤2，直到生成树中有 <code>V-1</code>
条边</strong>（<code>V</code> 是顶点数）。</li>
</ol>
<p><strong>6. Python 实现示例</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">UnionFind</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size</span>):</span><br><span class="line">        <span class="variable language_">self</span>.parent = <span class="built_in">list</span>(<span class="built_in">range</span>(size))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">find</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.parent[x] != x:</span><br><span class="line">            <span class="variable language_">self</span>.parent[x] = <span class="variable language_">self</span>.find(<span class="variable language_">self</span>.parent[x])  <span class="comment"># 路径压缩</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.parent[x]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">union</span>(<span class="params">self, x, y</span>):</span><br><span class="line">        rootX = <span class="variable language_">self</span>.find(x)</span><br><span class="line">        rootY = <span class="variable language_">self</span>.find(y)</span><br><span class="line">        <span class="keyword">if</span> rootX == rootY:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span>  <span class="comment"># 已在同一个集合</span></span><br><span class="line">        <span class="variable language_">self</span>.parent[rootY] = rootX  <span class="comment"># 合并</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">kruskal</span>(<span class="params">n, edges</span>):</span><br><span class="line">    <span class="comment"># edges: [(权重, u, v), ...]</span></span><br><span class="line">    edges.sort()</span><br><span class="line">    uf = UnionFind(n)</span><br><span class="line">    mst = []</span><br><span class="line">    cost = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> weight, u, v <span class="keyword">in</span> edges:</span><br><span class="line">        <span class="keyword">if</span> uf.union(u, v):</span><br><span class="line">            mst.append((u, v))</span><br><span class="line">            cost += weight</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(mst) == n - <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">break</span>  <span class="comment"># 已选够 n-1 条边</span></span><br><span class="line">    <span class="keyword">return</span> mst, cost</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例</span></span><br><span class="line">n = <span class="number">5</span>  <span class="comment"># 顶点数（0~4）</span></span><br><span class="line">edges = [</span><br><span class="line">    (<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>),  <span class="comment"># A(0)-B(1)</span></span><br><span class="line">    (<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>),  <span class="comment"># B(1)-C(2)</span></span><br><span class="line">    (<span class="number">3</span>, <span class="number">2</span>, <span class="number">3</span>),  <span class="comment"># C(2)-D(3)</span></span><br><span class="line">    (<span class="number">4</span>, <span class="number">3</span>, <span class="number">4</span>),  <span class="comment"># D(3)-E(4)</span></span><br><span class="line">    (<span class="number">5</span>, <span class="number">0</span>, <span class="number">4</span>),  <span class="comment"># A(0)-E(4)</span></span><br><span class="line">    (<span class="number">6</span>, <span class="number">1</span>, <span class="number">3</span>)   <span class="comment"># B(1)-D(3)</span></span><br><span class="line">]</span><br><span class="line">mst, total = kruskal(n, edges)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;MST 边：&quot;</span>, mst)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;总权重：&quot;</span>, total)</span><br></pre></td></tr></table></figure>
<h3 id="动态规划">动态规划</h3>
<h4 id="完全背包">完全背包</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">def unbounded_knapsack_2d(weights, values, capacity):</span><br><span class="line">    n = len(weights)</span><br><span class="line">    dp = [[0] * (capacity + 1) for _ in range(n + 1)]</span><br><span class="line"></span><br><span class="line">    for i in range(1, n + 1):</span><br><span class="line">        for j in range(1, capacity + 1):</span><br><span class="line">            if weights[i-1] &lt;= j:</span><br><span class="line">                dp[i][j] = max(</span><br><span class="line">                    dp[i-1][j],</span><br><span class="line">                    dp[i][j - weights[i-1]] + values[i-1]</span><br><span class="line">                )</span><br><span class="line">            else:</span><br><span class="line">                dp[i][j] = dp[i-1][j]</span><br><span class="line">    </span><br><span class="line">    return dp[n][capacity]</span><br><span class="line"></span><br><span class="line"># 示例</span><br><span class="line">weights = [1, 2, 3]</span><br><span class="line">values = [15, 20, 50]</span><br><span class="line">capacity = 5</span><br><span class="line">print(unbounded_knapsack_2d(weights, values, capacity))  # 输出 80</span><br></pre></td></tr></table></figure>
<h4 id="最优二叉搜索树">最优二叉搜索树</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">def optimal_bst(p, q, n):</span><br><span class="line">    # 初始化 dp 和 w 数组（大小为 (n+2) x (n+2)，避免越界）</span><br><span class="line">    dp = [[0] * (n+2) for _ in range(n+2)]</span><br><span class="line">    w = [[0] * (n+2) for _ in range(n+2)]</span><br><span class="line">    root = [[0] * (n+2) for _ in range(n+2)]</span><br><span class="line"></span><br><span class="line">    # 初始化虚拟键的权重</span><br><span class="line">    for i in range(n+1):</span><br><span class="line">        w[i][i] = q[i]</span><br><span class="line">    </span><br><span class="line">    # 填表顺序：链长从 1 到 n</span><br><span class="line">    for l in range(1, n+1):  # l 为关键字数量</span><br><span class="line">        for i in range(n - l + 1):</span><br><span class="line">            j = i + l</span><br><span class="line">            w[i][j] = w[i][j-1] + p[j] + q[j]</span><br><span class="line">            dp[i][j] = float(&#x27;inf&#x27;)</span><br><span class="line">            # 枚举根节点 r（i &lt; r ≤ j）</span><br><span class="line">            for r in range(i+1, j+1):</span><br><span class="line">                cost = dp[i][r-1] + dp[r][j]</span><br><span class="line">                if cost &lt; dp[i][j]:</span><br><span class="line">                    dp[i][j] = cost</span><br><span class="line">                    root[i][j] = r</span><br><span class="line">    </span><br><span class="line">    return dp[0][n], root</span><br><span class="line"></span><br><span class="line"># 示例输入</span><br><span class="line">p = [0, 0.15, 0.1, 0.05]  # 关键字概率（从 k₁ 开始）</span><br><span class="line">q = [0.05, 0.1, 0.05, 0.05]  # 虚拟键概率（从 d₀ 开始）</span><br><span class="line">n = 3  # 关键字数量</span><br><span class="line">min_cost, root = optimal_bst(p, q, n)</span><br><span class="line">print(&quot;最小期望搜索代价:&quot;, min_cost)</span><br></pre></td></tr></table></figure>
<h3 id="回溯法">回溯法</h3>
<h4 id="八皇后">八皇后</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">https://leetcode.cn/problems/n-queens/</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">n=4</span><br><span class="line">ans=[]</span><br><span class="line">path=[]</span><br><span class="line">onpath=[False]*n#记录哪一列有皇后</span><br><span class="line">diag1=[False]*(2*n-1)#记录主对角线是否有皇后</span><br><span class="line">diag2=[False]*(2*n-1)#记录副对角线是否有皇后</span><br><span class="line">def dfs(row,path:list):</span><br><span class="line">    if row==n:</span><br><span class="line">        #print(path)</span><br><span class="line">        chess=[]</span><br><span class="line">        # 生成棋盘</span><br><span class="line">        for i in range(n):</span><br><span class="line">            chess.append(&quot;.&quot;*path[i]+&quot;Q&quot;+&quot;.&quot;*(n-path[i]-1))</span><br><span class="line">        ans.append(chess)</span><br><span class="line">        return</span><br><span class="line">    for col in range(n):</span><br><span class="line">        if isvalid(row,col):</span><br><span class="line">            path.append(col)#放置皇后</span><br><span class="line">            onpath[col]=diag1[row+col]=diag2[row-col+n-1]=True</span><br><span class="line">            dfs(row+1,path)#递归下一行</span><br><span class="line">            path.pop()#回溯，取消放置</span><br><span class="line">            onpath[col]=diag1[row+col]=diag2[row-col+n-1]=False</span><br><span class="line">    </span><br><span class="line">def isvalid(row,col):</span><br><span class="line">    if onpath[col] or diag1[row+col] or diag2[row-col+n-1]:</span><br><span class="line">        return False</span><br><span class="line">    return True</span><br><span class="line">dfs(0,path)</span><br><span class="line">print(ans)</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/06/10/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E6%9F%A5%E6%BC%8F%E8%A1%A5%E7%BC%BA%EF%BC%89/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/06/10/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E6%9F%A5%E6%BC%8F%E8%A1%A5%E7%BC%BA%EF%BC%89/" class="post-title-link" itemprop="url">机器学习——期末复习（查漏补缺）</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-06-10 00:00:00" itemprop="dateCreated datePublished" datetime="2025-06-10T00:00:00+08:00">2025-06-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-06-16 07:35:40" itemprop="dateModified" datetime="2025-06-16T07:35:40+08:00">2025-06-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%A4%A7%E4%BA%8C%E4%B8%8B/" itemprop="url" rel="index"><span itemprop="name">大二下</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="高斯核rbf核中-σ²-的作用及其对模型的影响">高斯核（RBF核）中 σ²
的作用及其对模型的影响</h3>
<p>高斯核（RBF核）的形式为： <span class="math display">$$
K(x, x') = \exp\left(-\frac{\|x - x'\|^2}{2\sigma^2}\right)
$$</span> 其中 $ |x - x’| $ 是两个样本点之间的欧氏距离，$ ^2 $
是高斯核的方差参数，控制核函数的“宽度”或“局部性”。</p>
<p><strong>1. σ² 的几何意义：核函数的“影响范围”</strong></p>
<ul>
<li><p><strong>σ² 较小时</strong>：<br>
分母较小，指数项中的 $ $ 会更大，导致指数函数值快速衰减。<br>
<strong>结果</strong>：只有当 $ x $ 和 $ x’ $
非常接近时，核函数值才接近1；稍远一点的距离会导致核函数值迅速趋近于0。<br>
<strong>直观理解</strong>：模型只关注局部区域内的样本点，决策边界会围绕每个样本点“弯曲”，形成复杂的非线性形状。</p></li>
<li><p><strong>σ² 较大时</strong>：<br>
分母较大，指数项中的 $ $ 会更小，指数函数值衰减缓慢。<br>
<strong>结果</strong>：即使 $ x $ 和 $ x’ $
相距较远，核函数值仍可能较大。<br>
<strong>直观理解</strong>：模型会考虑更大范围的样本点，决策边界更平滑，接近线性分隔。</p></li>
</ul>
<p><strong>2. σ² 如何影响模型的复杂度</strong></p>
<ul>
<li><strong>σ² 小 → 局部敏感，高复杂度</strong>：
<ul>
<li>每个样本点的影响范围有限，模型需要“记住”每个局部区域的细节。<br>
</li>
<li>决策边界会围绕每个样本点剧烈弯曲，甚至形成孤立的环形区域（如图1）。<br>
</li>
<li>容易过拟合：模型过度适应训练数据的噪声和细节。</li>
</ul></li>
<li><strong>σ² 大 → 全局平滑，低复杂度</strong>：
<ul>
<li>样本点的影响范围扩大，模型倾向于用简单的全局模式区分数据。<br>
</li>
<li>决策边界接近线性（如图2），可能无法捕捉数据中的非线性结构。<br>
</li>
<li>容易欠拟合：模型无法拟合数据中的局部特征。</li>
</ul></li>
</ul>
<p><strong>3. 数学与直观示例</strong></p>
<p>假设两个样本点 $ x_1 $ 和 $ x_2 $ 距离为 $ d $，核函数值 $ K(x_1,
x_2) $ 随 $ ^2 $ 的变化如下：</p>
<table>
<colgroup>
<col style="width: 13%">
<col style="width: 22%">
<col style="width: 31%">
<col style="width: 32%">
</colgroup>
<thead>
<tr class="header">
<th>$ ^2 $</th>
<th>$ d = 1 $</th>
<th>$ d = 2 $</th>
<th>$ d = 3 $</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>$ ^2 = 0.1 $</td>
<td>$ (-5) $</td>
<td>$ (-20) ^{-9} $</td>
<td>$ (-45) ^{-20} $</td>
</tr>
<tr class="even">
<td>$ ^2 = 1 $</td>
<td>$ (-0.5) $</td>
<td>$ (-2) $</td>
<td>$ (-4.5) $</td>
</tr>
<tr class="odd">
<td>$ ^2 = 10 $</td>
<td>$ (-0.05) $</td>
<td>$ (-0.2) $</td>
<td>$ (-0.45) $</td>
</tr>
</tbody>
</table>
<ul>
<li><strong>σ² 小（如
0.1）</strong>：距离稍大的样本点之间几乎无关联，模型仅依赖极邻近的点做决策。<br>
</li>
<li><strong>σ² 大（如
10）</strong>：即使距离较远的样本点仍有显著关联，模型决策边界更平滑。</li>
</ul>
<h3 id="为什么使用高斯核之前要归一化">为什么使用高斯核之前要归一化</h3>
<p>在使用高斯核（RBF核）之前对数据进行归一化，是机器学习中至关重要的预处理步骤。其核心原因是<strong>高斯核对特征的尺度（scale）极度敏感</strong>，而归一化能消除特征间尺度差异带来的负面影响。以下是详细解释：</p>
<ol type="1">
<li><strong>高斯核的本质依赖距离计算</strong></li>
</ol>
<p>高斯核的公式为： <span class="math display">$$
K(x, x') = \exp\left(-\frac{\|x - x'\|^2}{2\sigma^2}\right)
$$</span> 其中 <span class="math inline">∥<em>x</em> − <em>x</em>′∥</span>
是两个样本点之间的欧氏距离。<br>
<strong>问题</strong>：欧氏距离的计算受特征尺度影响极大。例如： -
假设特征A的取值范围是 [0,1]，特征B的取值范围是 [0,1000]。 -
此时特征B的差异会主导距离计算（如 $ (0.5)^2 + (500)^2
$），特征A的贡献几乎被忽略。</p>
<p><strong>结果</strong>：模型决策边界会过度依赖尺度大的特征，导致性能下降。</p>
<ol start="2" type="1">
<li><strong>归一化消除特征尺度差异</strong></li>
</ol>
<p>归一化（如标准化或最小-最大缩放）将所有特征调整到相似的数值范围（如
[0,1] 或均值为0、方差为1）。<br>
<strong>效果</strong>： -
<strong>公平比较特征</strong>：每个特征对距离的贡献权重均衡。 -
<strong>防止“大尺度特征主导”</strong>：避免模型因某些特征数值过大而忽略其他重要特征。</p>
<p><strong>示例</strong>：<br>
假设两个样本：<br>
- 未归一化：$ x_1 = [1, 100], x_2 = [2, 200] $，距离为 $ <span class="math inline">。 − <em>归</em><em>一</em><em>化</em><em>后</em>（<em>假</em><em>设</em><em>缩</em><em>放</em><em>到</em>[0,1]）：</span>
x_1 = [0.1, 0.1], x_2 = [0.2, 0.2] $，距离为 $ $。<br>
此时两个特征的贡献比例从 1:100 变为 1:1。</p>
<ol start="3" type="1">
<li><strong>高斯核参数 σ² 的有效性依赖归一化</strong></li>
</ol>
<p>高斯核的参数 σ²（或 γ =
1/σ²）决定了核函数的“局部性”（即模型关注局部还是全局模式）。<br>
- <strong>未归一化时</strong>：σ²
的选择必须同时适应不同尺度的特征，导致参数调优困难。 -
例如：若某特征尺度极大，需要极小的 σ²
才能捕捉其局部变化，但这可能使其他小尺度特征的核函数失效。 -
<strong>归一化后</strong>：所有特征尺度一致，σ²
的调参只需关注数据整体分布，而非单个特征的尺度。</p>
<h3 id="svm的hinge损失函数">SVM的Hinge损失函数</h3>
<p>Hinge损失函数是支持向量机（SVM）中用于分类任务的核心损失函数，其核心思想是<strong>最大化分类间隔</strong>，同时惩罚分类错误或置信度不足的样本。以下是详细解析：</p>
<p><strong>1. 数学定义</strong></p>
<p>对于二分类问题，假设真实标签 $ y {+1, -1} $，模型输出 $ f(x) = w^T x
+ b $，则 <strong>Hinge损失</strong> 的定义为： <span class="math display">ℒ(<em>y</em>,<em>f</em>(<em>x</em>)) = max (0,1−<em>y</em>⋅<em>f</em>(<em>x</em>))</span>
- <strong>关键含义</strong>： - 当 $ y f(x)
$：样本被正确分类且置信度足够（位于间隔边界外），损失为0。 - 当 $ y f(x)
&lt; 1 $：样本位于间隔内或被错误分类，损失随 $ y f(x) $ 线性增长。</p>
<p><strong>2. 几何意义：最大化间隔</strong></p>
<p>Hinge损失的设计与SVM的<strong>硬间隔（Hard
Margin）</strong>和<strong>软间隔（Soft Margin）</strong>目标直接相关：
- <strong>硬间隔</strong>：要求所有样本严格满足 $ y_i (w^T x_i + b)
$，即完全线性可分。 -
<strong>软间隔</strong>：允许部分样本违反间隔约束，通过Hinge损失将约束转化为优化目标：
<span class="math display">$$
  \min_{w,b} \left( \frac{1}{2} \|w\|^2 + C \sum_{i=1}^n \max(0, 1 - y_i
(w^T x_i + b)) \right)
  $$</span> - <strong>第一项</strong> $ |w|^2 $：最大化间隔（间隔宽度与
$ |w| $ 成反比）。 - <strong>第二项</strong>
Hinge损失：惩罚违反间隔约束的样本，$ C $ 控制惩罚强度。</p>
<h3 id="为什么树的数量增加不会导致过拟合">为什么树的数量增加不会导致过拟合？</h3>
<p><strong>核心原因</strong>：随机森林通过<strong>集成学习</strong>和<strong>多样性机制</strong>抑制了单棵决策树的过拟合风险。具体来说：</p>
<ol type="1">
<li><p><strong>Bagging（自助聚合）机制</strong>：<br>
每棵树的训练数据是通过有放回采样（Bootstrap）得到的子集，这意味着每棵树看到的数据略有不同，减少了对训练数据的“记忆”依赖。</p></li>
<li><p><strong>特征随机选择</strong>：<br>
每次分裂节点时，仅从随机选择的特征子集中挑选最优特征，进一步降低了各树之间的相关性。</p></li>
<li><p><strong>投票/平均机制</strong>：<br>
多棵树的预测结果通过投票（分类）或平均（回归）结合，高方差的个体树被平滑，整体模型的泛化能力增强。</p></li>
<li><p><strong>收敛性保证</strong>：<br>
随着树的数量增加，模型性能逐渐收敛到一个稳定值。即使继续增加树的数量，也不会显著提升训练集性能，更不会过拟合。</p></li>
</ol>
<h3 id="欧式距离的特性分析">欧式距离的特性分析</h3>
<p><strong>欧式距离</strong>（Euclidean
Distance）是衡量欧几里得空间中两点之间直线距离的常用方法，其公式为：
<span class="math display">$$
d(x, y) = \sqrt{\sum_{i=1}^n (x_i - y_i)^2}
$$</span> 以下是对其特性的详细分析：</p>
<p><strong>A. 旋转不变性</strong></p>
<p><strong>正确</strong><br>
- <strong>定义</strong>：若坐标系旋转，两点间的欧式距离保持不变。<br>
- <strong>原因</strong>：旋转是刚性变换（rigid
transformation），仅改变点的坐标表示，但不改变几何距离。<br>
- <strong>示例</strong>：在二维平面中，将坐标系旋转θ角度，两点 <span class="math inline">(<em>x</em><sub>1</sub>,<em>y</em><sub>1</sub>)</span>
和 <span class="math inline">(<em>x</em><sub>2</sub>,<em>y</em><sub>2</sub>)</span>
的旋转后坐标分别为： <span class="math display">(<em>x</em><sub>1</sub>′,<em>y</em><sub>1</sub>′) = (<em>x</em><sub>1</sub>cos<em>θ</em>−<em>y</em><sub>1</sub>sin<em>θ</em>,<em>x</em><sub>1</sub>sin<em>θ</em>+<em>y</em><sub>1</sub>cos<em>θ</em>)</span>
<span class="math display">(<em>x</em><sub>2</sub>′,<em>y</em><sub>2</sub>′) = (<em>x</em><sub>2</sub>cos<em>θ</em>−<em>y</em><sub>2</sub>sin<em>θ</em>,<em>x</em><sub>2</sub>sin<em>θ</em>+<em>y</em><sub>2</sub>cos<em>θ</em>)</span>
计算旋转后的距离仍等于原始距离。</p>
<p><strong>B. 尺度缩放不变性</strong></p>
<p><strong>错误</strong><br>
-
<strong>定义</strong>：若对坐标轴进行非均匀或均匀缩放，欧式距离会发生变化。<br>
- <strong>反例</strong>：假设对某维特征缩放 <span class="math inline"><em>k</em></span> 倍（如将 <span class="math inline"><em>x</em><sub><em>i</em></sub></span> 变为 <span class="math inline"><em>k</em><em>x</em><sub><em>i</em></sub></span>），则距离变为原来的
<span class="math inline"><em>k</em></span> 倍。<br>
-
<strong>结论</strong>：欧式距离<strong>依赖于特征的绝对尺度</strong>，不具备缩放不变性。</p>
<p><strong>C. 不受量纲影响的特性</strong></p>
<p><strong>错误</strong><br>
-
<strong>定义</strong>：若不同特征的量纲不同（如身高[m]与体重[kg]），欧式距离的计算会因量纲差异而失真。<br>
- <strong>反例</strong>：<br>
- 点A：(1.8m, 70kg)，点B：(1.7m, 65kg)<br>
-
若不标准化，身高差（0.1m）与体重差（5kg）的贡献会被直接相加，但两者量纲不同，结果无实际意义。<br>
-
<strong>解决方法</strong>：需通过标准化（如Z-score归一化）消除量纲影响。</p>
<h3 id="下列哪个不属于特征提取">下列哪个不属于特征提取</h3>
<p><strong>答案：D. 主成分分析</strong></p>
<p><strong>解析：</strong></p>
<p>在文本分类的特征选择中，常用的方法包括：</p>
<ul>
<li><strong>A.
卡方检验值</strong>：通过统计检验评估特征与类别的相关性，属于过滤式特征选择方法。</li>
<li><strong>B.
互信息</strong>：基于信息论，衡量特征与类别的依赖关系，属于无监督或半监督的特征选择方法。</li>
<li><strong>C.
信息增益</strong>：基于熵的指标，评估特征对分类的贡献，常用于决策树等算法中的特征选择。</li>
</ul>
<p>而 <strong>D. 主成分分析（PCA）</strong> 是一种
<strong>降维技术</strong>，通过线性变换将高维数据映射到低维空间，其核心目标是保留数据的主要方差，而非直接选择原始特征。它属于
<strong>特征提取</strong>（Feature Extraction）而非传统意义上的
<strong>特征选择</strong>（Feature
Selection）。因此，主成分分析不属于常用的文本分类特征选择算法。</p>
<p>### ridge回归和lasso回归</p>
<p>Ridge回归（岭回归）和Lasso回归（套索回归）是两种常用的<strong>正则化线性回归方法</strong>，主要用于解决线性回归中的<strong>过拟合问题</strong>和<strong>特征选择问题</strong>。它们的核心思想是在损失函数中添加正则化项（惩罚项），从而限制模型参数的大小，提升模型的泛化能力。</p>
<p><strong>1. Ridge回归（岭回归）</strong></p>
<p><strong>目标函数</strong> <span class="math display">$$
\min_{\mathbf{w}} \left\{ \sum_{i=1}^n (y_i - \mathbf{w}^T
\mathbf{x}_i)^2 + \lambda \|\mathbf{w}\|_2^2 \right\}
$$</span> - 第一项是普通线性回归的均方误差（MSE）。 -
第二项是L2正则化项（权重平方的和），<span class="math inline"><em>λ</em> ≥ 0</span>
是正则化系数，控制惩罚强度。</p>
<p><strong>特点</strong></p>
<ul>
<li><strong>L2正则化</strong>：通过缩小权重系数（但不会完全置零）来减少模型复杂度。</li>
<li><strong>解决多重共线性</strong>：当特征之间存在高度相关性时，Ridge回归能稳定回归系数。</li>
<li><strong>唯一解</strong>：目标函数是凸函数，且严格凸，因此有唯一最优解。</li>
<li><strong>计算效率高</strong>：可以通过解析解（闭式解）求解： <span class="math display"><strong>w</strong><sub>Ridge</sub> = (<strong>X</strong><sup><em>T</em></sup><strong>X</strong>+<em>λ</em><strong>I</strong>)<sup>−1</sup><strong>X</strong><sup><em>T</em></sup><strong>y</strong></span></li>
</ul>
<p><strong>应用场景</strong></p>
<ul>
<li>特征维度较低，但存在多重共线性。</li>
<li>需要保留所有特征，但希望抑制其影响（如基因数据分析）。</li>
</ul>
<p><strong>2. Lasso回归（套索回归）</strong></p>
<p><strong>目标函数</strong> <span class="math display">$$
\min_{\mathbf{w}} \left\{ \sum_{i=1}^n (y_i - \mathbf{w}^T
\mathbf{x}_i)^2 + \lambda \|\mathbf{w}\|_1 \right\}
$$</span> - 第一项是均方误差。 -
第二项是L1正则化项（权重绝对值的和），<span class="math inline"><em>λ</em> ≥ 0</span> 是正则化系数。</p>
<p><strong>特点</strong></p>
<ul>
<li><strong>L1正则化</strong>：强制部分权重系数为零，实现特征选择。</li>
<li><strong>稀疏模型</strong>：适用于高维数据（如文本分类、基因数据），自动筛选关键特征。</li>
<li><strong>非唯一解</strong>：目标函数是凸函数，但可能有多个解（当特征高度相关时）。</li>
<li><strong>计算复杂度较高</strong>：通常需要迭代优化算法（如坐标下降法、近端梯度下降）。</li>
</ul>
<p><strong>应用场景</strong></p>
<ul>
<li>特征维度极高（如万维以上），需降维。</li>
<li>需要可解释性强的模型（如金融风控中的关键特征筛选）。</li>
</ul>
<p><strong>3. 总结</strong></p>
<ul>
<li><strong>Ridge回归</strong>：适合特征较少且需要稳定系数的场景。</li>
<li><strong>Lasso回归</strong>：适合高维数据和特征选择场景。</li>
<li><strong>实际选择</strong>：
<ul>
<li>如果特征数量远大于样本数量（<span class="math inline"><em>p</em> ≫ <em>n</em></span>），优先使用Lasso。</li>
<li>如果特征间存在强相关性，优先使用Ridge或弹性网络。</li>
</ul></li>
</ul>
<p>通过调整正则化系数 <span class="math inline"><em>λ</em></span>，可以控制模型的复杂度与泛化能力。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/06/09/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/06/09/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0/" class="post-title-link" itemprop="url">计算机系统基础——期末复习</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-06-09 00:00:00" itemprop="dateCreated datePublished" datetime="2025-06-09T00:00:00+08:00">2025-06-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-06-30 11:46:47" itemprop="dateModified" datetime="2025-06-30T11:46:47+08:00">2025-06-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%A4%A7%E4%BA%8C%E4%B8%8B/" itemprop="url" rel="index"><span itemprop="name">大二下</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/" itemprop="url" rel="index"><span itemprop="name">计算机系统基础</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="第一章">第一章</h3>
<p><strong>时钟频率（f）</strong>
：单位时间内完成的时钟周期数，单位为赫兹（Hz）。 例如：800MHz
表示每秒完成 800×106 个周期。</p>
<p><strong>时钟周期（T）</strong>
：完成一个时钟周期所需的时间，单位为秒（s）。 例如：800MHz 的时钟周期为
<em>T</em>=800×1061​s=1.25ns （纳秒）。</p>
<p><strong>CPI</strong>（<strong>Cycles Per
Instruction</strong>，每条指令所需的时钟周期数）是衡量计算机体系结构性能的关键指标之一，用于描述<strong>CPU执行一条指令平均需要多少个时钟周期</strong>。它直接影响程序的执行速度和系统性能。</p>
<ul>
<li><strong>CPI</strong>
表示每条指令执行所需的平均时钟周期数，计算公式为： <span class="math display">$$
\text{CPI} = \frac{\text{总时钟周期数}}{\text{总指令数}}
$$</span></li>
<li><strong>执行时间</strong> 与 CPI 的关系： <span class="math display">执行时间 = 指令数 × CPI × 时钟周期时间</span>
其中，时钟周期时间 = 1 / 时钟频率。</li>
</ul>
<p><strong>MIPS（Million Instructions Per Second）</strong>
是衡量计算机处理器性能的一个经典指标，表示
<strong>每秒执行的百万条指令数</strong>，用于量化 CPU
的指令处理能力。其核心思想是：<strong>数值越大，性能越强</strong>，但需注意其局限性。</p>
<ul>
<li><strong>MIPS</strong> = 指令数 / (执行时间 × 10⁶)<br>
</li>
<li>或通过 <strong>时钟频率</strong> 和 <strong>CPI（Cycles Per
Instruction）</strong> 计算：<br>
<span class="math display">$$
\text{MIPS} = \frac{\text{时钟频率（Hz）}}{\text{CPI} \times 10^6}
$$</span></li>
</ul>
<p><strong>举例</strong>：<br>
- 若 CPU 主频为 <strong>2 GHz</strong>（2×10⁹ Hz），平均
CPI=4，则：<br>
<span class="math display">$$
  \text{MIPS} = \frac{2 \times 10^9}{4 \times 10^6} = 500 \text{ MIPS}
  $$</span></p>
<p>数量级：</p>
<p>G，吉，十的九次方</p>
<p>n，纳，十的负九次方</p>
<p><strong>m（milli，毫）的数量级是 10−3 （千分之一）</strong> 。</p>
<h3 id="第二章">第二章</h3>
<h4 id="补码">补码</h4>
<p><strong>1. 补码的定义</strong></p>
<p>补码（Two’s
Complement）是计算机中表示有符号整数的标准方法，其核心作用是将减法运算转化为加法运算，从而简化硬件设计。</p>
<p><strong>2. 如何求一个数的补码？</strong></p>
<p>以 <strong>8位二进制</strong> 为例： - <strong>正数</strong>：补码 =
原码（符号位为0，其余位直接表示数值）。<br>
例如：<code>+5</code> 的补码是 <code>00000101</code>。</p>
<ul>
<li><strong>负数</strong>：补码 =
原码的符号位不变，其余位取反（反码），然后末位加1。<br>
例如：求 <code>-5</code> 的补码：
<ol type="1">
<li>原码：<code>10000101</code>（符号位为1，其余位为5的二进制）。</li>
<li>取反（符号位保留）：<code>11111010</code>（反码）。</li>
<li>加1：<code>11111010 + 1 = 11111011</code>（补码）。</li>
</ol></li>
</ul>
<p><strong>3. 数学原理：模运算</strong></p>
<p>补码的本质是基于 <strong>模（Modulo）运算</strong>。<br>
- 对于 <strong>n位二进制数</strong>，其模为 <span class="math inline">2<sup><em>n</em></sup></span>。<br>
- 负数的补码表示为：<br>
<span class="math display"> − <em>x</em> ≡ 2<sup><em>n</em></sup> − <em>x</em> (mod 2<sup><em>n</em></sup>)</span>
例如，8位二进制数的模是 <span class="math inline">2<sup>8</sup> = 256</span>，因此：<br>
<span class="math inline"> − 5</span> 的补码 = <span class="math inline">256 − 5 = 251</span>，二进制表示为
<code>11111011</code>。</p>
<p><strong>4. 为什么“取反 + 1”有效？</strong></p>
<ul>
<li><strong>取反</strong>：相当于将数值部分取反（即 <span class="math inline"><em>x</em> → (2<sup><em>n</em> − 1</sup>−1−<em>x</em>)</span>）。</li>
<li><strong>加1</strong>：最终得到 <span class="math inline">2<sup><em>n</em></sup> − <em>x</em></span>，即补码的数学定义。</li>
</ul>
<p>以 <code>-5</code> 为例（8位）： 1.
原码：<code>10000101</code>（符号位为1，数值部分为5）。 2.
取反：<code>11111010</code>（数值部分取反，符号位保留）。 3.
加1：<code>11111010 + 1 = 11111011</code>，即 <span class="math inline">251 = 256 − 5</span>。</p>
<p><strong>5. 补码的优势</strong></p>
<ul>
<li><strong>唯一零表示</strong>：补码中只有
<strong>一个零</strong>（<code>00000000</code>），而原码和反码存在
<code>+0</code> 和 <code>-0</code> 的问题。</li>
<li><strong>加减统一</strong>：所有加减运算均通过加法器完成，无需单独的减法器。<br>
例如：<code>5 - 3 = 5 + (-3)</code>，直接通过补码相加即可。</li>
<li><strong>溢出自动处理</strong>：超过范围的高位会自然丢弃（模运算特性）。</li>
</ul>
<p><strong>6. 特殊情况：最小负数</strong></p>
<p>对于 <strong>n位补码</strong>，能表示的范围是：<br>
<span class="math display">[−2<sup><em>n</em> − 1</sup>, 2<sup><em>n</em> − 1</sup>−1]</span>
- 例如，8位补码范围是：<code>-128</code>（<code>10000000</code>）到
<code>+127</code>（<code>01111111</code>）。 -
<strong>最小负数（-128）</strong> 没有对应的正数（因为 <span class="math inline"> + 128</span> 超出范围），其补码直接定义为
<code>10000000</code>，无法通过“取反 + 1”从原码推导（因为原码中不存在
<code>+128</code>）。</p>
<h4 id="移码offset-binary详解"><strong>移码（Offset
Binary）详解</strong></h4>
<p><strong>1. 移码的定义</strong></p>
<p>移码是一种<strong>带偏移量的编码方式</strong>，主要用于表示<strong>浮点数的阶码</strong>（Exponent）。其核心思想是将真值（实际数值）加上一个固定的偏移量（Bias），使得所有数值映射到<strong>非负数范围</strong>，从而简化比较和运算。</p>
<p><strong>公式</strong>：<br>
<span class="math display">移码 = 真值 + 偏移量</span></p>
<p><strong>2. 移码的核心作用</strong></p>
<ul>
<li><strong>简化比较</strong>：<br>
移码将负数范围映射到正数范围，使得可以直接通过<strong>无符号整数比较</strong>来判断阶码的大小。
<ul>
<li>例如：<br>
在浮点数中，阶码 <span class="math inline"> − 3</span> 和 <span class="math inline"> + 2</span> 的移码分别为 <span class="math inline">125</span> 和 <span class="math inline">130</span>（偏移量为127），直接比较 <span class="math inline">125 &lt; 130</span> 即可得出 <span class="math inline"> − 3 &lt;  + 2</span>。</li>
</ul></li>
<li><strong>消除负数表示</strong>：<br>
移码将负数转换为正数表示，避免了补码中负数符号位的影响。</li>
</ul>
<p><strong>3. 偏移量的选择</strong></p>
<p>偏移量通常为 <span class="math inline">2<sup><em>n</em> − 1</sup></span> 或 <span class="math inline">2<sup><em>n</em> − 1</sup> − 1</span>（<span class="math inline"><em>n</em></span> 为位数）： -
<strong>单精度浮点数（32位）</strong>：偏移量为 <span class="math inline">127</span>（即 <span class="math inline">2<sup>7</sup> − 1</span>）。<br>
- <strong>双精度浮点数（64位）</strong>：偏移量为 <span class="math inline">1023</span>（即 <span class="math inline">2<sup>10</sup> − 1</span>）。</p>
<p><strong>4. 移码与补码的关系</strong></p>
<ul>
<li><strong>符号位取反</strong>：<br>
移码可以看作是<strong>补码的符号位取反</strong>。例如：
<ul>
<li>补码 <code>10000000</code>（<span class="math inline"> − 128</span>）的移码为 <code>00000000</code>（<span class="math inline"> − 128 + 128 = 0</span>）。<br>
</li>
<li>补码 <code>00000000</code>（<span class="math inline">0</span>）的移码为 <code>10000000</code>（<span class="math inline">0 + 128 = 128</span>）。</li>
</ul></li>
<li><strong>本质区别</strong>：
<ul>
<li><strong>补码</strong>：用于定点数的加减运算，支持负数和正数的统一处理。<br>
</li>
<li><strong>移码</strong>：用于浮点数阶码的表示，便于直接比较大小。</li>
</ul></li>
</ul>
<p><strong>5. 移码的应用场景</strong></p>
<ul>
<li><strong>IEEE 754浮点数标准</strong>：<br>
移码用于表示浮点数的阶码（Exponent），使得阶码可以直接按无符号整数比较。
<ul>
<li><strong>单精度（32位）</strong>：<br>
阶码占8位，偏移量为127。<br>
真值 <span class="math inline"><em>E</em></span> 的移码为 <span class="math inline"><em>E</em> + 127</span>。<br>
</li>
<li><strong>双精度（64位）</strong>：<br>
阶码占11位，偏移量为1023。<br>
真值 <span class="math inline"><em>E</em></span> 的移码为 <span class="math inline"><em>E</em> + 1023</span>。</li>
</ul></li>
</ul>
<h4 id="浮点数表示">浮点数表示</h4>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1VK4y1f7o6?spm_id_from=333.788.videopod.sections&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">【CSAPP-深入理解计算机系统】2-4.浮点数(上)_哔哩哔哩_bilibili</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Le4y137gU/?spm_id_from=333.337.search-card.all.click&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">【计算机知识】定点数与浮点数（2）浮点数法表示方法！_哔哩哔哩_bilibili</a></p>
<h4 id="进制转换">进制转换</h4>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1ke411T7Qr?spm_id_from=333.788.videopod.sections&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">【计算机基础】进制转换(3)
小数部分如何进行转换？_哔哩哔哩_bilibili</a></p>
<h4 id="整数加减">整数加减</h4>
<h4 id="浮点数加减">浮点数加减</h4>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1894y1C7br/?spm_id_from=333.337.search-card.all.click&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">浮点数加减法运算
白中英计算机组成原理期末速成_哔哩哔哩_bilibili</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1ue4y1s71Z/?spm_id_from=333.337.search-card.all.click&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">(自用)计算机组成原理
题型三 浮点数加减法运算题_哔哩哔哩_bilibili</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1ej411J71a/?spm_id_from=333.337.search-card.all.click&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">浮点运算（浮点数加减运算）计算机组成原理（看了包会）_哔哩哔哩_bilibili</a></p>
<p>ieee</p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1nwTXz7EVi/?spm_id_from=333.337.search-card.all.click&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">计算机组成原理期末复习（5分钟）：IEEE754浮点数加减计算！_哔哩哔哩_bilibili</a></p>
<h4 id="位数">位数</h4>
<p>short 16位</p>
<h3 id="第三章-程序的转换与机器级表示"><strong>第三章
程序的转换与机器级表示</strong></h3>
<h4 id="结构体与联合体">结构体与联合体</h4>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1754y1Y7Ut?spm_id_from=333.788.player.switch&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">【CSAPP-深入理解计算机系统】3-9.结构体与联合体_哔哩哔哩_bilibili</a></p>
<h4 id="数组的分配和访问">数组的分配和访问</h4>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1ho4y1d7J6?spm_id_from=333.788.videopod.sections&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">【CSAPP-深入理解计算机系统】3-8.数组的分配和访问_哔哩哔哩_bilibili</a></p>
<h4 id="过程调用">过程调用</h4>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1By4y1x7Yh/?spm_id_from=333.337.search-card.all.click&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">C程序在内存中的栈_哔哩哔哩_bilibili</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV19X4y1P7Pn?spm_id_from=333.788.videopod.sections&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">【CSAPP-深入理解计算机系统】3-7.
过程（函数调用）_哔哩哔哩_bilibili</a></p>
<h4 id="att格式">AT&amp;T格式</h4>
<p>AT&amp;T格式是汇编语言中的一种语法风格，主要用于x86/x64架构的汇编代码编写。它与Intel格式并列为最常见的两种汇编语法，两者在语法细节上有显著差异。以下是AT&amp;T格式的核心特点、示例及常见用途：</p>
<p><strong>主要特点</strong></p>
<table>
<colgroup>
<col style="width: 16%">
<col style="width: 47%">
<col style="width: 36%">
</colgroup>
<thead>
<tr class="header">
<th>特性</th>
<th>AT&amp;T格式语法</th>
<th>对比Intel格式语法</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>寄存器</strong></td>
<td>前缀 <code>%</code>（如 <code>%eax</code>）</td>
<td>无前缀（如 <code>eax</code>）</td>
</tr>
<tr class="even">
<td><strong>立即数</strong></td>
<td>前缀 <code>$</code>（如 <code>$0x10</code>）</td>
<td>直接使用数值（如 <code>10</code>）</td>
</tr>
<tr class="odd">
<td><strong>操作数顺序</strong></td>
<td>源操作数在前，目标在后</td>
<td>目标在前，源在后</td>
</tr>
<tr class="even">
<td><strong>内存寻址</strong></td>
<td><code>offset(base, index, scale)</code></td>
<td><code>[base + index*scale + offset]</code></td>
</tr>
<tr class="odd">
<td><strong>指令后缀</strong></td>
<td>通过后缀标明操作数大小（如 <code>l</code> 表示32位）</td>
<td>无后缀，由操作数推断</td>
</tr>
</tbody>
</table>
<h4 id="寄存器种类">寄存器种类</h4>
<ul>
<li>8 个通用寄存器，其中
<ul>
<li><code>EAX, EBX, ECX, EDX</code> 均为 32 位寄存器</li>
<li><code>AX, BX, CX, DX</code> 均为 16 位寄存器</li>
<li><code>AH, BH, CH, DH</code> 均为高 8 位寄存器</li>
<li><code>AL, BL, CL, DL</code> 均为低 8 位寄存器</li>
</ul></li>
<li>2 个专用寄存器</li>
<li>6 个段寄存器</li>
</ul>
<h4 id="操作数寻址方式">操作数寻址方式</h4>
<p><strong>1. 基础内存寻址模式</strong></p>
<p><strong>(1) 直接寻址（Direct Addressing）</strong>
cccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc</p>
<ul>
<li><strong>语法</strong>：<code>offset</code>（AT&amp;T格式）或
<code>[offset]</code>（Intel格式）。</li>
<li><strong>用途</strong>：直接访问全局变量或静态数据。</li>
<li><strong>示例</strong>： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">movl var(%rip), %eax  # AT&amp;T格式（RIP相对寻址，64位模式推荐）</span><br></pre></td></tr></table></figure> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mov eax, [var]        # Intel格式（32位模式）</span><br></pre></td></tr></table></figure></li>
</ul>
<p><strong>(2) 寄存器间接寻址（Register Indirect
Addressing）</strong></p>
<ul>
<li><strong>语法</strong>：<code>(base_register)</code> 或
<code>[base_register]</code></li>
<li><strong>用途</strong>：指针解引用。</li>
<li><strong>示例</strong>： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">movl (%eax), %ebx     # 将EAX指向的内存值传入EBX</span><br></pre></td></tr></table></figure> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mov ebx, [eax]</span><br></pre></td></tr></table></figure></li>
</ul>
<p><strong>(3) 基址寻址（Base Addressing）</strong></p>
<ul>
<li><strong>语法</strong>：<code>offset(base_register)</code> 或
<code>[base_register + offset]</code></li>
<li><strong>用途</strong>：访问栈帧中的局部变量或结构体成员。</li>
<li><strong>示例</strong>： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">movl 8(%ebp), %ecx    # 从栈帧偏移8处读取数据到ECX</span><br></pre></td></tr></table></figure> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mov ecx, [ebp + 8]</span><br></pre></td></tr></table></figure></li>
</ul>
<p><strong>(4) 变址寻址（Indexed Addressing）比例寻址</strong></p>
<ul>
<li><p><strong>语法</strong>：<code>array(, index_register, scale)</code>
或 <code>[array + index_register*scale]</code></p></li>
<li><p><strong>用途</strong>：数组元素访问。</p></li>
<li><p><strong>示例</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">movl array(,%eax,4), %edx  # 数组array + EAX*4位置的值传入EDX（数组索引）</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mov edx, [array + eax*4]</span><br></pre></td></tr></table></figure></li>
</ul>
<p><strong>2. 组合寻址模式</strong></p>
<p><strong>(1) 基址 + 变址（Base + Index）</strong></p>
<ul>
<li><p><strong>语法</strong>：<code>(base_register, index_register)</code>
或 <code>[base_register + index_register]</code></p></li>
<li><p><strong>用途</strong>：访问二维数组或动态分配的数组。</p></li>
<li><p><strong>示例</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">movl (%ebx, %esi), %edi  # 将EBX + ESI指向的内存值传入EDI</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mov edi, [ebx + esi]</span><br></pre></td></tr></table></figure></li>
</ul>
<p>**(2) 基址 + 比例变址（Base + Index*Scale）**</p>
<ul>
<li><strong>语法</strong>：<code>(base_register, index_register, scale)</code>
或 <code>[base_register + index_register*scale]</code></li>
<li><strong>用途</strong>：按元素大小（scale）访问数组。</li>
<li><strong>示例</strong>： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">movl (%ebx, %esi, 4), %edi  # 将EBX + ESI*4指向的内存值传入EDI（4字节元素）</span><br></pre></td></tr></table></figure> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mov edi, [ebx + esi*4]</span><br></pre></td></tr></table></figure></li>
</ul>
<p>**(3) 基址 + 比例变址 + 偏移（Base + Index*Scale + Offset）**</p>
<ul>
<li><strong>语法</strong>：<code>offset(base_register, index_register, scale)</code>
或 <code>[base_register + index_register*scale + offset]</code></li>
<li><strong>用途</strong>：访问结构体数组或复杂数据结构。</li>
<li><strong>示例</strong>： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">movl 12(%ebx, %esi, 8), %edi  # 结构体数组中第ESI个元素的偏移12处数据传入EDI</span><br></pre></td></tr></table></figure> <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mov edi, [ebx + esi*8 + 12]</span><br></pre></td></tr></table></figure></li>
</ul>
<p><strong>3.总结</strong></p>
<table>
<colgroup>
<col style="width: 27%">
<col style="width: 28%">
<col style="width: 43%">
</colgroup>
<thead>
<tr class="header">
<th>寻址模式</th>
<th>AT&amp;T格式语法</th>
<th>Intel格式语法</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>直接寻址</td>
<td><code>var(%rip)</code></td>
<td><code>[rip + var]</code>（64位）或 <code>var</code></td>
</tr>
<tr class="even">
<td>寄存器间接寻址</td>
<td><code>(%eax)</code></td>
<td><code>[eax]</code></td>
</tr>
<tr class="odd">
<td>基址寻址</td>
<td><code>8(%ebp)</code></td>
<td><code>[ebp + 8]</code></td>
</tr>
<tr class="even">
<td>变址寻址</td>
<td><code>array(,%eax,4)</code></td>
<td><code>[array + eax*4]</code></td>
</tr>
<tr class="odd">
<td>基址+比例变址</td>
<td><code>(%ebx, %esi, 4)</code></td>
<td><code>[ebx + esi*4]</code></td>
</tr>
<tr class="even">
<td>基址+比例变址+偏移</td>
<td><code>12(%ebx, %esi, 8)</code></td>
<td><code>[ebx + esi*8 + 12]</code></td>
</tr>
</tbody>
</table>
<h4 id="指令后缀">指令后缀</h4>
<p>在 AT&amp;T 汇编格式中，<strong>指令后缀</strong>（如
<code>b</code>、<code>w</code>、<code>l</code>、<code>q</code>）用于明确操作数的大小，确保汇编器正确生成机器码。判断后缀的核心规则是：<strong>根据操作数的大小选择对应的后缀</strong>，尤其是寄存器的位数或内存操作数的显式指定。以下是详细说明：</p>
<p><strong>后缀与操作数大小的对应关系</strong></p>
<table>
<thead>
<tr class="header">
<th>后缀</th>
<th>操作数大小</th>
<th>示例寄存器/操作数</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>b</code></td>
<td>byte（8位）</td>
<td><code>%al</code>, <code>$0x10</code>,
<code>12(%ebp)</code>（需显式指定）</td>
</tr>
<tr class="even">
<td><code>w</code></td>
<td>word（16位）</td>
<td><code>%ax</code>, <code>%bx</code>,
<code>12(%ebp)</code>（需显式指定）</td>
</tr>
<tr class="odd">
<td><code>l</code></td>
<td>long（32位）</td>
<td><code>%eax</code>, <code>%ebx</code>,
<code>12(%ebp)</code>（需显式指定）</td>
</tr>
<tr class="even">
<td><code>q</code></td>
<td>quad（64位）</td>
<td><code>%rax</code>, <code>%rbx</code>,
<code>12(%ebp)</code>（需显式指定）</td>
</tr>
</tbody>
</table>
<blockquote>
<p><strong>立即数默认为32位</strong></p>
</blockquote>
<h4 id="判断指针与临时变量">判断“指针”与“临时变量</h4>
<p><strong>（1）<code>%edx</code>：临时变量</strong></p>
<ul>
<li><strong>特征</strong>：直接从寄存器 <code>%edx</code>
读取数据，不涉及内存地址的间接访问。</li>
<li><strong>对应C语言</strong>：<br>
如果 <code>%edx</code> 存储的是某个局部变量或计算结果（如
<code>temp = a + b</code>），则对应<strong>临时变量</strong>。</li>
</ul>
<p><strong>（2）<code>(%ecx)</code>：指针</strong></p>
<ul>
<li><strong>特征</strong>：<code>%ecx</code>
中存储的是内存地址，<code>(%ecx)</code> 表示解引用该地址（类似C语言的
<code>*ptr</code>）。<br>
</li>
<li><strong>对应C语言</strong>：<br>
如果 <code>%ecx</code> 存储的是一个指针变量（如
<code>int *ptr</code>），则 <code>(%ecx)</code>
对应<strong>指针解引用</strong>。</li>
</ul>
<p><strong>关键结论</strong></p>
<table>
<colgroup>
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 78%">
</colgroup>
<thead>
<tr class="header">
<th>操作数</th>
<th>类型</th>
<th>判断依据</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>%edx</code></td>
<td>临时变量</td>
<td>直接从寄存器读取数据，无间接内存访问（无括号）。</td>
</tr>
<tr class="even">
<td><code>(%ecx)</code></td>
<td>指针</td>
<td>使用括号 <code>(%ecx)</code> 表示解引用内存地址（类似C语言的
<code>*ptr</code>）。</td>
</tr>
</tbody>
</table>
<p><strong>常见模式对比</strong></p>
<table>
<colgroup>
<col style="width: 24%">
<col style="width: 17%">
<col style="width: 58%">
</colgroup>
<thead>
<tr class="header">
<th>汇编指令</th>
<th>C语言对应操作</th>
<th>解释</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>movl %eax, (%ebx)</code></td>
<td><code>*ptr = temp;</code></td>
<td><code>%ebx</code> 是指针（存储地址），<code>%eax</code>
是临时变量。</td>
</tr>
<tr class="even">
<td><code>movl (%ebx), %eax</code></td>
<td><code>temp = *ptr;</code></td>
<td>从指针 <code>ptr</code> 读取值到临时变量 <code>temp</code>。</td>
</tr>
<tr class="odd">
<td><code>movl $0x1, %eax</code></td>
<td><code>temp = 1;</code></td>
<td><code>%eax</code> 是临时变量，直接赋值。</td>
</tr>
</tbody>
</table>
<h4 id="汇编语言中m的作用">汇编语言中M的作用</h4>
<p>在汇编语言中，<strong>M</strong> 通常表示
<strong>内存（Memory）</strong>，用于指示操作数来自内存地址。在你的问题中，<code>M[R[eax]]</code>
的含义是：</p>
<p><strong><code>M</code> 的作用</strong></p>
<ul>
<li><strong><code>M[地址]</code></strong> 表示从 <strong>内存地址为
<code>地址</code> 的位置读取数据</strong>。</li>
<li><strong><code>R[eax]</code></strong> 表示寄存器 <code>EAX</code>
的值（即 <code>EAX</code> 中存储的内容）。</li>
<li>因此，<code>M[R[eax]]</code> 的含义是： &gt; <strong>以
<code>EAX</code>
寄存器的值作为内存地址，从该地址读取数据</strong>。</li>
</ul>
<p>** AT&amp;T 汇编中的等价写法**</p>
<p>在 AT&amp;T 汇编语法中，<code>M[R[eax]]</code> 对应的写法是：
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">addl (%eax), %edx</span><br></pre></td></tr></table></figure> - <strong>含义</strong>： - <code>(%eax)</code>：以
<code>EAX</code> 的值为内存地址，读取该地址的内容（默认是 4 字节，即 32
位）。 - <code>addl</code>：执行 32 位加法。 -
<code>%edx</code>：目标寄存器，存储结果。</p>
<p><strong>关键点总结</strong></p>
<table>
<thead>
<tr class="header">
<th>符号</th>
<th>含义</th>
<th>示例</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>R</code></td>
<td>寄存器（Register）</td>
<td><code>R[eax]</code> → <code>EAX</code> 的值</td>
</tr>
<tr class="even">
<td><code>M</code></td>
<td>内存（Memory）</td>
<td><code>M[地址]</code> → 从地址读取数据</td>
</tr>
<tr class="odd">
<td><code>()</code></td>
<td>AT&amp;T 汇编中表示内存寻址</td>
<td><code>(%eax)</code> → 等价于 <code>M[R[eax]]</code></td>
</tr>
</tbody>
</table>
<h4 id="常见att格式汇编指令">常见AT&amp;T格式汇编指令</h4>
<table>
<colgroup>
<col style="width: 12%">
<col style="width: 15%">
<col style="width: 28%">
<col style="width: 43%">
</colgroup>
<thead>
<tr class="header">
<th>指令类型</th>
<th>操作目的</th>
<th>影响标志位</th>
<th>典型用途</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>addl</code></td>
<td>加法</td>
<td>OF, SF, ZF, CF</td>
<td>数值运算、地址偏移</td>
</tr>
<tr class="even">
<td><code>subl</code></td>
<td>减法</td>
<td>OF, SF, ZF, CF</td>
<td>数值运算、条件判断</td>
</tr>
<tr class="odd">
<td><code>orl</code></td>
<td>按位或</td>
<td>OF=0, SF, ZF, CF=0</td>
<td>位掩码操作</td>
</tr>
<tr class="even">
<td><code>testl</code></td>
<td>按位与测试</td>
<td>OF=0, SF, ZF, CF=0</td>
<td>条件判断（如检查位是否设置）</td>
</tr>
<tr class="odd">
<td><code>imull</code></td>
<td>有符号乘法</td>
<td>OF, CF</td>
<td>数值运算</td>
</tr>
<tr class="even">
<td><code>leal</code></td>
<td>地址计算</td>
<td>无影响</td>
<td>高效数组索引计算</td>
</tr>
<tr class="odd">
<td><code>decl</code></td>
<td>递减</td>
<td>OF, SF, ZF, CF</td>
<td>循环计数、边界检查</td>
</tr>
</tbody>
</table>
<p><strong><code>sall</code>（Shift Arithmetic Left）——
左移指令</strong></p>
<p><strong>功能</strong></p>
<ul>
<li><strong>作用</strong> ：将操作数的二进制位 <strong>向左移动</strong>
指定的位数，低位补0。</li>
<li><strong>效果</strong> ：相当于将操作数乘以 2<em>n</em> （n
为移动的位数）。</li>
</ul>
<p><strong><code>and</code>（Logical AND）—— 逻辑与指令</strong></p>
<p><strong>功能</strong></p>
<ul>
<li><strong>作用</strong> ：对两个操作数进行 <strong>按位与运算</strong>
，结果写入目标操作数。</li>
<li><strong>效果</strong> ：只有对应位都为1时，结果位才为1。</li>
</ul>
<p><code>shrl</code> 是 <strong>逻辑右移指令</strong> （Shift Right
Logical），用于对操作数进行 <strong>无符号右移</strong> ，即高位补
0，低位移出。</p>
<p><code>leal</code> 是 <strong>加载有效地址（Load Effective
Address）</strong> 的指令，其功能是
<strong>计算内存地址并存储到目标寄存器</strong> ，但
<strong>不会访问内存</strong> 。它常用于 <strong>地址计算</strong> 和
<strong>高效算术运算</strong></p>
<h4 id="标志位">标志位</h4>
<p>以下是 <strong>x86/x64
架构中常见的四个状态标志位</strong>（OF、SF、ZF、CF）的详细说明及其判断方法：</p>
<p><strong>1. 标志位概述</strong></p>
<table>
<colgroup>
<col style="width: 7%">
<col style="width: 16%">
<col style="width: 75%">
</colgroup>
<thead>
<tr class="header">
<th>标志</th>
<th>全称</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>CF</strong></td>
<td>Carry Flag</td>
<td><strong>无符号溢出标志</strong>：表示无符号数运算是否产生进位或借位。</td>
</tr>
<tr class="even">
<td><strong>ZF</strong></td>
<td>Zero Flag</td>
<td><strong>零标志</strong>：表示运算结果是否为零。</td>
</tr>
<tr class="odd">
<td><strong>SF</strong></td>
<td>Sign Flag</td>
<td><strong>符号标志</strong>：表示运算结果的最高位（符号位）是否为1（负数）。</td>
</tr>
<tr class="even">
<td><strong>OF</strong></td>
<td>Overflow Flag</td>
<td><strong>溢出标志</strong>：表示有符号数运算是否溢出（结果超出数据类型表示范围）。</td>
</tr>
</tbody>
</table>
<p><strong>2. 判断方法详解</strong></p>
<p><strong>(1) 进位标志（CF）</strong></p>
<ul>
<li><strong>用途</strong>：判断 <strong>无符号数运算</strong>
是否溢出。</li>
<li><strong>判断规则</strong>：
<ul>
<li><strong>加法</strong>：若结果最高位（最高有效位）发生进位（超过数据类型的最大值），CF=1。</li>
<li><strong>减法</strong>：若结果需要借位（被减数 &lt;
减数），CF=1。</li>
</ul></li>
</ul>
<p><strong>(2) 零标志（ZF）</strong></p>
<ul>
<li><strong>用途</strong>：判断运算结果是否为零。</li>
<li><strong>判断规则</strong>：
<ul>
<li><strong>结果为0</strong> → ZF=1</li>
<li><strong>结果非0</strong> → ZF=0</li>
</ul></li>
</ul>
<p><strong>(3) 符号标志（SF）</strong></p>
<ul>
<li><strong>用途</strong>：表示运算结果的符号（正/负）。</li>
<li><strong>判断规则</strong>：
<ul>
<li><strong>结果最高位为1</strong>（负数）→ SF=1</li>
<li><strong>结果最高位为0</strong>（正数）→ SF=0</li>
</ul></li>
</ul>
<p><strong>(4) 溢出标志（OF）</strong></p>
<ul>
<li><strong>用途</strong>：判断 <strong>有符号数运算</strong>
是否溢出。</li>
<li><strong>判断规则</strong>：
<ul>
<li><strong>溢出条件</strong>：两个正数相加结果为负，或两个负数相加结果为正
→ OF=1。</li>
<li><strong>无溢出</strong>：其他情况 → OF=0。</li>
</ul></li>
</ul>
<h4 id="栈帧布局和参数偏移计算规则">栈帧布局和参数偏移计算规则</h4>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1sV411b7c1/?spm_id_from=333.337.search-card.all.click&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">【CSAPP-深入理解计算机系统】3-3.栈与数据传送指令_哔哩哔哩_bilibili</a></p>
<p><strong>1. 参数压栈顺序</strong></p>
<p>C语言默认使用 <strong><code>cdecl</code>
调用约定</strong>，参数<strong>从右到左</strong>压入栈中。例如，函数调用
<code>operate(x, y, z, k)</code> 的压栈顺序为： <figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">push k;     <span class="comment">// 第四个参数（最右边）</span></span><br><span class="line">push z;     <span class="comment">// 第三个参数</span></span><br><span class="line">push y;     <span class="comment">// 第二个参数</span></span><br><span class="line">push x;     <span class="comment">// 第一个参数（最左边）</span></span><br><span class="line">call operate;</span><br></pre></td></tr></table></figure>
栈中参数布局（高地址 → 低地址）： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">高地址</span><br><span class="line">| k  (参数4) | ← 栈顶（ESP）</span><br><span class="line">| z  (参数3) |</span><br><span class="line">| y  (参数2) |</span><br><span class="line">| x  (参数1) |</span><br><span class="line">| 返回地址   |</span><br><span class="line">低地址</span><br></pre></td></tr></table></figure></p>
<p><strong>2. 栈帧建立过程</strong></p>
<p>进入函数 <code>operate</code> 后，通过以下指令建立栈帧：
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pushl %ebp        ; 保存旧的EBP（栈帧基址）</span><br><span class="line">movl %esp, %ebp   ; 将当前栈顶（ESP）赋值给EBP，作为新栈帧的基址</span><br></pre></td></tr></table></figure> 此时栈帧布局如下： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">高地址</span><br><span class="line">| k  (参数4) | ← EBP + 20</span><br><span class="line">| z  (参数3) | ← EBP + 16</span><br><span class="line">| y  (参数2) | ← EBP + 12</span><br><span class="line">| x  (参数1) | ← EBP + 8</span><br><span class="line">| 返回地址   | ← EBP + 4</span><br><span class="line">| 旧 EBP     | ← EBP</span><br><span class="line">低地址</span><br></pre></td></tr></table></figure></p>
<p><strong>3. 参数地址的计算逻辑</strong></p>
<ul>
<li><strong><code>EBP + 4</code></strong>：返回地址（由
<code>call</code> 指令自动压栈）。<br>
</li>
<li><strong><code>EBP + 8</code></strong>：第一个参数（<code>x</code>）。<br>
</li>
<li><strong><code>EBP + 12</code></strong>：第二个参数（<code>y</code>）。<br>
</li>
<li><strong><code>EBP + 16</code></strong>：第三个参数（<code>z</code>）。<br>
</li>
<li><strong><code>EBP + 20</code></strong>：第四个参数（<code>k</code>）。</li>
</ul>
<p><strong>原因</strong>：<br>
1.
<strong>参数顺序</strong>：参数从右到左压栈，导致第一个参数（<code>x</code>）位于栈的最低地址（<code>EBP + 8</code>），而第四个参数（<code>k</code>）位于最高地址（<code>EBP + 20</code>）。<br>
2. <strong>偏移计算</strong>：每个参数占用4字节（32位系统中
<code>int</code> 和指针大小），因此偏移量依次递增4。<br>
3. <strong>栈帧基址</strong>：<code>EBP</code> 指向旧的 <code>EBP</code>
值，其上方是返回地址（<code>EBP + 4</code>），再上方是参数。</p>
<h4 id="汇编语言表示程序函数的过程调用">汇编语言表示程序函数的过程调用</h4>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Nt4y1G728/?spm_id_from=333.337.search-card.all.click&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">超硬核！408考研重点！汇编语言表示程序函数的过程调用！23王道计算机组成原理指令系统_哔哩哔哩_bilibili</a></p>
<h4 id="反汇编">反汇编</h4>
<p>反汇编代码是将二进制机器码（如可执行文件、内存转储）转换为
<strong>人类可读的汇编指令</strong>
的结果。它是逆向工程、漏洞分析、调试等领域的核心工具。以下是详细说明：</p>
<p><strong>1. 反汇编代码的定义</strong></p>
<ul>
<li><strong>本质</strong>：将机器码（二进制/十六进制）转换为对应的汇编指令。</li>
<li><strong>作用</strong>：帮助开发者理解程序逻辑、分析恶意软件、调试崩溃原因或研究编译器优化。</li>
</ul>
<p><strong>2. 反汇编代码的典型格式</strong></p>
<p>反汇编代码通常包含以下部分： | <strong>字段</strong> |
<strong>说明</strong> | <strong>示例</strong> | | ————————- |
————————————————– | —————————- | | <strong>地址（Address）</strong> |
指令在内存中的地址（十六进制）。 | <code>0x804838c</code> | |
<strong>机器码（Opcode）</strong> |
对应的原始十六进制机器码（机器指令的二进制表示）。 | <code>74 08</code>
| | <strong>汇编指令（Mnemonic）</strong> | 汇编助记符（如
<code>mov</code>, <code>jmp</code>, <code>call</code>）及操作数。 |
<code>je 0x8048396</code> | | <strong>注释（Comment, 可选）</strong> |
开发者添加的注释（某些工具会自动生成符号信息）。 |
<code>; if (eax == 0) goto label</code> |</p>
<p><strong>示例反汇编代码（AT&amp;T格式）</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">0804838c &lt;main&gt;:</span><br><span class="line">804838c:    74 08                   je     8048396 &lt;main+0xa&gt;</span><br><span class="line">804838e:    b8 00 00 00 00          mov    $0x0, %eax</span><br><span class="line">8048393:    e9 0e 00 00 00          jmp    80483a6 &lt;main+0x1a&gt;</span><br></pre></td></tr></table></figure>
<h4 id="大端小端">大端小端</h4>
<p><strong>小端方式（Little-Endian）</strong> 是一种
<strong>数据在内存中的存储顺序</strong>，其核心特点是： &gt;
<strong>数据的低位字节（LSB, Least Significant
Byte）存储在内存的低地址处，高位字节（MSB, Most Significant
Byte）存储在高地址处</strong>。</p>
<p><strong>1. 小端 vs 大端</strong></p>
<table>
<colgroup>
<col style="width: 14%">
<col style="width: 42%">
<col style="width: 42%">
</colgroup>
<thead>
<tr class="header">
<th><strong>特性</strong></th>
<th><strong>小端（Little-Endian）</strong></th>
<th><strong>大端（Big-Endian）</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>存储顺序</strong></td>
<td>低位字节在前（低地址），高位在后</td>
<td>高位字节在前（低地址），低位在后</td>
</tr>
<tr class="even">
<td><strong>示例</strong></td>
<td><code>0x12345678</code> → 存储为 <code>78 56 34 12</code></td>
<td><code>0x12345678</code> → 存储为 <code>12 34 56 78</code></td>
</tr>
<tr class="odd">
<td><strong>常见平台</strong></td>
<td>x86/x64 架构（Intel/AMD 处理器）</td>
<td>ARM（部分模式）、网络协议（TCP/IP）</td>
</tr>
</tbody>
</table>
<p><strong>2. 小端方式的直观理解</strong></p>
<p><strong>示例：32位整数 <code>0x12345678</code></strong></p>
<ul>
<li><strong>内存地址分配</strong>： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">地址 →    0x1000    0x1001    0x1002    0x1003</span><br><span class="line">         +---------+---------+---------+---------+</span><br><span class="line">         |  0x78   |  0x56   |  0x34   |  0x12   |</span><br><span class="line">         +---------+---------+---------+---------+</span><br></pre></td></tr></table></figure></li>
<li><strong>解释</strong>：
<ul>
<li>数据的最低位字节 <code>0x78</code> 存储在最低地址
<code>0x1000</code>。</li>
<li>高位字节 <code>0x12</code> 存储在最高地址 <code>0x1003</code>。</li>
</ul></li>
</ul>
<h4 id="转移目标地址的计算">转移目标地址的计算</h4>
<p>在 IA-32（x86）架构中，<strong>转移目标地址的计算</strong>依赖于
<strong>指令的长度</strong> 和
<strong>相对偏移量（Displacement）</strong>。以下是详细分析：</p>
<p><strong>1. 转移指令的基本原理</strong></p>
<ul>
<li><strong>相对跳转（Relative Jump）</strong>：转移目标地址 =
<strong>下一条指令地址</strong> + <strong>偏移量</strong>。</li>
<li><strong>偏移量</strong>：有符号的 8 位、16 位或 32 位整数，表示从
<strong>下一条指令地址</strong> 开始的偏移（正向或负向）。</li>
<li><strong>小端方式（Little-Endian）</strong>：多字节偏移量需按小端方式存储（低位字节在前）。</li>
</ul>
<p><strong>2. 示例：<code>call</code> 指令的地址计算</strong></p>
<p><strong>(1) 已知条件</strong></p>
<ul>
<li><strong>指令地址</strong>：<code>0x804838e</code>（<code>call</code>
指令的起始地址）。</li>
<li><strong>机器码</strong>：<code>E8 1E 00 00 00</code>。
<ul>
<li><code>E8</code> 是 <code>call</code> 的操作码。</li>
<li><code>1E 00 00 00</code> 是偏移量（小端方式存储）。</li>
</ul></li>
</ul>
<p><strong>(2) 计算步骤</strong></p>
<ol type="1">
<li><strong>确定指令长度</strong>：
<ul>
<li><code>call</code> 指令占 <strong>5 字节</strong>（1 字节操作码 + 4
字节偏移量）。</li>
</ul></li>
<li><strong>计算下一条指令地址</strong>：
<ul>
<li>下一条指令地址 = 当前指令地址 + 指令长度<br>
= <code>0x804838e + 5 = 0x8048393</code>。</li>
</ul></li>
<li><strong>解析偏移量</strong>：
<ul>
<li>偏移量字段为 <code>1E 00 00 00</code>（小端方式）→ 转换为大端顺序为
<code>0x0000001E</code>（十进制 30）。</li>
</ul></li>
<li><strong>计算转移目标地址</strong>：
<ul>
<li>转移目标地址 = 下一条指令地址 + 偏移量<br>
= <code>0x8048393 + 0x1E = 0x80483B1</code>。</li>
</ul></li>
</ol>
<p><strong>3. 核心公式</strong> <span class="math display">目标地址 = (当前指令地址+指令长度) + 偏移量</span>
- <strong>当前指令地址</strong>：指令的起始地址（如
<code>0x804838e</code>）。 -
<strong>指令长度</strong>：由操作码和操作数决定（如 <code>call</code> 占
5 字节）。 -
<strong>偏移量</strong>：从指令的操作数中提取并转换为有符号整数。</p>
<p><strong>9. 其他指令示例</strong></p>
<p><strong>(1) <code>je</code> 指令</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">804838c:    74 08                   je     0x8048396</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>当前地址</strong>：<code>0x804838c</code>。</li>
<li><strong>指令长度</strong>：2 字节。</li>
<li><strong>偏移量</strong>：<code>0x08</code>（单字节，无需反转）。</li>
<li><strong>目标地址</strong>：<code>0x804838c + 2 + 0x08 = 0x8048396</code>。</li>
</ul>
<p><strong>(2) <code>jmp</code> 指令</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">80483a4:    E9 F6 FF FF FF          jmp    0x804839f</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>当前地址</strong>：<code>0x80483a4</code>。</li>
<li><strong>指令长度</strong>：5 字节。</li>
<li><strong>偏移量</strong>：<code>F6 FF FF FF</code>（小端）→ 补码为
<code>-10</code>（十进制）。</li>
<li><strong>目标地址</strong>：<code>0x80483a4 + 5 + (-10) = 0x804839f</code>。</li>
</ul>
<h4 id="计算下一条指令地址"><strong>计算下一条指令地址</strong></h4>
<p>下一条指令地址=当前指令地址+当前指令长度</p>
<h3 id="第四章-程序的链接">第四章 程序的链接</h3>
<h4 id="重定位">重定位</h4>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1JL411L7ku?spm_id_from=333.788.videopod.sections&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">【CSAPP-深入理解计算机系统】7-6.
重定位_哔哩哔哩_bilibili</a></p>
<h4 id="其他">其他</h4>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1oe411n72U/?spm_id_from=333.337.search-card.all.click">3分钟彻底理解链接器_哔哩哔哩_bilibili</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/gzxb1995/article/details/105088502">计算机系统基础摘记——程序的链接_引入链接的好处是什么-CSDN博客</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1oS4y1T7Uf?spm_id_from=333.788.player.switch&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">【CSAPP-深入理解计算机系统】7-5.
静态库的解析过程_哔哩哔哩_bilibili</a></p>
<h3 id="其他-1">其他</h3>
<p>gdb调试</p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Sg41167B1/?spm_id_from=333.337.search-card.all.click&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">一分钟学会GDB程序调试_哔哩哔哩_bilibili</a></p>
<h3 id="参考资料">参考资料</h3>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV17K4y1N7Q2?spm_id_from=333.788.videopod.episodes&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">深入理解计算机系统合集（周更中）_哔哩哔哩_bilibili</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/" class="post-title-link" itemprop="url">机器学习——期末复习（下）</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-06-06 00:00:00" itemprop="dateCreated datePublished" datetime="2025-06-06T00:00:00+08:00">2025-06-06</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-06-22 19:53:19" itemprop="dateModified" datetime="2025-06-22T19:53:19+08:00">2025-06-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%A4%A7%E4%BA%8C%E4%B8%8B/" itemprop="url" rel="index"><span itemprop="name">大二下</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="期末复习">期末复习</h2>
<h3 id="方差与偏差">方差与偏差</h3>
<p>方差（Variance）和偏差（Bias）是机器学习中衡量模型性能的两个核心概念，它们共同构成了<strong>偏差-方差权衡</strong>（Bias-Variance
Tradeoff）的基础框架。以下是两者的定义与区别：</p>
<p><strong>1. 偏差（Bias）</strong></p>
<ul>
<li><strong>定义</strong>：偏差是指模型预测的期望值与真实值之间的差异。它反映了模型本身的拟合能力，即是否能够准确捕捉数据中的规律。</li>
</ul>
<p><strong>2. 方差（Variance）</strong></p>
<ul>
<li><strong>定义</strong>：方差是指模型在不同训练数据集下预测结果的波动程度。它衡量了模型对训练数据中噪声或微小变化的敏感性。</li>
</ul>
<p><strong>3. 如何降低偏差与方差</strong></p>
<table>
<colgroup>
<col style="width: 11%">
<col style="width: 56%">
<col style="width: 32%">
</colgroup>
<thead>
<tr class="header">
<th><strong>目标</strong></th>
<th><strong>方法</strong></th>
<th><strong>示例</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>降低偏差</strong></td>
<td>增加模型复杂度（如更多特征、更深的神经网络）、减少正则化强度</td>
<td>使用多项式回归替代线性回归</td>
</tr>
<tr class="even">
<td><strong>降低方差</strong></td>
<td>增加训练数据、引入正则化（L1/L2）、使用集成方法（如
Bagging、Boosting）</td>
<td>随机森林（Bagging）降低决策树的方差</td>
</tr>
</tbody>
</table>
<p><strong>4. 总结</strong></p>
<ul>
<li><strong>偏差</strong>关注模型是否能准确拟合数据（<strong>学习能力</strong>），而<strong>方差</strong>关注模型对数据波动的稳定性（<strong>泛化能力</strong>）。</li>
<li>实际应用中需通过交叉验证、正则化或集成学习等技术平衡两者的关系。</li>
</ul>
<h3 id="监督学习与无监督学习">监督学习与无监督学习</h3>
<p>以下是关于监督学习与无监督学习的核心区别总结：</p>
<p><strong>1. 监督学习（Supervised Learning）</strong></p>
<p><strong>任务类型</strong>：<br>
-
<strong>分类（Classification）</strong>：预测离散类别标签（如垃圾邮件/非垃圾邮件）。<br>
-
<strong>回归（Regression）</strong>：预测连续数值标签（如房价预测）。</p>
<p><strong>特点</strong>：<br>
- 需要<strong>带标签的样本</strong>（Labeled
Data），即每个训练样本都有明确的输入 $ x $ 和输出 $ y $。<br>
- 模型通过学习输入与标签之间的映射关系进行预测。</p>
<p><strong>2. 无监督学习（Unsupervised Learning）</strong></p>
<p><strong>任务类型</strong>：</p>
<ul>
<li><strong>聚类（Clustering）</strong>：将样本划分为具有相似特征的群体（如客户分群）。<br>
</li>
<li><strong>降维（Dimensionality
Reduction）</strong>：压缩数据维度同时保留关键信息（如PCA）。</li>
</ul>
<p><strong>特点</strong>：<br>
- 仅需<strong>无标签的样本</strong>（Unlabeled
Data），无需预先定义输出目标。<br>
- 模型自主挖掘数据内在结构或分布规律。</p>
<h3 id="贝叶斯分类">贝叶斯分类</h3>
<h4 id="贝叶斯分类器">贝叶斯分类器</h4>
<h5 id="贝叶斯决策论">贝叶斯决策论</h5>
<p>本质思想：寻找合适的参数使得「当前的样本情况发生的概率」最大。</p>
<p>又由于假设每一个样本相互独立（概率条件理想的情况下），因此可以用连乘的形式表示上述概率，当然由于概率较小导致连乘容易出现浮点数精度损失，因此尝尝采用取对数的方式来避免「下溢」问题。也就是所谓的「对数似然估计」方法。</p>
<p>在已知样本特征 $ $ 的条件下，选择分类结果 $ c_i
$，使得分类的期望损失（Risk）最小<strong>。</strong></p>
<p>**(1) 损失函数 $ _{ij} $**</p>
<ul>
<li><strong>定义</strong>：$ _{ij} $ 是将真实类别为 $ c_j $
的样本误分类为 $ c_i $ 所产生的损失。
<ul>
<li>例如：
<ul>
<li>在医学诊断中，若 $ c_1 $ 表示“患病”，$ c_2 $ 表示“未患病”：
<ul>
<li>$ _{21} <span class="math inline">：<em>将</em><em>实</em><em>际</em><em>患</em><em>病</em>（</span>
c_1 <span class="math inline">）<em>误</em><em>判</em><em>为</em><em>未</em><em>患</em><em>病</em>（</span>
c_2 $）的损失（可能更高）。</li>
<li>$ _{12} <span class="math inline">：<em>将</em><em>实</em><em>际</em><em>未</em><em>患</em><em>病</em>（</span>
c_2 <span class="math inline">）<em>误</em><em>判</em><em>为</em><em>患</em><em>病</em>（</span>
c_1 $）的损失（可能较低）。</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<p><strong>(2) 条件风险（单个样本的期望损失）</strong></p>
<p>对于给定样本 $ $，若将其分类为 $ c_i
$，则其<strong>条件风险</strong>为： <span class="math display">$$
R(c_i | \mathbf{x}) = \sum_{j=1}^N \lambda_{ij} P(c_j | \mathbf{x})
$$</span> - <strong>含义</strong>：在已知 $ $ 的情况下，分类为 $ c_i $
的平均损失。 - <strong>推导</strong>： - $ P(c_j | ) $：样本 $ $
真实属于 $ c_j $ 的后验概率。 - $ <em>{ij} $：若真实类别是 $ c_j
$，但被分到 $ c_i $，则产生损失 $ </em>{ij} $。 -
因此，总期望损失是所有可能真实类别的加权和（权重为后验概率）。</p>
<p><strong>(3) 总体风险</strong></p>
<p>对于整个数据集，分类器 $ h() $ 的<strong>总体风险</strong>为： <span class="math display"><em>R</em>(<em>h</em>) = 𝔼<sub><strong>x</strong></sub>[<em>R</em>(<em>h</em>(<strong>x</strong>)|<strong>x</strong>)] = ∫<em>R</em>(<em>h</em>(<strong>x</strong>)|<strong>x</strong>)<em>p</em>(<strong>x</strong>)<em>d</em><strong>x</strong></span>
- <strong>含义</strong>：所有样本的平均条件风险。h为分类器（模型） -
<strong>目标</strong>：找到使 $ R(h) $ 最小的分类器 $ h() $。</p>
<h5 id="贝叶斯决策规则"><strong>贝叶斯决策规则</strong></h5>
<p>根据上述定义，贝叶斯决策论的分类规则是： &gt; <strong>对于样本 $
$，选择使其条件风险 $ R(c_i | ) $ 最小的类别 $ c_i $
作为预测结果。</strong></p>
<p>即： <span class="math display">$$
h^*(\mathbf{x}) = \arg\min_{c_i} R(c_i | \mathbf{x}) = \arg\min_{c_i}
\sum_{j=1}^N \lambda_{ij} P(c_j | \mathbf{x})
$$</span></p>
<h5 id="特殊情况0-1-损失函数"><strong>特殊情况：0-1
损失函数</strong></h5>
<p>当所有误分类的损失相同（即 $ <em>{ij} = 1 $ 对于 $ i j <span class="math inline">，</span> </em>{ii} = 0 $）<strong>0-1
损失函数</strong>： <span class="math display">$$
\lambda_{ij} =
\begin{cases}
0, &amp; \text{if } i = j \\
1, &amp; \text{otherwise}
\end{cases}
$$</span> 此时条件风险简化为： <span class="math display"><em>R</em>(<em>c</em><sub><em>i</em></sub>|<strong>x</strong>) = ∑<sub><em>j</em> ≠ <em>i</em></sub><em>P</em>(<em>c</em><sub><em>j</em></sub>|<strong>x</strong>) = 1 − <em>P</em>(<em>c</em><sub><em>i</em></sub>|<strong>x</strong>)</span>
原因：概率之和为 1：$ <em>{j=1}^N P(c_j | ) = 1 $，因此 $ </em>{j i}
P(c_j | ) = 1 - P(c_i | ) $。</p>
<p>此时，最小化风险等价于<strong>最大化后验概率</strong>，即： <span class="math display"><em>h</em><sup>*</sup>(<strong>x</strong>) = arg max<sub><em>c</em><sub><em>i</em></sub></sub><em>P</em>(<em>c</em><sub><em>i</em></sub>|<strong>x</strong>)</span>
这正是传统贝叶斯分类器的决策规则。</p>
<blockquote>
<p>即在x样本的情况下，分类正确的概率最大</p>
</blockquote>
<h4 id="后验概率与先验概率">后验概率与先验概率</h4>
<h5 id="后验概率">后验概率</h5>
<p>后验概率（Posterior
Probability）是贝叶斯理论中的核心概念，指的是<strong>在观察到新证据（数据）后，对事件发生概率的修正</strong>
。 其本质是：</p>
<blockquote>
<p><strong>“已知结果（数据），反推原因（类别或参数）的概率”</strong>
。</p>
</blockquote>
<p>已知结果（数据）B，反推最可能的原因A（后验概率
<em>P</em>(<em>A</em>∣<em>B</em>) ）</p>
<h5 id="先验概率prior-probability"><strong>先验概率（Prior
Probability）</strong></h5>
<p>先验概率是贝叶斯统计中的核心概念，指的是在<strong>观察到新数据之前</strong>，对某一事件或假设的概率估计。它是基于<strong>已有知识、经验或假设</strong>得出的初始概率，后续会通过新数据更新为更准确的<strong>后验概率</strong>。</p>
<p><strong>1. 核心定义</strong></p>
<ul>
<li><strong>数学表达</strong>：<br>
<span class="math display"><em>P</em>(<em>A</em>)</span>
<ul>
<li>$ P(A) $：事件 $ A $ 的先验概率。</li>
<li>例如：$ A $ 表示“某人患有某种疾病”，则 $ P(A) $
是该疾病的已知发病率（在未进行检测前的概率）。</li>
</ul></li>
<li><strong>与后验概率的区别</strong>：
<ul>
<li><strong>先验概率</strong>：$ P(A) $，在无新数据时的概率。<br>
</li>
<li><strong>后验概率</strong>：$ P(A|B) $，在观察到数据 $ B $
后更新的概率（通过贝叶斯定理计算）。</li>
</ul></li>
</ul>
<p><strong>2. 直观理解</strong></p>
<p><strong>(1) 类比：医学诊断</strong></p>
<ul>
<li><strong>先验概率</strong>：某种疾病的已知发病率（如 1%）。<br>
</li>
<li><strong>新数据</strong>：患者接受检测，结果为阳性。<br>
</li>
<li><strong>后验概率</strong>：结合发病率和检测结果，计算实际患病的概率（如
8.7%，参考贝叶斯定理的经典医学测试案例）。</li>
</ul>
<h4 id="生成式模型和判别式模型">生成式模型和判别式模型</h4>
<h5 id="核心区别"><strong>核心区别</strong></h5>
<table>
<colgroup>
<col style="width: 10%">
<col style="width: 44%">
<col style="width: 44%">
</colgroup>
<thead>
<tr class="header">
<th><strong>模型类型</strong></th>
<th><strong>建模目标</strong></th>
<th><strong>数学表达</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>判别式模型</strong></td>
<td>直接建模 $ P(c</td>
<td>) $</td>
</tr>
<tr class="even">
<td><strong>生成式模型</strong></td>
<td>先建模联合概率 $ P(, c) $，再推导 $ P(c</td>
<td>) $</td>
</tr>
</tbody>
</table>
<h5 id="详细解释"><strong>详细解释</strong></h5>
<p><strong>1. 判别式模型（Discriminative Model）</strong></p>
<ul>
<li><strong>目标</strong>：直接学习从输入 $ $ 到标签 $ c $
的映射关系。</li>
<li><strong>数学本质</strong>：建模条件概率 $ P(c|) $，即“已知特征 $
$，预测类别 $ c $”。</li>
<li><strong>特点</strong>：
<ul>
<li>不关心数据本身的分布，只关注分类边界。</li>
<li>例如：逻辑回归、支持向量机（SVM）、神经网络等。</li>
</ul></li>
</ul>
<p><strong>2. 生成式模型（Generative Model）</strong></p>
<ul>
<li><p><strong>目标</strong>：先学习数据的生成过程，即联合概率 $ P(, c)
$，再通过贝叶斯定理推导条件概率 $ P(c|) $。</p></li>
<li><p><strong>数学步骤</strong>：</p>
<ol type="1">
<li>建模 $ P(|c) $（特征在类别 $ c $ 下的分布）和 $ P(c)
$（类别先验）。</li>
<li>根据贝叶斯定理计算后验概率： <span class="math display">$$
P(c|\mathbf{x}) = \frac{P(\mathbf{x}|c)P(c)}{P(\mathbf{x})}
$$</span></li>
<li>选择使 $ P(c|) $ 最大的类别作为预测结果。</li>
</ol></li>
</ul>
<h5 id="示例二分类问题"><strong>示例：二分类问题</strong></h5>
<p>假设我们要判断一封邮件是否为垃圾邮件（$ c=spam $ 或 $ ham $）。</p>
<p><strong>判别式模型（逻辑回归）</strong></p>
<p>直接建模： <span class="math display">$$
P(spam|\mathbf{x}) = \frac{1}{1 + e^{-(w^T \mathbf{x} + b)}}
$$</span> 若 $ P(spam|) &gt; 0.5 $，则判定为垃圾邮件。</p>
<p><strong>生成式模型（朴素贝叶斯）</strong></p>
<ol type="1">
<li>建模联合概率： <span class="math display"><em>P</em>(<strong>x</strong>,<em>s</em><em>p</em><em>a</em><em>m</em>) = <em>P</em>(<em>s</em><em>p</em><em>a</em><em>m</em>)∏<sub><em>i</em></sub><em>P</em>(<em>w</em><em>o</em><em>r</em><em>d</em><sub><em>i</em></sub>|<em>s</em><em>p</em><em>a</em><em>m</em>)</span>
<span class="math display"><em>P</em>(<strong>x</strong>,<em>h</em><em>a</em><em>m</em>) = <em>P</em>(<em>h</em><em>a</em><em>m</em>)∏<sub><em>i</em></sub><em>P</em>(<em>w</em><em>o</em><em>r</em><em>d</em><sub><em>i</em></sub>|<em>h</em><em>a</em><em>m</em>)</span></li>
<li>计算后验概率： <span class="math display">$$
P(spam|\mathbf{x}) = \frac{P(\mathbf{x}|spam)P(spam)}{P(\mathbf{x})}
$$</span> <span class="math display">$$
P(ham|\mathbf{x}) = \frac{P(\mathbf{x}|ham)P(ham)}{P(\mathbf{x})}
$$</span></li>
<li>选择概率更大的类别。</li>
</ol>
<h4 id="生成式模型的建模思路">生成式模型的建模思路</h4>
<p>根据概率论的基本定义： <span class="math display">$$
P(c|\mathbf{x}) = \frac{P(\mathbf{x}, c)}{P(\mathbf{x})}
$$</span> - <strong>含义</strong>： - $ P(, c) $：联合概率，表示特征 $ $
和类别 $ c $ 同时发生的概率。 - $ P() $：边缘概率（证据），表示特征 $ $
出现的概率，用于归一化。</p>
<p>根据贝叶斯定理，联合概率 $ P(, c) $ 可以分解为： <span class="math display"><em>P</em>(<strong>x</strong>,<em>c</em>) = <em>P</em>(<em>c</em>) ⋅ <em>P</em>(<strong>x</strong>|<em>c</em>)</span>
其中： - $ P(c) $：类先验概率（Prior Probability），表示类别 $ c $
在数据中的整体占比。 - $ P(|c) $：似然度（Likelihood），表示在类别 $ c $
下，特征 $ $ 出现的概率。</p>
<p>将上述分解代入条件概率公式，得到： <span class="math display">$$
P(c|\mathbf{x}) = \frac{P(c) \cdot P(\mathbf{x}|c)}{P(\mathbf{x})}
$$</span> 产生问题：</p>
<p>在贝叶斯分类中，需要计算联合概率
<em>P</em>(<strong>x</strong>∣<em>c</em>) ，即在类别 <em>c</em>
下，特征向量 <strong>x</strong>=(<em>x</em>1,<em>x</em>2,…,*x**d<em>)
的条件概率。 若直接建模联合概率，需估计 </em>d*
个特征的所有可能组合的概率。例如：</p>
<ul>
<li>若每个特征有 <em>k</em> 个取值，类别数为 <em>K</em> ，则需要估计
<em>K</em>⋅*k**d* 个参数。</li>
<li>当特征维度 <em>d</em>
很大时（如文本分类中成千上万的词汇），参数数量呈指数级增长，导致计算不可行（<strong>维度灾难</strong>
）。</li>
</ul>
<p>举例：</p>
<ul>
<li><strong>低维空间</strong> ：假设只有 2
个特征（如“免费”和“中奖”），每个特征取值为 0 或 1，则特征空间共有 22=4
个可能的组合（即四个格子）。
<ul>
<li>如果有 100 封邮件，每个格子平均有 25 封邮件（数据较密集）。</li>
</ul></li>
<li><strong>高维空间</strong> ： 当特征维度增加到 <em>d</em>=10,000
时，特征空间的组合数是 210,000 ，远大于宇宙中原子的数量（约 1080 ）。
<ul>
<li>即使有 100 万封邮件，每个组合几乎都是空的（数据极度稀疏）。</li>
</ul></li>
</ul>
<p><strong>结果</strong> ：
在高维空间中，训练数据无法覆盖所有可能的特征组合，导致模型无法可靠估计联合概率
<em>P</em>(x∣c) 。</p>
<p>因此产生<strong>属性条件独立性假设</strong></p>
<h4 id="朴素贝叶斯分类器">朴素贝叶斯分类器</h4>
<p>朴素贝叶斯分类器的核心思想是通过<strong>贝叶斯定理</strong>和<strong>属性条件独立性假设</strong>来简化计算，从而高效地进行分类。</p>
<h5 id="属性条件独立性假设">属性条件独立性假设</h5>
<p>朴素贝叶斯的核心假设是：<strong>在已知类别 $ c $
的条件下，所有属性（特征）之间相互独立</strong>。<br>
因此，联合概率 $ P(|c) $ 可以分解为各属性独立概率的乘积： <span class="math display">$$
P(\mathbf{x}|c) = \prod_{i=1}^d P(x_i|c)
$$</span> 其中 $ d $ 是特征的数量，$ x_i $ 是第 $ i $ 个特征的取值。</p>
<p>将此代入贝叶斯公式： <span class="math display">$$
P(c|\mathbf{x}) = \frac{P(c) \cdot \prod_{i=1}^d
P(x_i|c)}{P(\mathbf{x})}
$$</span></p>
<h5 id="为何可以忽略-p"><strong>为何可以忽略 $ P() $?</strong></h5>
<p>在分类任务中，我们的目标是比较不同类别 $ c $ 的后验概率 $ P(c|)
$，并选择最大值。由于 $ P() $
对所有类别来说是相同的常量（与类别无关），因此在最大化过程中可以忽略：
<span class="math display">$$
\arg\max_{c} P(c|\mathbf{x}) = \arg\max_{c} \left[ \frac{P(c) \cdot
\prod_{i=1}^d P(x_i|c)}{P(\mathbf{x})} \right] = \arg\max_{c} \left[
P(c) \cdot \prod_{i=1}^d P(x_i|c) \right]
$$</span> 这就是公式中 $ P() $ 被省略的原因。</p>
<blockquote>
<p>在比较的过程中，分母相同，可以忽略</p>
</blockquote>
<h5 id="朴素贝叶斯的最终决策规则"><strong>朴素贝叶斯的最终决策规则</strong></h5>
<p>简化后的决策规则为： <span class="math display">$$
h_{nb}(\mathbf{x}) = \arg\max_{c} \left[ P(c) \cdot \prod_{i=1}^d
P(x_i|c) \right]
$$</span> 即： - 计算每个类别的先验概率 $ P(c) $。 -
计算每个特征在该类别下的条件概率 $ P(x_i|c) $。 -
将这些概率相乘，选择乘积最大的类别作为预测结果。</p>
<h5 id="类先验概率-pc-的估计方法"><strong>类先验概率 $ P(c) $
的估计方法</strong></h5>
<p>基于<strong>大数定律</strong> <span class="math display">$$
P(c) = \frac{|D_c|}{|D|}
$$</span> - <strong>符号含义</strong>： - $ D $：训练集，包含所有样本。
- $ D_c $：训练集中类别为 $ c $ 的样本子集。 - $ |D_c| $：类别 $ c $
的样本数量。 - $ |D| $：训练集总样本数量。</p>
<ul>
<li><strong>直观解释</strong>：
类先验概率等于该类别样本数占总样本数的比例。</li>
</ul>
<h5 id="条件概率-px_i-c-的估计方法"><strong>条件概率 $ P(x_i | c) $
的估计方法</strong></h5>
<p>在生成式模型（如朴素贝叶斯分类器）中，<strong>条件概率 $ P(x_i | c)
$</strong> 表示在类别 $ c $ 下，第 $ i $ 个属性取值为 $ x_i $
的概率。根据属性类型（离散或连续），其估计方法不同：</p>
<p><strong>1. 离散属性的条件概率估计</strong></p>
<p><strong>公式</strong>： <span class="math display">$$
P(x_i | c) = \frac{|D_{c,x_i}|}{|D_c|}
$$</span> - <strong>符号含义</strong>： - $ D_c $：训练集中类别为 $ c $
的样本集合。 - $ D_{c,x_i} <span class="math inline">：</span> D_c $
中第 $ i $ 个属性取值为 $ x_i $ 的样本子集。 - $ |D_{c,x_i}| <span class="math inline">：</span> D_{c,x_i} $ 的样本数量。 - $ |D_c| $：类别
$ c $ 的总样本数量。</p>
<p><strong>直观解释</strong>：</p>
<ul>
<li>在类别 $ c $ 的样本中，统计第 $ i $ 个属性取值为 $ x_i $
的频率，作为 $ P(x_i | c) $ 的估计。</li>
<li><strong>示例</strong>：<br>
若类别 $ c=spam $（垃圾邮件）有 200 封，其中 150 封包含“免费”一词，则：
<span class="math display">$$
P(\text{“免费”} | spam) = \frac{150}{200} = 0.75
$$</span></li>
</ul>
<p><strong>注意事项</strong>：</p>
<ul>
<li><strong>零概率问题</strong>：若某属性值在类别 $ c $ 中未出现，则 $
P(x_i | c) = 0 $，可能导致后续计算失效。<br>
<strong>解决方案</strong>：使用<strong>拉普拉斯平滑（Laplace
Smoothing）</strong>，将公式改为： <span class="math display">$$
P(x_i | c) = \frac{|D_{c,x_i}| + 1}{|D_c| + K}
$$</span> 其中 $ K $ 是该属性的取值总数。</li>
</ul>
<p><strong>2. 连续属性的条件概率估计</strong></p>
<p><strong>假设</strong>：属性服从正态分布（高斯分布） <span class="math display">$$
p(x_i | c) = \frac{1}{\sqrt{2\pi}\sigma_{c,i}} \exp\left( -\frac{(x_i -
\mu_{c,i})^2}{2\sigma_{c,i}^2} \right)
$$</span> - <strong>符号含义</strong>： - $ <em>{c,i} $：类别 $ c $ 在第
$ i $ 个属性上的均值。 - $ </em>{c,i}^2 $：类别 $ c $ 在第 $ i $
个属性上的方差。</p>
<p><strong>直观解释</strong>：</p>
<ul>
<li>假设在类别 $ c $ 下，属性 $ x_i $ 服从均值为 $ <em>{c,i} $、方差为 $
</em>{c,i}^2 $ 的正态分布。</li>
<li><strong>示例</strong>：<br>
若类别 $ c=spam $ 的“字数”属性均值 $ <em>{spam, } = 500 $，方差 $
</em>{spam, }^2 = 100 $，则： <span class="math display">$$
p(600 | spam) = \frac{1}{\sqrt{2\pi \cdot 100}} \exp\left( -\frac{(600 -
500)^2}{2 \cdot 100} \right) \approx 0.004
$$</span></li>
</ul>
<p><strong>注意事项</strong>：</p>
<ul>
<li><strong>分布假设</strong>：若实际数据不符合正态分布，需调整假设（如使用核密度估计、对数变换等）。</li>
<li><strong>参数估计</strong>：均值和方差通过训练数据计算： <span class="math display">$$
\mu_{c,i} = \frac{1}{|D_c|} \sum_{x \in D_c} x_i, \quad \sigma_{c,i}^2 =
\frac{1}{|D_c|} \sum_{x \in D_c} (x_i - \mu_{c,i})^2
$$</span></li>
</ul>
<h4 id="半朴素贝叶斯分类器">半朴素贝叶斯分类器</h4>
<p>半朴素贝叶斯分类器是对传统<strong>朴素贝叶斯</strong>的改进，它在保留计算效率的同时，<strong>适当引入部分属性间的依赖关系</strong>，从而在分类性能和计算复杂度之间取得平衡。</p>
<h5 id="独依赖估计ode方法"><strong>独依赖估计（ODE）方法</strong></h5>
<p><strong>(1) 定义</strong></p>
<p>独依赖估计（One-Dependent Estimator,
ODE）是半朴素贝叶斯的一种实现方式，其核心假设是： &gt; <strong>每个属性
$ x_i $ 在类别 $ c $ 之外最多依赖于一个其他属性（称为父属性 $ pa_i
$）</strong>。</p>
<p>数学表达式为： <span class="math display">$$
P(c|\mathbf{x}) \propto P(c) \prod_{i=1}^d P(x_i | c, pa_i)
$$</span> 其中： - $ pa_i $：属性 $ x_i $ 的父属性（依赖的单一属性）。 -
$ P(x_i | c, pa_i) $：在类别 $ c $ 和父属性 $ pa_i $ 下，属性 $ x_i $
的条件概率。</p>
<p><strong>(2) 直观理解</strong></p>
<ul>
<li>每个属性 $ x_i $ 的分布不仅受类别 $ c $ 影响，还受其父属性 $ pa_i $
的影响。</li>
<li>例如，在文本分类中，若属性 $ x_1 $ 是“免费”，$ x_2 $
是“中奖”，可设定 $ pa_2 = x_1
$，表示“中奖”在类别和“免费”的共同作用下出现。</li>
</ul>
<h5 id="超父独依赖估计spode"><strong>超父独依赖估计（SPODE）</strong></h5>
<p>超父独依赖估计（Super Parent One-Dependent Estimator,
SPODE）是<strong>半朴素贝叶斯分类器</strong>的一种扩展，其核心思想是：
&gt; <strong>所有属性都依赖于同一个“超父”属性 $ x_i
$</strong>，从而在保留部分依赖关系的同时避免完全联合概率的计算。</p>
<p><strong>(1) 贝叶斯定理展开</strong> <span class="math display">$$
P(c|\mathbf{x}) = \frac{P(\mathbf{x}, c)}{P(\mathbf{x})}
$$</span> 其中： - $ P(, c) $：联合概率，表示特征 $ $ 和类别 $ c $
同时发生的概率。 - $ P() $：证据（归一化因子）。</p>
<p><strong>(2) 引入“超父”属性 $ x_i $</strong></p>
<p>假设所有属性 $ x_j (j i) $ 在类别 $ c $ 下仅依赖于 $ x_i $，则：
<span class="math display"><em>P</em>(<strong>x</strong>,<em>c</em>) = <em>P</em>(<em>c</em>,<em>x</em><sub><em>i</em></sub>) ⋅ <em>P</em>(<em>x</em><sub>1</sub>,…,<em>x</em><sub><em>i</em> − 1</sub>,<em>x</em><sub><em>i</em> + 1</sub>,…,<em>x</em><sub><em>d</em></sub>|<em>c</em>,<em>x</em><sub><em>i</em></sub>)</span>
进一步分解为： <span class="math display"><em>P</em>(<strong>x</strong>,<em>c</em>) = <em>P</em>(<em>c</em>,<em>x</em><sub><em>i</em></sub>) ⋅ ∏<sub><em>j</em> ≠ <em>i</em></sub><em>P</em>(<em>x</em><sub><em>j</em></sub>|<em>c</em>,<em>x</em><sub><em>i</em></sub>)</span></p>
<p><strong>(3) 最终形式</strong></p>
<p>由于 $ P() $ 对所有类别相同，可忽略，最终决策规则为： <span class="math display">$$
P(c|\mathbf{x}) \propto P(c, x_i) \cdot \prod_{j=1}^d P(x_j | c, x_i)
$$</span> 其中： - $ P(c, x_i) $：类别 $ c $ 和属性 $ x_i $ 的联合概率。
- $ P(x_j | c, x_i) $：在类别 $ c $ 和 $ x_i $ 的条件下，属性 $ x_j $
的概率。</p>
<h5 id="树增强朴素贝叶斯tan-tree-augmented-naive-bayes"><strong>树增强朴素贝叶斯（TAN:
Tree-Augmented Naive Bayes）</strong></h5>
<p><strong>TAN</strong>（Tree-Augmented Naive
Bayes）是<strong>半朴素贝叶斯分类器</strong>的一种扩展，旨在通过引入属性间的<strong>树状依赖关系</strong>，在保留计算效率的同时，显著提升分类性能。它结合了<strong>贝叶斯网络</strong>的建模能力和<strong>生成式模型</strong>的概率推理优势。</p>
<p><strong>1. 核心思想</strong></p>
<p>TAN 的核心假设是： &gt; <strong>所有属性（特征）在类别 $ c $
的基础上，形成一个以属性为节点的树状依赖结构</strong>，即每个属性最多依赖一个其他属性（父属性），且整个依赖图是一棵无环的树。</p>
<p><strong>数学表达</strong>： <span class="math display">$$
P(c|\mathbf{x}) \propto P(c) \cdot \prod_{i=1}^d P(x_i | c, pa_i)
$$</span> 其中： - $ pa_i $：属性 $ x_i $ 的父属性（依赖的单一属性）。 -
$ P(x_i | c, pa_i) $：在类别 $ c $ 和父属性 $ pa_i $ 的条件下，属性 $
x_i $ 的条件概率。</p>
<p><strong>2. TAN 的构建步骤</strong></p>
<p>TAN 通过以下步骤构建属性间的依赖结构：</p>
<p><strong>(1) 计算互信息（Mutual Information）</strong></p>
<p>互信息衡量两个属性之间的相关性： <span class="math display">$$
I(x_i, x_j) = \sum_{x_i, x_j} P(x_i, x_j) \log \frac{P(x_i,
x_j)}{P(x_i)P(x_j)}
$$</span> -
<strong>含义</strong>：互信息越大，两个属性之间的依赖关系越强。</p>
<p><strong>(2) 构建带权图</strong></p>
<ul>
<li>将所有属性视为图中的节点。</li>
<li>每对属性间的边权重设为互信息 $ I(x_i, x_j) $。</li>
</ul>
<p><strong>(3) 最大带权生成树（Maximum Weight Spanning Tree,
MWST）</strong></p>
<p>使用克鲁斯卡尔（Kruskal）算法或普里姆（Prim）算法，选择一棵连接所有属性节点的树，使得：
- 树的边权重（互信息）总和最大。 - 树中无环。</p>
<p><strong>(4) 确定依赖方向</strong></p>
<ul>
<li>随机选择一个根节点（或根据领域知识指定）。</li>
<li>从根节点出发，确定每条边的方向（父属性 → 子属性）。</li>
</ul>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250605223036362.png" alt="image-20250605223036362">
<figcaption aria-hidden="true">image-20250605223036362</figcaption>
</figure>
<h4 id="贝叶斯网">贝叶斯网</h4>
<p>待学习</p>
<h4 id="em算法">EM算法</h4>
<p>EM算法（Expectation-Maximization
Algorithm）是一种<strong>迭代优化算法</strong>，用于处理<strong>含有隐变量</strong>（Hidden
Variables）或<strong>缺失数据</strong>的概率模型参数估计问题。它的核心思想是通过交替执行<strong>期望（E）步</strong>和<strong>最大化（M）步</strong>，逐步逼近模型参数的最大似然估计。</p>
<h5 id="核心思想解决隐变量问题"><strong>1.
核心思想：解决隐变量问题</strong></h5>
<p><strong>(1) 什么是隐变量？</strong></p>
<p>隐变量（Latent
Variables）是模型中<strong>不可观测但影响观测数据</strong>的变量。例如：
-
<strong>混合高斯模型（GMM）</strong>：每个样本属于哪个高斯分布是隐变量。
- <strong>聚类任务</strong>：样本所属的聚类标签是隐变量。</p>
<p><strong>(2) 问题挑战</strong></p>
<p>当存在隐变量时，直接最大化似然函数变得困难。例如： <span class="math display">log <em>P</em>(<strong>x</strong>|<em>θ</em>) = log ∑<sub><em>z</em></sub><em>P</em>(<strong>x</strong>,<em>z</em>|<em>θ</em>)</span>
其中 $ z $ 是隐变量，$ $
是模型参数。由于对数中包含求和，直接求导无法分离参数。</p>
<p><strong>(3) EM算法的解决方案</strong></p>
<p>EM算法通过以下步骤迭代求解： 1.
<strong>E步（期望）</strong>：用当前参数估计隐变量的后验分布（即“责任”分配）。
2.
<strong>M步（最大化）</strong>：基于隐变量的后验分布，最大化期望似然函数以更新参数。</p>
<h5 id="算法流程"><strong>2. 算法流程</strong></h5>
<p><strong>(1) 初始化参数</strong></p>
<p>选择初始参数 $ ^{(0)} $，例如随机初始化或通过启发式方法设定。</p>
<p><strong>(2) E步：计算隐变量后验分布</strong></p>
<p>给定当前参数 $ ^{(t)} $，计算隐变量 $ z $ 的后验概率： <span class="math display"><em>Q</em><sup>(<em>t</em>)</sup>(<em>z</em>) = <em>P</em>(<em>z</em>|<strong>x</strong>,<em>θ</em><sup>(<em>t</em>)</sup>)</span>
这一步为每个样本分配隐变量的概率分布（如样本属于某个聚类的概率）。</p>
<p><strong>(3) M步：最大化期望似然</strong></p>
<p>基于 $ Q^{(t)}(z) $，构造期望似然函数并最大化： <span class="math display"><em>θ</em><sup>(<em>t</em>+1)</sup> = arg max<sub><em>θ</em></sub>∑<sub><em>z</em></sub><em>Q</em><sup>(<em>t</em>)</sup>(<em>z</em>)log <em>P</em>(<strong>x</strong>,<em>z</em>|<em>θ</em>)</span>
这一步更新参数 $ $，使得期望似然最大。</p>
<p><strong>(4) 收敛判断</strong></p>
<p>重复E步和M步直到参数收敛（如 $ |^{(t+1)} - ^{(t)}| &lt;
$）或达到最大迭代次数。</p>
<h5 id="示例混合高斯模型gmm"><strong>3.
示例：混合高斯模型（GMM）</strong></h5>
<p>假设数据由多个高斯分布生成，但不知道每个样本属于哪个分布。</p>
<p><strong>(1) 模型定义</strong></p>
<ul>
<li>观测变量 $ x_i ^d $：第 $ i $ 个样本。</li>
<li>隐变量 $ z_i {1, …, K} $：样本 $ x_i $ 所属的高斯分布。</li>
<li>参数 $ = {_k, _k, <em>k}</em>{k=1}^K $：
<ul>
<li>$ _k $：第 $ k $ 个高斯分布的均值。</li>
<li>$ _k $：第 $ k $ 个高斯分布的协方差矩阵。</li>
<li>$ _k $：第 $ k $ 个高斯分布的权重（先验概率）。</li>
</ul></li>
</ul>
<p><strong>(2) E步：计算责任分配</strong></p>
<p>对于每个样本 $ x_i $ 和类别 $ k $，计算责任（responsibility）： <span class="math display">$$
\gamma_{ik}^{(t)} = P(z_i=k|x_i, \theta^{(t)}) = \frac{\pi_k^{(t)}
\mathcal{N}(x_i|\mu_k^{(t)}, \Sigma_k^{(t)})}{\sum_{j=1}^K \pi_j^{(t)}
\mathcal{N}(x_i|\mu_j^{(t)}, \Sigma_j^{(t)})}
$$</span> 含义：在当前参数下，样本 $ x_i $ 属于类别 $ k $ 的概率。</p>
<p><strong>(3) M步：更新参数</strong></p>
<p>根据责任 $ _{ik} $ 更新参数： - <strong>均值更新</strong>： <span class="math display">$$
  \mu_k^{(t+1)} = \frac{\sum_{i=1}^N \gamma_{ik}^{(t)} x_i}{\sum_{i=1}^N
\gamma_{ik}^{(t)}}
  $$</span> - <strong>协方差更新</strong>： <span class="math display">$$
  \Sigma_k^{(t+1)} = \frac{\sum_{i=1}^N \gamma_{ik}^{(t)} (x_i -
\mu_k^{(t+1)})(x_i - \mu_k^{(t+1)})^T}{\sum_{i=1}^N \gamma_{ik}^{(t)}}
  $$</span> - <strong>权重更新</strong>： <span class="math display">$$
  \pi_k^{(t+1)} = \frac{\sum_{i=1}^N \gamma_{ik}^{(t)}}{N}
  $$</span></p>
<p><strong>(4) 迭代终止</strong></p>
<p>当参数变化小于阈值或达到最大迭代次数时停止。</p>
<h4 id="作业">作业</h4>
<h5 id="section">1</h5>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250606143758565.png" alt="image-20250606143758565">
<figcaption aria-hidden="true">image-20250606143758565</figcaption>
</figure>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250606143819352.png" alt="image-20250606143819352">
<figcaption aria-hidden="true">image-20250606143819352</figcaption>
</figure>
<h5 id="section-1">2</h5>
<p>已知观测数据-67，-48，6，8，14，16，23，24，28，29，41，49，56，60，75，试估计两个分量的高斯混合模型的5个参数。</p>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250606150830535.png" alt="image-20250606150830535">
<figcaption aria-hidden="true">image-20250606150830535</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.mixture <span class="keyword">import</span> GaussianMixture</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化观测数据</span></span><br><span class="line">data = np.array([-<span class="number">67</span>, -<span class="number">48</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">14</span>, <span class="number">16</span>, <span class="number">23</span>, <span class="number">24</span>, <span class="number">28</span>, <span class="number">29</span>, <span class="number">41</span>, <span class="number">49</span>, <span class="number">56</span>, <span class="number">60</span>,</span><br><span class="line">                 <span class="number">75</span>]).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 聚类</span></span><br><span class="line">gmmModel = GaussianMixture(n_components=<span class="number">2</span>)</span><br><span class="line">gmmModel.fit(data)</span><br><span class="line">labels = gmmModel.predict(data)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;labels =&quot;</span>, labels)</span><br><span class="line">labels = [<span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(labels)):</span><br><span class="line">    <span class="keyword">if</span> labels[i] == <span class="number">0</span>:</span><br><span class="line">        plt.scatter(i, data.take(i), s=<span class="number">15</span>, c=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">    <span class="keyword">elif</span> labels[i] == <span class="number">1</span>:</span><br><span class="line">        plt.scatter(i, data.take(i), s=<span class="number">15</span>, c=<span class="string">&#x27;blue&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Gaussian Mixture Model&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;y&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;means =&quot;</span>, gmmModel.means_.reshape(<span class="number">1</span>, -<span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;covariances =&quot;</span>, gmmModel.covariances_.reshape(<span class="number">1</span>, -<span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;weights = &quot;</span>, gmmModel.weights_.reshape(<span class="number">1</span>, -<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250606150906475.png" alt="image-20250606150906475">
<figcaption aria-hidden="true">image-20250606150906475</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># means = [[ 32.98489643 -57.51107027]]</span></span><br><span class="line"><span class="comment"># covariances = [[429.45764867  90.24987882]]</span></span><br><span class="line"><span class="comment"># weights =  [[0.86682762 0.13317238]]</span></span><br></pre></td></tr></table></figure>
<h5 id="section-2">3</h5>
<p>简要阐述下EM算法的原理，并给出EM算法对高斯混合模型GMM进行求解的具体过程。</p>
<h6 id="em算法的原理">EM算法的原理</h6>
<p>EM算法（期望最大化算法）是一种用于含有隐变量的概率模型参数估计的迭代优化方法。其核心思想是通过交替执行两个步骤来最大化观测数据的似然函数：</p>
<ol type="1">
<li><strong>E步（期望步）</strong>：计算隐变量的后验期望（即责任），给定当前参数估计。</li>
<li><strong>M步（最大化步）</strong>：基于责任，最大化完全数据的期望似然函数以更新参数。</li>
</ol>
<p>EM算法通过不断优化似然函数的下界，最终收敛到局部最优解。以下具体阐述EM算法对高斯混合模型（GMM）的求解过程。</p>
<h6 id="em算法对gmm的具体求解过程"><strong>EM算法对GMM的具体求解过程</strong></h6>
<p><strong>1. GMM模型定义</strong></p>
<p>GMM假设数据由 $ K $ 个高斯分布线性组合生成，其概率密度函数为： <span class="math display">$$
p(\mathbf{x}|\theta) = \sum_{k=1}^K \alpha_k \cdot
\mathcal{N}(\mathbf{x}|\mu_k, \Sigma_k)
$$</span> 其中： - $ <em>k $：第 $ k $ 个高斯分布的权重（$ </em>{k=1}^K
_k = 1 $）。 - $ _k $：第 $ k $ 个高斯分布的均值向量。 - $ _k $：第 $ k
$ 个高斯分布的协方差矩阵。 - $ = {_k, _k, <em>k}</em>{k=1}^K
$：模型参数。</p>
<p>隐变量 $ z_i {1,,K} $ 表示样本 $ _i $ 的类别标签（未知）。</p>
<p><strong>2. EM算法步骤</strong></p>
<p><strong>(1) 初始化参数</strong></p>
<p>随机或通过K-means初始化： - 每个高斯分布的均值 $ _k^{(0)} $、协方差 $
_k^{(0)} $、权重 $ _k^{(0)} $。</p>
<p><strong>(2) 迭代优化（E步与M步）</strong></p>
<p><strong>E步：计算责任（后验概率）</strong> 对每个样本 <span class="math inline"><strong>x</strong><sub><em>i</em></sub></span>
和每个簇 $ k $，计算其属于第 $ k $ 个高斯分布的后验概率 <span class="math display">$$
\gamma(z_{ik}) = \frac{\alpha_k \cdot \mathcal{N}(\mathbf{x}_i | \mu_k,
\Sigma_k)}{\sum_{j=1}^K \alpha_j \cdot \mathcal{N}(\mathbf{x}_i | \mu_j,
\Sigma_j)}
$$</span> 此概率表示在当前参数下，样本 $ _i $ 属于第 $ k $
个高斯分布的“责任”。</p>
<p><strong>M步：更新参数</strong><br>
基于责任 $ (z_{ik}) $，最大化完全数据似然函数的期望，更新参数：</p>
<ul>
<li><strong>权重更新</strong>： <span class="math display">$$
\alpha_k^{(new)} = \frac{1}{N} \sum_{i=1}^N \gamma(z_{ik})
$$</span></li>
<li><strong>均值更新</strong>： <span class="math display">$$
\mu_k^{(new)} = \frac{\sum_{i=1}^N \gamma(z_{ik})
\mathbf{x}_i}{\sum_{i=1}^N \gamma(z_{ik})}
$$</span></li>
<li><strong>协方差更新</strong>： <span class="math display">$$
\Sigma_k^{(new)} = \frac{\sum_{i=1}^N \gamma(z_{ik}) (\mathbf{x}_i -
\mu_k^{(new)})(\mathbf{x}_i - \mu_k^{(new)})^\top}{\sum_{i=1}^N
\gamma(z_{ik})}
$$</span> 若为单变量高斯分布，则更新方差： <span class="math display">$$
\sigma_k^{(new)} = \frac{\sum_{i=1}^N \gamma(z_{ik}) (x_i -
\mu_k^{(new)})^2}{\sum_{i=1}^N \gamma(z_{ik})}
$$</span></li>
</ul>
<p><strong>(3) 收敛判断</strong></p>
<p>计算对数似然函数： <span class="math display">$$
\log p(\mathbf{X}|\theta) = \sum_{i=1}^N \log \left( \sum_{k=1}^K
\alpha_k \cdot \mathcal{N}(\mathbf{x}_i|\mu_k, \Sigma_k) \right)
$$</span>
若对数似然的变化量小于阈值或达到最大迭代次数，则停止；否则重复E步和M步。。</p>
<h4 id="参考资料">参考资料</h4>
<p>[<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1RT411G7jJ/?spm_id_from=333.788.recommend_more_video.0&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">5分钟学算法]
#06 EM算法 你到底是哪个班级的_哔哩哔哩_bilibili</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/396007256">《统计学习方法_第二版》学习笔记第九章
- 知乎</a></p>
<h3 id="集成学习">集成学习</h3>
<h4 id="个体与集成">个体与集成</h4>
<h5 id="集成学习的基本概念"><strong>集成学习的基本概念</strong></h5>
<p>集成学习（Ensemble
Learning）通过构建并结合<strong>多个学习器（基模型）</strong>来完成学习任务，其核心思想是“<strong>优而不同</strong>”，即<strong>通过多个弱学习器的协作提升整体性能</strong>，通常能获得比单一学习器更优的泛化能力
。</p>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250606154731476.png" alt="image-20250606154731476">
<figcaption aria-hidden="true">image-20250606154731476</figcaption>
</figure>
<p>在上图的集成模型中，若个体学习器都属于同一类别，例如都是决策树或都是神经网络，则称该集成为同质的（homogeneous）;若个体学习器包含多种类型的学习算法，例如既有决策树又有神经网络，则称该集成为异质的（heterogenous）。</p>
<blockquote>
<p><strong>同质集成</strong>：个体学习器称为“基学习器”（base
learner），对应的学习算法为“基学习算法”（base learning algorithm）。</p>
<p><strong>异质集成</strong>：个体学习器称为“组件学习器”（component
learner）或直称为“个体学习器”。</p>
</blockquote>
<p>集成学习的两个重要概念：<strong>准确性</strong>和<strong>多样性</strong>（diversity）。准确性指的是个体学习器不能太差，要有一定的准确度；多样性则是个体学习器之间的输出要具有差异性。</p>
<p>通过下面的这三个例子可以很容易看出这一点，准确度较高，差异度也较高，可以较好地提升集成性能。</p>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250606155939884.png" alt="image-20250606155939884">
<figcaption aria-hidden="true">image-20250606155939884</figcaption>
</figure>
<p><strong>集成策略</strong>：如何结合多个基模型的预测结果，例如：</p>
<ul>
<li><strong>投票法</strong>（Voting）：多数投票（硬投票）或概率加权（软投票）。<br>
</li>
<li><strong>加权平均法</strong>：对基模型的输出赋予不同权重 。<br>
</li>
<li><strong>Stacking</strong>：用元模型（Meta-Model）学习基模型的输出作为新特征
。</li>
</ul>
<h5 id="基于投票法的集成个体学习器的收敛性保证"><strong>基于投票法的集成个体学习器的收敛性保证</strong>：</h5>
<p><strong>公式解析</strong> <span class="math display">$$
P(H(\boldsymbol{x}) \neq f(\boldsymbol{x})) = \sum_{k=0}^{\lfloor T/2
\rfloor} \binom{T}{k} (1-\epsilon)^k \epsilon^{T-k} \leq
\exp\left(-\frac{1}{2} T (1 - 2\epsilon)^2\right)
$$</span></p>
<p><strong>1. 公式含义</strong></p>
<ul>
<li><strong><span class="math inline"><em>H</em>(<strong>x</strong>)</span></strong>：集成学习器的最终预测结果（如多数投票结果）。</li>
<li><strong><span class="math inline"><em>f</em>(<strong>x</strong>)</span></strong>：真实标记。</li>
<li><strong><span class="math inline"><em>ϵ</em></span></strong>：单个弱学习器的错误率（即
<span class="math inline"><em>P</em>(<em>h</em><sub><em>t</em></sub>(<strong>x</strong>)≠<em>f</em>(<strong>x</strong>))</span>），默认小于0.5。</li>
<li><strong><span class="math inline"><em>T</em></span></strong>：基学习器的数量。</li>
<li><strong>左边</strong>：集成学习器预测错误的概率（即至少有超过 <span class="math inline"><em>T</em>/2</span>
个基学习器预测错误的概率）。</li>
<li><strong>右边</strong>：对左边概率的指数级上限估计。</li>
</ul>
<p><strong>2. 推导思路</strong></p>
<ul>
<li>假设每个基学习器独立且错误率为 <span class="math inline"><em>ϵ</em></span>，则错误次数服从<strong>二项分布</strong>
<span class="math inline"><em>B</em>(<em>T</em>,<em>ϵ</em>)</span>。</li>
<li>集成错误的条件是“超过半数基学习器错误”，即错误次数 <span class="math inline"><em>k</em> ≤ ⌊<em>T</em>/2⌋</span>。</li>
</ul>
<p><strong>两个基本结论</strong></p>
<p><strong>1. 收敛速率随个体学习器数量 <span class="math inline"><em>T</em></span> 指数下降</strong></p>
<ul>
<li><strong>数学体现</strong>：错误概率的上界是 <span class="math inline">exp (−<em>c</em><em>T</em>)</span> 形式，其中 <span class="math inline">$c = \frac{1}{2}(1 - 2\epsilon)^2$</span>。</li>
</ul>
<p><strong>2. <span class="math inline"><em>ϵ</em> = 0.5</span>
的个体学习器对收敛没有作用</strong></p>
<ul>
<li><strong>数学原因</strong>：当 <span class="math inline"><em>ϵ</em> = 0.5</span> 时，<span class="math inline">(1−2<em>ϵ</em>)<sup>2</sup> = 0</span>，指数项变为
0，错误概率上界为 <span class="math inline">exp (0) = 1</span>，即错误概率无法降低。</li>
</ul>
<p>根据个体学习器的<strong>生成方式</strong>，目前集成学习可分为两类，代表作如下：</p>
<ol type="1">
<li>个体学习器直接存在强依赖关系，必须串行生成的序列化方法：<strong>Boosting</strong>；</li>
<li>个体学习器间不存在强依赖关系，可以同时生成的并行化方法：<strong>Bagging</strong>
和 <strong>随机森林 (Random Forest)</strong>。</li>
</ol>
<h4 id="boosting"><strong>Boosting</strong></h4>
<p>Boosting是一种<strong>串行</strong>的工作机制，即<strong>个体学习器的训练存在依赖关系</strong>，必须一步一步序列化进行。</p>
<p>其<strong>基本思想</strong>是：<strong>增加前一个基学习器在训练过程中预测错误样本的权重，使得后续基学习器更加关注这些打标错误的训练样本，尽可能纠正这些错误，然后基于调整后的样本分布训练下一个基学习器</strong>，如此重复，一直向下串行直至产生需要的T个基学习器，Boosting最终对这T个学习器进行加权结合，产生学习器委员会。</p>
<p>Boosting族算法最著名、使用最为广泛的就是<strong>AdaBoost</strong>，因此下面主要是对AdaBoost算法进行介绍。</p>
<p>AdaBoost使用的是<strong>指数损失函数</strong>，因此AdaBoost的权值与样本分布的更新都是围绕着最小化指数损失函数进行的。</p>
<blockquote>
<p>看到这里回想一下之前的机器学习算法，<strong>不难发现机器学习的大部分带参模型只是改变了最优化目标中的损失函数</strong>：如果是Square
loss，那就是最小二乘了；如果是Hinge
Loss，那就是著名的SVM了；如果是log-Loss，那就是Logistic
Regression了。</p>
</blockquote>
<h5 id="adaboost">AdaBoost</h5>
<h5 id="公式解析"><strong>公式解析</strong></h5>
<p><span class="math display">$$
H(\boldsymbol{x}) = \sum_{t=1}^T \alpha_t h_t(\boldsymbol{x})
$$</span> <span class="math display">ℓ<sub>exp</sub>(<em>H</em>|𝒟) = 𝔼<sub><strong>x</strong> ∼ 𝒟</sub>[<em>e</em><sup>−<em>f</em>(<strong>x</strong>)<em>H</em>(<strong>x</strong>)</sup>]</span></p>
<p><strong>1. 符号含义</strong></p>
<ul>
<li><strong><span class="math inline"><em>H</em>(<strong>x</strong>)</span></strong>：最终集成模型的预测结果，是
<span class="math inline"><em>T</em></span> 个基学习器 <span class="math inline"><em>h</em><sub><em>t</em></sub>(<strong>x</strong>)</span>
的加权和。</li>
<li><strong><span class="math inline"><em>α</em><sub><em>t</em></sub></span></strong>：第
<span class="math inline"><em>t</em></span>
个基学习器的权重，表示其在集成中的重要性。</li>
<li><strong><span class="math inline"><em>h</em><sub><em>t</em></sub>(<strong>x</strong>)</span></strong>：第
<span class="math inline"><em>t</em></span>
个基学习器（如决策树、感知机等）。</li>
<li><strong><span class="math inline"><em>f</em>(<strong>x</strong>)</span></strong>：真实标签，通常取值为
<span class="math inline">{ − 1,  + 1}</span>（二分类问题）。</li>
<li><strong><span class="math inline">𝒟</span></strong>：训练数据分布。</li>
<li><strong><span class="math inline">ℓ<sub>exp</sub></span></strong>：指数损失函数（Exponential
Loss）。</li>
</ul>
<p><strong>2. 指数损失函数的意义</strong></p>
<p>指数损失函数的形式为： <span class="math display">ℓ<sub>exp</sub>(<em>H</em>|𝒟) = 𝔼<sub><strong>x</strong> ∼ 𝒟</sub>[<em>e</em><sup>−<em>f</em>(<strong>x</strong>)<em>H</em>(<strong>x</strong>)</sup>]</span>
- <strong>直观解释</strong>： - 当 <span class="math inline"><em>H</em>(<strong>x</strong>)</span> 与 <span class="math inline"><em>f</em>(<strong>x</strong>)</span>
同号时（预测正确），指数项 <span class="math inline"><em>e</em><sup>−<em>f</em>(<strong>x</strong>)<em>H</em>(<strong>x</strong>)</sup></span>
接近 0，损失小。 - 当 <span class="math inline"><em>H</em>(<strong>x</strong>)</span> 与 <span class="math inline"><em>f</em>(<strong>x</strong>)</span>
异号时（预测错误），指数项趋近于正无穷，损失极大。 -
因此，该损失函数对错误样本的惩罚非常严格，迫使模型优先修正错误。</p>
<h5 id="adaboost的优化目标"><strong>AdaBoost的优化目标</strong></h5>
<p>AdaBoost的目标是选择基学习器 <span class="math inline"><em>h</em><sub><em>t</em></sub></span> 和权重 <span class="math inline"><em>α</em><sub><em>t</em></sub></span>，使得集成模型
<span class="math inline"><em>H</em>(<strong>x</strong>)</span>
能够<strong>最小化指数损失函数</strong>： <span class="math display">$$
\min_{\alpha_1, h_1, \dots, \alpha_T, h_T} \mathbb{E}_{\boldsymbol{x}
\sim \mathcal{D}} \left[ e^{-f(\boldsymbol{x}) \sum_{t=1}^T \alpha_t
h_t(\boldsymbol{x})} \right]
$$</span></p>
<p><strong>优化策略</strong></p>
<p>AdaBoost采用<strong>前向分步算法（Forward Stagewise
Algorithm）</strong>，逐轮迭代优化： 1.
<strong>初始化样本权重</strong>：初始时所有样本权重相等。 2.
<strong>训练基学习器 <span class="math inline"><em>h</em><sub><em>t</em></sub></span></strong>：在当前样本权重分布下，训练一个弱学习器
<span class="math inline"><em>h</em><sub><em>t</em></sub></span>。 3.
<strong>计算权重 <span class="math inline"><em>α</em><sub><em>t</em></sub></span></strong>：根据
<span class="math inline"><em>h</em><sub><em>t</em></sub></span>
的错误率 <span class="math inline"><em>ϵ</em><sub><em>t</em></sub></span> 计算其权重：
<span class="math display">$$
   \alpha_t = \frac{1}{2} \ln \left( \frac{1 - \epsilon_t}{\epsilon_t}
\right)
   $$</span> 4. <strong>更新样本权重</strong>：提高被 <span class="math inline"><em>h</em><sub><em>t</em></sub></span>
错分类样本的权重，降低正确分类样本的权重。 5. <strong>重复步骤
2-4</strong>，直到训练完成 <span class="math inline"><em>T</em></span>
轮。</p>
<h5 id="示例二分类问题-1"><strong>示例：二分类问题</strong></h5>
<p>假设一个二分类任务，真实标签 <span class="math inline"><em>f</em>(<strong>x</strong>) ∈ { − 1,  + 1}</span>，集成模型预测值
<span class="math inline">$H(\boldsymbol{x}) = \sum_{t=1}^T \alpha_t
h_t(\boldsymbol{x})$</span>： - 若 <span class="math inline"><em>H</em>(<strong>x</strong>) &gt; 0</span>，预测为
<span class="math inline"> + 1</span>； - 若 <span class="math inline"><em>H</em>(<strong>x</strong>) &lt; 0</span>，预测为
<span class="math inline"> − 1</span>。</p>
<p>此时，指数损失函数的值反映了模型对错误样本的惩罚程度： -
正确预测时，<span class="math inline"><em>e</em><sup>−<em>f</em>(<strong>x</strong>)<em>H</em>(<strong>x</strong>)</sup> ≈ 0</span>；
- 错误预测时，<span class="math inline"><em>e</em><sup>−<em>f</em>(<strong>x</strong>)<em>H</em>(<strong>x</strong>)</sup> ≫ 1</span>。</p>
<h5 id="adaboost的算法流程">AdaBoost的算法流程</h5>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250606172851748.png" alt="image-20250606172851748">
<figcaption aria-hidden="true">image-20250606172851748</figcaption>
</figure>
<h5 id="重赋权法与重采样法">重赋权法与重采样法</h5>
<p>在集成学习中，<strong>Boosting
算法的核心在于动态调整样本权重</strong> ，以逐步聚焦难分类样本。Boosting
主要通过两种方法实现样本权重的更新：<strong>重赋权法（re-weighting）</strong>
和 <strong>重采样法（re-sampling）</strong> 。</p>
<blockquote>
<p><strong>重赋权法</strong> :
对每个样本附加一个权重，这时涉及到样本属性与标签的计算，都需要乘上一个权值。
<strong>重采样法</strong> :
对于一些无法接受带权样本的及学习算法，适合用“重采样法”进行处理。方法大致过程是，根据各个样本的权重，对训练数据进行重采样，初始时样本权重一样，每个样本被采样到的概率一致，每次从N个原始的训练样本中按照权重有放回采样N个样本作为训练集，然后计算训练集错误率，然后调整权重，重复采样，集成多个基学习器。</p>
</blockquote>
<p>从偏差-方差分解来看：Boosting算法主要关注于降低偏差，每轮的迭代都关注于训练过程中预测错误的样本，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成学习器。</p>
<h5 id="拓展gradient-boosting">拓展：Gradient Boosting</h5>
<p>任务分为分类，回归，聚类，降维等，而分类中还分为二分类和多分类</p>
<p>从AdaBoost的算法流程来看，标准的AdaBoost只适用于二分类问题。</p>
<p>通过改造AdaBoost对样本分类的限制和损失函数，可以实现多分类或回归问题，这样改造出来的算法框架成为<strong>Gradient
Boosting</strong></p>
<h6 id="gbdtgradient-boosting-decision-tree与xgboost"><strong>GBDT（Gradient
Boosting Decision Tree）与XGBoost</strong></h6>
<p><strong>1. GBDT 的核心思想</strong></p>
<p>GBDT 是基于<strong>梯度提升（Gradient
Boosting）</strong>框架的集成学习方法，其特点包括： -
<strong>基学习器</strong>：使用<strong>CART（分类与回归树）</strong>作为个体学习器。
- <strong>损失函数</strong>： -
<strong>回归问题</strong>：平方损失（Squared Loss）： <span class="math display">err(<em>H</em><sub><em>t</em></sub>(<strong>x</strong>),<em>f</em>(<strong>x</strong>)) = (<em>H</em><sub><em>t</em></sub>(<strong>x</strong>)−<em>f</em>(<strong>x</strong>))<sup>2</sup></span>
- <strong>二分类问题</strong>：对数似然损失（Log-Likelihood
Loss，类似逻辑回归）： <span class="math display">err(<em>H</em><sub><em>t</em></sub>(<strong>x</strong>),<em>f</em>(<strong>x</strong>)) = log (1+exp(−<em>f</em>(<strong>x</strong>)<em>H</em><sub><em>t</em></sub>(<strong>x</strong>)))</span>
- <strong>多分类问题</strong>：扩展为多分类对数损失。</p>
<p><strong>2. XGBoost 的定位</strong></p>
<p>XGBoost（eXtreme Gradient Boosting）是 GBDT
的一种<strong>高效实现和改进</strong>，类似于 LIBSVM 对 SVM
的优化关系。其核心目标是： -
<strong>提升训练速度</strong>：通过<strong>并行计算</strong>、<strong>内存优化</strong>等工程技巧。
-
<strong>增强模型性能</strong>：引入<strong>正则化项</strong>、<strong>缺失值处理</strong>、<strong>自适应学习率</strong>等改进。</p>
<blockquote>
<p>XGBoost即eXtremeGradient
Boosting的缩写，XGBoost与GBDT的关系可以类比为
LIBSVM和SVM的关系，即XGBoOst是GBDT的一种高效实现和改进。</p>
<p>它并非一个全新的算法框架，而是对标准 GBDT
进行了<strong>大量的工程优化和算法增强</strong>。</p>
</blockquote>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250606175536310.png" alt="image-20250606175536310">
<figcaption aria-hidden="true">image-20250606175536310</figcaption>
</figure>
<h4 id="bagging">Bagging</h4>
<p>Bagging是一种<strong>并行式</strong>的集成学习方法，即<strong>基学习器的训练之间没有前后顺序可以同时进行</strong></p>
<p>Bagging使用<strong>“有放回”采样的方式选取训练集</strong>，对于包含m个样本的训练集，进行m次有放回的随机采样操作，从而得到m个样本的采样集，这样训练集中有<strong>接近36.8%</strong>的样本没有被采到，可用作验证集来对泛化性能进行“包外估计”(out-of-bag
estimate)。</p>
<p>按照相同的方式重复进行，我们就可以采集到T个包含m个样本的数据集，从而训练出<strong>T个基学习器</strong>，最终对<strong>这T个基学习器的输出进行结合</strong>。</p>
<h5 id="bagging与boosting的差异">Bagging与Boosting的差异</h5>
<p>Boosting算法一大特点是串行，这样诚然可以降低模型的偏差，增强拟合能力，但是当数据过大时，一大缺点就是会降低学习效率</p>
<p>Bagging作为并行式的集成学习方法，通过综合多个基学习器的结果，可以增加学习效率</p>
<p>二者差异性：</p>
<p>1.对目标的拟合程度：Boosting对目标有更好的拟合能力（偏差小）；Bagging则偏差相对大一些</p>
<p>2.运行效率：由于并行的特点，Bagging的运行效率是大于Boosting的</p>
<p>3.泛化能力：由于Bagging每个学习器不会受其他学习器的影响，泛化能力（方差大）相对于Boosting</p>
<p>更好</p>
<h5 id="bagging的算法流程">Bagging的算法流程</h5>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250606182733022.png" alt="image-20250606182733022">
<figcaption aria-hidden="true">image-20250606182733022</figcaption>
</figure>
<p>可以看出Bagging主要通过<strong>样本的扰动</strong>来增加基学习器之间的多样性，因此Bagging的基学习器应为那些对训练集十分敏感的不稳定学习算法，例如：神经网络与决策树等。</p>
<p>从偏差-方差分解来看，Bagging算法主要关注于降低方差，即通过多次重复训练提高稳定性。</p>
<p>不同于AdaBoost的是，Bagging可以十分简单地移植到多分类、回归等问题。总的说起来则是：<strong>AdaBoost关注于降低偏差，而Bagging关注于降低方差。</strong></p>
<h5 id="自助采样法bootstrap-sampling">自助采样法（Bootstrap
Sampling）</h5>
<p>在机器学习中，<strong>自助采样法（Bootstrap Sampling）</strong> 是
Bagging
算法的核心技术之一。其核心思想是从原始数据集中有放回地随机抽取样本，形成新的训练子集。这一过程的一个重要数学性质是：当样本量
<span class="math inline"><em>n</em></span> 趋近于无穷大时，每个样本在
Bootstrap 样本集中<strong>未被抽中</strong>的概率趋近于 <span class="math inline">$\frac{1}{e} \approx
36.6\%$</span>。以下是详细解析：</p>
<p><strong>1. 公式推导</strong></p>
<p>假设我们从 <span class="math inline"><em>n</em></span>
个样本中<strong>有放回地</strong>抽取 <span class="math inline"><em>n</em></span> 次，形成一个 Bootstrap
样本集。对于任意一个特定样本（如第 <span class="math inline"><em>i</em></span>
个样本），它在某次抽样中<strong>未被选中</strong>的概率为： <span class="math display">$$
1 - \frac{1}{n}
$$</span> 因此，它在整个 <span class="math inline"><em>n</em></span>
次抽样中<strong>从未被选中</strong>的概率为： <span class="math display">$$
\left(1 - \frac{1}{n}\right)^n
$$</span> 当 <span class="math inline"><em>n</em> → ∞</span>
时，该概率的极限为： <span class="math display">$$
\lim_{n \to \infty} \left(1 - \frac{1}{n}\right)^n = \frac{1}{e} \approx
0.3679 \quad (\text{即 } 36.6\%)
$$</span> 在每次 Bootstrap 采样中，约有 <strong>36.6%
的样本未被选中</strong> ，这些样本称为
<strong>Out-of-Bag（OOB，包外估计）样本</strong> 。</p>
<p><strong>2. OOB 样本的应用</strong></p>
<p>在 Bagging 算法中，OOB 样本具有以下重要作用： 1.
<strong>无偏验证</strong>：<br>
每个基学习器的训练数据不包含其对应的 OOB
样本，因此可以用这些样本直接评估模型性能（即 OOB
误差），无需额外的交叉验证。 2. <strong>特征重要性评估</strong>：<br>
在随机森林中，通过比较 OOB
样本在打乱某个特征后的预测误差变化，可以衡量该特征的重要性。</p>
<p><strong>3. 与其他采样方法的对比</strong></p>
<table>
<colgroup>
<col style="width: 25%">
<col style="width: 17%">
<col style="width: 32%">
<col style="width: 24%">
</colgroup>
<thead>
<tr class="header">
<th><strong>采样方法</strong></th>
<th><strong>是否放回</strong></th>
<th><strong>样本覆盖范围</strong></th>
<th><strong>典型应用场景</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Bootstrap 采样</strong></td>
<td>是</td>
<td>约 63.4% 样本被重复使用</td>
<td>Bagging、随机森林</td>
</tr>
<tr class="even">
<td><strong>简单随机采样</strong></td>
<td>否</td>
<td>所有样本唯一出现</td>
<td>传统交叉验证</td>
</tr>
</tbody>
</table>
<h5 id="随机森林">随机森林</h5>
<p>随机森林（Random
Forest）是Bagging的一个拓展体，它的基学习器固定为<strong>决策树</strong>，多棵树也就组成了森林，而<strong>“随机”则在于选择划分属性的随机</strong>，随机森林在训练基学习器时，也采用有放回采样的方式添加样本扰动，同时它还引入了一种<strong>属性扰动</strong>，即在基决策树的训练过程中，在选择划分属性时，RF先从候选属性集中随机挑选出一个包含K个属性的子集，再从这个子集中选择最优划分属性
。</p>
<p>这样随机森林中基学习器的<strong>多样性不仅来自样本扰动，还来自属性扰动</strong>，从而进一步提升了基学习器之间的差异度。相比决策树的Bagging集成，随机森林的起始性能较差（由于属性扰动，基决策树的准确度有所下降），但随着基学习器数目的增多，随机森林往往会收敛到更低的泛化误差。同时不同于Bagging中决策树从所有属性集中选择最优划分属性，<strong>随机森林只在属性集的一个子集中选择划分属性，因此训练效率更高</strong>。</p>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250606184958951.png" alt="image-20250606184958951">
<figcaption aria-hidden="true">image-20250606184958951</figcaption>
</figure>
<h4 id="结合策略">结合策略</h4>
<p>在集成学习中，结合策略是将多个基学习器的输出整合为最终预测结果的关键步骤。以下是针对回归和分类问题的不同结合策略及其核心要点：</p>
<p><strong>定义</strong>：在训练好多个基学习器后，如何将其输出组合成集成模型的最终输出。</p>
<h5 id="平均法回归问题"><strong>1.平均法（回归问题）</strong></h5>
<ol type="1">
<li><p><strong>简单平均法（Simple Averaging）</strong></p>
<ul>
<li><strong>公式</strong>：<br>
<span class="math display">$$
H(x) = \frac{1}{T} \sum_{i=1}^{T} h_i(x)
$$</span></li>
<li><strong>特点</strong>：
<ul>
<li>直接对所有基学习器的预测结果取算术平均。<br>
</li>
<li>计算简单，适合基学习器性能相近的场景。<br>
</li>
<li>若部分基学习器表现较差，可能拖累整体性能。</li>
</ul></li>
</ul></li>
<li><p><strong>加权平均法（Weighted Averaging）</strong></p>
<ul>
<li><strong>公式</strong>：<br>
<span class="math display">$$
H(x) = \sum_{i=1}^{T} w_i h_i(x)
$$</span> 其中，$ w_i $ 且 $ _{i=1}^{T} w_i = 1 $。<br>
</li>
<li><strong>特点</strong>：
<ul>
<li>通过权重 $ w_i $ 调节各基学习器的贡献，灵活性更高。<br>
</li>
<li>适用于基学习器性能差异较大的情况，可提升鲁棒性。<br>
</li>
<li>权重可通过验证集性能（如RMSE、MAE）或优化算法（如梯度下降）确定。</li>
</ul></li>
</ul></li>
</ol>
<h5 id="投票法分类问题"><strong>2.投票法（分类问题）</strong></h5>
<ol type="1">
<li><strong>简单投票法（Majority Voting）</strong>
<ul>
<li><strong>原理</strong>：<br>
每个基学习器对样本进行分类投票，最终结果由得票最多的类别决定。<br>
</li>
<li><strong>公式</strong>（二分类示例）：<br>
<span class="math display">$$
H(x) =
\begin{cases}
1 &amp; \text{若} \sum_{i=1}^{T} I(h_i(x) = 1) &gt; T/2 \\
0 &amp; \text{否则}
\end{cases}
$$</span> 其中，$ I() $ 为指示函数。<br>
</li>
<li><strong>特点</strong>：
<ul>
<li>简单高效，适合基学习器性能相近的场景。<br>
</li>
<li>对异常分类器的鲁棒性较弱。</li>
</ul></li>
</ul></li>
<li><strong>加权投票法（Weighted Voting）</strong>
<ul>
<li><strong>原理</strong>：<br>
给不同基学习器分配权重，最终结果由加权票数最高的类别决定。<br>
</li>
<li><strong>公式</strong>（二分类示例）：<br>
<span class="math display">$$
H(x) =
\begin{cases}
1 &amp; \text{若} \sum_{i=1}^{T} w_i I(h_i(x) = 1) &gt; 0.5
\sum_{i=1}^{T} w_i \\
0 &amp; \text{否则}
\end{cases}
$$</span></li>
<li><strong>特点</strong>：
<ul>
<li>权重可根据基学习器的验证集准确率或领域知识设定。<br>
</li>
<li>更适合处理性能差异较大的基学习器。</li>
</ul></li>
</ul></li>
</ol>
<p>绝对多数投票法（majority
voting）提供了拒绝选项，这在可靠性要求很高的学习任务中是一个很好的机制。同时，对于分类任务，各个基学习器的输出值有两种类型，分别为类标记和类概率。</p>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250606195241433.png" alt="image-20250606195241433">
<figcaption aria-hidden="true">image-20250606195241433</figcaption>
</figure>
<p>一些在产生类别标记的同时也生成置信度的学习器，置信度可转化为类概率使用，<strong>一般基于类概率进行结合往往比基于类标记进行结合的效果更好</strong>，需要注意的是对于异质集成，其类概率不能直接进行比较，此时需要将类概率转化为类标记输出，然后再投票。</p>
<h5 id="学习法stacking"><strong>3.学习法（Stacking）</strong></h5>
<p><strong>学习法</strong>是一种更高级的结合策略，其核心思想是通过训练一个<strong>次级学习器（Meta-Learner）</strong>
来动态融合多个基学习器的输出。其中，<strong>Stacking（堆叠泛化）</strong>
是学习法的典型代表，它通过将基学习器的预测结果作为新特征，进一步训练一个次级模型，最终实现更优的泛化性能。</p>
<p><strong>Stacking 的基本原理</strong></p>
<p><strong>步骤概述</strong>：</p>
<ul>
<li><strong>训练基学习器</strong>：使用原始数据训练 $ T $
个基学习器（如决策树、SVM、神经网络等）。<br>
</li>
<li><strong>生成新特征</strong>：对于每个样本，将 $ T $
个基学习器的输出（预测结果）作为该样本的新特征，形成一个 $ m T $
的数据集（$ m $ 为样本数量）。<br>
</li>
<li><strong>训练次级学习器</strong>：使用新数据集（基学习器输出 +
真实标签）训练一个次级学习器（如逻辑回归、梯度提升树等），该学习器负责融合基学习器的预测结果。</li>
</ul>
<p><strong>Stacking 的优势</strong></p>
<ol type="1">
<li><strong>动态权重分配</strong>：<br>
次级学习器可以自动学习基学习器的权重，无需人工设定。例如，若某个基学习器表现优异，次级学习器会赋予其更高的权重。<br>
</li>
<li><strong>异质模型融合</strong>：<br>
可以混合不同类型的基学习器（如线性模型与树模型），充分利用各自的特性。<br>
</li>
<li><strong>提升泛化能力</strong>：<br>
次级学习器通过学习基学习器的输出模式，能够捕捉更复杂的决策边界。</li>
</ol>
<p><strong>Stacking 的实现细节</strong></p>
<ol type="1">
<li><strong>数据划分</strong>：
<ul>
<li>通常需将原始数据分为两部分：
<ul>
<li><strong>训练集</strong>：用于训练基学习器。<br>
</li>
<li><strong>验证集</strong>：用于生成基学习器的输出（避免过拟合）。<br>
</li>
</ul></li>
<li>或采用交叉验证（如 $ k
$-折）生成基学习器的预测结果，确保次级学习器的训练数据不被污染。</li>
</ul></li>
<li><strong>基学习器输出类型</strong>：
<ul>
<li><strong>分类任务</strong>：基学习器输出类概率（Soft
Voting），而非类别标签（Hard Voting）。例如，逻辑回归输出 $ P(c_j | x)
$，随机森林输出节点样本的类别分布。<br>
</li>
<li><strong>回归任务</strong>：基学习器直接输出预测值（如线性回归的
<span class="math inline"><em>ŷ</em></span>）。</li>
</ul></li>
<li><strong>次级学习器选择</strong>：
<ul>
<li><strong>多响应线性回归（MLR）</strong>：适用于基学习器输出可加权平均的情况，计算简单且鲁棒。<br>
<span class="math display">$$
H(x) = \sum_{i=1}^{T} w_i h_i(x)
$$</span></li>
<li><strong>复杂模型</strong>：如梯度提升树、神经网络，可捕捉基学习器输出之间的非线性关系。</li>
</ul></li>
</ol>
<h4 id="多样性">多样性</h4>
<p>在集成学习中，<strong>多样性增强（Diversity Enhancement）</strong>
是提升模型性能的关键策略。通过引入多样性，可以降低基学习器之间的相关性，从而减少误差传递和过拟合风险。以下是四种常见的多样性增强方法及其核心要点：</p>
<p><strong>1. 数据样本扰动（Data Perturbation）</strong></p>
<p><strong>原理</strong>：通过修改训练数据的分布或采样方式，使每个基学习器看到不同的数据子集。</p>
<p><strong>实现方式</strong>：<br>
- <strong>Bagging（如随机森林）</strong>：<br>
- 随机有放回地采样（Bootstrap），生成多个不同的训练集。<br>
- 对输入扰动敏感的基学习器（如决策树、神经网络）效果显著。<br>
- <strong>示例</strong>：<br>
- 决策树对数据扰动敏感，Bagging 可有效提升其泛化能力。<br>
- 线性模型（如线性回归、SVM）对数据扰动不敏感，Bagging 效果有限。</p>
<p><strong>2. 输入属性扰动（Input Attribute Perturbation）</strong></p>
<p><strong>原理</strong>：通过改变输入特征的表示或选择，增加基学习器间的差异。</p>
<p><strong>实现方式</strong>：<br>
-
<strong>特征子集采样</strong>：每次随机选择部分特征进行训练（如随机森林中的列扰动）。<br>
- <strong>特征变换</strong>：对特征进行缩放、旋转或加噪声等操作。<br>
- <strong>适用场景</strong>：<br>
- 数据包含大量冗余属性时，可大幅加速训练并提升多样性。<br>
- 对高维数据（如图像、文本）尤其有效。</p>
<p><strong>3. 输出属性扰动（Output Attribute Perturbation）</strong></p>
<p><strong>原理</strong>：通过修改训练样本的标签，间接影响基学习器的学习过程。</p>
<p><strong>实现方式</strong>：<br>
-
<strong>随机翻转标签</strong>：对部分样本的标记进行随机更改（需谨慎使用，避免干扰模型）。<br>
- <strong>Dropout（神经网络）</strong>：<br>
- 在训练过程中随机“关闭”部分神经元，强制网络学习更鲁棒的特征。<br>
- 类似于对输出属性的随机扰动，可提升模型泛化能力。</p>
<p><strong>4. 算法参数扰动（Algorithm Parameter
Perturbation）</strong></p>
<p><strong>原理</strong>：通过调整基学习器的超参数，生成不同的模型行为。</p>
<p><strong>实现方式</strong>：</p>
<ul>
<li><strong>正则化方法</strong>：L1/L2 正则化（如
Ridge、Lasso）限制模型复杂度，降低过拟合风险。</li>
<li><strong>随机初始化</strong>：
神经网络的随机权重初始化可能导致收敛到不同局部最优解。</li>
</ul>
<h4 id="作业-1">作业</h4>
<h5 id="section-3">1</h5>
<p>集成学习中常见的两种方法是什么？请分别介绍它们的原理和特点。集成学习相比于单个模型有什么优势和应用场景？</p>
<p><strong>集成学习常见方法、原理、特点及优势</strong></p>
<p><strong>常见方法</strong>：Bagging 和 Boosting<br>
<strong>原理与特点</strong>：</p>
<table>
<colgroup>
<col style="width: 9%">
<col style="width: 45%">
<col style="width: 45%">
</colgroup>
<thead>
<tr class="header">
<th><strong>方法</strong></th>
<th><strong>原理</strong></th>
<th><strong>特点</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Bagging</strong></td>
<td>1. <strong>自助采样</strong>：从训练集有放回抽取多个子集<br>2.
<strong>并行训练</strong>基模型<br>3.
<strong>聚合预测</strong>（投票/平均）</td>
<td>- 降低方差<br>- 适合高方差模型（如未剪枝决策树）<br>-
并行化，训练快<br>- 代表：随机森林</td>
</tr>
<tr class="even">
<td><strong>Boosting</strong></td>
<td>1. <strong>顺序训练</strong>：后一个模型修正前一个模型的错误<br>2.
<strong>加权困难样本</strong><br>3. <strong>加权组合</strong>模型</td>
<td>- 降低偏差<br>- 需弱学习器（如树桩）<br>- 易过拟合（需正则化）<br>-
代表：AdaBoost, GBDT, XGBoost</td>
</tr>
</tbody>
</table>
<p><strong>集成学习的优势</strong>：<br>
-
<strong>提升泛化能力</strong>：降低过拟合（Bagging）或欠拟合（Boosting）风险<br>
- <strong>增强鲁棒性</strong>：减少异常值/噪声影响（如投票机制）<br>
- <strong>突破性能上限</strong>：组合多个弱模型达到强模型效果</p>
<p><strong>应用场景</strong>：<br>
- <strong>分类任务</strong>：医疗诊断（整合多模型减少误诊）<br>
- <strong>回归任务</strong>：房价预测（融合不同树模型提升精度）<br>
- <strong>不平衡数据</strong>：Boosting 加权少数类样本<br>
- <strong>高维数据</strong>：随机森林自动特征选择</p>
<h5 id="section-4">2</h5>
<p>如果在完全相同的训练集上训练了五个不同的模型，并且它们都达到了95%的准确率，是否还有机会通过结合这些模型来获得更好的结果？如果可以，该怎么做？如果不行，为什么？</p>
<p><strong>模型结合提升性能的可能性与方法</strong></p>
<p><strong>是否可能提升</strong>：<strong>是</strong>，但需满足条件：<strong>模型错误不相关</strong>（即犯错样本不同）。</p>
<p><strong>如何实现</strong>：</p>
<ol type="1">
<li><strong>投票法（分类）</strong>：
<ul>
<li>多数投票：5个模型对样本 (x) 的预测为 ([A, A, B, A, C]) → 最终输出
(A)<br>
</li>
<li><strong>关键要求</strong>：模型存在<strong>多样性</strong>（如使用SVM、决策树等不同算法）<br>
</li>
</ul></li>
<li><strong>加权平均（回归）</strong>：
<ul>
<li>若模型精度不同，分配权重：$ y_{} = w_1 y_1 + w_2 y_2 + + w_5
y_5$</li>
<li>权重可通过验证集性能确定</li>
</ul></li>
</ol>
<p><strong>若无法提升的情况</strong>：<br>
-
<strong>原因</strong>：模型高度相关（如相同算法、相同特征、相同超参）<br>
- <strong>数学解释</strong>：误差相关性 <span class="math inline"><em>r</em><em>h</em><em>o</em> ≈ 1</span>
时，集成误差 <span class="math inline">≈</span>单一模型误差</p>
<h5 id="section-5">3</h5>
<p>是否可以通过在多个服务器上并行来加速随机森林的训练？AdaBoost集成呢？为什么？</p>
<table>
<colgroup>
<col style="width: 13%">
<col style="width: 18%">
<col style="width: 68%">
</colgroup>
<thead>
<tr class="header">
<th><strong>算法</strong></th>
<th><strong>是否支持并行</strong></th>
<th><strong>原因</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>随机森林</strong></td>
<td>✅ <strong>是</strong></td>
<td>1. 树之间独立训练<br>2. 可分布式分配Bootstrap样本到不同服务器<br>3.
特征分裂也可并行（如选特征子集）</td>
</tr>
<tr class="even">
<td><strong>AdaBoost</strong></td>
<td>❌ <strong>否</strong></td>
<td>1.
模型必须<strong>顺序训练</strong>：后一个模型依赖前一个模型的样本权重更新<br>2.
无法解耦迭代过程</td>
</tr>
</tbody>
</table>
<h3 id="聚类">聚类</h3>
<h4 id="聚类任务"><strong>聚类任务</strong></h4>
<blockquote>
<p>我们之前学习的分类/回归任务都属于 有监督学习
需要我们提供样本与标签</p>
<p>而马上要学习的聚类任务和后续学习的降维则属于 无监督学习
仅需提供样本</p>
</blockquote>
<p>聚类是一种经典的<strong>无监督学习</strong>(unsupervised
learning)方法，<strong>无监督学习的目标是通过对无标记训练样本的学习，发掘和揭示数据集本身潜在的结构与规律</strong>，即不依赖于训练数据集的类标记信息。</p>
<p>聚类试图将数据集中的样本划分为若干个通常是不相交的子集,<strong>每个子集称为一个“簇”(
cluster)</strong>。通过这样的划分,每簇可能对应于一些潜在的概念(类别),如“浅色瓜”“深色瓜”,“有籽瓜”“无籽瓜”,甚至“本地瓜”“外地瓜”等;需说明的是,这些概念对聚类算法而言事先是未知的,聚类过程仅能自动形成簇结构,
<strong>簇所对应的概念语义需由使用者来把握和命名</strong>。</p>
<p>直观上来说，聚类是将相似的样本聚在一起，从而形成一个<strong>类簇（cluster）</strong>。涉及两个问题</p>
<ul>
<li>如何<strong>度量相似性</strong>（similarity
measure），这便是<strong>距离度量</strong>(distance
measure)，在生活中我们说差别小则相似，对应到多维样本，每个样本可以对应于高维空间中的一个数据点，若它们的距离相近，我们便可以称它们相似。</li>
<li>如何<strong>评价聚类结果</strong>，这便是<strong>性能度量</strong>(validity
index)</li>
</ul>
<h4 id="距离度量">距离度量</h4>
<h5 id="连续离散有序">连续/离散有序</h5>
<p><strong>明可夫斯基距离（Minkowski Distance）</strong></p>
<p>明可夫斯基距离是一组常用的<strong>连续型距离度量</strong>，通过调整参数
$ p $ 可以统一表示多种距离形式，是欧氏距离和曼哈顿距离的推广。</p>
<p><strong>1. 公式定义</strong></p>
<p>对于两个 $ n $ 维向量 $ <em>i = (x</em>{i1}, x_{i2}, , x_{in}) $ 和 $
<em>j = (x</em>{j1}, x_{j2}, , x_{jn}) $，明可夫斯基距离的计算公式为：
<span class="math display">$$
\text{dist}_{\text{mk}}(\boldsymbol{x}_i, \boldsymbol{x}_j) = \left(
\sum_{u=1}^{n} |x_{iu} - x_{ju}|^p \right)^{\frac{1}{p}}
$$</span> 其中，$ p $ 是一个可调节的参数。</p>
<p><strong>2. 特殊情况</strong></p>
<ul>
<li><strong>当 $ p = 2 $</strong>：退化为<strong>欧氏距离（Euclidean
Distance）</strong><br>
<span class="math display">$$
\text{dist}_{\text{ed}}(\boldsymbol{x}_i, \boldsymbol{x}_j) =
\sqrt{\sum_{u=1}^{n} |x_{iu} - x_{ju}|^2}
$$</span>
<ul>
<li><strong>几何意义</strong>：两点之间的直线距离。<br>
</li>
<li><strong>适用场景</strong>：大多数机器学习算法（如KNN、PCA）默认使用欧氏距离。</li>
</ul></li>
<li><strong>当 $ p = 1 $</strong>：退化为<strong>曼哈顿距离（Manhattan
Distance）</strong><br>
<span class="math display">$$
\text{dist}_{\text{man}}(\boldsymbol{x}_i, \boldsymbol{x}_j) =
\sum_{u=1}^{n} |x_{iu} - x_{ju}|
$$</span>
<ul>
<li><strong>几何意义</strong>：沿坐标轴移动的总距离（如棋盘格路径）。<br>
</li>
<li><strong>适用场景</strong>：高维稀疏数据（如文本特征）、计算资源受限的场景。</li>
</ul></li>
<li><strong>当 $ p $</strong>：退化为<strong>切比雪夫距离（Chebyshev
Distance）</strong><br>
<span class="math display">dist<sub>che</sub>(<strong>x</strong><sub><em>i</em></sub>,<strong>x</strong><sub><em>j</em></sub>) = max<sub><em>u</em></sub>|<em>x</em><sub><em>i</em><em>u</em></sub>−<em>x</em><sub><em>j</em><em>u</em></sub>|</span>
<ul>
<li><strong>几何意义</strong>：各维度差值的最大值。<br>
</li>
<li><strong>适用场景</strong>：关注最坏情况下的误差（如游戏AI路径规划）。</li>
</ul></li>
</ul>
<p><strong>3. 参数 $ p $ 的影响</strong></p>
<ul>
<li><strong>$ p $
越小</strong>：距离计算越关注较小的维度差异（如曼哈顿距离对单个维度的扰动更敏感）。<br>
</li>
<li><strong>$ p $
越大</strong>：距离计算越关注较大的维度差异（如切比雪夫距离仅关注最大差值）。<br>
</li>
<li><strong>选择依据</strong>：
<ul>
<li>数据分布是否均匀：若某些维度差异显著，可增大 $ p $。<br>
</li>
<li>算法需求：如KNN中，高维数据可能更适合曼哈顿距离（缓解“维度灾难”）。</li>
</ul></li>
</ul>
<h5 id="离散无序">离散无序</h5>
<p>我们知道属性分为两种：<strong>连续属性</strong>(continuous
attribute)和<strong>离散属性</strong>（catergorical
attribute有限个取值）。对于连续值的属性，一般都可以被学习器所用，有时会根据具体的情形作相应的预处理，例如：归一化等；而对于离散值的属性，需要作下面进一步的处理：</p>
<blockquote>
<p>若属性值之间<strong>存在序关系</strong>(ordinal
attribute)，则可以将其转化为连续值，例如：身高属性“高”“中等”“矮”，可转化为{1,
0.5, 0}。</p>
<p>若属性值之间<strong>不存在序关系</strong>(non-ordinal
attribute)，则通常将其转化为向量的形式，例如：性别属性“男”“女”，可转化为{（1,0）,（0,1）}。</p>
</blockquote>
<p><strong>连续属性和存在序关系的离散属性都可以直接参与计算</strong>，而不存在序关系的<strong>无序属性，我们一般采用VDM（Value
Difference Metric）进行距离的计算</strong></p>
<p>VDM
是一种专门用于<strong>离散无序属性</strong>的距离度量方法，通过统计信息量化不同类别间的差异。其核心思想是：<strong>若两个类别的样本在目标变量上的分布差异越大，则它们的距离越大</strong>。</p>
<p><strong>1. 公式解析</strong> <span class="math display">$$
\text{VDM}_p(a, b) = \sum_{i=1}^{k} \left| \frac{m_{u,a,i}}{m_{u,a}} -
\frac{m_{u,b,i}}{m_{u,b}} \right|^p
$$</span> - <strong>符号含义</strong>：<br>
- $ a, b $：两个不同的类别值（如性别“男”和“女”）。<br>
- $ m_{u,a,i} $：在属性 $ u $ 的第 $ i $ 个取值下，类别 $ a $
的样本数量。<br>
- $ m_{u,a} $：类别 $ a $ 的总样本数量。<br>
- $ k $：属性 $ u $ 的不同取值数目（如颜色属性有红、蓝、绿三种取值，则 $
k=3 $）。<br>
- $ p $：距离幂指数（通常取 $ p=1 $ 或 $ p=2 $）。</p>
<p><strong>2. 核心思想</strong></p>
<ul>
<li><strong>统计分布差异</strong>：<br>
对于每个属性取值 $ i $，计算类别 $ a $ 和 $ b $ 的样本比例差异：<br>
<span class="math display">$$
\left| \frac{m_{u,a,i}}{m_{u,a}} - \frac{m_{u,b,i}}{m_{u,b}} \right|
$$</span> 该值越大，说明两个类别在该取值上的分布差异越大。<br>
</li>
<li><strong>加权求和</strong>：<br>
将所有属性取值的差异按 $ p $ 次方加权求和，得到最终的距离。</li>
</ul>
<p><strong>3. 示例说明</strong></p>
<p>假设我们有一个“颜色”属性（红、蓝、绿），目标变量是“是否购买商品”（0/1）。统计结果如下：</p>
<table>
<thead>
<tr class="header">
<th>颜色</th>
<th>购买（1）</th>
<th>不购买（0）</th>
<th>总计</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>红</td>
<td>10</td>
<td>5</td>
<td>15</td>
</tr>
<tr class="even">
<td>蓝</td>
<td>8</td>
<td>12</td>
<td>20</td>
</tr>
<tr class="odd">
<td>绿</td>
<td>3</td>
<td>7</td>
<td>10</td>
</tr>
</tbody>
</table>
<p>计算“红”与“蓝”之间的 VDM 距离（$ p=1 <span class="math inline">）：1.<em>计</em><em>算</em><em>每</em><em>个</em><em>颜</em><em>色</em><em>在</em><em>购</em><em>买</em>/<em>不</em><em>购</em><em>买</em><em>的</em><em>比</em><em>例</em>： − <em>红</em>：</span>
P(1) = 10/15 <span class="math inline">，</span> P(0) = 5/15 $<br>
- 蓝：$ P(1) = 8/20 = 0.4 <span class="math inline">，</span> P(0) =
12/20 = 0.6 $<br>
2. 计算差异并求和：<br>
<span class="math display">VDM<sub>1</sub>(红,蓝) = |0.67−0.4| + |0.33−0.6| = 0.27 + 0.27 = 0.54</span></p>
<h4 id="性能度量">性能度量</h4>
<p>于聚类算法不依赖于样本的真实类标，就不能像监督学习的分类那般，通过计算分对分错（即精确度或错误率）来评价学习器的好坏或作为学习过程中的优化目标。</p>
<p>直观上看,我们希望<strong>“物以类聚”</strong>,即同一簇的样本尽可能彼此相似,不同簇的样本尽可能不同换言之,聚类结果的<strong>“簇内相似度”(
intra-cluster similarity)高且“簇间相似度” inter-cluster
similarity)低</strong></p>
<p><strong>聚类性能度量有两类</strong></p>
<ul>
<li>“外部指标”(external
index)：所谓外部指标就是已经有一个“参考模型”存在了，将当前模型与参考模型的比对结果作为指标。</li>
<li>“内部指标”( internal
index)：所谓内部指标就是仅仅考虑当前模型的聚类结果。</li>
</ul>
<h5 id="外部指标">外部指标</h5>
<p><strong>1.基本概念</strong></p>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250607110103570.png" alt="image-20250607110103570">
<figcaption aria-hidden="true">image-20250607110103570</figcaption>
</figure>
<p>显然，$ a + b + c + d = $ 。</p>
<p><strong>2. 常用外部指标</strong></p>
<p><strong>（1）Jaccard系数（JC）</strong> <span class="math display">$$
\text{JC} = \frac{a}{a + b + c}
$$</span> -
<strong>含义</strong>：衡量两个划分的重叠程度，仅考虑正确匹配（$ a <span class="math inline">）<em>与</em><em>矛</em><em>盾</em><em>情</em><em>况</em>（</span>
b + c <span class="math inline">）。 −  *  * <em>范</em><em>围</em> *  * ：</span>
[0, 1] $，值越大越好。<br>
- <strong>特点</strong>：对称性差，对噪声敏感 。</p>
<p><strong>（2）Fowlkes-Mallows指数（FMI）</strong> <span class="math display">$$
\text{FMI} = \sqrt{\frac{a}{a + b} \cdot \frac{a}{a + c}}
$$</span> - <strong>含义</strong>：结合查准率（$ <span class="math inline">）<em>和</em><em>查</em><em>全</em><em>率</em>（</span>
<span class="math inline">），<em>反</em><em>映</em><em>正</em><em>确</em><em>匹</em><em>配</em><em>的</em><em>综</em><em>合</em><em>能</em><em>力</em>。 −  *  * <em>范</em><em>围</em> *  * ：</span>
[0, 1] $，值越大越好。<br>
- <strong>特点</strong>：平衡性较好，适合小样本 。</p>
<p><strong>（3）Rand指数（RI）</strong> <span class="math display">$$
\text{RI} = \frac{2(a + d)}{m(m - 1)}
$$</span> - <strong>含义</strong>：同时考虑正确匹配（$ a + d <span class="math inline">）<em>与</em><em>总</em><em>样</em><em>本</em><em>对</em><em>数</em>，<em>适</em><em>用</em><em>于</em><em>大</em><em>规</em><em>模</em><em>数</em><em>据</em>。 −  *  * <em>范</em><em>围</em> *  * ：</span>
[0, 1] $，值越大越好。<br>
- <strong>特点</strong>：计算简单，但对噪声较鲁棒 。</p>
<p><strong>常用指标</strong></p>
<ul>
<li><strong>调整兰德指数（Adjusted Rand Index, ARI）</strong>
<ul>
<li><strong>定义</strong>：衡量聚类结果与真实标签的匹配程度，调整随机聚类的影响，取值范围
[-1, 1]，值越大越好。<br>
</li>
<li><strong>公式</strong>：<br>
<span class="math display">$$
\text{ARI} = \frac{\text{RI} - \mathbb{E}[\text{RI}]}{\max(\text{RI}) -
\mathbb{E}[\text{RI}]}
$$</span> 其中 RI 是兰德指数（匹配样本对的比例）。</li>
</ul></li>
<li><strong>归一化互信息（Normalized Mutual Information, NMI）</strong>
<ul>
<li><strong>定义</strong>：衡量聚类结果与真实标签的信息共享程度，值越大越好。<br>
</li>
<li><strong>公式</strong>：<br>
<span class="math display">$$
\text{NMI} = \frac{I(C; K)}{\sqrt{H(C) H(K)}}
$$</span> 其中 $ I(C; K) $ 是互信息，$ H(C) $ 和 $ H(K) $ 是熵。</li>
</ul></li>
<li><strong>Fowlkes-Mallows 指数（FMI）</strong>
<ul>
<li><strong>定义</strong>：基于聚类结果与真实标签的 TP、FP、TN、FN
计算，值越大越好。<br>
</li>
<li><strong>公式</strong>：<br>
<span class="math display">$$
\text{FMI} = \sqrt{\frac{\text{TP}}{\text{TP} + \text{FP}} \cdot
\frac{\text{TP}}{\text{TP} + \text{FN}}}
$$</span></li>
</ul></li>
</ul>
<p><strong>优点</strong></p>
<ul>
<li>在有真实标签时，能更客观地评估聚类效果。</li>
<li>适用于验证聚类结果的业务意义（如客户分群是否符合预期）。</li>
</ul>
<p><strong>局限性</strong></p>
<ul>
<li>需要真实标签，不适用于纯无监督任务。</li>
<li>对标签噪声敏感（如标签错误会误导 $ K $ 的选择）。</li>
</ul>
<p><strong>3. 应用示例</strong></p>
<p>假设一个包含4个样本的数据集，参考标签为 <span class="math inline">{<em>A</em>, <em>A</em>, <em>B</em>, <em>B</em>}</span>，聚类结果为
<span class="math inline">{<em>C</em>, <em>C</em>, <em>D</em>, <em>D</em>}</span>：
- <strong>计算样本对</strong>：<br>
- $ a = 2 $（样本1-2同簇，参考与聚类均同类）。<br>
- $ b = 0 $（参考同类但聚类不同类）。<br>
- $ c = 0 $（参考不同类但聚类同类）。<br>
- $ d = 2 $（参考不同类且聚类不同类）。<br>
- <strong>指标结果</strong>：<br>
- JC = $ = 1 $（完美匹配）。<br>
- FMI = $ = 1 $。<br>
- RI = $ = $。</p>
<h5 id="内部指标">内部指标</h5>
<p>内部指标不依赖任何外部参考模型，直接通过<strong>簇内紧凑性</strong>和<strong>簇间分离性</strong>评估聚类结果。其核心思想是：
- <strong>簇内高内聚</strong>：同一簇的样本尽可能相似（距离小）。<br>
- <strong>簇间低耦合</strong>：不同簇的样本尽可能不同（距离大）。</p>
<p><strong>1. 基本定义</strong></p>
<p>设聚类结果为 $ C = {C_1, C_2, , C_k} $，定义以下四个关键距离：</p>
<p><strong>（1）簇内平均距离（avg(C)）</strong> <span class="math display">$$
\text{avg}(C) = \frac{2}{|C|(|C| - 1)} \sum_{1 \leq i &lt; j \leq |C|}
\text{dist}(\boldsymbol{x}_i, \boldsymbol{x}_j)
$$</span> - <strong>含义</strong>：簇内所有样本对的平均距离。<br>
- <strong>目标</strong>：越小越好，表示簇内样本更紧密。</p>
<p><strong>（2）簇内最大距离（diam(C)）</strong> <span class="math display">diam(<em>C</em>) = max<sub>1 ≤ <em>i</em> &lt; <em>j</em> ≤ |<em>C</em>|</sub>dist(<strong>x</strong><sub><em>i</em></sub>,<strong>x</strong><sub><em>j</em></sub>)</span>
- <strong>含义</strong>：簇内最远的两个样本之间的距离。<br>
- <strong>目标</strong>：越小越好，避免簇内存在离群点。</p>
<p><strong>（3）簇间最小距离（$ d_{}(C_i, C_j) $）</strong> <span class="math display"><em>d</em><sub>min</sub>(<em>C</em><sub><em>i</em></sub>,<em>C</em><sub><em>j</em></sub>) = min<sub><strong>x</strong><sub><em>i</em></sub> ∈ <em>C</em><sub><em>i</em></sub>, <strong>x</strong><sub><em>j</em></sub> ∈ <em>C</em><sub><em>j</em></sub></sub>dist(<strong>x</strong><sub><em>i</em></sub>,<strong>x</strong><sub><em>j</em></sub>)</span>
- <strong>含义</strong>：簇 $ C_i $ 和 $ C_j $
之间最近的两个样本的距离。<br>
- <strong>目标</strong>：越大越好，表示簇间分离度高。</p>
<p><strong>（4）簇中心距离（$ d_{}(C_i, C_j) $）</strong> <span class="math display"><em>d</em><sub>cen</sub>(<em>C</em><sub><em>i</em></sub>,<em>C</em><sub><em>j</em></sub>) = dist(<strong>μ</strong><sub><em>i</em></sub>,<strong>μ</strong><sub><em>j</em></sub>)</span>
- <strong>含义</strong>：簇 $ C_i $ 和 $ C_j $
的中心点（均值向量）之间的距离。<br>
- <strong>目标</strong>：越大越好，表示簇中心相隔较远。</p>
<p><strong>2. 常用内部指标</strong></p>
<p><strong>1. DB指数（Davies-Bouldin Index, DBI）</strong></p>
<ul>
<li><strong>公式</strong>：<br>
<span class="math display">$$
\text{DBI} = \frac{1}{k} \sum_{i=1}^{k} \max_{j \neq i} \left(
\frac{\text{avg}(C_i) + \text{avg}(C_j)}{d_{\text{cen}}(\mu_i, \mu_j)}
\right)
$$</span>
<ul>
<li><strong>符号含义</strong>：
<ul>
<li>$ k $：簇的数量。<br>
</li>
<li>$ (C_i) $：簇 $ C_i $ 内部样本的平均距离。<br>
</li>
<li>$ d_{}(_i, _j) $：簇 $ C_i $ 和 $ C_j $
的中心点（均值向量）之间的距离。<br>
</li>
</ul></li>
<li><strong>目标</strong>：越小越好。<br>
</li>
<li><strong>核心思想</strong>：对于每个簇 $ C_i $，找到与其“最竞争”的簇
$ C_j $（即 $ $ 最大的簇），并取所有簇的平均值。</li>
</ul></li>
<li><strong>示例</strong>：<br>
若簇 $ C_1 $ 和 $ C_2 $ 的平均距离分别为 2 和 3，中心距离为
5，则它们的比值为 $ = 1 $。若这是 $ C_1 $ 的最大比值，则 $ C_1 $ 对 DBI
的贡献为 1。最终 DBI 是所有簇贡献的平均值。</li>
</ul>
<p><strong>2. Dunn指数（Dunn Index, DI）</strong></p>
<ul>
<li><strong>公式</strong>：<br>
<span class="math display">$$
\text{DI} = \min_{1 \leq i \leq k} \left\{ \frac{\min_{j \neq i}
d_{\min}(C_i, C_j)}{\max_{1 \leq l \leq k} \text{diam}(C_l)} \right\}
$$</span>
<ul>
<li><strong>符号含义</strong>：
<ul>
<li>$ d_{}(C_i, C_j) $：簇 $ C_i $ 和 $ C_j $
之间的最小距离（最近样本对的距离）。<br>
</li>
<li>$ (C_l) $：簇 $ C_l $ 内的最大距离（最远样本对的距离）。<br>
</li>
</ul></li>
<li><strong>目标</strong>：越大越好。<br>
</li>
<li><strong>核心思想</strong>：
<ul>
<li>分子：所有簇对之间的最小距离中的最小值（即最“脆弱”的簇间分离度）。<br>
</li>
<li>分母：所有簇中的最大直径（最“松散”的簇内紧凑度）。<br>
</li>
<li>指数越大，表示簇间分离度高且簇内紧凑。</li>
</ul></li>
</ul></li>
<li><strong>示例</strong>：<br>
假设簇对 $ (C_1, C_2) $ 的最小距离为 5，簇 $ C_3 $ 的最大直径为 10，则
DI 为 $ = 0.5 $。</li>
</ul>
<p><strong>3. 轮廓系数（Silhouette Coefficient）</strong></p>
<ul>
<li><p><strong>单一样本的轮廓系数</strong>：<br>
<span class="math display">$$
s = \frac{b - a}{\max(a, b)}
$$</span></p>
<ul>
<li><strong>符号含义</strong>：
<ul>
<li>$ a $：样本到同簇其他样本的平均距离（簇内凝聚度）。<br>
</li>
<li>$ b $：样本到最近簇中样本的平均距离（簇间分离度）。<br>
</li>
</ul></li>
<li><strong>取值范围</strong>：$ [-1, 1] $，越接近 1
表示聚类效果越好。<br>
</li>
<li><strong>核心思想</strong>：
<ul>
<li>若 $ a &lt; b $（同簇紧密，异簇疏远），则 $ s &gt; 0
$，样本分类合理。<br>
</li>
<li>若 $ a &gt; b $（同簇松散，异簇更近），则 $ s &lt; 0
$，样本可能被错误分类。</li>
</ul></li>
</ul></li>
<li><p><strong>整体轮廓系数</strong>：所有样本轮廓系数的平均值。</p></li>
<li><p><strong>示例</strong>：<br>
若某样本 $ a = 2 <span class="math inline">，</span> b = 5 $，则 $ s = =
0.6 $，表明该样本分类合理。</p></li>
</ul>
<p><strong>4.肘部法则（Elbow Method）</strong></p>
<p>肘部法则是一种<strong>经验性方法</strong>，常用于确定K-means等聚类算法的最优簇数（$
K $）。其核心思想是通过观察误差平方和（SSE, Sum of Squared Errors）随 $
K $ 值变化的趋势，寻找“肘部点”（即 SSE
下降速度明显减缓的拐点），从而选择最优的 $ K $ 值</p>
<ul>
<li><p><strong>SSE（误差平方和）</strong>：衡量每个样本到其所属簇中心的距离平方和，公式为：
<span class="math display">$$
\text{SSE} = \sum_{i=1}^n \|x_i - \mu_{c_i}\|^2
$$</span> 其中 $ x_i $ 是样本点，$ _{c_i} $ 是其所属簇中心。</p></li>
<li><p><strong>趋势分析</strong>：</p>
<ul>
<li>当 $ K $ 增大时，SSE
会不断减小（因为簇越多，每个簇的样本越密集）。</li>
<li>但当 $ K $ 增加到某个值后，SSE
的下降速度会显著放缓，形成“肘部”形状。</li>
</ul></li>
<li><p><strong>肘部点的意义</strong>：<br>
肘部点对应的 $ K $
值是<strong>模型复杂度</strong>（簇数）与<strong>聚类效果</strong>（SSE）之间的平衡点。</p></li>
</ul>
<p><strong>指标对比与选择</strong></p>
<table style="width:100%;">
<colgroup>
<col style="width: 8%">
<col style="width: 32%">
<col style="width: 13%">
<col style="width: 22%">
<col style="width: 22%">
</colgroup>
<thead>
<tr class="header">
<th><strong>指标</strong></th>
<th><strong>计算方式</strong></th>
<th><strong>目标</strong></th>
<th><strong>适用场景</strong></th>
<th><strong>局限性</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>DBI</td>
<td>簇内平均距离与簇中心距离的比值</td>
<td>越小越好</td>
<td>球形簇，需指定 $ k $</td>
<td>对离群点敏感</td>
</tr>
<tr class="even">
<td>Dunn指数</td>
<td>簇间最小距离与簇内最大直径的比值</td>
<td>越大越好</td>
<td>强调簇间分离与簇内紧凑</td>
<td>计算复杂，受离群点影响</td>
</tr>
<tr class="odd">
<td>轮廓系数</td>
<td>样本到同簇/异簇的平均距离差</td>
<td>越接近 1 越好</td>
<td>快速评估，适合 K-Means</td>
<td>对非球形簇不敏感</td>
</tr>
</tbody>
</table>
<h4 id="原型聚类与kmeans">原型聚类与kmeans</h4>
<h5 id="原型聚类">原型聚类</h5>
<p>原型聚类即“<strong>基于原型的聚类</strong>”（prototype-based
clustering），原型表示模板的意思，就是通过参考一个模板向量或模板分布的方式来完成聚类的过程，通常情形下算法先对原型进行初始化,然后对原型进行迭代更新求解。采用不同的原型表、不同的求解方式,将产生不同的算法。</p>
<p>常见的K-Means便是基于簇中心（原型向量）来实现聚类，混合高斯聚类则是基于簇分布（概率模型）来实现聚类。</p>
<h5 id="k-means-聚类算法详解"><strong>K-Means 聚类算法详解</strong></h5>
<p><strong>目标函数</strong>：最小化所有样本到其所属簇中心的平方距离之和：<br>
<span class="math display">$$
E = \sum_{i=1}^{k} \sum_{\boldsymbol{x} \in C_i} \|\boldsymbol{x} -
\boldsymbol{\mu}_i\|_2^2
$$</span> 其中，$ <em>i = </em>{ C_i} $ 是簇 $ C_i $ 的均值向量。</p>
<p><strong>算法步骤</strong></p>
<ol type="1">
<li><strong>初始化簇中心</strong>：随机选择 $ k $ 个样本作为初始簇中心。
<ul>
<li><strong>改进方法</strong>：K-Means++
算法可提升初始中心的质量。<br>
</li>
</ul></li>
<li><strong>分配样本到最近簇</strong>：对每个样本 $
$，计算其到所有簇中心的距离，将其分配到距离最近的簇 $ C_i $。<br>
</li>
<li><strong>更新簇中心</strong>：重新计算每个簇的均值向量 $ _i $。<br>
</li>
<li><strong>迭代终止条件</strong>：
<ul>
<li>达到预设的最大迭代次数；<br>
</li>
<li>簇中心不再显著变化（如变化幅度小于阈值 $ $）；<br>
</li>
<li>样本分配不再改变。</li>
</ul></li>
</ol>
<p><strong>如何选择 $ k $ 值？</strong></p>
<ul>
<li><strong>肘部法则（Elbow Method）</strong>：绘制 $ k $ 与误差 $ E $
的关系曲线，选择误差下降显著变缓的 $ k $ 值。<br>
</li>
<li><strong>轮廓系数（Silhouette
Coefficient）</strong>：计算每个样本的轮廓系数，选择平均轮廓系数最大的 $
k $。</li>
</ul>
<h5 id="k-means的算法流程">K-Means的算法流程</h5>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250607114743211.png" alt="image-20250607114743211">
<figcaption aria-hidden="true">image-20250607114743211</figcaption>
</figure>
<h5 id="k-means">K-means++</h5>
<p>此法相对于 K-means 做出了一个小的改进。在一开始选择 k
个聚类中心时，并不是随机初始化 k 个，而是首先随机出 1 个，然后循环
k−1<em>k</em>−1 次选择剩下的 k-1
个聚类中心。选择的规则是：每次选择最不可能成为新的聚类中心的样本，或者是到所有聚类中心的最小距离最大的样本。</p>
<h5 id="优势"><strong>优势</strong></h5>
<p><strong>避免不良初始化</strong>
：传统K-means随机初始化可能导致中心过于集中，而K-means++通过“最大化最小距离”策略，使初始中心分布更均匀。</p>
<h5 id="bisecting-k-means">Bisecting K-means</h5>
<p>此法叫做二分 K-means
算法。具体的，在一开始将所有的样本划分为一个簇，然后每次选择一个误差最大的簇进行二分裂，不断分裂直到收敛。这种方法不能使得
Loss 最小，但是可以作为 K-means
算法的一个预热，比如可以通过这种方法得到一个相对合理的簇中心，然后再利用
K-means 算法进行聚类。</p>
<h5 id="优势-1"><strong>优势</strong></h5>
<p><strong>降低计算复杂度</strong>
：每次仅对一个簇进行二分，时间复杂度为
<em>O</em>(<em>k</em>⋅<em>m</em>⋅<em>n</em>) ，适合大规模数据。</p>
<p><strong>提供合理初始中心</strong>
：可作为传统K-means的预处理，减少随机初始化的影响。</p>
<h5 id="lvq学习向量量化"><strong>LVQ（学习向量量化）</strong></h5>
<p><strong>核心思想</strong>：<br>
LVQ
是一种<strong>有监督的原型聚类算法</strong>，结合了神经网络与向量量化技术。它通过维护一组<strong>原型向量</strong>（Prototype
Vectors）来代表不同类别，并利用这些原型对数据进行分类或聚类。与 K-Means
类似，LVQ
会为每个簇分配一个原型向量，但其更新规则受类别标签的指导，因此更适用于分类任务
。</p>
<p><strong>算法特点</strong>：</p>
<ul>
<li><strong>有监督学习</strong>：需要已知类别标签来调整原型向量，使同类样本更接近对应原型，异类样本远离原型。<br>
</li>
<li><strong>拓扑结构建模</strong>：通过原型向量捕捉数据的局部特征，类似于自组织映射（SOM），但更具针对性。<br>
</li>
<li><strong>硬聚类</strong>：每个样本最终被分配到最近的原型对应的类别，不提供概率输出
。</li>
</ul>
<h5 id="高斯混合聚类gaussian-mixture-model-gmm"><strong>高斯混合聚类（Gaussian
Mixture Model, GMM）</strong></h5>
<p>一句话概述算法：高斯混合聚类算法是一种概率模型，假设数据由多个高斯分布混合而成，通过迭代优化参数以拟合数据分布，常用于无监督学习中的聚类任务。</p>
<p>算法过程：</p>
<p>初始化参数： 随机初始化每个分量的均值、协方差矩阵和混合系数。</p>
<p>E 步（Expectation）：
对每个数据点，计算它属于每个分量的后验概率，即计算每个分量的权重。</p>
<p>M 步（Maximization）：
使用E步计算得到的后验概率，更新每个分量的均值、协方差矩阵和混合系数。</p>
<p>迭代： 重复执行E步和M步，直到模型参数收敛或达到预定的迭代次数。</p>
<p>GMM的优点包括对各种形状和方向的聚类簇建模能力，以及对数据分布的灵活性。它在许多领域，如模式识别、图像处理和自然语言处理等，都有广泛的应用。
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250611180451162.png" alt="image-20250611180451162"></p>
<p>以下是高斯混合聚类（GMM）算法的详细步骤及EM算法中E步与M步的解释：</p>
<p><strong>算法流程解析</strong></p>
<p><strong>输入</strong>：样本集 $ D = {x_1, x_2, , x_m} $，混合成分个数
$ k $。<br>
<strong>输出</strong>：簇划分 $ C = {C_1, C_2, , C_k} $。</p>
<p><strong>步骤详解</strong></p>
<ol type="1">
<li><p><strong>初始化模型参数</strong><br>
随机初始化或通过K-means初步估计以下参数：</p>
<ul>
<li><strong>混合系数</strong> $ <em>i $（满足 $ </em>{i=1}^k _i = 1
$）。</li>
<li><strong>均值向量</strong> $ _i $。</li>
<li><strong>协方差矩阵</strong> $ _i $。</li>
</ul></li>
<li><p><strong>迭代优化参数（EM循环）</strong><br>
重复以下步骤直到收敛（如对数似然变化小于阈值）：</p>
<ul>
<li><p><strong>E步（期望步）</strong>：<br>
对每个样本 $ x_j $，计算其由第 $ i $
个高斯分布生成的<strong>后验概率</strong>（责任度 $ _{ji} $）： <span class="math display">$$
\gamma_{ji} = p(z_j = i | x_j) = \frac{\alpha_i \mathcal{N}(x_j | \mu_i,
\Sigma_i)}{\sum_{l=1}^k \alpha_l \mathcal{N}(x_j | \mu_l, \Sigma_l)}
$$</span> 其中 $ (x | , ) $ 是高斯分布的概率密度函数。</p></li>
<li><p><strong>M步（最大化步）</strong>：<br>
根据当前的责任度 $ _{ji} $，更新模型参数：</p>
<ol type="1">
<li><strong>新均值向量</strong>： <span class="math display">$$
\mu_i' = \frac{\sum_{j=1}^m \gamma_{ji} x_j}{\sum_{j=1}^m \gamma_{ji}}
$$</span></li>
<li><strong>新协方差矩阵</strong>： <span class="math display">$$
\Sigma_i' = \frac{\sum_{j=1}^m \gamma_{ji} (x_j - \mu_i')(x_j -
\mu_i')^\top}{\sum_{j=1}^m \gamma_{ji}}
$$</span></li>
<li><strong>新混合系数</strong>： <span class="math display">$$
\alpha_i' = \frac{\sum_{j=1}^m \gamma_{ji}}{m}
$$</span></li>
</ol></li>
</ul></li>
<li><p><strong>簇划分</strong></p>
<ul>
<li>初始化空簇 $ C_i = $。</li>
<li>对每个样本 $ x_j $，计算其属于各簇的后验概率 $ _j = <em>i </em>{ji}
$。</li>
<li>将 $ x_j $ 分配到簇 $ C_{_j} $ 中。</li>
</ul></li>
</ol>
<p><strong>E步与M步的核心作用</strong></p>
<p><strong>E步（期望步）</strong></p>
<ul>
<li><strong>目标</strong>：基于当前参数 $ (_i, _i, <em>i)
$，计算每个样本 $ x_j $ 属于各高斯分布的<strong>责任度</strong> $
</em>{ji} $。</li>
<li><strong>意义</strong>：
<ul>
<li>责任度反映了在当前模型下，样本 $ x_j $ 由第 $ i $
个高斯分布生成的概率。</li>
<li><strong>软分配</strong>：允许样本部分属于多个簇，而非硬划分。</li>
</ul></li>
</ul>
<p><strong>M步（最大化步）</strong></p>
<ul>
<li><strong>目标</strong>：根据责任度 $ _{ji} $，重新估计模型参数 $
(_i’, _i’, _i’) $，以最大化数据的对数似然。</li>
<li><strong>关键公式</strong>：
<ul>
<li><strong>均值更新</strong>：加权平均样本点，权重为责任度。</li>
<li><strong>协方差更新</strong>：加权样本点的方差，反映簇内数据分布。</li>
<li><strong>混合系数更新</strong>：各簇样本的“有效数量”占总样本的比例。</li>
</ul></li>
</ul>
<h4 id="参考资料-1">参考资料</h4>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/smileyan9/article/details/135398479">西瓜书读书笔记整理（九）
—— 第九章 聚类_西瓜书笔记第9章-CSDN博客</a></p>
<h4 id="密度聚类与dbscan">密度聚类与DBSCAN</h4>
<blockquote>
<p>若样本分布为同心的两个环，kmeans则无法做到良好的聚类效果，因此引出密度聚类</p>
</blockquote>
<p>密度聚类是一种基于<strong>样本分布密集程度</strong>的无监督学习方法，其核心思想是：<strong>将高密度区域划分为同一簇，低密度区域视为噪声或边界</strong>。</p>
<p>DBSCAN（Density-Based Spatial Clustering of Applications with
Noise）是密度聚类的典型代表，通过两个关键参数 $ $ 和 $ MinPts $
描述样本分布的紧密性。</p>
<h5 id="核心概念"><strong>1. 核心概念</strong></h5>
<ol type="1">
<li><strong>$ $-邻域</strong>
<ul>
<li>定义：与样本 $ x $ 距离不超过 $ $ 的所有样本集合。<br>
</li>
<li>作用：衡量样本周围的局部密度。<br>
</li>
</ul></li>
<li><strong>核心对象（Core Object）</strong>
<ul>
<li>定义：若样本 $ x $ 的 $ $-邻域内包含至少 $ MinPts $ 个样本，则 $ x $
是核心对象。<br>
</li>
<li>作用：作为簇的生长起点，确保簇的最小密度要求。<br>
</li>
</ul></li>
<li><strong>密度直达（Directly Density-Reachable）</strong>
<ul>
<li>定义：若样本 $ x_j $ 位于核心对象 $ x_i $ 的 $ $-邻域内，则称 $ x_i
$ 可密度直达 $ x_j $。<br>
</li>
<li>作用：建立核心对象与邻近样本的直接连接。<br>
</li>
</ul></li>
<li><strong>密度可达（Density-Reachable）</strong>
<ul>
<li>定义：若存在样本序列 $ x_i, p_1, p_2, , p_n, x_j $，其中 $ p_i $
密度直达 $ p_{i+1} $，则称 $ x_i $ 可密度可达 $ x_j $。<br>
</li>
<li>作用：通过链式传递扩展簇的范围。<br>
</li>
</ul></li>
<li><strong>密度相连（Density-Connected）</strong>
<ul>
<li>定义：若样本 $ x_i $ 和 $ x_j $ 均可密度可达某个公共样本 $ x_k
$，则称 $ x_i $ 和 $ x_j $ 密度相连。<br>
</li>
<li>作用：确保簇的连通性，避免碎片化。</li>
</ul></li>
</ol>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250607124326529.png" alt="image-20250607124326529">
<figcaption aria-hidden="true">image-20250607124326529</figcaption>
</figure>
<p><strong>DBSCN定义的簇</strong></p>
<ul>
<li>定义：最大密度相连的样本集合为一个簇</li>
<li>有两个性质：1.连接性：同一个簇内任意两样本，必然密度相连2.最大性：密度可达的两个样本必
定属于同一个簇</li>
</ul>
<h5 id="dbscan-算法流程"><strong>2. DBSCAN 算法流程</strong></h5>
<p>简单来理解DBSCAN：<strong>找出一个核心对象所有密度可达的样本集合形成簇</strong>。首先从数据集中任选一个核心对象A，找出所有A密度可达的样本集合，将这些样本形成一个密度相连的类簇，直到所有的核心对象都遍历完。DBSCAN算法的流程如下图所示：</p>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250607124446432.png" alt="image-20250607124446432">
<figcaption aria-hidden="true">image-20250607124446432</figcaption>
</figure>
<h5 id="参数选择与影响"><strong>3. 参数选择与影响</strong></h5>
<ul>
<li><strong>$ $（邻域半径）</strong>：
<ul>
<li>过小：可能导致多数样本被标记为噪声，簇数量增加。<br>
</li>
<li>过大：可能导致不同簇合并，簇数量减少。<br>
</li>
<li><strong>选择方法</strong>：通过<strong>K-Distance图</strong>（排序后的第
$ k $ 近邻距离）观察“拐点”。</li>
</ul></li>
<li><strong>$ MinPts $（最小样本数）</strong>：
<ul>
<li>控制簇的最小密度阈值。<br>
</li>
<li>通常取 $ d+1 <span class="math inline">（</span> d $
为特征维度），避免在高维空间中误判噪声。</li>
</ul></li>
</ul>
<h4 id="层次聚类与agnes">层次聚类与AGNES</h4>
<p>层次聚类是一种通过构建<strong>树状结构（Dendrogram）</strong>将数据划分为不同层次的聚类方法。其核心思想是：<br>
-
<strong>凝聚型（Agglomerative）</strong>：从每个样本作为一个独立簇开始，逐步合并最相似的簇，直到达到预设的簇数或形成一个唯一簇。<br>
-
<strong>分裂型（Divisive）</strong>：与凝聚型相反，从整个数据集作为一个簇开始，逐步分裂为更小的簇。</p>
<p>本节重点介绍<strong>AGNES（Agglomerative
Nesting）</strong>，一种经典的自底向上的层次聚类算法。</p>
<h5 id="agnes-算法流程"><strong>1. AGNES 算法流程</strong></h5>
<ol type="1">
<li><strong>初始化</strong>：每个样本作为一个独立簇。<br>
</li>
<li><strong>迭代合并</strong>：
<ul>
<li>计算所有簇对之间的距离。<br>
</li>
<li>合并距离最近的两个簇。<br>
</li>
</ul></li>
<li><strong>终止条件</strong>：
<ul>
<li>达到预设的簇数 $ k $；<br>
</li>
<li>所有簇之间的距离大于阈值。</li>
</ul></li>
</ol>
<h5 id="簇间距离的定义"><strong>2. 簇间距离的定义</strong></h5>
<p>AGNES
的关键在于如何定义<strong>簇间距离</strong>，常见的三种方法如下：</p>
<p><strong>（1）最小距离（Single Linkage）</strong> <span class="math display"><em>d</em><sub>min</sub>(<em>C</em><sub><em>i</em></sub>,<em>C</em><sub><em>j</em></sub>) = min<sub><strong>x</strong> ∈ <em>C</em><sub><em>i</em></sub>, <strong>z</strong> ∈ <em>C</em><sub><em>j</em></sub></sub>dist(<strong>x</strong>,<strong>z</strong>)</span>
- <strong>含义</strong>：两个簇之间最近的两个样本的距离。</p>
<p><strong>（2）最大距离（Complete Linkage）</strong> <span class="math display"><em>d</em><sub>max</sub>(<em>C</em><sub><em>i</em></sub>,<em>C</em><sub><em>j</em></sub>) = max<sub><strong>x</strong> ∈ <em>C</em><sub><em>i</em></sub>, <strong>z</strong> ∈ <em>C</em><sub><em>j</em></sub></sub>dist(<strong>x</strong>,<strong>z</strong>)</span>
- <strong>含义</strong>：两个簇之间最远的两个样本的距离。</p>
<p><strong>（3）平均距离（Average Linkage）</strong> <span class="math display">$$
d_{\text{avg}}(C_i, C_j) = \frac{1}{|C_i| |C_j|} \sum_{\boldsymbol{x}
\in C_i} \sum_{\boldsymbol{z} \in C_j} \text{dist}(\boldsymbol{x},
\boldsymbol{z})
$$</span> - <strong>含义</strong>：两个簇所有样本对距离的平均值。</p>
<h5 id="层次聚类法的算法流程如下所示">层次聚类法的算法流程如下所示：</h5>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250607125338029.png" alt="image-20250607125338029">
<figcaption aria-hidden="true">image-20250607125338029</figcaption>
</figure>
<h4 id="作业-2">作业</h4>
<h5 id="section-6">1</h5>
<p>假设任务是将下面8个点聚类成3个簇：A1(2,10), A2(2,5), A3(8,4),
B1(5,8), B2(7,5), B3(6,4), C1(1,2),
C3(4,9)，距离函数是欧式距离。假设初始选择A1，B1，C1分别作为每个聚类的中心，用Kmeans算法给出计算过程。</p>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250607125506436.png" alt="image-20250607125506436">
<figcaption aria-hidden="true">image-20250607125506436</figcaption>
</figure>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250607125606040.png" alt="image-20250607125606040">
<figcaption aria-hidden="true">image-20250607125606040</figcaption>
</figure>
<h5 id="section-7">2</h5>
<p>Kmeans初始类簇中心如何选取？K值如何确定？请简要阐述。</p>
<p><strong>一、初始类簇中心的选取 (如何选好的起始点？)</strong></p>
<p>传统K-means随机选择初始中心点，容易导致结果不稳定（多次运行结果不同）或陷入局部最优（效果差）。改进方法主要有：</p>
<ol type="1">
<li><strong>K-means++ (最常用且推荐)：</strong>
<ul>
<li><strong>核心思想：</strong> 让初始中心点彼此尽量远离。</li>
<li><strong>步骤：</strong>
<ol type="1">
<li>随机选择<strong>第一个</strong>中心点。</li>
<li>计算每个数据点到<strong>当前已选中心点</strong>的最短距离（即离最近中心的距离）。</li>
<li>以<strong>与这个最短距离平方成正比</strong>的概率，随机选择下一个中心点（距离越大的点，被选中的概率越大）。</li>
<li>重复步骤2和3，直到选出K个中心点。</li>
</ol></li>
<li><strong>优点：</strong>
显著提高聚类质量和稳定性，计算开销增加不大。</li>
</ul></li>
<li><strong>多次运行+选取最优：</strong>
<ul>
<li>独立运行K-means算法多次（每次随机初始化）。</li>
<li>每次运行完成后，计算所有数据点与其所属簇中心的距离平方和（SSE, Sum
of Squared Errors）。</li>
<li>选择SSE最小的那次运行结果作为最终结果。</li>
<li><strong>优点：</strong> 简单，增加找到更好解的机会。</li>
<li><strong>缺点：</strong> 计算开销随运行次数增加。</li>
</ul></li>
<li><strong>基于样本密度/距离：</strong>
<ul>
<li>选择数据空间中样本密度高的区域点作为中心。</li>
<li>或选择相互之间距离较远的点作为中心（类似K-means++的思想，但实现方式可能不同）。</li>
</ul></li>
</ol>
<p><strong>二、K值（簇数量）的确定 (如何知道分几类？)</strong></p>
<p>K值通常需要预先指定，但没有绝对正确的答案。常用方法基于评估不同K值下聚类结果的“质量”，寻找拐点或最优值：</p>
<ol type="1">
<li><strong>肘部法则：</strong>
<ul>
<li><strong>核心思想：</strong>
随着K增大，簇内样本聚合更紧密，簇内平方和误差（SSE）会下降，但下降幅度会逐渐变缓。找到SSE下降速率发生显著变化的“肘点”。</li>
<li><strong>做法：</strong> 计算不同K值（如K=1, 2, 3, …,
max）对应的SSE。绘制<code>K值 - SSE</code>曲线图。观察曲线，寻找SSE下降幅度突然变得平缓的那个K值（形如手臂的“肘关节”）。</li>
<li><strong>优点：</strong> 直观。</li>
<li><strong>缺点：</strong>
“肘点”有时不明显或不存在，需要主观判断。</li>
</ul></li>
<li><strong>轮廓系数：</strong>
<ul>
<li><strong>核心思想：</strong>
综合衡量一个样本与其自身簇的紧密度(<code>a</code>)和与其他簇的分离度(<code>b</code>)。</li>
<li><strong>计算：</strong> 对于每个样本i：
<ul>
<li><code>a(i)</code> = i
到同簇内所有其他点的平均距离（簇内不相似度）。</li>
<li><code>b(i)</code> = i
到所有<strong>其他簇</strong>中点的平均距离的最小值（最近邻簇的不相似度）。</li>
<li>样本i的轮廓系数：<code>s(i) = (b(i) - a(i)) / max(a(i), b(i))</code>。值在[-1,
1]之间。</li>
</ul></li>
<li><strong>整体评估：</strong>
计算所有样本轮廓系数的平均值，作为该K值下聚类的整体轮廓系数。</li>
<li><strong>选择K：</strong>
尝试不同K值，选择<strong>平均轮廓系数最大</strong>对应的K值。轮廓系数越接近1，表示聚类效果越好（簇内紧凑，簇间分离）。</li>
<li><strong>优点：</strong> 量化评估，结果在[-1, 1]之间有界。</li>
<li><strong>缺点：</strong> 计算量较大，尤其对于大数据集。</li>
</ul></li>
</ol>
<h4 id="参考资料-2">参考资料</h4>
<p><a target="_blank" rel="noopener" href="https://cloud.tencent.com.cn/developer/article/1802143">《机器学习》–
第九章 聚类-腾讯云开发者社区-腾讯云</a></p>
<h3 id="降维与度量学习">降维与度量学习</h3>
<h4 id="knn">KNN</h4>
<p>k近邻算法简称<strong>kNN（k-Nearest
Neighbor）</strong>，是一种经典的监督学习方法，是数据挖掘十大算法之一。其工作机制十分简单：给定某个测试样本，kNN基于某种<strong>距离度量</strong>在训练集中找出与其距离最近的k个带有真实标记的训练样本，然后基于这k个邻居的真实标记来进行预测，类似于集成学习中的基学习器结合策略：分类任务采用投票法，回归任务则采用平均法。</p>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250607150256290.png" alt="image-20250607150256290">
<figcaption aria-hidden="true">image-20250607150256290</figcaption>
</figure>
<p><strong>核心思想</strong></p>
<p>1NN 分类器通过将测试样本 $ $ 分配到其最近邻样本 $ $
的类别来完成预测。其错误概率取决于两个关键因素： - <strong>$ $
的真实类别</strong>：$ P(c | ) $，即给定 $ $ 属于类别 $ c $
的概率。<br>
- <strong>$ $ 的类别</strong>：$ P(c | ) $，即 $ $ 属于类别 $ c $
的概率。</p>
<p><strong>错误概率公式</strong></p>
<p>若测试样本 $ $ 的最近邻为 $ $，则 1NN 分类器出错的概率为： <span class="math display"><em>P</em>(err) = 1 − <em>P</em>(correct) = 1 − ∑<sub><em>c</em> ∈ 𝒞</sub><em>P</em>(<em>c</em>|<strong>x</strong>)<em>P</em>(<em>c</em>|<strong>z</strong>)</span>
其中： - $ $ 是所有可能的类别集合。<br>
- $ P(c | ) <span class="math inline">：</span> $ 属于类别 $ c $
的条件概率。<br>
- $ P(c | ) <span class="math inline">：</span> $ 属于类别 $ c $
的条件概率。</p>
<p>通过证明可以发现一个令人震惊的结论：<strong>最近邻分类器的错误率不超过贝叶斯最优分类器错误率的两倍</strong>。</p>
<p>对于距离度量，<strong>不同的度量方法得到的k个近邻不尽相同，从而对最终的投票结果产生了影响</strong>，因此选择一个合适的距离度量方法也十分重要。</p>
<p>在上一篇聚类算法中，在度量样本相似性时介绍了常用的几种距离计算方法，包括<strong>闵可夫斯基距离，曼哈顿距离，VDM</strong>等。在实际应用中，<strong>kNN的距离度量函数一般根据样本的特性来选择合适的距离度量，同时应对数据进行去量纲/归一化处理来消除大量纲属性的强权政治影响</strong>。</p>
<h4 id="低维嵌入">低维嵌入</h4>
<p><strong>使用knn的前提是样本空间的密度要一定大，但是这个条件在现实中很难满足，因此引出降维操作</strong></p>
<blockquote>
<p>kNN的重要假设: 任意测试样本 附近任意小的
距离范围内总能找到一个训练样本，即训练样本的采样密度足够大，或称为
<strong>“密采样”( dense sample)</strong>
。然而，这个假设在现实任务中通常很难满足</p>
</blockquote>
<p>样本的<strong>特征数</strong>也称为<strong>维数</strong>（dimensionality），当维数非常大时，也就是通常所说的“<strong>维数灾难</strong>”(curse
of
dimensionality)，具体表现在：在高维情形下，<strong>数据样本变得十分稀疏</strong>，因为此时要满足训练样本为“<strong>密采样</strong>”的总体样本数目是一个触不可及的天文数字。<strong>训练样本的稀疏使得其代表总体分布的能力大大减弱，从而消减了学习器的泛化能力</strong>；同时当维数很高时，<strong>计算距离也变得十分复杂</strong>，甚至连计算内积都不再容易</p>
<p>缓解维数灾难的一个重要途径就是<strong>降维（dimension
reduction），即通过某种数学变换将原始高维空间转变到一个低维的子空间</strong>。在这个子空间中，样本的密度将大幅提高，同时距离计算也变得容易。这</p>
<p>时也许会有疑问，降维之后不是会丢失原始数据的一部分信息吗？</p>
<p>实际上，在很多实际问题中，虽然训练数据是高维的，但是与学习任务相关也许仅仅是其中的一个低维子空间，也称为一个<strong>低维嵌入</strong>，例如：数据属性中存在噪声属性、相似属性或冗余属性等，<strong>对高维数据进行降维能在一定程度上达到提炼低维优质属性或降噪的效果</strong>。</p>
<h4 id="mds算法"><strong>MDS算法</strong></h4>
<p>MDS（Multidimensional
Scaling，多维尺度分析）是一种经典的<strong>降维技术</strong>，其核心目标是将高维数据映射到低维空间（如二维或三维），同时<strong>尽可能保留原始数据中样本点之间的距离关系</strong>。以下是其核心原理与应用要点：</p>
<p><strong>1. 核心思想</strong></p>
<ul>
<li><strong>输入</strong>：一个样本点之间的距离矩阵 $ D
$（如欧氏距离、余弦距离等）。<br>
</li>
<li><strong>输出</strong>：低维空间中样本点的坐标矩阵 $ Z
$，使得低维空间中的距离与原始距离尽可能一致 。<br>
</li>
<li><strong>关键假设</strong>：高维数据的内在结构可通过样本间的距离关系描述，降维后需最小化这种关系的失真。</li>
</ul>
<p><strong>2. 算法步骤</strong></p>
<p>MDS 的核心是通过<strong>矩阵分解</strong>从距离矩阵推导低维坐标： 1.
<strong>构建距离矩阵 $ D $</strong>：<br>
对于 $ r $ 个样本，计算两两之间的距离，形成 $ r r $ 的矩阵 $ D $，其中 $
D_{ij} $ 表示样本 $ i $ 和 $ j $ 的距离 。</p>
<ol start="2" type="1">
<li><p><strong>双中心化（Double Centering）</strong>：<br>
构造矩阵 $ B = - H D^{(2)} H $，其中 $ D^{(2)} $ 是距离的平方矩阵，$ H =
I - ^$ 是中心化矩阵 。</p></li>
<li><p><strong>特征值分解</strong>：<br>
对 $ B $ 进行特征值分解，得到 $ B = V V^$，其中 $ $
是按降序排列的特征值对角矩阵，$ V $ 是对应的特征向量矩阵 。</p></li>
<li><p><strong>构造低维坐标</strong>：<br>
选择前 $ d’ $ 个最大特征值（$ d’ $
为目标维度）和对应的特征向量，计算低维坐标矩阵：<br>
<span class="math display"><em>Z</em> = <em>Λ</em><sup>1/2</sup><em>V</em><sup>⊤</sup></span>
其中 $ ^{1/2} $ 是特征值矩阵的平方根 。</p></li>
</ol>
<p><strong>3. 关键特性</strong></p>
<ul>
<li><strong>保留距离关系</strong>：MDS
直接优化低维空间中的距离与原始距离的一致性，适用于需精确保留样本相似性的场景（如生物信息学中的基因关系分析）。<br>
</li>
<li><strong>非线性适应性</strong>：与 PCA 不同，MDS
不要求数据线性分布，更适合处理非线性结构（如环形、流形数据）。<br>
</li>
<li><strong>灵活性</strong>：支持任意距离度量（如自定义的相似性指标），而
PCA 仅适用于欧氏距离 。</li>
</ul>
<h4 id="线性降维方法"><strong>线性降维方法</strong></h4>
<p>线性降维通过<strong>线性变换</strong>将高维数据 $ ^{d m} $
投影到低维空间 $ ^{d’ m} <span class="math inline">（</span> d’ d
$），保留数据的主要信息。其数学表达为： <span class="math display"><strong>Z</strong> = <strong>W</strong><sup>⊤</sup><strong>X</strong></span></p>
<ul>
<li><p><strong>变换矩阵 $ ^{d d’} $</strong>：<br>
每一列是正交的基向量，构成低维子空间的坐标系。<br>
</p></li>
<li><p><strong>目标</strong>：选择 $ $ 使得低维表示 $ $
最大化保留原始数据的信息（如方差、距离等）。</p></li>
<li><p><strong>MDS</strong>：<br>
直接以<strong>保留高维空间中样本点之间的距离关系</strong>为目标。降维后的低维空间需尽可能保持原始样本两两之间的距离（如欧氏距离、自定义相似性距离）。</p>
<ul>
<li><strong>示例</strong>：在基因数据分析中，MDS可确保基因表达相似的样本在低维空间中仍紧密分布。</li>
</ul></li>
<li><p><strong>其他线性方法（如PCA、LDA）</strong>：</p>
<ul>
<li><strong>PCA</strong>：最大化数据在低维空间的方差，强调保留全局结构而非具体距离。<br>
</li>
<li><strong>LDA</strong>：在监督学习中最大化类间分离度，忽略类内距离。</li>
</ul></li>
</ul>
<h4 id="主成分分析">主成分分析</h4>
<p>不同于MDS采用距离保持的方法，主成分分析（Principal Component Analysis
,PCA）是一种经典的<strong>无监督降维算法</strong>
，其核心目标是通过线性变换将高维数据映射到低维空间，同时保留数据的<strong>最大方差信息</strong>
（即信息损失最小）</p>
<p>直接通过一个<strong>线性变换</strong>，将原始空间中的样本<strong>投影</strong>到新的低维空间中。</p>
<p>简单来理解这一过程便是：<strong>PCA采用一组新的基（向量）来表示样本点，其中每一个基向量都是原始空间基向量的线性组合，通过使用尽可能少的新基向量来表出样本，从而达到降维的目的。</strong></p>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250607155733314.png" alt="image-20250607155733314">
<figcaption aria-hidden="true">image-20250607155733314</figcaption>
</figure>
<p>假设使用d’个新基向量来表示原来样本，实质上是将样本投影到一个由d’个基向量确定的一个<strong>超平面</strong>上（<strong>即舍弃了一些维度</strong>），要用一个超平面对空间中所有高维样本进行恰当的表达，最理想的情形是：<strong>若这些样本点都能在超平面上表出且这些表出在超平面上都能够很好地分散开来</strong>。但是一般使用较原空间低一些维度的超平面来做到这两点十分不容易，因此我们退一步海阔天空，要求这个超平面应具有如下两个性质：</p>
<blockquote>
<p><strong>最近重构性</strong>：样本点到超平面的距离足够近，即尽可能在超平面附近；</p>
<p><strong>最大可分性</strong>：样本点在超平面上的投影尽可能地分散开来，即投影后的坐标具有区分性。</p>
</blockquote>
<p>这里十分神奇的是：<strong>最近重构性与最大可分性虽然从不同的出发点来定义优化问题中的目标函数，但最终这两种特性得到了完全相同的优化问题</strong>：</p>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250607165159235.png" alt="image-20250607165159235">
<figcaption aria-hidden="true">image-20250607165159235</figcaption>
</figure>
<h5 id="协方差矩阵与优化求解"><strong>协方差矩阵与优化求解</strong></h5>
<p>若数据已<strong>中心化</strong>（均值为零），则 $ ^$
是<strong>样本协方差矩阵</strong>的 $ m $
倍。此时，PCA的优化问题转化为： <span class="math display">$$
\begin{aligned}
&amp; \underset{\mathbf{W}}{\text{maximize}}
&amp; &amp; \text{tr}\left( \mathbf{W}^\top \mathbf{X} \mathbf{X}^\top
\mathbf{W} \right) \\
&amp; \text{subject to}
&amp; &amp; \mathbf{W}^\top \mathbf{W} = \mathbf{I}
\end{aligned}
$$</span> 通过拉格朗日乘数法，该问题的解为 $ ^$ 的前 $ d’ $
个最大特征值对应的特征向量</p>
<h5 id="pca的数学推导"><strong>PCA的数学推导</strong></h5>
<ul>
<li><p><strong>优化目标</strong>：<br>
<span class="math display">max<sub><strong>W</strong></sub>  tr(<strong>W</strong><sup>⊤</sup><strong>X</strong><strong>X</strong><sup>⊤</sup><strong>W</strong>)  s.t.  <strong>W</strong><sup>⊤</sup><strong>W</strong> = <strong>I</strong></span>
其中，$ ^{d m} $ 是中心化后的数据矩阵（均值为零）。</p></li>
<li><p><strong>拉格朗日乘数法</strong>：<br>
引入拉格朗日乘子 $ $，构造拉格朗日函数： <span class="math display">ℒ(<strong>W</strong>,<em>Λ</em>) = tr(<strong>W</strong><sup>⊤</sup><strong>X</strong><strong>X</strong><sup>⊤</sup><strong>W</strong>) − tr(<em>Λ</em>(<strong>W</strong><sup>⊤</sup><strong>W</strong>−<strong>I</strong>))</span>
对 $ $ 求导并令导数为零，得到： <span class="math display"><strong>X</strong><strong>X</strong><sup>⊤</sup><strong>W</strong> = <em>Λ</em><strong>W</strong></span>
即 $ ^$ 的特征向量 $ _i $ 满足： <span class="math display"><strong>X</strong><strong>X</strong><sup>⊤</sup><strong>w</strong><sub><em>i</em></sub> = <em>λ</em><sub><em>i</em></sub><strong>w</strong><sub><em>i</em></sub></span></p></li>
</ul>
<h5 id="pca特征向量选择">PCA特征向量选择</h5>
<p><strong>1. 核心问题</strong></p>
<p>在PCA中，我们希望找到一个 $ d’ d $ 的变换矩阵 $
$，其列向量是协方差矩阵 $ ^$ 的特征向量，且满足正交约束 $ ^ =
$。关键问题是：<strong>如何从 $ d $ 个特征向量中选择 $ d’ $
个最优的？</strong></p>
<p><strong>2. 数学推导</strong></p>
<ol type="1">
<li><p><strong>特征值分解</strong>：<br>
协方差矩阵 $ <sup></sup>{d d} $ 可分解为： <span class="math display"><strong>X</strong><strong>X</strong><sup>⊤</sup><strong>W</strong> = <strong>W</strong><strong>Λ</strong></span>
其中，$ = (_1, _2, , _d) $ 是特征值对角矩阵，$ $
是特征向量矩阵。</p></li>
<li><p><strong>优化目标转化</strong>：<br>
PCA的目标是最大化 $ (^ ^) $。利用特征值分解，可得： <span class="math display"><strong>W</strong><sup>⊤</sup><strong>X</strong><strong>X</strong><sup>⊤</sup><strong>W</strong> = <strong>W</strong><sup>⊤</sup>(<strong>W</strong><strong>Λ</strong>) = <strong>Λ</strong></span>
因此，优化目标变为： <span class="math display">$$
\max_{\mathbf{W}} \quad \text{tr}(\boldsymbol{\Lambda}) =
\sum_{i=1}^{d'} \lambda_i
$$</span> 即选择 $ d’ $ 个最大的特征值 $ _i $ 对应的特征向量组成 $
$。</p></li>
</ol>
<p><strong>3. 特征向量选择策略</strong></p>
<ul>
<li><strong>按特征值排序</strong>：<br>
特征值 $ _i $ 表示数据沿特征向量 $ _i $ 方向的方差。选择前 $ d’ $
个最大特征值对应的特征向量，可保留最多信息。<br>
</li>
<li><strong>正交性保证</strong>：<br>
特征向量矩阵 $ $ 的列自动满足 $ ^ = $，无需额外正交化。</li>
</ul>
<h5 id="pca算法的整个流程如下图所示">PCA算法的整个流程如下图所示：</h5>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250607170020467.png" alt="image-20250607170020467">
<figcaption aria-hidden="true">image-20250607170020467</figcaption>
</figure>
<h4 id="核化线性降维"><strong>核化线性降维</strong></h4>
<p>待学习</p>
<h4 id="流形学习">流形学习</h4>
<p><strong>流形学习（manifold
learning）</strong>是一种借助拓扑流形概念的降维方法，流形是指在<strong>局部与欧式空间同胚的空间</strong>，即在局部与欧式空间具有相同的性质，能用欧氏距离计算样本之间的距离。这样即使高维空间的分布十分复杂，但是在局部上依然满足欧式空间的性质，基于流形学习的降维正是这种
<strong>“邻域保持”</strong> 的思想。其中
<strong>等度量映射（Isomap）试图在降维前后保持邻域内样本之间的距离，而局部线性嵌入（LLE）则是保持邻域内样本之间的线性关系</strong>
。</p>
<h5 id="等度量映射isomap">等度量映射Isomap</h5>
<p>等度量映射的基本出发点是：高维空间中的直线距离具有误导性，因为有时高维空间中的直线距离在低维空间中是不可达的。<strong>因此利用流形在局部上与欧式空间同胚的性质，可以使用近邻距离来逼近测地线距离</strong>，即对于一个样本点，它与近邻内的样本点之间是可达的，且距离使用欧式距离计算，这样整个样本空间就形成了一张近邻图，高维空间中两个样本之间的距离就转为最短路径问题。可采用著名的<strong>Dijkstra算法</strong>或<strong>Floyd算法</strong>计算最短距离，得到高维空间中任意两点之间的距离后便可以使用
MDS 算法来其计算低维空间中的坐标。</p>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250607171119645.png" alt="image-20250607171119645">
<figcaption aria-hidden="true">image-20250607171119645</figcaption>
</figure>
<p>Isomap算法流程如下图：</p>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250607171258284.png" alt="image-20250607171258284">
<figcaption aria-hidden="true">image-20250607171258284</figcaption>
</figure>
<p>对于近邻图的构建，常用的有两种方法：<strong>一种是指定近邻点个数</strong>，像kNN一样选取k个最近的邻居；<strong>另一种是指定邻域半径</strong>，距离小于该阈值的被认为是它的近邻点。但两种方法均会出现下面的问题：</p>
<blockquote>
<p>若<strong>邻域范围指定过大，则会造成“短路问题”</strong>，即本身距离很远却成了近邻，将距离近的那些样本扼杀在摇篮。</p>
<p>若<strong>邻域范围指定过小，则会造成“断路问题”</strong>，即有些样本点无法可达了，整个世界村被划分为互不可达的小部落。</p>
</blockquote>
<h5 id="局部线性嵌入">局部线性嵌入</h5>
<p>待学习</p>
<h4 id="度量学习">度量学习</h4>
<p><strong>1. 核心思想</strong></p>
<p>度量学习（Metric
Learning）的核心目标是<strong>学习一个合理的距离度量</strong>，使得相似样本距离更近，不相似样本距离更远。传统欧式距离（Euclidean
Distance）虽然简单，但其固定权重无法反映不同特征的实际重要性。因此，我们引入<strong>加权欧式距离</strong>，通过可调节的参数（权重）优化距离计算。</p>
<p><strong>2. 欧式距离与加权欧式距离</strong></p>
<ul>
<li><p><strong>标准欧式距离</strong>：<br>
<span class="math display">$$
\text{dist}_{\text{ed}}^2(\boldsymbol{x}_i, \boldsymbol{x}_j) =
\|\boldsymbol{x}_i - \boldsymbol{x}_j\|_2^2 = \sum_{k=1}^d
(\boldsymbol{x}_{i,k} - \boldsymbol{x}_{j,k})^2
$$</span>
每个特征维度对距离的贡献相同，未考虑特征的重要性差异。</p></li>
<li><p><strong>加权欧式距离</strong>：<br>
<span class="math display">dist<sub>wed</sub><sup>2</sup>(<strong>x</strong><sub><em>i</em></sub>,<strong>x</strong><sub><em>j</em></sub>) = (<strong>x</strong><sub><em>i</em></sub>−<strong>x</strong><sub><em>j</em></sub>)<sup>⊤</sup><strong>W</strong>(<strong>x</strong><sub><em>i</em></sub>−<strong>x</strong><sub><em>j</em></sub>)</span>
其中，$ = () $ 是对角权重矩阵，$ w_k $ 表示第 $ k $ 个特征的权重。<br>
展开后为： <span class="math display">$$
\text{dist}_{\text{wed}}^2(\boldsymbol{x}_i, \boldsymbol{x}_j) =
\sum_{k=1}^d w_k (\boldsymbol{x}_{i,k} - \boldsymbol{x}_{j,k})^2
$$</span></p></li>
</ul>
<p><strong>3. 权重的作用</strong></p>
<ul>
<li><strong>特征重要性调节</strong>：
<ul>
<li>高权重 $ w_k $：强调第 $ k $
维特征对距离的影响（如图像的颜色通道比位置更重要）。<br>
</li>
<li>低权重 $ w_k $：弱化噪声或冗余特征的影响。<br>
</li>
</ul></li>
<li><strong>几何意义</strong>：<br>
加权欧式距离相当于在各特征维度上进行缩放，将数据映射到一个新的空间，使得关键特征的差异更显著。</li>
</ul>
<p><strong>4. 度量学习的目标</strong></p>
<p>通过学习最优权重 $ <span class="math inline">，<em>使</em><em>以</em><em>下</em><em>目</em><em>标</em><em>成</em><em>立</em>： −  *  * <em>相</em><em>似</em><em>样</em><em>本</em> *  * ：<em>加</em><em>权</em><em>距</em><em>离</em><em>小</em>（</span>
_{}^2(_i, <em>j) <span class="math inline">）。 −  *  * <em>不</em><em>相</em><em>似</em><em>样</em><em>本</em> *  * ：<em>加</em><em>权</em><em>距</em><em>离</em><em>大</em>（</span>
</em>{}^2(_i, _j) $）。</p>
<p>典型优化问题形式： <span class="math display">min<sub><strong>w</strong></sub>  ∑<sub>(<strong>x</strong><sub><em>i</em></sub>,<strong>x</strong><sub><em>j</em></sub>) ∈ <em>S</em></sub>dist<sub>wed</sub><sup>2</sup>(<strong>x</strong><sub><em>i</em></sub>,<strong>x</strong><sub><em>j</em></sub>) + <em>λ</em>∥<strong>w</strong>∥<sub>2</sub><sup>2</sup></span>
其中，$ S $ 是相似样本对集合，$ $ 是正则化项防止过拟合。</p>
<blockquote>
<p>总结来说，</p>
<ul>
<li><strong>降维是将原高维空间嵌入到一个合适的低维子空间中，接着在低维空间中进行学习任务</strong></li>
<li><strong>度量学习则是试图去学习出一个 *距离度量*
来等效降维的效果</strong></li>
</ul>
</blockquote>
<h5 id="lmnnlarge-margin-nearest-neighbors详解"><strong>LMNN（Large
Margin Nearest Neighbors）详解</strong></h5>
<p><strong>1. 核心思想</strong></p>
<p>LMNN
是一种<strong>监督度量学习方法</strong>，其目标是通过学习一个线性变换矩阵
$
$，使<strong>同类样本在变换后的空间中更紧密</strong>，<strong>不同类样本被推开</strong>，从而提升KNN等基于距离的算法性能。其核心是引入<strong>最大边距（Large
Margin）</strong>的概念，类似于SVM的分类边界。</p>
<p><strong>2. 损失函数</strong></p>
<p>LMNN 的优化目标由两部分组成： - <strong>Pull
Loss（拉力损失）</strong>：<br>
使同类样本对的距离尽可能小，公式为： <span class="math display">$$
  \varepsilon_{\text{pull}}(\mathbf{L}) = \sum_{j \sim i}
\|\mathbf{L}(\bar{\boldsymbol{x}}_i - \bar{\boldsymbol{x}}_j)\|^2
  $$</span> 其中，$ j i $ 表示与样本 $ i $ 同类的最近邻样本。</p>
<ul>
<li><p><strong>Push Loss（推力损失）</strong>：<br>
使不同类样本对的距离至少保持一个固定边距 $ <em>{ijl} $，公式为： <span class="math display">$$
\varepsilon_{\text{push}}(\mathbf{L}) = \sum_{i,j,l} (1 - y_{il})
\left[1 + \|\mathbf{L}(\bar{\boldsymbol{x}}_i -
\bar{\boldsymbol{x}}_j)\|^2 - \|\mathbf{L}(\bar{\boldsymbol{x}}_i -
\bar{\boldsymbol{x}}_l)\|^2\right]_+
$$</span> 其中，$ y</em>{il} = 1 $ 表示样本 $ i $ 和 $ l $
属于同一类，否则为0；$ []_+ $ 表示取正值部分。</p></li>
<li><p><strong>总损失函数</strong>：<br>
<span class="math display"><em>ε</em>(<strong>L</strong>) = (1−<em>μ</em>)<em>ε</em><sub>pull</sub>(<strong>L</strong>) + <em>μ</em><em>ε</em><sub>push</sub>(<strong>L</strong>)</span>
参数 $ $ 控制两类损失的权重。</p></li>
</ul>
<p><strong>3. 优化问题</strong></p>
<p>LMNN 的目标是最小化总损失函数，同时满足以下约束： <span class="math display">$$
\begin{aligned}
&amp; \min_{\mathbf{M}, \boldsymbol{\xi}} \quad (1 - \mu) \sum_{i,j \sim
i} (\bar{\boldsymbol{x}}_i - \bar{\boldsymbol{x}}_j)^\top \mathbf{M}
(\bar{\boldsymbol{x}}_i - \bar{\boldsymbol{x}}_j) + \mu \sum_{i,j \sim
i,l} (1 - y_{il}) \xi_{ijl} \\
&amp; \text{s.t.} \quad (\bar{\boldsymbol{x}}_i -
\bar{\boldsymbol{x}}_l)^\top \mathbf{M} (\bar{\boldsymbol{x}}_i -
\bar{\boldsymbol{x}}_l) - (\bar{\boldsymbol{x}}_i -
\bar{\boldsymbol{x}}_j)^\top \mathbf{M} (\bar{\boldsymbol{x}}_i -
\bar{\boldsymbol{x}}_j) \geq 1 - \xi_{ijl}, \\
&amp; \quad \quad \quad \xi_{ijl} \geq 0, \quad \mathbf{M} \succeq 0.
\end{aligned}
$$</span> -
<strong>约束（1）</strong>：确保不同类样本对的距离比同类样本对大至少 $ 1
- <em>{ijl} $。<br>
- <strong>约束（2）</strong>：松弛变量 $ </em>{ijl} $
允许部分样本对违反约束。<br>
- <strong>约束（3）</strong>：$ $
必须是半正定矩阵，保证距离的非负性和三角不等式。</p>
<h4 id="作业-3">作业</h4>
<h5 id="section-8">1</h5>
<p>数据降维有哪些常用的方法？阐述主成分分析（PCA）算法的计算流程，并讨论PCA
降维之后的维度如何确定？</p>
<p><strong>（1）常用数据降维方法</strong></p>
<ol type="1">
<li><strong>主成分分析（PCA）</strong>：通过线性变换保留最大方差方向，适用于去噪和压缩数据
。<br>
</li>
<li><strong>线性判别分析（LDA）</strong>：在监督学习中最大化类间分离度，适用于分类任务
。</li>
</ol>
<p><strong>（2）主成分分析（PCA）的计算流程</strong></p>
<ol type="1">
<li><strong>数据标准化</strong>：对原始数据去均值、方差归一化，消除量纲影响
。<br>
</li>
<li><strong>计算协方差矩阵</strong>：<br>
<span class="math display">$$
\mathbf{\Sigma} = \frac{1}{m} \mathbf{X} \mathbf{X}^\top
$$</span> 其中 $ $ 是中心化后的数据矩阵 。<br>
</li>
<li><strong>特征值分解</strong>：对协方差矩阵进行特征值分解，得到特征值
$ _i $ 和单位正交特征向量 $ _i $ 。<br>
</li>
<li><strong>选择主成分</strong>：按特征值大小排序，选择前 $ d’ $
个最大特征值对应的特征向量构成变换矩阵 $ = [_1, <em>2, , </em>{d’}]
$。<br>
</li>
<li><strong>降维投影</strong>：计算低维表示 $ = ^ $，其中 $ ^{d’ m} $
。</li>
</ol>
<p><strong>（3）PCA降维后维度的确定</strong></p>
<ul>
<li><strong>累积方差贡献率</strong>：选择前 $ d’ $
个主成分，使累计方差占比达到阈值（如95%）。<br>
</li>
<li><strong>肘部法则（Elbow
Method）</strong>：绘制特征值随维度变化的曲线，选择“拐点”作为 $ d’
$。</li>
</ul>
<h5 id="section-9">2</h5>
<p>度量学习的目标是什么？LMNN算法中三元组损失是什么？如何计算？</p>
<p><strong>（1）度量学习的目标</strong></p>
<p>度量学习旨在学习一个合理的距离度量，使得： -
<strong>相似样本</strong>：距离尽可能小（如同类样本）。<br>
- <strong>不相似样本</strong>：距离尽可能大（如异类样本）。<br>
典型应用包括推荐系统（优化用户-商品相似性）、图像检索（提升匹配精度）和生物识别（增强类间可分性）。</p>
<p><strong>（2）LMNN中的三元组损失</strong></p>
<p>LMNN（Large Margin Nearest
Neighbor）是一种监督度量学习方法，其核心思想是通过优化距离度量来提升KNN的分类性能。虽然LMNN本身主要使用对比损失（Contrastive
Loss），但三元组损失（Triplet
Loss）是深度度量学习中常见的损失函数，其计算方式如下：<strong>三元组损失的定义</strong></p>
<p>三元组损失基于锚点（Anchor）、正例（Positive）和负例（Negative）三个样本，目标是使锚点与正例的距离小于锚点与负例的距离，公式为：
<span class="math display">ℒ = ∑<sub><em>i</em>, <em>j</em>, <em>l</em></sub>max (0,∥<strong>z</strong><sub><em>i</em></sub>−<strong>z</strong><sub><em>j</em></sub>∥<sup>2</sup>−∥<strong>z</strong><sub><em>i</em></sub>−<strong>z</strong><sub><em>l</em></sub>∥<sup>2</sup>+<em>m</em>)</span>
- $ _i $：锚点样本的嵌入表示。<br>
- $ _j $：与锚点同类的正例样本。<br>
- $ _l $：与锚点不同类的负例样本。<br>
- $ m $：预设的边界值（Margin），控制正负样本距离的最小差距 。</p>
<p><strong>LMNN的损失函数</strong></p>
<p>LMNN 的损失函数包含两部分： 1. <strong>拉力损失（Pull
Loss）</strong>：最小化同类样本对的距离：<br>
<span class="math display">$$
   \varepsilon_{\text{pull}} = \sum_{i,j \sim i}
\|\mathbf{L}(\bar{\boldsymbol{x}}_i - \bar{\boldsymbol{x}}_j)\|^2
   $$</span> 2. <strong>推力损失（Push
Loss）</strong>：最大化异类样本对的距离：<br>
<span class="math display">$$
   \varepsilon_{\text{push}} = \sum_{i,j \sim i,l} (1 - y_{il}) \left[1
+ \|\mathbf{L}(\bar{\boldsymbol{x}}_i - \bar{\boldsymbol{x}}_j)\|^2 -
\|\mathbf{L}(\bar{\boldsymbol{x}}_i -
\bar{\boldsymbol{x}}_l)\|^2\right]_+
   $$</span> 其中 $ $ 是线性变换矩阵，$ y_{il} $ 表示样本对是否同类，$
[]_+ $ 表示取正值部分 。</p>
<p><strong>优化目标</strong></p>
<p>LMNN 的总损失为拉力和推力损失的加权和： <span class="math display"><em>ε</em>(<strong>L</strong>) = (1−<em>μ</em>)<em>ε</em><sub>pull</sub> + <em>μ</em><em>ε</em><sub>push</sub></span>
参数 $ $ 平衡两类损失的权重，最终通过优化 $ $ 得到最优距离度量 。</p>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250607175825520.png" alt="image-20250607175825520">
<figcaption aria-hidden="true">image-20250607175825520</figcaption>
</figure>
<h3 id="半监督学习">半监督学习</h3>
<p>监督学习解决现实问题有哪些难点?
1.标记数据获取成本高：在许多领域如医疗，获取标记数据是昂贵且耗时的。
2.未标记数据大量存在且易得：相对而言，未标记数据大量存在且容易获取。
3.提升模型的泛化能力：通过利用未标记数据，可以增强模型的泛化能力。
举例：在医疗领域，获取医生标记的诊断数据非常昂贵，但有大量未标记的病人记录。
半监督学习可以帮助利用这些未标记数据，提高疾病预测模型的准确性。</p>
<p><img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250607181345721.png" alt="image-20250607181345721">半监督学习结合了有监督学习和无监督学习，半监督学习使用<strong>少量的标记数据</strong>和<strong>大量的未标记数据</strong>来训练模型，主要目标是提升模型在未标记数据上的表现。</p>
<h5 id="基于生成模型的方法">基于生成模型的方法</h5>
<p>假设所有数据（无论是否有标记）都是由一个<strong>潜在的模型</strong>“生成”的。那么无标记的数据可以帮助更准确的估计潜在模型的参数。
比如右图中可以看到数据可以由两个高斯分布近似，则无监督的数据可以被用来更好得做高斯分布的参数估计</p>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250607183201926.png" alt="image-20250607183201926">
<figcaption aria-hidden="true">image-20250607183201926</figcaption>
</figure>
<h5 id="半监督svm"><strong>半监督SVM</strong></h5>
<p>监督学习中的SVM试图找到一个划分超平面，使得两侧支持向量之间的间隔最大，即
<strong>最大划分间隔</strong> 思想。对于半监督SVM (Semi-Supervised
Support Vector Machine, S3VM)
则考虑超平面在能将两类标记样本分隔的同时，<strong>穿过数据低密度的区域</strong>。</p>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250607183349866.png" alt="image-20250607183349866">
<figcaption aria-hidden="true">image-20250607183349866</figcaption>
</figure>
<h6 id="tsvmtransductive-support-vector-machine">TSVM(Transductive
Support Vector Machine)</h6>
<p><strong>1. 核心思想</strong></p>
<p>TSVM 是一种<strong>半监督学习方法</strong>，通过结合有标记数据 $ D_l
$ 和未标记数据 $ D_u
$，利用伪标签（Pseudo-labels）和迭代优化策略，最大化分类超平面的间隔。其损失函数需同时考虑：
- <strong>有标记样本</strong>：最小化分类错误（Hinge Loss）。<br>
- <strong>未标记样本</strong>：通过伪标签引入约束，逐步调整超平面。</p>
<p><strong>2. 损失函数推导</strong></p>
<p>TSVM 的目标是找到一个超平面 $ ^ + b = 0 $，使得： 1.
<strong>有标记样本</strong>的分类误差最小。<br>
2. <strong>未标记样本</strong>的伪标签与超平面预测结果一致。</p>
<p><strong>标准SVM的损失函数</strong>为： <span class="math display">$$
\min_{\boldsymbol{w}, b, \xi} \quad \frac{1}{2} \|\boldsymbol{w}\|^2 + C
\sum_{i=1}^l \xi_i
$$</span> 其中，$ _i $ 是松弛变量，表示样本 $ (_i, y_i) $
的分类误差。</p>
<p><strong>TSVM的扩展</strong>：<br>
引入未标记样本 $ D_u $ 的伪标签 $ _j <span class="math inline">（</span>
j = l+1, , l+u $），并赋予其较小的惩罚系数 $ C_u $（初始阶段 $ C_u C_l
$）： <span class="math display">$$
\min_{\boldsymbol{w}, b, \xi} \quad \frac{1}{2} \|\boldsymbol{w}\|^2 +
C_l \sum_{i=1}^l \xi_i + C_u \sum_{j=l+1}^{l+u} \xi_j
$$</span> 其中： - $ C_l $：有标记样本的惩罚系数。<br>
- $ C_u
$：未标记样本的惩罚系数，初始值很小，逐步增大以增强伪标签的影响。</p>
<p><strong>3. 迭代优化流程</strong></p>
<ol type="1">
<li><strong>初始化</strong>：
<ul>
<li>用有标记数据 $ D_l $ 训练初始 SVM，得到 $ _0, b_0 $。<br>
</li>
<li>对未标记数据 $ D_u $ 预测伪标签 $ _j = (_0^_j + b_0) $。</li>
</ul></li>
<li><strong>伪标签调整</strong>：
<ul>
<li>若存在冲突（如 $ _i _j &lt; 0 $ 且 $ _i + _j &gt; 2
$），翻转其中一个伪标签（如 $ _i -_i $）。<br>
</li>
<li>重新求解优化问题，更新 $ , b $。</li>
</ul></li>
<li><strong>参数调整</strong>：
<ul>
<li>逐步增大 $ C_u $（如 $ C_u {2C_u, C_l}
$），增强未标记样本的影响。</li>
</ul></li>
</ol>
<p><strong>4. 关键数学细节</strong></p>
<ul>
<li><p><strong>Hinge Loss</strong>：<br>
对每个样本 $ (_i, y_i) $，损失为： <span class="math display"><em>ξ</em><sub><em>i</em></sub> = max (0,1−<em>y</em><sub><em>i</em></sub>(<strong>w</strong><sup>⊤</sup><strong>x</strong><sub><em>i</em></sub>+<em>b</em>))</span>
未标记样本的伪标签 $ _j $ 同样代入此公式，但惩罚系数为 $ C_u
$。</p></li>
<li><p><strong>正则化项</strong>：<br>
$ ||^2 $ 确保超平面的泛化能力，防止过拟合。</p></li>
<li><p><strong>伪标签翻转条件</strong>：<br>
当两个未标记样本 $ i, j $ 满足： <span class="math display"><em>ŷ</em><sub><em>i</em></sub><em>ŷ</em><sub><em>j</em></sub> &lt; 0  且  <em>ξ</em><sub><em>i</em></sub> &gt; 0, <em>ξ</em><sub><em>j</em></sub> &gt; 0,  <em>ξ</em><sub><em>i</em></sub> + <em>ξ</em><sub><em>j</em></sub> &gt; 2</span>
表示它们被错误分类且距离超平面较近，需翻转其中一个标签以减少冲突。</p></li>
</ul>
<h5 id="图半监督学习"><strong>图半监督学习</strong></h5>
<p>给定一个数据集，我们可将其映射为一个图，数据集中每个样本对应于图结点，若两个样本之间的相似度很高(或相关性很强)，则对应的结点之间存在一条边，边的“强度”(strength)
正比于样本之间的相似度(或相关性)。</p>
<p>可将有标记样本所对应的结点想象为染过色，标记样本所对应的结点尚未染色。半监督学习就对应于“颜色”在图上扩散或传播的过程。由于个图对应了一个矩阵，我们就能基于矩阵运算来进行半监督学习算法的推导与分析。</p>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250607184534217.png" alt="image-20250607184534217">
<figcaption aria-hidden="true">image-20250607184534217</figcaption>
</figure>
<p><strong>图半监督学习中的能量函数推导详解</strong></p>
<p><strong>1. 图结构与亲和矩阵</strong></p>
<p>给定有标记数据集 $ D_l = {(<em>1, y_1), (<em>2, y_2), , (<em>l, y_l)}
$ 和未标记数据集 $ D_u = {</em>{l+1}, </em>{l+2}, , </em>{l+u}}
$，构建图 $ G = (V, E) <span class="math inline">： −  *  * <em>结</em><em>点</em><em>集</em> *  * ：</span>
V = {<em>1, , <em>l, </em>{l+1}, , </em>{l+u}} $，包含所有样本。<br>
- <strong>边集</strong>：通过亲和矩阵 $ $ 表示，元素定义为： <span class="math display">$$
  (\mathbf{W})_{ij} =
  \begin{cases}
  \exp\left(-\frac{\|\boldsymbol{x}_i -
\boldsymbol{x}_j\|^2}{2\sigma^2}\right), &amp; i \neq j \\
  0, &amp; \text{otherwise}
  \end{cases}
  $$</span> 其中，$ $ 是高斯核的带宽参数，控制邻接关系的敏感性。</p>
<p><strong>2. 能量函数的定义与推导</strong></p>
<p>假设分类模型的输出标记为 $ f(_i) $（取值为类别标签，如 $
$），定义能量函数 $ E(f) $ 为： <span class="math display">$$
E(f) = \frac{1}{2} \sum_{i=1}^m \sum_{j=1}^m (\mathbf{W})_{ij}
(f(\boldsymbol{x}_i) - f(\boldsymbol{x}_j))^2
$$</span> 其中 $ m = l + u $ 是总样本数。</p>
<p><strong>3. 能量函数的展开与简化</strong></p>
<ol type="1">
<li><strong>展开平方项</strong> <span class="math display">$$
E(f) = \frac{1}{2} \sum_{i=1}^m \sum_{j=1}^m (\mathbf{W})_{ij} \left[
f^2(\boldsymbol{x}_i) - 2 f(\boldsymbol{x}_i) f(\boldsymbol{x}_j) +
f^2(\boldsymbol{x}_j) \right]
$$</span></li>
<li><strong>利用对称性简化</strong> 由于 $ $ 是对称矩阵（<span class="math inline">(<strong>W</strong>)<sub><em>i</em><em>j</em></sub> = (<strong>W</strong>)<sub><em>j</em><em>i</em></sub></span>），可交换求和顺序：
<span class="math display">$$
\sum_{i=1}^m \sum_{j=1}^m (\mathbf{W})_{ij} f^2(\boldsymbol{x}_j) =
\sum_{j=1}^m \sum_{i=1}^m (\mathbf{W})_{ji} f^2(\boldsymbol{x}_j) =
\sum_{i=1}^m \sum_{j=1}^m (\mathbf{W})_{ij} f^2(\boldsymbol{x}_i)
$$</span> 因此，能量函数变为 <span class="math display">$$
E(f) = \frac{1}{2} \left( 2 \sum_{i=1}^m \sum_{j=1}^m (\mathbf{W})_{ij}
f^2(\boldsymbol{x}_i) - 2 \sum_{i=1}^m \sum_{j=1}^m (\mathbf{W})_{ij}
f(\boldsymbol{x}_i) f(\boldsymbol{x}_j) \right)
$$</span></li>
<li><strong>引入度矩阵</strong> 定义度矩阵 $ $
为对角矩阵，其对角线元素为： <span class="math display">$$
d_i = \sum_{j=1}^m (\mathbf{W})_{ij}
$$</span> 最终能量函数可表示为： <span class="math display">$$
E(f) = \sum_{i=1}^m d_i f^2(\boldsymbol{x}_i) - \sum_{i=1}^m
\sum_{j=1}^m (\mathbf{W})_{ij} f(\boldsymbol{x}_i) f(\boldsymbol{x}_j) =
\boldsymbol{f}^\top (\mathbf{D} - \mathbf{W}) \boldsymbol{f}
$$</span> 其中，$ = [f(_1), f(_2), , f(_m)]^$。</li>
</ol>
<p><strong>图半监督学习方法推导详解</strong></p>
<p><strong>1. 分块矩阵表示</strong></p>
<p>将亲和矩阵 $ $ 和度矩阵 $ $ 按有标记数据（前 $ l $
行列）和未标记数据（后 $ u $ 行列）分块： <span class="math display">$$
\mathbf{W} =
\begin{bmatrix}
\mathbf{W}_{ll} &amp; \mathbf{W}_{lu} \\
\mathbf{W}_{ul} &amp; \mathbf{W}_{uu}
\end{bmatrix}, \quad
\mathbf{D} =
\begin{bmatrix}
\mathbf{D}_{ll} &amp; \mathbf{0}_{lu} \\
\mathbf{0}_{ul} &amp; \mathbf{D}_{uu}
\end{bmatrix}
$$</span> 其中： - $ <em>{ll} $：有标记数据间的亲和度。<br>
- $ </em>{lu} $：有标记与未标记数据间的亲和度。<br>
- $ <em>{uu} $：未标记数据间的亲和度。<br>
- $ </em>{ll}, _{uu} $：对应子图的度矩阵。</p>
<p><strong>2. 能量函数的分块展开</strong></p>
<p>能量函数 $ E(f) = ^( - ) $ 可展开为</p>
<p>展开后得到： <span class="math display"><em>E</em>(<em>f</em>) = <strong>f</strong><sub><em>l</em></sub><sup>⊤</sup>(<strong>D</strong><sub><em>l</em><em>l</em></sub>−<strong>W</strong><sub><em>l</em><em>l</em></sub>)<strong>f</strong><sub><em>l</em></sub> − 2<strong>f</strong><sub><em>u</em></sub><sup>⊤</sup><strong>W</strong><sub><em>u</em><em>l</em></sub><strong>f</strong><sub><em>l</em></sub> + <strong>f</strong><sub><em>u</em></sub><sup>⊤</sup>(<strong>D</strong><sub><em>u</em><em>u</em></sub>−<strong>W</strong><sub><em>u</em><em>u</em></sub>)<strong>f</strong><sub><em>u</em></sub></span></p>
<p>**3. 对未标记数据 $ _u $ 求偏微分**</p>
<p>目标是最小化 $ E(f) $，对 $ _u $ 求偏导并令其为零： <span class="math display">$$
\frac{\partial E(f)}{\partial \boldsymbol{f}_u} = -2 \mathbf{W}_{ul}
\boldsymbol{f}_l + 2 (\mathbf{D}_{uu} - \mathbf{W}_{uu})
\boldsymbol{f}_u = 0
$$</span> 解得： <span class="math display"><strong>f</strong><sub><em>u</em></sub> = (<strong>D</strong><sub><em>u</em><em>u</em></sub>−<strong>W</strong><sub><em>u</em><em>u</em></sub>)<sup>−1</sup><strong>W</strong><sub><em>u</em><em>l</em></sub><strong>f</strong><sub><em>l</em></sub></span></p>
<h4 id="协同训练">协同训练</h4>
<p>协同训练（Co-training）是一种经典的<strong>半监督学习方法</strong>，由Blum和Mitchell于1998年首次提出，主要用于处理<strong>多视图数据</strong>（Multi-view
Data）。其核心思想是通过多个分类器的协作，利用少量标记数据和大量未标记数据提升模型性能。以下是详细解析：</p>
<p><strong>1. 核心思想与假设</strong></p>
<p><strong>（1）多视图数据</strong></p>
<ul>
<li><strong>定义</strong>：每个样本可被划分为多个<strong>充分冗余且条件独立</strong>的视图（View）。
<ul>
<li><strong>充分冗余</strong>：每个视图本身包含足够信息，可独立完成学习任务。<br>
</li>
<li><strong>条件独立性</strong>：在给定类别标签的条件下，不同视图的特征相互独立。<br>
例如，网页数据可划分为“文本内容”和“超链接结构”两个视图，它们共同描述网页内容。</li>
</ul></li>
</ul>
<p><strong>（2）协作机制</strong></p>
<ul>
<li><strong>双分类器设计</strong>：使用两个分类器 $ h_1 $ 和 $ h_2
$，分别基于视图 $ V_1 $ 和 $ V_2 $ 进行训练。<br>
</li>
<li><strong>伪标签生成</strong>：分类器 $ h_1 $
对未标记数据的高置信度预测结果会被 $ h_2 $
使用，反之亦然，形成迭代优化。<br>
</li>
<li><strong>目标</strong>：通过分类器间的互补性，逐步扩展标记数据集，提升模型泛化能力。</li>
</ul>
<p><strong>2. 算法流程</strong></p>
<ol type="1">
<li><strong>初始化阶段</strong>：
<ul>
<li>使用少量标记数据 $ D_l $，分别训练分类器 $ h_1 $（基于视图 $ V_1
$）和 $ h_2 $（基于视图 $ V_2 $）。<br>
</li>
</ul></li>
<li><strong>伪标签生成</strong>：
<ul>
<li>对未标记数据 $ D_u <span class="math inline">，</span> h_1 $
预测视图 $ V_1 $ 的伪标签，$ h_2 $ 预测视图 $ V_2 $ 的伪标签。<br>
</li>
<li>选择置信度高于阈值的样本加入训练集（如 $ h_1 $ 的预测结果用于更新 $
h_2 $ 的训练数据，反之亦然）。<br>
</li>
</ul></li>
<li><strong>迭代优化</strong>：
<ul>
<li>重复伪标签生成和模型训练，直到未标记数据耗尽或模型收敛。</li>
</ul></li>
</ol>
<p><strong>3. 核心优势</strong></p>
<ul>
<li><strong>减少对标注数据的依赖</strong>：仅需少量标记数据即可训练高性能模型，尤其适合标注成本高的场景（如医疗影像分析）。<br>
</li>
<li><strong>提升模型鲁棒性</strong>：分类器间的协作可纠正彼此的错误，降低单一模型过拟合风险。<br>
</li>
<li><strong>多视图互补性</strong>：不同视图的信息融合能捕捉更全面的特征（如图像的RGB通道与纹理特征）。</li>
</ul>
<h4 id="作业-4">作业</h4>
<h5 id="section-10">1</h5>
<p>什么是半监督学习？请简要描述其基本思想。半监督学习相比于监督学习和无监督学习有什么优势和应用场景？</p>
<p><strong>（1）定义与基本思想</strong></p>
<p>半监督学习（Semi-Supervised
Learning）是结合<strong>监督学习</strong>（利用标记数据）和<strong>无监督学习</strong>（利用未标记数据）的机器学习方法，其核心思想是通过少量标记数据与大量未标记数据的联合训练，提升模型的泛化能力和鲁棒性。<br>
-
<strong>监督学习</strong>：依赖大量人工标注数据（如分类、回归）。<br>
-
<strong>无监督学习</strong>：仅利用数据分布规律（如聚类、降维）。<br>
-
<strong>半监督学习</strong>：在标记数据稀缺时，通过未标记数据挖掘潜在结构，降低标注成本
。</p>
<p><strong>（2）优势</strong></p>
<ul>
<li><strong>减少标注依赖</strong>：仅需少量标记数据即可训练高性能模型，适用于标注成本高的场景（如医疗影像分析）。<br>
</li>
<li><strong>提升模型性能</strong>：利用未标记数据增强数据多样性，缓解过拟合风险。<br>
</li>
<li><strong>平衡效率与精度</strong>：在资源有限时，兼顾监督学习的准确性与无监督学习的高效性
。</li>
</ul>
<p><strong>（3）应用场景</strong></p>
<ul>
<li><strong>医学诊断</strong>：利用少量标注的病理图像和大量未标注数据训练疾病预测模型。<br>
</li>
<li><strong>推荐系统</strong>：结合用户行为（有标记）与商品属性（未标记）优化排序模型。<br>
</li>
<li><strong>自然语言处理</strong>：通过预训练模型（如GPT）的“预训练+微调”框架，减少人工标注需求
。</li>
</ul>
<h5 id="section-11">2</h5>
<p>协同训练算法的作用是什么？请简述算法主要流程和所需条件。</p>
<p><strong>（1）作用与核心思想</strong></p>
<p>协同训练是一种典型的半监督学习方法，适用于<strong>多视图数据</strong>（Multi-view
Data）。其核心思想是通过多个分类器的协作，利用未标记数据扩展训练集，最终提升模型性能。</p>
<ul>
<li><strong>多视图条件</strong>：
<ul>
<li><strong>充分冗余</strong>：每个视图本身包含足够信息，可独立完成任务。<br>
</li>
<li><strong>条件独立性</strong>：在给定类别标签的条件下，不同视图的特征相互独立
。</li>
</ul></li>
</ul>
<p><strong>（2）算法流程</strong></p>
<ol type="1">
<li><strong>初始化阶段</strong>：
<ul>
<li>使用少量标记数据 $ D_l $，分别训练两个分类器 $ h_1 $（基于视图 $ V_1
$）和 $ h_2 $（基于视图 $ V_2 $）。<br>
</li>
</ul></li>
<li><strong>伪标签生成</strong>：
<ul>
<li>对未标记数据 $ D_u <span class="math inline">，</span> h_1 $ 预测 $
V_2 $ 的伪标签，$ h_2 $ 预测 $ V_1 $ 的伪标签。<br>
</li>
<li>选择置信度高于阈值的样本加入训练集（如 $ h_1 $ 的预测结果用于更新 $
h_2 $ 的训练数据，反之亦然）。<br>
</li>
</ul></li>
<li><strong>迭代优化</strong>：
<ul>
<li>重复伪标签生成和模型训练，直到未标记数据耗尽或模型收敛 。</li>
</ul></li>
</ol>
<p><strong>（3）所需条件</strong></p>
<ul>
<li><strong>多视图划分</strong>：数据需满足“充分冗余”和“条件独立性”（如图像的RGB通道与纹理特征）。<br>
</li>
<li><strong>分类器多样性</strong>：选择差异较大的分类器（如SVM +
决策树），增强互补性。<br>
</li>
<li><strong>伪标签可靠性</strong>：初始模型需有一定性能，避免错误伪标签污染训练集
。</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="上一页" aria-label="上一页" href="/page/2/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/12/">12</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/4/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">张熙浚</span>
  </div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="本站访问数 fa fa-user 次"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="本站总访问量 fa fa-eye 次"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script size="400" alpha="0.6" zIndex="-1" src="https://cdnjs.cloudflare.com/ajax/libs/ribbon.js/1.0.2/ribbon.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.1.0/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script src="/js/pjax.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  <script src="/js/third-party/pace.js"></script>


  
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
