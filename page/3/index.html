<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha256-XOqroi11tY4EFQMR9ZYwZWKj5ZXiftSx36RRuC3anlA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-bounce.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.20.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="zxj Blogs">
<meta property="og:type" content="website">
<meta property="og:title" content="Zhang XiJun">
<meta property="og:url" content="http://example.com/page/3/index.html">
<meta property="og:site_name" content="Zhang XiJun">
<meta property="og:description" content="zxj Blogs">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="张熙浚">
<meta property="article:tag" content="zxj">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/page/3/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"page/3/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Zhang XiJun</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Zhang XiJun</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">BLOGS</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="张熙浚"
      src="/images/zxjavatar.gif">
  <p class="site-author-name" itemprop="name">张熙浚</p>
  <div class="site-description" itemprop="description">zxj Blogs</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">123</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">45</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">53</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/zxj-2023" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zxj-2023" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="http://wpa.qq.com/msgrd?v=3&uin=2902065320&site=qq&menu=yes" title="QQ → http:&#x2F;&#x2F;wpa.qq.com&#x2F;msgrd?v&#x3D;3&amp;uin&#x3D;2902065320&amp;site&#x3D;qq&amp;menu&#x3D;yes" rel="noopener me" target="_blank"><i class="fab fa-qq fa-fw"></i>QQ</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
    <div class="sidebar-inner sidebar-blogroll">
      <div class="links-of-blogroll animated">
        <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
          链接
        </div>
        <ul class="links-of-blogroll-list">
            <li class="links-of-blogroll-item">
              <a href="https://zxj-2023.github.io/" title="https:&#x2F;&#x2F;zxj-2023.github.io" rel="noopener" target="_blank">Zhang XiJun</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://theme-next.js.org/" title="https:&#x2F;&#x2F;theme-next.js.org" rel="noopener" target="_blank">NexT</a>
            </li>
        </ul>
      </div>
    </div>
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/07/18/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/libreoffice/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/07/18/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/libreoffice/" class="post-title-link" itemprop="url">Libreoffice</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-07-18 00:00:00" itemprop="dateCreated datePublished" datetime="2025-07-18T00:00:00+08:00">2025-07-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-07-30 15:26:37" itemprop="dateModified" datetime="2025-07-30T15:26:37+08:00">2025-07-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AE%9E%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">实习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/" itemprop="url" rel="index"><span itemprop="name">晨晟智控</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="libreoffice部署"><a href="#libreoffice部署" class="headerlink" title="libreoffice部署"></a>libreoffice部署</h3><h4 id="查看Linux发行版"><a href="#查看Linux发行版" class="headerlink" title="查看Linux发行版"></a>查看Linux发行版</h4><p><img src="/2025/07/18/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/libreoffice/image-20250718095931232.png" alt="image-20250718095931232"></p>
<p>系统是 <strong>银河麒麟高级服务器操作系统 V10（Kylin Linux Advanced Server V10）</strong>，属于 <strong>中国国产、兼容 CentOS/RHEL 生态</strong> 的 Linux 发行版。</p>
<p>因此它使用 <strong>RPM 包管理</strong>（<code>dnf</code>/<code>yum</code>），而不是 <code>.deb</code>。</p>
<h4 id="查看CPU-处理器架构"><a href="#查看CPU-处理器架构" class="headerlink" title="查看CPU 处理器架构"></a>查看<strong>CPU 处理器架构</strong></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">uname -m</span><br></pre></td></tr></table></figure>
<p>是<strong>x86_64</strong></p>
<h4 id="不用部署了，镜像里有，直接用了"><a href="#不用部署了，镜像里有，直接用了" class="headerlink" title="不用部署了，镜像里有，直接用了"></a>不用部署了，镜像里有，直接用了</h4><h4 id="使用libreoffice处理doc文件，转成pdf"><a href="#使用libreoffice处理doc文件，转成pdf" class="headerlink" title="使用libreoffice处理doc文件，转成pdf"></a>使用libreoffice处理doc文件，转成pdf</h4><p>将当前目录下所有 <code>.doc</code> 和 <code>.docx</code> 转为 PDF：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">libreoffice --headless --convert-to pdf *.doc *.docx --outdir ./pdf_output/</span><br><span class="line"></span><br><span class="line"># 检查是否有残留进程</span><br><span class="line">ps aux | grep libreoffice</span><br><span class="line"></span><br><span class="line"># 如果有残留进程，杀死它们</span><br><span class="line">killall soffice.bin 2&gt;/dev/null</span><br></pre></td></tr></table></figure>
<h4 id="python"><a href="#python" class="headerlink" title="python"></a>python</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import subprocess</span><br><span class="line">import argparse</span><br><span class="line">import glob</span><br><span class="line">from pathlib import Path</span><br><span class="line">def batch_convert_documents(input_path, output_dir):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    批量转换文档的函数版本</span><br><span class="line">    </span><br><span class="line">    Args:</span><br><span class="line">        input_path (str): 输入路径（文件、目录或通配符）</span><br><span class="line">        output_dir (str): 输出目录</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">        bool: 转换是否成功</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    # 确保输出目录存在</span><br><span class="line">    Path(output_dir).mkdir(parents=True, exist_ok=True)</span><br><span class="line">    </span><br><span class="line">    # 收集所有要转换的文件</span><br><span class="line">    files_to_convert = []</span><br><span class="line">    </span><br><span class="line">    if os.path.isfile(input_path):</span><br><span class="line">        # 单个文件</span><br><span class="line">        if input_path.lower().endswith((&#x27;.doc&#x27;, &#x27;.docx&#x27;)):</span><br><span class="line">            files_to_convert.append(input_path)</span><br><span class="line">    elif os.path.isdir(input_path):</span><br><span class="line">        # 目录中的所有doc/docx文件</span><br><span class="line">        for pattern in [&#x27;*.doc&#x27;, &#x27;*.docx&#x27;]:</span><br><span class="line">            files_to_convert.extend(glob.glob(os.path.join(input_path, pattern)))</span><br><span class="line">    else:</span><br><span class="line">        # 通配符模式</span><br><span class="line">        files_to_convert = glob.glob(input_path)</span><br><span class="line">        # 过滤出doc和docx文件</span><br><span class="line">        files_to_convert = [f for f in files_to_convert if f.lower().endswith((&#x27;.doc&#x27;, &#x27;.docx&#x27;))]</span><br><span class="line">    </span><br><span class="line">    if not files_to_convert:</span><br><span class="line">        print(&quot;未找到要转换的文档文件&quot;)</span><br><span class="line">        return False</span><br><span class="line">    </span><br><span class="line">    print(f&quot;找到 &#123;len(files_to_convert)&#125; 个文件需要转换&quot;)</span><br><span class="line">    </span><br><span class="line">    # 构建命令</span><br><span class="line">    cmd = [</span><br><span class="line">        &#x27;libreoffice&#x27;,</span><br><span class="line">        &#x27;--headless&#x27;,</span><br><span class="line">        &#x27;--convert-to&#x27;, &#x27;pdf&#x27;,</span><br><span class="line">        &#x27;--outdir&#x27;, output_dir</span><br><span class="line">    ] + files_to_convert</span><br><span class="line">    </span><br><span class="line">    try:</span><br><span class="line">        print(&quot;正在转换文件...&quot;)</span><br><span class="line">        result = subprocess.run(cmd, capture_output=True, text=True, timeout=600)</span><br><span class="line">        </span><br><span class="line">        if result.returncode == 0:</span><br><span class="line">            print(f&quot;成功转换 &#123;len(files_to_convert)&#125; 个文件&quot;)</span><br><span class="line">            return True</span><br><span class="line">        else:</span><br><span class="line">            print(f&quot;转换失败: &#123;result.stderr&#125;&quot;)</span><br><span class="line">            return False</span><br><span class="line">            </span><br><span class="line">    except Exception as e:</span><br><span class="line">        print(f&quot;转换过程中发生错误: &#123;e&#125;&quot;)</span><br><span class="line">        return False</span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    success = batch_convert_documents(&quot;./docs&quot;, &quot;./pdf_output&quot;)</span><br></pre></td></tr></table></figure>
<h3 id="Linux扫盲"><a href="#Linux扫盲" class="headerlink" title="Linux扫盲"></a>Linux扫盲</h3><h4 id="发行版"><a href="#发行版" class="headerlink" title="发行版"></a>发行版</h4><p>像Ubuntu，CentOS都属于Linux的发行版，就像Windows11属于Windows的关系</p>
<p>常见发行版分类：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>系列</th>
<th>代表发行版</th>
<th>包格式</th>
<th>特点</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Debian 系</strong></td>
<td>Debian、Ubuntu、Kali、Linux Mint</td>
<td><code>.deb</code></td>
<td>包多、社区大、教程多</td>
</tr>
<tr>
<td><strong>Red Hat 系</strong></td>
<td>CentOS、RHEL、Rocky、Alma、Fedora</td>
<td><code>.rpm</code></td>
<td>企业级稳定、官方支持长</td>
</tr>
<tr>
<td><strong>SUSE 系</strong></td>
<td>openSUSE Leap / Tumbleweed</td>
<td><code>.rpm</code></td>
<td>YaST 管理工具、欧洲流行</td>
</tr>
<tr>
<td><strong>Arch 系</strong></td>
<td>Arch Linux、Manjaro</td>
<td><code>.pkg.tar.zst</code></td>
<td>滚动更新、极客向</td>
</tr>
<tr>
<td><strong>轻量/最小</strong></td>
<td>Alpine、Debian netinst、CentOS Stream Minimal</td>
<td>任意</td>
<td>镜像小、资源占用低</td>
</tr>
</tbody>
</table>
</div>
<p>如何查看Linux发行版</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /etc/os-release</span><br></pre></td></tr></table></figure>
<h4 id="deb和rpm"><a href="#deb和rpm" class="headerlink" title="deb和rpm"></a>deb和rpm</h4><p> <code>.deb</code> 和 <code>.rpm</code> 想象成 <strong>“Linux 世界里的安装程序”</strong>，就像 Windows 的 <code>.exe</code> / <code>.msi</code></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>格式</th>
<th>适用系统</th>
<th>安装命令</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong><code>.deb</code></strong></td>
<td>Debian、Ubuntu、Linux Mint、Kali 等</td>
<td><code>sudo dpkg -i xxx.deb</code> 或 <code>sudo apt install ./xxx.deb</code></td>
</tr>
<tr>
<td><strong><code>.rpm</code></strong></td>
<td>CentOS、RHEL、Fedora、openSUSE、Rocky、Alma 等</td>
<td><code>sudo rpm -ivh xxx.rpm</code> 或 <code>sudo dnf/yum install xxx.rpm</code></td>
</tr>
</tbody>
</table>
</div>
<h4 id="cpu处理器架构"><a href="#cpu处理器架构" class="headerlink" title="cpu处理器架构"></a>cpu处理器架构</h4><div class="table-container">
<table>
<thead>
<tr>
<th>目录名</th>
<th>代表架构</th>
<th>适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>x86_64</strong></td>
<td><strong>Intel/AMD 64 位</strong></td>
<td>绝大多数台式机、服务器（如 Xeon、EPYC、Core、Ryzen）</td>
</tr>
<tr>
<td><strong>aarch64</strong></td>
<td><strong>ARM 64 位</strong></td>
<td>树莓派 4/5、苹果 M 系列（Asahi Linux）、鲲鹏、飞腾、Ampere ARM 服务器等</td>
</tr>
</tbody>
</table>
</div>
<p>查看处理器架构：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">uname -m</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/07/18/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/%E5%AE%9E%E6%88%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/07/18/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/%E5%AE%9E%E6%88%98/" class="post-title-link" itemprop="url">实战</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-07-18 00:00:00" itemprop="dateCreated datePublished" datetime="2025-07-18T00:00:00+08:00">2025-07-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-07-30 09:15:18" itemprop="dateModified" datetime="2025-07-30T09:15:18+08:00">2025-07-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AE%9E%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">实习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/" itemprop="url" rel="index"><span itemprop="name">晨晟智控</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="日志"><a href="#日志" class="headerlink" title="日志"></a>日志</h3><p>部署日志见实习日志那篇文章</p>
<h3 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h3><p>查看mineru提取的markdown文档后，发现mineru暂时无法提取多级标题，所以可以舍弃根据markdown文档结构的分块策略</p>
<p>UnstructuredMarkdownLoader会丢失表格格式，不能使用</p>
<h3 id="实战"><a href="#实战" class="headerlink" title="实战"></a>实战</h3><p>将 <code>Markdown</code> 文档加载到 LangChain <a target="_blank" rel="noopener" href="https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html#langchain_core.documents.base.Document">文档</a> 对象中，以便我们可以在后续使用</p>
<h4 id="使用UnstructuredMarkdownLoader-对象"><a href="#使用UnstructuredMarkdownLoader-对象" class="headerlink" title="使用UnstructuredMarkdownLoader 对象"></a>使用<a target="_blank" rel="noopener" href="https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.markdown.UnstructuredMarkdownLoader.html">UnstructuredMarkdownLoader</a> 对象</h4><p>UnstructuredMarkdownLoader 是 LangChain 中的一个 文档加载器（Document Loader） ，专门用于加载和解析 Markdown ( .md ) 文件。它的特别之处在于，它底层依赖于一个强大的开源库 unstructured 。</p>
<p><strong>UnstructuredMarkdownLoader 的核心功能</strong></p>
<p>传统的文本加载器可能会将整个 Markdown 文件作为一个大字符串读入。而 UnstructuredMarkdownLoader 凭借 unstructured 库的能力，可以 智能地识别 Markdown 文件内部的结构 。</p>
<p>它能够将文档分解成多个有意义的<strong>“元素” (Elements)</strong>，例如：</p>
<ul>
<li>标题 (Titles)</li>
<li>叙述性文本 (Narrative Text / Paragraphs)</li>
<li>列表项 (List Items)</li>
<li>代码块 (Code Blocks)<br>这种智能分区对于后续的 RAG (Retrieval-Augmented Generation) 应用非常重要，因为它能让您以更小的、逻辑上连贯的块来处理文本，从而提高检索的准确性。</li>
</ul>
<p><strong>重要的 mode 参数</strong></p>
<p>UnstructuredMarkdownLoader 的构造函数中有一个非常重要的 mode 参数，它决定了文档的加载方式：</p>
<ol>
<li><p>mode=”single” (默认值)</p>
<ul>
<li>将整个 Markdown 文件的所有内容加载成 一个单独的 Document 对象 。</li>
<li>page_content 包含文件的全部文本。</li>
</ul>
</li>
<li><p>mode=”elements”</p>
<ul>
<li>这是它最强大的模式。它会将文件解析成多个 Document 对象， 每个对象对应一个识别出的元素 （如一个标题、一个段落）。</li>
<li>这对于创建精细的文本块以进行向量嵌入和检索非常有用。</li>
</ul>
</li>
</ol>
<h4 id="使用libreoffice处理doc文件，转成pdf"><a href="#使用libreoffice处理doc文件，转成pdf" class="headerlink" title="使用libreoffice处理doc文件，转成pdf"></a>使用libreoffice处理doc文件，转成pdf</h4><p>将当前目录下所有 <code>.doc</code> 和 <code>.docx</code> 转为 PDF：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">libreoffice --headless --convert-to pdf *.doc *.docx --outdir ./pdf_output/</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/07/15/%E5%AD%A6%E4%B9%A0/ai%E6%A1%86%E6%9E%B6/langchain%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/07/15/%E5%AD%A6%E4%B9%A0/ai%E6%A1%86%E6%9E%B6/langchain%E5%AD%A6%E4%B9%A0/" class="post-title-link" itemprop="url">LangChain学习</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-07-15 00:00:00" itemprop="dateCreated datePublished" datetime="2025-07-15T00:00:00+08:00">2025-07-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-08-02 15:52:08" itemprop="dateModified" datetime="2025-08-02T15:52:08+08:00">2025-08-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ai%E6%A1%86%E6%9E%B6/" itemprop="url" rel="index"><span itemprop="name">ai框架</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ai%E6%A1%86%E6%9E%B6/langchain/" itemprop="url" rel="index"><span itemprop="name">langchain</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="实战demo"><a href="#实战demo" class="headerlink" title="实战demo"></a>实战demo</h3><h4 id="agent实战"><a href="#agent实战" class="headerlink" title="agent实战"></a>agent实战</h4><p>langchain的agent与langgraph的agent主要差异点在create_openai_functions_agent, AgentExecutor这两个函数</p>
<p>前者的作用类似<strong>构建 Runnable 链</strong>，返回一个<code>RunnablePassthrough.assign(...)|prompt|llm_with_tools|ToolsAgentOutputParser()</code>，但其invoke仅能完成单步的调用，而<code>AgentExecutor</code> 会自动完成3 步循环（调用工具→拼回结果→再次调用 LLM），直到任务结束。</p>
<blockquote>
<p>以下为ai的解释</p>
<p><strong>直接使用 <code>agent</code> (Runnable) 的局限性:</strong></p>
<ol>
<li><strong>单步执行</strong>: 你直接调用 <code>agent.invoke()</code> 或 <code>agent.ainvoke()</code> 时，它通常只执行<strong>一步</strong>。对于像 <code>create_tool_calling_agent</code> 生成的 <code>agent</code> 来说，这一步就是：<ul>
<li>接收输入（包括历史消息和 <code>agent_scratchpad</code>）。</li>
<li>让 LLM 决定是给出最终答案 (<code>AgentFinish</code>) 还是调用工具 (<code>AgentAction</code>)。</li>
<li>返回这个决定。</li>
</ul>
</li>
<li><strong>工具调用需要手动处理</strong>: 如果 LLM 决定调用工具（返回 <code>AgentAction</code>），<strong>你</strong>需要负责：<ul>
<li>从返回的 <code>AgentAction</code> 中找出工具名称和输入参数。</li>
<li>在你的工具列表中找到对应的工具。</li>
<li>执行这个工具。</li>
<li>获取工具的输出（Observation）。</li>
<li><strong>再次手动调用 <code>agent.invoke(...)</code></strong>，把工具的输出（通常需要格式化成 <code>ToolMessage</code>）放回 <code>agent_scratchpad</code> 或 <code>intermediate_steps</code> 中。</li>
<li>重复这个过程，直到 <code>agent</code> 最终返回 <code>AgentFinish</code>。</li>
</ul>
</li>
</ol>
<p><strong>使用 <code>AgentExecutor</code> 的优势:</strong></p>
<p><code>AgentExecutor</code> 就是为了解决上述问题而设计的。它本质上是一个<strong>自动化的执行引擎</strong>，为你管理整个 Agent 的思考-行动-观察循环。</p>
<ol>
<li><strong>自动化循环</strong>: <code>AgentExecutor</code> 内部会自动运行那个循环：<ul>
<li>调用 <code>agent</code> (Runnable)。</li>
<li>检查返回的是 <code>AgentAction</code> 还是 <code>AgentFinish</code>。</li>
<li>如果是 <code>AgentAction</code>，它会自动根据你提供的 <code>tools</code> 列表找到并执行对应的工具。</li>
<li>它会自动将工具的输出（Observation）记录下来，并作为下一步的输入（放入 <code>agent_scratchpad</code>）再次调用 <code>agent</code>。</li>
<li>这个过程会一直重复。</li>
</ul>
</li>
</ol>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">from langchain.agents import create_openai_functions_agent, AgentExecutor</span><br><span class="line">from langchain.tools import tool</span><br><span class="line">from langchain_openai import ChatOpenAI</span><br><span class="line">from pydantic import BaseModel, Field</span><br><span class="line"></span><br><span class="line"># 1. 定义工具</span><br><span class="line">class WeatherInput(BaseModel):</span><br><span class="line">    location: str = Field(description=&quot;城市名称&quot;)</span><br><span class="line"></span><br><span class="line">@tool(&quot;get_weather&quot;, args_schema=WeatherInput)</span><br><span class="line">def get_weather(location: str) -&gt; str:</span><br><span class="line">    &quot;&quot;&quot;查询城市天气&quot;&quot;&quot;</span><br><span class="line">    return f&quot;&#123;location&#125; 今天是晴天，25°C&quot;</span><br><span class="line"></span><br><span class="line"># 2. 创建Agent</span><br><span class="line">llm = ChatOpenAI(model=&quot;gpt-4&quot;)</span><br><span class="line">tools = [get_weather]</span><br><span class="line">prompt = ChatPromptTemplate.from_messages([</span><br><span class="line">    (&quot;system&quot;, &quot;你是一个助手，可以调用工具&quot;),</span><br><span class="line">    (&quot;human&quot;, &quot;&#123;input&#125;&quot;)</span><br><span class="line">])</span><br><span class="line">agent = create_openai_functions_agent(llm, tools, prompt)</span><br><span class="line">agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)</span><br><span class="line"></span><br><span class="line"># 3. 执行</span><br><span class="line">result = agent_executor.invoke(&#123;&quot;input&quot;: &quot;北京天气如何？&quot;&#125;)</span><br><span class="line">print(result[&quot;output&quot;])</span><br></pre></td></tr></table></figure>
<h4 id="工具调用"><a href="#工具调用" class="headerlink" title="工具调用"></a>工具调用</h4><p>利用bind_tools绑定工具，当大模型需要调用工具的时候，会返回工具信息，tool_calls，如下</p>
<p>[{‘name’: ‘add_numbers’, ‘args’: {‘a’: 15, ‘b’: 27}, ‘id’: ‘4e7b261cce6d4e3da09134086c704c3c’, ‘type’: ‘tool_call’}]</p>
<blockquote>
<p><code>llm_with_tools.invoke(...)</code> 只是一个<strong>单步调用</strong>，LLM 返回的是<strong>“我想调用哪个工具、传什么参数”</strong>（即 <code>tool_calls</code>）。<br><strong>但 LLM 并不会自动执行工具</strong>，所以你必须：</p>
<ol>
<li><strong>手动执行工具</strong>（或让 AgentExecutor 帮你执行）。</li>
<li><strong>把执行结果拼回对话</strong>（作为 <code>ToolMessage</code>）。</li>
<li><strong>再次调用 LLM</strong>，让它基于工具返回的结果生成最终答案。</li>
</ol>
</blockquote>
<p>这里展示的是手动拼接，并传给大模型，如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 将工具的输出发送回LLM，让LLM基于结果生成最终回答</span><br><span class="line">            # 这是一个关键步骤，通常在Agent中自动处理。这里手动演示。</span><br><span class="line">            final_response = llm_with_tools.invoke([</span><br><span class="line">                HumanMessage(content=&quot;What is 15 + 27?&quot;),</span><br><span class="line">                AIMessage(content=&quot;&quot;, tool_calls=[tool_call]), # 告知LLM它之前建议的工具调用</span><br><span class="line">                ToolMessage(content=str(result), tool_call_id=tool_call[&#x27;id&#x27;]) # 告知LLM工具的执行结果</span><br><span class="line">            ])</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line">from langchain_core.tools import tool</span><br><span class="line">from typing import Literal</span><br><span class="line">from langchain_openai import ChatOpenAI</span><br><span class="line">from langchain_core.messages import HumanMessage, AIMessage, ToolMessage</span><br><span class="line"></span><br><span class="line"># 定义一个加法工具</span><br><span class="line">@tool</span><br><span class="line">def add_numbers(a: float, b: float) -&gt; float:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Adds two numbers together.</span><br><span class="line"></span><br><span class="line">    Args:</span><br><span class="line">        a: The first number.</span><br><span class="line">        b: The second number.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    return a + b</span><br><span class="line"></span><br><span class="line"># 我们可以定义更多的工具，例如一个乘法工具</span><br><span class="line">@tool</span><br><span class="line">def multiply_numbers(a: float, b: float) -&gt; float:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Multiplies two numbers together.</span><br><span class="line"></span><br><span class="line">    Args:</span><br><span class="line">        a: The first number.</span><br><span class="line">        b: The second number.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    return a * b</span><br><span class="line"></span><br><span class="line"># 将我们定义的工具放在一个列表中</span><br><span class="line">tools = [add_numbers, multiply_numbers]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 初始化LLM</span><br><span class="line">llm = ChatOpenAI(</span><br><span class="line">    temperature=0.5,</span><br><span class="line">    model_name=&quot;deepseek-v3-0324&quot;, # 聊天模型通常使用&quot;gpt-3.5-turbo&quot;或&quot;gpt-4&quot;</span><br><span class="line">    openai_api_base=&quot;https://api.qnaigc.com/v1&quot;, # 例如，您可以指定base_url</span><br><span class="line">    openai_api_key=&quot;sk-&quot; # 直接在此处设置API密钥，或者通过环境变量设置</span><br><span class="line">)</span><br><span class="line"># 将工具绑定到LLM</span><br><span class="line"># LLM现在知道了add_numbers和multiply_numbers这两个工具及其功能</span><br><span class="line">llm_with_tools = llm.bind_tools(tools)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 场景一：LLM直接回答，不需要工具</span><br><span class="line">print(&quot;--- 场景一：LLM直接回答 ---&quot;)</span><br><span class="line">response1 = llm_with_tools.invoke([HumanMessage(content=&quot;Hello, what&#x27;s your name?&quot;)])</span><br><span class="line">print(response1.content) # LLM直接生成文本回复</span><br><span class="line"></span><br><span class="line">print(&quot;\n--- 场景二：LLM决定调用工具 ---&quot;)</span><br><span class="line"># 场景二：LLM决定调用工具</span><br><span class="line"># 当LLM的响应中包含tool_calls时，意味着它想要调用一个或多个工具</span><br><span class="line">response2 = llm_with_tools.invoke([HumanMessage(content=&quot;What is 15 + 27?&quot;)])</span><br><span class="line">print(response2.tool_calls) # 打印LLM决定调用的工具信息</span><br><span class="line"></span><br><span class="line"># 检查并执行LLM建议的工具调用</span><br><span class="line">if response2.tool_calls:</span><br><span class="line">    for tool_call in response2.tool_calls:</span><br><span class="line">        if tool_call[&#x27;name&#x27;] == &quot;add_numbers&quot;:</span><br><span class="line">            # 提取LLM为工具调用生成的参数</span><br><span class="line">            args = tool_call[&#x27;args&#x27;]</span><br><span class="line">            result = add_numbers.invoke(args) # 执行工具</span><br><span class="line">            print(f&quot;Tool call: add_numbers(&#123;args[&#x27;a&#x27;]&#125;, &#123;args[&#x27;b&#x27;]&#125;) = &#123;result&#125;&quot;)</span><br><span class="line"></span><br><span class="line">            # 将工具的输出发送回LLM，让LLM基于结果生成最终回答</span><br><span class="line">            # 这是一个关键步骤，通常在Agent中自动处理。这里手动演示。</span><br><span class="line">            final_response = llm_with_tools.invoke([</span><br><span class="line">                HumanMessage(content=&quot;What is 15 + 27?&quot;),</span><br><span class="line">                AIMessage(content=&quot;&quot;, tool_calls=[tool_call]), # 告知LLM它之前建议的工具调用</span><br><span class="line">                ToolMessage(content=str(result), tool_call_id=tool_call[&#x27;id&#x27;]) # 告知LLM工具的执行结果</span><br><span class="line">            ])</span><br><span class="line">            print(&quot;Final LLM response based on tool output:&quot;)</span><br><span class="line">            print(final_response.content)</span><br></pre></td></tr></table></figure>
<h3 id="概念扫盲"><a href="#概念扫盲" class="headerlink" title="概念扫盲"></a>概念扫盲</h3><h4 id="Document-对象"><a href="#Document-对象" class="headerlink" title="Document 对象"></a>Document 对象</h4><p>Document 对象是 LangChain 用来封装和处理文本数据的基本单位。无论您是从 PDF、Markdown 文件、网站还是数据库加载数据，LangChain 都会将这些数据转换成一个或多个 Document 对象，以便在后续的流程中使用。</p>
<p>一个 Document 对象主要包含两个部分：</p>
<ol>
<li><p>page_content (字符串)</p>
<ul>
<li>这是文档对象的核心，存储了原始的文本内容。例如，如果加载一个 Markdown 文件， page_content 就会包含该文件的所有文本。</li>
</ul>
</li>
<li><p>metadata (字典)</p>
<ul>
<li>这是一个字典，用于存储关于文档的“元数据”或附加信息。这些信息对于过滤、追踪或增强文档处理流程非常有用。常见的元数据包括：<ul>
<li>source ：文档的来源，比如文件名、URL等。</li>
<li>page ：如果文档来自多页文件（如PDF），这里可以存储页码。</li>
<li>其他自定义信息：您可以添加任何有助于您应用的信息，如作者、创建日期等。</li>
</ul>
</li>
</ul>
</li>
</ol>
<p>除了通过文档加载器（Loaders）自动创建，您也可以手动创建一个 Document 对象。这在测试或处理简单文本时非常方便。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 创建一个简单的 Document 对象</span><br><span class="line">doc = Document(</span><br><span class="line">    page_content=&quot;这是文档的主要内容。LangChain 真酷！&quot;,</span><br><span class="line">    metadata=&#123;</span><br><span class="line">        &#x27;source&#x27;: &#x27;my_notebook.ipynb&#x27;,</span><br><span class="line">        &#x27;author&#x27;: &#x27;AI Assistant&#x27;,</span><br><span class="line">        &#x27;chapter&#x27;: 2</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h4 id="Runnable协议"><a href="#Runnable协议" class="headerlink" title="Runnable协议"></a>Runnable协议</h4><p><a target="_blank" rel="noopener" href="https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable">“Runnable”</a>协议</p>
<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV12TLAzuEni/?spm_id_from=333.337.search-card.all.click&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">2025最新版！langchain入门到精通实战教程！结合实战案例，干货拉满！99%的人不知道的暴利玩法，学完敢谷歌工程师叫板！_哔哩哔哩_bilibili</a></p>
<p><a target="_blank" rel="noopener" href="https://www.langchain.com.cn/docs/introduction/">introduction | LangChain中文网</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1XudVYzEcW?spm_id_from=333.788.videopod.episodes&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">跟着官网学langchain2025(version 0.3)_哔哩哔哩_bilibili</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.langchain.com/langgraph-platform">LangGraph Platform - Docs by LangChain</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/07/15/%E5%AD%A6%E4%B9%A0/ai%E6%A1%86%E6%9E%B6/langsmith/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/07/15/%E5%AD%A6%E4%B9%A0/ai%E6%A1%86%E6%9E%B6/langsmith/" class="post-title-link" itemprop="url">Langsmith</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2025-07-15 00:00:00 / 修改时间：17:55:00" itemprop="dateCreated datePublished" datetime="2025-07-15T00:00:00+08:00">2025-07-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ai%E6%A1%86%E6%9E%B6/" itemprop="url" rel="index"><span itemprop="name">ai框架</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ai%E6%A1%86%E6%9E%B6/langsmith/" itemprop="url" rel="index"><span itemprop="name">langsmith</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="配置langsmith">配置langsmith</h3>
<h4 id="安装langsmith-sdk">安装LangSmith SDK</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install langsmith</span><br></pre></td></tr></table></figure>
<h4 id="环境变量">环境变量</h4>
<p>获取api<a target="_blank" rel="noopener" href="https://smith.langchain.com/o/56031c2a-c402-41d4-83ed-ed15d0693548/settings/apikeys">LangSmith</a></p>
<p>设置相应的环境变量。这将把跟踪记录到<code>default</code>项目（尽管您可以轻松更改）。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export LANGSMITH_TRACING=true</span><br><span class="line">export LANGSMITH_API_KEY=</span><br><span class="line">export LANGSMITH_PROJECT=default</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">LANGSMITH_TRACING=true</span><br><span class="line">LANGSMITH_ENDPOINT=&quot;https://api.smith.langchain.com&quot;</span><br><span class="line">LANGSMITH_API_KEY=&quot;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&quot;</span><br><span class="line">LANGSMITH_PROJECT=&quot;langgraph&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="资源">资源</h3>
<p>官网<a target="_blank" rel="noopener" href="https://smith.langchain.com/o/56031c2a-c402-41d4-83ed-ed15d0693548/">《LangSmith》
— LangSmith</a></p>
<p>参考文档<a target="_blank" rel="noopener" href="https://langsmith.langchain.ac.cn/">LangSmith 入门 |
🦜️🛠️ LangSmith 文档</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.smith.langchain.com/">Get started with
LangSmith | 🦜️🛠️ LangSmith</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/07/15/%E5%AD%A6%E4%B9%A0/ai%E6%A1%86%E6%9E%B6/langgraph%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/07/15/%E5%AD%A6%E4%B9%A0/ai%E6%A1%86%E6%9E%B6/langgraph%E5%AD%A6%E4%B9%A0/" class="post-title-link" itemprop="url">LangGraph学习——快速入门</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-07-15 00:00:00" itemprop="dateCreated datePublished" datetime="2025-07-15T00:00:00+08:00">2025-07-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-08-05 17:31:03" itemprop="dateModified" datetime="2025-08-05T17:31:03+08:00">2025-08-05</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ai%E6%A1%86%E6%9E%B6/" itemprop="url" rel="index"><span itemprop="name">ai框架</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ai%E6%A1%86%E6%9E%B6/langgraph/" itemprop="url" rel="index"><span itemprop="name">langgraph</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="构建langgraph聊天机器人的基本流程"><a href="#构建langgraph聊天机器人的基本流程" class="headerlink" title="构建langgraph聊天机器人的基本流程"></a>构建langgraph聊天机器人的基本流程</h3><h4 id="创建一个-StateGraph"><a href="#创建一个-StateGraph" class="headerlink" title="创建一个 StateGraph"></a>创建一个 <code>StateGraph</code></h4><p>首先创建一个 <code>StateGraph</code>。一个 <code>StateGraph</code> 对象将我们的聊天机器人结构定义为“状态机”。我们将添加 <code>节点</code> 来表示 LLM 和聊天机器人可以调用的函数，并添加 <code>边</code> 来指定机器人应如何在这些函数之间进行转换。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> Annotated</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> typing_extensions <span class="keyword">import</span> TypedDict</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> langgraph.graph <span class="keyword">import</span> StateGraph, START</span><br><span class="line"><span class="keyword">from</span> langgraph.graph.message <span class="keyword">import</span> add_messages</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">State</span>(<span class="title class_ inherited__">TypedDict</span>):</span><br><span class="line">    messages: Annotated[<span class="built_in">list</span>, add_messages]</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义状态</span></span><br><span class="line">graph_builder = StateGraph(State)</span><br></pre></td></tr></table></figure>
<p>在 langgraph 中，状态会在图的各个节点之间传递。当一个节点产生新的消息时，它会更新 State 中的 messages 字段。</p>
<p>我们的图现在可以处理两个关键任务</p>
<ol>
<li>每个 <code>节点</code> 都可以接收当前 <code>状态</code> 作为输入，并输出状态的更新。</li>
<li>对 <code>消息</code> 的更新将追加到现有列表而不是覆盖它，这得益于与 <code>Annotated</code> 语法一起使用的预构建 <a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langgraph/reference/graphs/?h=add+messages#add_messages"><code>add_messages</code></a> 函数。</li>
</ol>
<blockquote>
<p>langgraph中每个消息对象通常包含以下关键属性：</p>
<ul>
<li>role : 一个字符串，标识消息的发送者（例如 ‘human’ , ‘ai’ , ‘system’ ）。</li>
<li>content : 消息的具体内容，通常是字符串，但也可以是更复杂的结构（例如，用于多模态输入）。</li>
<li>id : 一个可选的唯一标识符。</li>
</ul>
<p>Annotated 的作用 : 通过使用 Annotated[list, add_messages] ，你改变了这个默认行为。 add_messages 函数（由 langgraph 提供或由你自定义）的逻辑是 追加 而不是覆盖。所以，当一个新节点返回消息时， langgraph 会调用 add_messages 函数，将新消息 追加 到现有 messages 列表的末尾。</p>
</blockquote>
<h4 id="定义一个聊天模型"><a href="#定义一个聊天模型" class="headerlink" title="定义一个聊天模型"></a>定义一个聊天模型</h4><p>两种方法：</p>
<p>1.使用 <code>init_chat_model</code>(通用高层封装)<br>这是一个通用的辅助函数，旨在提供一个统一的接口来初始化来自 不同提供商 的聊天模型。</p>
<p><a target="_blank" rel="noopener" href="https://python.langchain.com/api_reference/langchain/chat_models/langchain.chat_models.base.init_chat_model.html">init_chat_model — 🦜🔗 LangChain 文档 —- init_chat_model — 🦜🔗 LangChain documentation</a></p>
<p>2.使用如ChatOpenAI (特定于提供商的类)<br>这是一个专门为 OpenAI API 设计的类，提供了对 OpenAI 模型所有功能的完全访问。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from langchain.chat_models import init_chat_model</span><br><span class="line"></span><br><span class="line">os.environ[&quot;OPENAI_API_KEY&quot;] = &quot;sk-&quot;</span><br><span class="line">#使用‘&#123;model_provider&#125;:&#123;model&#125;’格式在单个参数中指定模型和模型提供者，例如“openai:o1”</span><br><span class="line">llm = init_chat_model(&quot;openai:qwen-plus-2025-04-28&quot;,</span><br><span class="line">                      base_url=&quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;</span><br><span class="line">                      )</span><br></pre></td></tr></table></figure>
<h4 id="添加一个节点"><a href="#添加一个节点" class="headerlink" title="添加一个节点"></a>添加一个节点</h4><p>现在我们可以将聊天模型集成到一个简单的节点中</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#定义节点chatbot</span><br><span class="line">def chatbot(state: State):</span><br><span class="line">    return &#123;&quot;messages&quot;: [llm.invoke(state[&quot;messages&quot;])]&#125;</span><br><span class="line"></span><br><span class="line">graph_builder.add_node(&quot;chatbot&quot;, chatbot)</span><br></pre></td></tr></table></figure>
<p> <code>chatbot</code> 节点函数如何将当前 <code>状态</code> 作为输入，并返回一个包含更新的 <code>消息</code> 列表的字典，键为“messages”。这是所有 LangGraph 节点函数的基本模式。</p>
<p>我们 <code>状态</code> 中的 <code>add_messages</code> 函数会将 LLM 的响应消息追加到状态中已有的消息之后。</p>
<h4 id="添加一个-入口-点"><a href="#添加一个-入口-点" class="headerlink" title="添加一个 入口 点"></a>添加一个 <code>入口</code> 点</h4><p>添加一个 <code>入口</code> 点，以告诉图每次运行时<strong>从何处开始工作</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">graph_builder.add_edge(START, &quot;chatbot&quot;)</span><br></pre></td></tr></table></figure>
<h4 id="编译图"><a href="#编译图" class="headerlink" title="编译图"></a>编译图</h4><p>在运行图之前，我们需要对其进行编译。我们可以通过在图构建器上调用 <code>compile()</code> 来完成。这将创建一个 <code>CompiledGraph</code>，我们可以在我们的状态上调用它。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">graph = graph_builder.compile()</span><br></pre></td></tr></table></figure>
<h4 id="可视化图"><a href="#可视化图" class="headerlink" title="可视化图"></a>可视化图</h4><p>您可以使用 <code>get_graph</code> 方法和其中一个“绘图”方法（例如 <code>draw_ascii</code> 或 <code>draw_png</code>）来可视化图。这些 <code>draw</code> 方法都需要额外的依赖项。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from IPython.display import Image, display</span><br><span class="line"></span><br><span class="line">try:</span><br><span class="line">    display(Image(graph.get_graph().draw_mermaid_png()))</span><br><span class="line">except Exception:</span><br><span class="line">    # This requires some extra dependencies and is optional</span><br><span class="line">    pass</span><br></pre></td></tr></table></figure>
<h4 id="运行聊天机器人"><a href="#运行聊天机器人" class="headerlink" title="运行聊天机器人"></a>运行聊天机器人</h4><p>运行聊天机器人</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">stream_graph_updates</span>(<span class="params">user_input: <span class="built_in">str</span></span>):</span><br><span class="line">    <span class="keyword">for</span> event <span class="keyword">in</span> graph.stream(&#123;<span class="string">&quot;messages&quot;</span>: [&#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: user_input&#125;]&#125;):</span><br><span class="line">        <span class="built_in">print</span>(event)</span><br><span class="line">        <span class="comment">#stream 返回的每个 event 通常是一个字典，键是图中节点的名称，值是该节点完成后的状态更新。</span></span><br><span class="line">        <span class="keyword">for</span> value <span class="keyword">in</span> event.values():</span><br><span class="line">            <span class="comment">#消息列表中的最后一条消息的文本内容</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Assistant:&quot;</span>, value[<span class="string">&quot;messages&quot;</span>][-<span class="number">1</span>].content)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        user_input = <span class="built_in">input</span>(<span class="string">&quot;User: &quot;</span>)</span><br><span class="line">        <span class="comment">#退出</span></span><br><span class="line">        <span class="keyword">if</span> user_input.lower() <span class="keyword">in</span> [<span class="string">&quot;quit&quot;</span>, <span class="string">&quot;exit&quot;</span>, <span class="string">&quot;q&quot;</span>]:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Goodbye!&quot;</span>)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="comment">#调用</span></span><br><span class="line">        stream_graph_updates(user_input)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="comment"># fallback if input() is not available</span></span><br><span class="line">        user_input = <span class="string">&quot;What do you know about LangGraph?&quot;</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;User: &quot;</span> + user_input)</span><br><span class="line">        stream_graph_updates(user_input)</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>graph.stream() 是 LangGraph 的核心功能之一。它会执行整个图（Graph），但不是一次性返回最终结果，而是像视频流一样，一步一步地返回中间过程的更新。这使得您可以实时看到模型生成内容的每一个部分。</p>
</blockquote>
<h3 id="添加网页搜索工具"><a href="#添加网页搜索工具" class="headerlink" title="添加网页搜索工具"></a>添加网页搜索工具</h3><h4 id="获取Tavily-api"><a href="#获取Tavily-api" class="headerlink" title="获取Tavily api"></a>获取Tavily api</h4><p><a target="_blank" rel="noopener" href="https://tavily.com/">Tavily 的搜索 API</a> 是一款专为 AI 代理 (LLM) 构建的搜索引擎，能够快速提供实时、准确和基于事实的结果。</p>
<p>每月 1,000 次免费搜索</p>
<p><a target="_blank" rel="noopener" href="https://python.langchain.ac.cn/docs/integrations/tools/tavily_search/">Tavily Search | 🦜️🔗 LangChain 框架</a></p>
<p>获取api<a target="_blank" rel="noopener" href="https://app.tavily.com/home">Tavily AI —- Tavily AI</a></p>
<h4 id="添加工具"><a href="#添加工具" class="headerlink" title="添加工具"></a>添加工具</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from langchain_tavily import TavilySearch</span><br><span class="line"></span><br><span class="line">tool = TavilySearch(</span><br><span class="line">    tavily_api_key=&quot;tvly-dev-&quot;,</span><br><span class="line">    max_results=2)</span><br><span class="line">tools = [tool]</span><br><span class="line">tool.invoke(&quot;李超是谁?&quot;)</span><br></pre></td></tr></table></figure>
<h4 id="定义图"><a href="#定义图" class="headerlink" title="定义图"></a>定义图</h4><p>在LLM上添加<code>bind_tools</code>。这让LLM知道如果它想使用搜索引擎，应使用正确的JSON格式。</p>
<p>定义聊天模型llm（代码同上）</p>
<p>将tools整合到<code>StateGraph</code>中</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> Annotated</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> typing_extensions <span class="keyword">import</span> TypedDict</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> langgraph.graph <span class="keyword">import</span> StateGraph, START, END</span><br><span class="line"><span class="keyword">from</span> langgraph.graph.message <span class="keyword">import</span> add_messages</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">State</span>(<span class="title class_ inherited__">TypedDict</span>):</span><br><span class="line">    messages: Annotated[<span class="built_in">list</span>, add_messages]</span><br><span class="line"></span><br><span class="line">graph_builder = StateGraph(State)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将一个或多个**工具（tools） 绑定到一个 大型语言模型（LLM）**上，从而创建一个新的、具备工具调用能力的 LLM 实例</span></span><br><span class="line">llm_with_tools = llm.bind_tools(tools)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">chatbot</span>(<span class="params">state: State</span>):</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&quot;messages&quot;</span>: [llm_with_tools.invoke(state[<span class="string">&quot;messages&quot;</span>])]&#125;</span><br><span class="line"></span><br><span class="line">graph_builder.add_node(<span class="string">&quot;chatbot&quot;</span>, chatbot)</span><br></pre></td></tr></table></figure>
<h4 id="创建一个运行工具的函数"><a href="#创建一个运行工具的函数" class="headerlink" title="创建一个运行工具的函数"></a>创建一个运行工具的函数</h4><p>现在，创建一个函数来运行被调用的工具。通过将工具添加到一个名为<code>BasicToolNode</code>的新节点来完成，该节点检查状态中的最新消息，如果消息包含<code>tool_calls</code>，则调用工具。它依赖于LLM的<code>tool_calling</code>支持，该支持在Anthropic、OpenAI、Google Gemini以及许多其他LLM提供商中可用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义 __call__ 方法，让这个类的实例可以像函数一样被调用</span></span><br><span class="line"><span class="comment"># inputs 是 langgraph 传进来的当前状态，是一个字典</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, inputs: <span class="built_in">dict</span></span>):</span><br><span class="line">    <span class="comment"># 1. 从状态中获取最新的消息</span></span><br><span class="line">    <span class="comment"># 使用了“海象操作符” :=，先从 inputs 中获取 &#x27;messages&#x27; 列表，如果不存在则返回空列表 []</span></span><br><span class="line">    <span class="comment"># 然后检查列表是否为空。如果不为空，则取出最后一条消息。</span></span><br><span class="line">    <span class="keyword">if</span> messages := inputs.get(<span class="string">&quot;messages&quot;</span>, []):</span><br><span class="line">        message = messages[-<span class="number">1</span>]  <span class="comment"># 通常，最后一条消息是 AI 发出的，其中包含工具调用请求</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 如果没有消息，就报错，因为这个节点不知道该做什么</span></span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;No message found in input&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. 准备一个列表，用来存放所有工具的执行结果</span></span><br><span class="line">    outputs = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. 遍历 AI 消息中请求的所有工具调用</span></span><br><span class="line">    <span class="keyword">for</span> tool_call <span class="keyword">in</span> message.tool_calls:</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 4. 执行工具</span></span><br><span class="line">        <span class="comment"># a. tool_call[&quot;name&quot;] 获取工具名称 (例如 &#x27;tavily_search_results_json&#x27;)</span></span><br><span class="line">        <span class="comment"># b. self.tools_by_name[...] 从预存的工具字典中找到对应的工具对象</span></span><br><span class="line">        <span class="comment"># c. .invoke(tool_call[&quot;args&quot;]) 使用 LLM 提供的参数来调用该工具</span></span><br><span class="line">        tool_result = <span class="variable language_">self</span>.tools_by_name[tool_call[<span class="string">&quot;name&quot;</span>]].invoke(</span><br><span class="line">            tool_call[<span class="string">&quot;args&quot;</span>]</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 5. 将工具执行结果打包成 ToolMessage</span></span><br><span class="line">        <span class="comment"># 这是 langgraph/langchain 的标准格式，用于告诉 LLM 工具执行的结果是什么</span></span><br><span class="line">        outputs.append(</span><br><span class="line">            ToolMessage(</span><br><span class="line">                content=json.dumps(tool_result),  <span class="comment"># 工具结果必须是字符串，所以用 json.dumps 序列化</span></span><br><span class="line">                name=tool_call[<span class="string">&quot;name&quot;</span>],  <span class="comment"># 告诉 LLM 这是哪个工具的结果</span></span><br><span class="line">                tool_call_id=tool_call[<span class="string">&quot;id&quot;</span>],  <span class="comment"># 必须提供原始请求的 ID，以便 LLM 知道这个结果对应哪个请求</span></span><br><span class="line">            )</span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 6. 返回结果，更新图的状态</span></span><br><span class="line">    <span class="comment"># 返回一个字典，其中 &#x27;messages&#x27; 键对应着包含所有 ToolMessage 的列表</span></span><br><span class="line">    <span class="comment"># langgraph 会将这个列表中的消息追加到主状态的 &#x27;messages&#x27; 列表中</span></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&quot;messages&quot;</span>: outputs&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>call</strong> 是 Python 中一个非常特殊的“魔术方法”（magic method）。它的作用是 让一个类的实例（对象）能够像函数一样被调用 。</p>
<p>这在 langgraph 中是一种常见且核心的设计模式。它的含义是：</p>
<ol>
<li>节点即函数 ： BasicToolNode 的实例（比如 tool_node ）本身就代表了图中的一个可执行节点。</li>
<li>执行逻辑 ：当 langgraph 的状态机运行到这个 tool_node 节点时，它会直接“调用”这个节点对象，并把当前的状态（ inputs 字典）传递给它</li>
</ol>
</blockquote>
<p>可以使用LangGraph预构建的<a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langgraph/reference/agents/#langgraph.prebuilt.tool_node.ToolNode">ToolNode</a>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langgraph.prebuilt <span class="keyword">import</span> ToolNode</span><br><span class="line"></span><br><span class="line">tool_node = ToolNode(tools=[tool])</span><br><span class="line">graph_builder.add_node(<span class="string">&quot;tools&quot;</span>, tool_node)</span><br></pre></td></tr></table></figure>
<h4 id="定义conditional-edges"><a href="#定义conditional-edges" class="headerlink" title="定义conditional_edges"></a>定义<code>conditional_edges</code></h4><p>添加了工具节点后，现在您可以定义<code>conditional_edges</code>。</p>
<p><strong>边（Edges）</strong>将控制流从一个节点路由到下一个节点。<strong>条件边（Conditional edges）</strong>从单个节点开始，通常包含“if”语句，根据当前图状态路由到不同的节点。这些函数接收当前的图<code>state</code>并返回一个字符串或字符串列表，指示接下来要调用哪个（或哪些）节点。</p>
<p>接下来，定义一个名为<code>route_tools</code>的路由函数，它检查聊天机器人输出中的<code>tool_calls</code>。通过调用<code>add_conditional_edges</code>将此函数提供给图，这会告诉图，无论何时<code>chatbot</code>节点完成，都要检查此函数以确定下一步去哪里。</p>
<p>如果存在工具调用，条件将路由到<code>tools</code>；如果不存在，则路由到<code>END</code>。由于条件可以返回<code>END</code>，因此这次您不需要明确设置<code>finish_point</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">route_tools</span>(<span class="params"></span></span><br><span class="line"><span class="params">     state: State,</span></span><br><span class="line"><span class="params"> </span>):</span><br><span class="line">     <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">     用于 conditional_edge 的路由函数：</span></span><br><span class="line"><span class="string">     - 如果最后一条消息包含工具调用，则路由到 ToolNode</span></span><br><span class="line"><span class="string">     - 否则路由到结束节点</span></span><br><span class="line"><span class="string">     &quot;&quot;&quot;</span></span><br><span class="line">     <span class="comment"># 处理 state 为列表的情况（可能是消息列表）</span></span><br><span class="line">     <span class="keyword">if</span> <span class="built_in">isinstance</span>(state, <span class="built_in">list</span>):</span><br><span class="line">         ai_message = state[-<span class="number">1</span>]  <span class="comment"># 获取最后一条消息</span></span><br><span class="line">     <span class="comment"># 处理 state 为字典的情况（包含 messages 字段）</span></span><br><span class="line">     <span class="keyword">elif</span> messages := state.get(<span class="string">&quot;messages&quot;</span>, []):</span><br><span class="line">         ai_message = messages[-<span class="number">1</span>]  <span class="comment"># 获取最后一条消息</span></span><br><span class="line">     <span class="keyword">else</span>:</span><br><span class="line">         <span class="keyword">raise</span> ValueError(<span class="string">f&quot;输入状态中没有找到消息: <span class="subst">&#123;state&#125;</span>&quot;</span>)</span><br><span class="line">     </span><br><span class="line">     <span class="comment"># 检查消息是否有工具调用</span></span><br><span class="line">     <span class="keyword">if</span> <span class="built_in">hasattr</span>(ai_message, <span class="string">&quot;tool_calls&quot;</span>) <span class="keyword">and</span> <span class="built_in">len</span>(ai_message.tool_calls) &gt; <span class="number">0</span>:</span><br><span class="line">         <span class="keyword">return</span> <span class="string">&quot;tools&quot;</span>  <span class="comment"># 有工具调用，返回 &quot;tools&quot; 路由到工具节点</span></span><br><span class="line">     <span class="keyword">return</span> END  <span class="comment"># 没有工具调用，返回 END 结束流程</span></span><br></pre></td></tr></table></figure>
<p>可以使用预构建的<a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langgraph/reference/prebuilt/#tools_condition">tools_condition</a>代替route_tools以使其更简洁。</p>
<blockquote>
<p><code>tools_condition</code> 函数在聊天机器人需要使用工具时返回 “tools”，如果可以不使用响应则返回 “END”。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langgraph.prebuilt <span class="keyword">import</span> tools_condition</span><br><span class="line">graph_builder.add_conditional_edges(</span><br><span class="line">    <span class="string">&quot;chatbot&quot;</span>,</span><br><span class="line">    tools_condition,</span><br><span class="line">    &#123;<span class="string">&quot;tools&quot;</span>: <span class="string">&quot;tools&quot;</span>, END: END&#125;,</span><br><span class="line">)</span><br><span class="line">graph_builder.add_edge(<span class="string">&quot;tools&quot;</span>, <span class="string">&quot;chatbot&quot;</span>)</span><br><span class="line">graph_builder.add_edge(START, <span class="string">&quot;chatbot&quot;</span>)</span><br><span class="line">graph = graph_builder.<span class="built_in">compile</span>()</span><br></pre></td></tr></table></figure>
<h4 id="可视化图-1"><a href="#可视化图-1" class="headerlink" title="可视化图"></a>可视化图</h4><p>如上</p>
<h4 id="向机器人提问"><a href="#向机器人提问" class="headerlink" title="向机器人提问"></a>向机器人提问</h4><p>现在您可以向聊天机器人提出超出其训练数据范围的问题。</p>
<p>如上</p>
<h3 id="添加记忆功能"><a href="#添加记忆功能" class="headerlink" title="添加记忆功能"></a>添加记忆功能</h3><p>LangGraph 通过<strong>持久性检查点</strong>解决了这个问题。如果您在编译图时提供一个<code>checkpointer</code>，并在调用图时提供一个<code>thread_id</code>，LangGraph 会在每一步之后自动保存状态。当您使用相同的<code>thread_id</code>再次调用图时，图会加载其保存的状态，允许聊天机器人从上次中断的地方继续。</p>
<p>我们稍后会看到，<strong>检查点</strong>比简单的聊天记忆功能<em>强大得多</em>——它允许您随时保存和恢复复杂状态，用于错误恢复、人工干预工作流、时间旅行交互等。但首先，让我们添加检查点以实现多轮对话。</p>
<h4 id="创建-MemorySaver-检查点"><a href="#创建-MemorySaver-检查点" class="headerlink" title="创建 MemorySaver 检查点"></a>创建 <code>MemorySaver</code> 检查点</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langgraph.checkpoint.memory <span class="keyword">import</span> MemorySaver</span><br><span class="line"></span><br><span class="line">memory = MemorySaver()</span><br></pre></td></tr></table></figure>
<p>这是一个内存中的检查点，方便本教程使用。然而，在生产应用程序中，您可能会将其更改为使用 <code>SqliteSaver</code> 或 <code>PostgresSaver</code> 并连接数据库。</p>
<h4 id="编译图-1"><a href="#编译图-1" class="headerlink" title="编译图"></a>编译图</h4><p>使用提供的检查点编译图，图在遍历每个节点时将对 <code>State</code> 进行检查点。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">graph = graph_builder.compile(checkpointer=memory)</span><br></pre></td></tr></table></figure>
<h4 id="与您的聊天机器人互动"><a href="#与您的聊天机器人互动" class="headerlink" title="与您的聊天机器人互动"></a>与您的聊天机器人互动</h4><ol>
<li><p>选择一个线程作为此对话的键。</p>
<p>thread_id决定对话窗口</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">config = &#123;&quot;configurable&quot;: &#123;&quot;thread_id&quot;: &quot;1&quot;&#125;&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>调用您的聊天机器人</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">user_input = &quot;我是谁&quot;</span><br><span class="line"></span><br><span class="line"># The config is the **second positional argument** to stream() or invoke()!</span><br><span class="line">events = graph.stream(</span><br><span class="line">    &#123;&quot;messages&quot;: [&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_input&#125;]&#125;,</span><br><span class="line">    config,</span><br><span class="line">    stream_mode=&quot;values&quot;,</span><br><span class="line">)</span><br><span class="line">for event in events:</span><br><span class="line">    event[&quot;messages&quot;][-1].pretty_print()</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="添加人工干预"><a href="#添加人工干预" class="headerlink" title="添加人工干预"></a>添加人工干预</h3><p>代理可能不可靠，并且可能需要人工输入才能成功完成任务。同样，对于某些操作，您可能需要在运行前要求人工批准，以确保一切按预期运行。</p>
<p>LangGraph 的<a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langgraph/concepts/persistence/">持久化</a>层支持<strong>人工干预</strong>工作流，允许根据用户反馈暂停和恢复执行。此功能的主要接口是<a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langgraph/how-tos/human_in_the_loop/add-human-in-the-loop/"><code>interrupt</code></a>函数。在节点内调用<code>interrupt</code>将暂停执行。通过传入<a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langgraph/concepts/low_level/#command">Command</a>，可以恢复执行并接收来自人工的新输入。<code>interrupt</code>在功能上类似于 Python 的内置<code>input()</code>，<a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langgraph/how-tos/human_in_the_loop/add-human-in-the-loop/">但有一些注意事项</a>。</p>
<h4 id="添加human-assistance工具"><a href="#添加human-assistance工具" class="headerlink" title="添加human_assistance工具"></a>添加<code>human_assistance</code>工具</h4><p>将<code>human_assistance</code>工具添加到聊天机器人。此工具使用<code>interrupt</code>从人工接收信息。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># 添加人工干预功能</span><br><span class="line"># 导入LangGraph的中断机制和命令类型</span><br><span class="line">from langgraph.types import Command, interrupt</span><br><span class="line"># 导入LangChain的工具装饰器</span><br><span class="line">from langchain_core.tools import tool</span><br><span class="line"></span><br><span class="line"># 使用@tool装饰器将函数标记为可被LLM调用的工具</span><br><span class="line">@tool</span><br><span class="line">def human_assistance(query: str) -&gt; str:</span><br><span class="line">    &quot;&quot;&quot;请求人工协助的工具函数。</span><br><span class="line">    当LLM遇到需要人工判断或帮助的情况时，会调用此工具。</span><br><span class="line">    该函数会暂停图的执行，等待人工操作员提供响应。</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    # interrupt()函数会暂停图的执行，等待人工输入</span><br><span class="line">    # 传入的字典包含查询信息，人工操作员会看到这个查询</span><br><span class="line">    human_response = interrupt(&#123;&quot;query&quot;: query&#125;)</span><br><span class="line">    </span><br><span class="line">    # 从人工响应中提取数据并返回给LLM</span><br><span class="line">    # human_response是一个字典，&quot;data&quot;字段包含人工提供的实际响应</span><br><span class="line">    return human_response[&quot;data&quot;]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>简单来说， <strong>调用哪个工具，以及何时调用，完全是由大语言模型（LLM）根据你给它的指令（Prompt）来决定的。</strong></p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tool = TavilySearch(max_results=2)</span><br><span class="line">tools = [tool, human_assistance]</span><br><span class="line">llm_with_tools = llm.bind_tools(tools)</span><br></pre></td></tr></table></figure>
<h4 id="定义chatbot"><a href="#定义chatbot" class="headerlink" title="定义chatbot"></a>定义chatbot</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">def chatbot(state: State):</span><br><span class="line">    # 调用绑定了工具的LLM（llm_with_tools），传入当前的消息历史</span><br><span class="line">    # LLM会根据最新的消息决定是生成文本回复，还是调用一个或多个工具</span><br><span class="line">    message = llm_with_tools.invoke(state[&quot;messages&quot;])</span><br><span class="line">    </span><br><span class="line">    # --- 关键断言逻辑 ---</span><br><span class="line">    assert len(message.tool_calls) &lt;= 1</span><br><span class="line">    </span><br><span class="line">    # 将LLM生成的新消息（可能是文本回复，也可能是工具调用请求）返回</span><br><span class="line">    # 这个返回值会以字典的形式更新到状态（State）对象中</span><br><span class="line">    return &#123;&quot;messages&quot;: [message]&#125;</span><br><span class="line"></span><br><span class="line"># 将 chatbot 函数作为名为 &quot;chatbot&quot; 的节点添加到图构建器中</span><br><span class="line">graph_builder.add_node(&quot;chatbot&quot;, chatbot)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>中断安全性的断言 ( assert ) : assert len(message.tool_calls) &lt;= 1 是在实现人工干预时一个非常重要的 安全措施 。</p>
<ul>
<li>问题 : 现代 LLM 支持并行工具调用（一次请求执行多个工具）。但如果其中一个工具是 human_assistance 并触发了中断，整个图会暂停。当人工操作完成后，图会从中断点恢复。此时，如果不对工具调用数量做限制，LangGraph 可能会重新尝试执行所有在中断前请求的工具，导致已经执行过的工具被再次调用。</li>
<li>解决方案 : 这个断言强制要求 LLM 在每一步最多只能请求调用一个工具。这样就保证了当中断发生并恢复后，不会有重复执行工具的风险，确保了流程的稳定性和可预测性。</li>
</ul>
</blockquote>
<h4 id="编译图-2"><a href="#编译图-2" class="headerlink" title="编译图"></a>编译图</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">memory = MemorySaver()</span><br><span class="line"></span><br><span class="line">graph = graph_builder.compile(checkpointer=memory)</span><br></pre></td></tr></table></figure>
<h4 id="调用聊天机器人并中断"><a href="#调用聊天机器人并中断" class="headerlink" title="调用聊天机器人并中断"></a>调用聊天机器人并中断</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">user_input = &quot;我需要一些关于构建 AI 代理的专家指导。你能帮我请求协助吗？&quot;</span><br><span class="line">config = &#123;&quot;configurable&quot;: &#123;&quot;thread_id&quot;: &quot;1&quot;&#125;&#125;</span><br><span class="line"></span><br><span class="line">events = graph.stream(</span><br><span class="line">    &#123;&quot;messages&quot;: [&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_input&#125;]&#125;,</span><br><span class="line">    config,</span><br><span class="line">    #它的作用是指定在进行流式处理时，你希望接收到的数据是以 完整的、累积的值 的形式返回，而不是以增量的、片段的形式返回。</span><br><span class="line">    stream_mode=&quot;values&quot;,</span><br><span class="line">)</span><br><span class="line">for event in events:</span><br><span class="line">    if &quot;messages&quot; in event:</span><br><span class="line">        event[&quot;messages&quot;][-1].pretty_print()</span><br></pre></td></tr></table></figure>
<p>聊天机器人生成了一个工具调用，但随后执行被中断。如果您检查图状态，您会看到它停止在工具节点</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">snapshot = graph.get_state(config)</span><br><span class="line">snapshot.next</span><br></pre></td></tr></table></figure>
<h4 id="恢复执行"><a href="#恢复执行" class="headerlink" title="恢复执行"></a>恢复执行</h4><p>要恢复执行，请传入一个包含工具所需数据的<a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langgraph/concepts/low_level/#command"><code>Command</code></a>对象。此数据的格式可以根据需要进行自定义。对于本示例，请使用一个带有键<code>&quot;data&quot;</code>的字典（由human_assistance决定）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">human_response = (</span><br><span class="line">&quot;我们专家在此为您提供帮助！我们建议您查看 LangGraph 来构建您的代理。它比简单的自主代理更可靠、更具可扩展性。&quot;</span><br><span class="line">)   </span><br><span class="line">#从暂停状态恢复执行</span><br><span class="line">human_command = Command(resume=&#123;&quot;data&quot;: human_response&#125;)</span><br><span class="line"></span><br><span class="line">events = graph.stream(human_command, config, stream_mode=&quot;values&quot;)</span><br><span class="line">for event in events:</span><br><span class="line">    if &quot;messages&quot; in event:</span><br><span class="line">        event[&quot;messages&quot;][-1].pretty_print()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>1.工具的定义 ( human_assistance function):</p>
<ul>
<li>当 LLM 调用 human_assistance 工具时，这个函数被执行。</li>
<li>函数内部， interrupt() 被调用，导致图暂停，并等待人工输入。</li>
<li>在图恢复后， interrupt() 函数会返回一个值，这个值就是您通过 Command(resume=…) 注入的内容，也就是 {“data”: human_response} 。</li>
<li>因此， human_assistance 函数中的 human_response 变量实际上就等于 {“data”: human_response} 。</li>
<li>最后， return human_response[“data”] 从这个字典中提取出 “data” 键对应的值 ，并将其作为 human_assistance 工具的最终返回结果。</li>
</ul>
<p>2.恢复指令 ( Command(resume=…) ):</p>
<ul>
<li>当您构建 Command(resume={“data”: human_response}) 时，您正在创建一个符合 human_assistance 函数期望的结构。</li>
<li>您将人工回复包装在一个字典里，并使用 “data” 作为键。</li>
<li>这个结构被传递回 interrupt() ，然后被 human_assistance 函数接收和解析。</li>
</ul>
<p>因为 human_assistance 函数的 return 语句期望从返回的字典中访问 “data” 键，所以我们在恢复执行时必须提供一个具有相同结构的字典。这是为了确保数据能够正确地在中断和恢复的过程中传递。</p>
</blockquote>
<p>在 LangGraph 中，<code>Command</code> 是一个用于<strong>控制图执行流程、更新状态、实现人机交互</strong>的核心类。它支持以下四个参数：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">参数名</th>
<th style="text-align:left">类型</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>update</code></td>
<td style="text-align:left"><code>dict</code></td>
<td style="text-align:left">用于更新图的状态（state）。例如：<code>Command(update=&#123;&quot;foo&quot;: &quot;bar&quot;&#125;)</code>。</td>
</tr>
<tr>
<td style="text-align:left"><code>resume</code></td>
<td style="text-align:left"><code>Any</code></td>
<td style="text-align:left">与 <code>interrupt()</code> 配合使用，用于恢复被中断的图执行，并传递用户输入。</td>
</tr>
<tr>
<td style="text-align:left"><code>goto</code></td>
<td style="text-align:left"><code>str</code> 或 <code>Send</code> 或 `List[str</td>
<td style="text-align:left">Send]`</td>
<td>控制下一步要执行的节点，支持跳转到指定节点、多个节点序列，或使用 <code>Send</code> 对象。</td>
</tr>
<tr>
<td style="text-align:left"><code>graph</code></td>
<td style="text-align:left"><code>str</code></td>
<td style="text-align:left">可选，指定命令作用的图。默认是当前图，也可以设为 <code>Command.PARENT</code> 表示父图。</td>
</tr>
</tbody>
</table>
</div>
<h3 id="自定义状态"><a href="#自定义状态" class="headerlink" title="自定义状态"></a>自定义状态</h3><p>在本教程中，您将向状态添加额外字段，以定义复杂行为，而无需依赖消息列表。聊天机器人将使用其搜索工具查找特定信息，并将其转发给人工进行审查。</p>
<h4 id="向状态添加键"><a href="#向状态添加键" class="headerlink" title="向状态添加键"></a>向状态添加键</h4><p>通过向状态添加 <code>name</code> 和 <code>birthday</code> 键，更新聊天机器人以研究实体的生日</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">class State(TypedDict):</span><br><span class="line">    messages: Annotated[list, add_messages]</span><br><span class="line">    name: str</span><br><span class="line">    birthday: str</span><br></pre></td></tr></table></figure>
<p>将此信息添加到状态中，可以使其轻松被其他图节点（例如存储或处理信息的下游节点）以及图的持久层访问。</p>
<h4 id="在工具内部更新状态"><a href="#在工具内部更新状态" class="headerlink" title="在工具内部更新状态"></a>在工具内部更新状态</h4><p>现在，在 <code>human_assistance</code> 工具内部填充状态键。这允许人工在信息存储到状态之前对其进行审查。使用 <a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langgraph/concepts/low_level/#using-inside-tools"><code>Command</code></a> 从<strong>工具内部</strong>发出状态更新。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"># 从 langchain_core.messages 导入 ToolMessage，用于创建工具调用的响应消息</span><br><span class="line">from langchain_core.messages import ToolMessage </span><br><span class="line"># 从 langchain_core.tools 导入 InjectedToolCallId（用于自动注入工具调用ID）和 tool（工具装饰器）</span><br><span class="line">from langchain_core.tools import InjectedToolCallId, tool </span><br><span class="line"></span><br><span class="line"># 从 langgraph.types 导入 Command（用于向图发送指令）和 interrupt（用于中断图的执行）</span><br><span class="line">from langgraph.types import Command, interrupt </span><br><span class="line">from typing import Annotated</span><br><span class="line"># @tool 装饰器将这个函数声明为一个可供 LLM 调用的工具</span><br><span class="line">@tool </span><br><span class="line">def human_assistance(</span><br><span class="line">    name: str, </span><br><span class="line">    birthday: str, </span><br><span class="line">    # tool_call_id 这个参数非常特殊。Annotated[...] 和 InjectedToolCallId 告诉 LangGraph：</span><br><span class="line">    # 1. 这个参数不应暴露给 LLM，LLM 在调用此工具时不需要提供它。</span><br><span class="line">    # 2. LangGraph 在执行时，会自动将触发此工具的那个工具调用的 ID 注入到这个参数中。</span><br><span class="line">    # 这个 ID 对于创建与原始请求相关联的 ToolMessage 至关重要。</span><br><span class="line">    tool_call_id: Annotated[str, InjectedToolCallId]</span><br><span class="line">) -&gt; str: </span><br><span class="line">    &quot;&quot;&quot;当需要人工确认或更正信息时，请求人类协助。&quot;&quot;&quot; </span><br><span class="line">    # 调用 interrupt() 来暂停图的执行，并向人类审核者呈现一个包含问题和待确认数据的字典。</span><br><span class="line">    # 图会在此处暂停，直到人类通过 resume 指令提供了响应。</span><br><span class="line">    human_response = interrupt( </span><br><span class="line">        &#123; </span><br><span class="line">            &quot;question&quot;: &quot;Is this correct?&quot;, </span><br><span class="line">            &quot;name&quot;: name, </span><br><span class="line">            &quot;birthday&quot;: birthday, </span><br><span class="line">        &#125;,</span><br><span class="line">    ) </span><br><span class="line">    # 检查人类的响应。如果响应中 &#x27;correct&#x27; 键的值是 &#x27;yes&#x27; 或 &#x27;y&#x27; 开头，</span><br><span class="line">    # 则认为信息是正确的。</span><br><span class="line">    if human_response.get(&quot;correct&quot;, &quot;&quot;).lower().startswith(&quot;y&quot;): </span><br><span class="line">        # 如果信息正确，直接使用从 LLM 获取的原始信息。</span><br><span class="line">        verified_name = name </span><br><span class="line">        verified_birthday = birthday </span><br><span class="line">        response = &quot;Correct&quot; </span><br><span class="line">    # 否则，认为人类审核者提供了更正后的信息。</span><br><span class="line">    else: </span><br><span class="line">        # 从人类的响应中获取更正后的姓名和生日。</span><br><span class="line">        # 如果人类没有提供新的值，则使用 .get() 的默认值，即原始值。</span><br><span class="line">        verified_name = human_response.get(&quot;name&quot;, name) </span><br><span class="line">        verified_birthday = human_response.get(&quot;birthday&quot;, birthday) </span><br><span class="line">        response = f&quot;Made a correction: &#123;human_response&#125;&quot; </span><br><span class="line"></span><br><span class="line">    # 在工具内部直接构造一个用于更新图状态的字典。</span><br><span class="line">    state_update = &#123; </span><br><span class="line">        &quot;name&quot;: verified_name, # 更新状态中的 &#x27;name&#x27; 字段</span><br><span class="line">        &quot;birthday&quot;: verified_birthday, # 更新状态中的 &#x27;birthday&#x27; 字段</span><br><span class="line">        # 创建一个 ToolMessage，将其添加到状态的 &#x27;messages&#x27; 列表中。</span><br><span class="line">        # 这个消息将作为此工具调用的正式“答复”出现在对话历史中。</span><br><span class="line">        # tool_call_id 是必需的，用于将此答复与 LLM 的原始工具调用请求关联起来。</span><br><span class="line">        &quot;messages&quot;: [ToolMessage(response, tool_call_id=tool_call_id)], </span><br><span class="line">    &#125; </span><br><span class="line">    # 这个工具不返回一个简单的字符串或数字，而是返回一个 Command 对象。</span><br><span class="line">    # Command(update=...) 是一个明确的指令，告诉 LangGraph 执行器：</span><br><span class="line">    # “请不要将我的返回值当作普通工具输出，而是用 state_update 字典里的内容来直接更新当前的图状态。”</span><br><span class="line">    return Command(update=state_update) </span><br></pre></td></tr></table></figure>
<p>图的其余部分保持不变。</p>
<h4 id="提示聊天机器人调用人工审查"><a href="#提示聊天机器人调用人工审查" class="headerlink" title="提示聊天机器人调用人工审查"></a>提示聊天机器人调用人工审查</h4><p>提示聊天机器人查找 LangGraph 库的“生日”，并在其获取所需信息后，指示聊天机器人使用 <code>human_assistance</code> 工具。通过在工具参数中设置 <code>name</code> 和 <code>birthday</code>，您将强制聊天机器人为这些字段生成提议。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">user_input = (</span><br><span class="line">    &quot;你能查一下 LangGraph 是什么时候发布的吗？ &quot;</span><br><span class="line">    &quot;当你有了答案后，使用 human_assistance 工具进行审查。&quot;</span><br><span class="line">)</span><br><span class="line">config = &#123;&quot;configurable&quot;: &#123;&quot;thread_id&quot;: &quot;1&quot;&#125;&#125;</span><br><span class="line"></span><br><span class="line">events = graph.stream(</span><br><span class="line">    &#123;&quot;messages&quot;: [&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_input&#125;]&#125;,</span><br><span class="line">    config,</span><br><span class="line">    stream_mode=&quot;values&quot;,</span><br><span class="line">)</span><br><span class="line">for event in events:</span><br><span class="line">    if &quot;messages&quot; in event:</span><br><span class="line">        event[&quot;messages&quot;][-1].pretty_print()</span><br></pre></td></tr></table></figure>
<p>我们再次在 <code>human_assistance</code> 工具中触发了 <code>interrupt</code>。</p>
<h4 id="添加人工协助"><a href="#添加人工协助" class="headerlink" title="添加人工协助"></a>添加人工协助</h4><p>聊天机器人未能识别正确的日期，因此为其提供信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">human_command = Command(</span><br><span class="line">    resume=&#123;</span><br><span class="line">        &quot;name&quot;: &quot;LangGraph&quot;,</span><br><span class="line">        &quot;birthday&quot;: &quot;Jan 17, 2024&quot;,</span><br><span class="line">    &#125;,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">events = graph.stream(human_command, config, stream_mode=&quot;values&quot;)</span><br><span class="line">for event in events:</span><br><span class="line">    if &quot;messages&quot; in event:</span><br><span class="line">        event[&quot;messages&quot;][-1].pretty_print()</span><br></pre></td></tr></table></figure>
<p>请注意，这些字段现在已反映在状态中</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">snapshot = graph.get_state(config)</span><br><span class="line"></span><br><span class="line">&#123;k: v for k, v in snapshot.values.items() if k in (&quot;name&quot;, &quot;birthday&quot;)&#125;</span><br></pre></td></tr></table></figure>
<p>这使得下游节点（例如，进一步处理或存储信息的节点）可以轻松访问它们。</p>
<h3 id="时间功能（从之前的某个状态开始）"><a href="#时间功能（从之前的某个状态开始）" class="headerlink" title="时间功能（从之前的某个状态开始）"></a>时间功能（从之前的某个状态开始）</h3><p>在典型的聊天机器人工作流程中，用户与机器人进行一次或多次交互以完成任务。<a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langgraph/tutorials/get-started/3-add-memory/">记忆</a>和<a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langgraph/tutorials/get-started/4-human-in-the-loop/">人工干预</a>功能可以为图状态启用检查点并控制未来的响应。</p>
<p>如果您希望用户能够从之前的响应开始并探索不同的结果，该怎么办？或者，如果您希望用户能够回溯聊天机器人的工作以纠正错误或尝试不同的策略，这在自主软件工程师等应用程序中很常见，那又该怎么办？</p>
<p>您可以使用 LangGraph 内置的<strong>时光旅行</strong>功能创建这些类型的体验。</p>
<h4 id="回溯您的图"><a href="#回溯您的图" class="headerlink" title="回溯您的图"></a>回溯您的图</h4><p>通过使用图的<code>get_state_history</code>方法获取检查点来回溯您的图。然后，您可以从之前的这个时间点恢复执行。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"># 初始化一个变量 to_replay 为 None，它将用于存储我们想要“时间旅行”回去的特定状态。</span><br><span class="line"># to_replay 将在循环中被赋值为我们感兴趣的那个历史状态快照。</span><br><span class="line">to_replay = None</span><br><span class="line"></span><br><span class="line"># 遍历 `graph` 在给定 `config` 下的所有历史状态。</span><br><span class="line"># `graph.get_state_history(config)` 会返回一个迭代器，其中包含了从开始到当前的所有状态快照。</span><br><span class="line">for state in graph.get_state_history(config):</span><br><span class="line">    # 打印当前状态快照中的一些信息，以便我们观察和选择。</span><br><span class="line">    # `len(state.values[&quot;messages&quot;])` 显示了到该状态为止，对话历史中的消息总数。</span><br><span class="line">    # `state.next` 显示了在该状态之后，图将要执行的下一个节点或步骤的名称。</span><br><span class="line">    print(&quot;Num Messages: &quot;, len(state.values[&quot;messages&quot;]), &quot;Next: &quot;, state.next)</span><br><span class="line">    </span><br><span class="line">    # 打印一条分隔线，使输出更易读。</span><br><span class="line">    print(&quot;-&quot; * 80)</span><br><span class="line">    </span><br><span class="line">    # 这里是选择“时间旅行”目标点的关键逻辑。</span><br><span class="line">    # 我们设定一个条件：当对话历史中的消息数量正好等于4时，我们就找到了想要回到的那个点。</span><br><span class="line">    # 这是一个为了演示而设定的任意条件，在实际应用中，您可以根据需要设置更复杂的选择逻辑。</span><br><span class="line">    if len(state.values[&quot;messages&quot;]) == 4:</span><br><span class="line">        # We are somewhat arbitrarily selecting a specific state based on the number of chat messages in the state.</span><br><span class="line">        # 将当前这个符合条件的状态（state）保存到 to_replay 变量中。</span><br><span class="line">        # 循环结束后，to_replay 变量将持有我们选中的那个历史时刻的完整状态，</span><br><span class="line">        # 之后我们就可以用它来恢复或修改执行流程。</span><br><span class="line">        to_replay = state</span><br></pre></td></tr></table></figure>
<p>图的每一步都会保存检查点。这<strong>跨越了调用</strong>，因此您可以回溯整个线程的历史。</p>
<h4 id="从特定时间点加载状态"><a href="#从特定时间点加载状态" class="headerlink" title="从特定时间点加载状态"></a>从特定时间点加载状态</h4><p>从<code>to_replay</code>状态恢复。从这一点恢复将接下来调用<strong>action</strong>节点。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(to_replay.next)</span><br><span class="line">print(to_replay.config)</span><br></pre></td></tr></table></figure>
<p>检查点的<code>to_replay.config</code>包含一个<code>checkpoint_id</code>时间戳。提供此<code>checkpoint_id</code>值会告诉 LangGraph 的检查点器从该时间点<strong>加载</strong>状态。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">for event in graph.stream(None, to_replay.config, stream_mode=&quot;values&quot;):</span><br><span class="line">    if &quot;messages&quot; in event:</span><br><span class="line">        event[&quot;messages&quot;][-1].pretty_print()</span><br></pre></td></tr></table></figure>
<h3 id="运行本地服务器"><a href="#运行本地服务器" class="headerlink" title="运行本地服务器"></a>运行本地服务器</h3><h4 id="安装-LangGraph-CLI"><a href="#安装-LangGraph-CLI" class="headerlink" title="安装 LangGraph CLI"></a>安装 LangGraph CLI</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># Python &gt;= 3.11 is required.</span><br><span class="line"></span><br><span class="line">pip install --upgrade &quot;langgraph-cli[inmem]&quot;</span><br></pre></td></tr></table></figure>
<h4 id="创建-LangGraph-应用-🌱"><a href="#创建-LangGraph-应用-🌱" class="headerlink" title="创建 LangGraph 应用 🌱"></a>创建 LangGraph 应用 🌱</h4><p>从 <a target="_blank" rel="noopener" href="https://github.com/langchain-ai/new-langgraph-project"><code>new-langgraph-project-python</code> 模板</a> 或 <a target="_blank" rel="noopener" href="https://github.com/langchain-ai/new-langgraphjs-project"><code>new-langgraph-project-js</code> 模板</a> 创建一个新应用。此模板展示了一个单节点应用程序，您可以根据自己的逻辑进行扩展。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">langgraph new . --template new-langgraph-project-python</span><br></pre></td></tr></table></figure>
<blockquote>
<p>使用 <code>langgraph new</code> 而不指定模板，系统将显示一个交互式菜单，您可以从中选择可用的模板列表。</p>
</blockquote>
<h4 id="使用uv安装依赖项"><a href="#使用uv安装依赖项" class="headerlink" title="使用uv安装依赖项"></a>使用uv安装依赖项</h4><p>uv 是一个用 Rust 编写的极速 Python 包和项目管理器 。它旨在解决传统 Python 包管理工具（如 pip 、 poetry 等）在速度和效率方面的痛点，提供更快的安装、依赖解析和环境管理。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install uv#安装uv</span><br></pre></td></tr></table></figure>
<p>运行<code>uv sync</code>会根据 <code>pyproject.toml</code>的依赖创建虚拟环境并安装依赖</p>
<blockquote>
<p><code>pyproject.toml</code> 文件是 Python 项目中用于统一配置项目元数据、构建系统、依赖管理和各种工具设置的标准化文件。它通常用于替代旧的 requirements.txt 文件，提供更现代和集中的项目配置方式。</p>
</blockquote>
<h4 id="创建一个-env-文件"><a href="#创建一个-env-文件" class="headerlink" title="创建一个 .env 文件"></a>创建一个 <code>.env</code> 文件</h4><p>您将在新 LangGraph 应用的根目录下找到一个 <code>.env.example</code> 文件。在新 LangGraph 应用的根目录下创建一个 <code>.env</code> 文件，并将 <code>.env.example</code> 文件的内容复制到其中，填入所需的 API 密钥。</p>
<p>添加环境变量如LANGSMITH_API_KEY，OPENAI_API_KEY等</p>
<h4 id="启动-LangGraph-服务器"><a href="#启动-LangGraph-服务器" class="headerlink" title="启动 LangGraph 服务器"></a>启动 LangGraph 服务器</h4><p>在本地启动 LangGraph API 服务器</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">langgraph dev</span><br></pre></td></tr></table></figure>
<p>示例输出</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;    Ready!</span><br><span class="line">&gt;</span><br><span class="line">&gt;    - API: [https://:2024](https://:2024/)</span><br><span class="line">&gt;</span><br><span class="line">&gt;    - Docs: https://:2024/docs</span><br><span class="line">&gt;</span><br><span class="line">&gt;    - LangGraph Studio Web UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024</span><br></pre></td></tr></table></figure>
<p>LangGraph 服务器（如您通过 langgraph dev 命令启动的服务器）的主要作用是提供一个运行环境和接口，用于开发、测试、部署和管理基于 LangGraph 构建的 AI 代理和应用程序。具体来说，它有以下几个主要用途：</p>
<ol>
<li>API 接口暴露 ：它将您用 LangGraph 定义的复杂代理逻辑（即图结构）通过标准的 RESTful API 接口暴露出来。这意味着其他应用程序、前端界面或者其他服务可以通过 HTTP 请求与您的 LangGraph 代理进行交互，而无需直接集成 LangGraph 的 Python 代码。</li>
<li>简化部署 ：通过将 LangGraph 应用程序打包成一个可运行的服务，您可以更容易地将其部署到云服务器、容器（如 Docker）或其他生产环境中。这使得 LangGraph 代理可以作为一个独立的微服务运行，方便扩展和管理。</li>
<li><p>开发和调试便利 ：</p>
<ul>
<li>实时预览和调试 ：服务器通常会提供一个 Studio UI（如您在 <a target="_blank" rel="noopener" href="http://127.0.0.1:2024/studio">http://127.0.0.1:2024/studio</a> 看到的），让开发者能够可视化地查看代理的图结构、执行流程、状态变化和中间步骤，这对于理解和调试复杂的代理行为至关重要。</li>
<li>API 文档 ：自动生成的 API 文档（如 <a target="_blank" rel="noopener" href="http://127.0.0.1:2024/docs">http://127.0.0.1:2024/docs</a> ）提供了所有可用接口的详细说明和交互式测试功能，极大地加速了开发和集成过程。</li>
</ul>
</li>
<li>状态管理和持久化 ：LangGraph 代理通常涉及复杂的状态管理。服务器可以负责处理这些状态的持久化，确保代理在多次交互之间能够记住上下文和历史信息。</li>
</ol>
<h4 id="在-LangGraph-Studio-中测试您的应用程序"><a href="#在-LangGraph-Studio-中测试您的应用程序" class="headerlink" title="在 LangGraph Studio 中测试您的应用程序"></a>在 LangGraph Studio 中测试您的应用程序</h4><p><a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langgraph/concepts/langgraph_studio/">LangGraph Studio</a> 是一个专门的 UI，您可以连接到 LangGraph API 服务器，以便在本地可视化、交互和调试您的应用程序。通过访问 <code>langgraph dev</code> 命令输出中提供的 URL，在 LangGraph Studio 中测试您的图。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;    - LangGraph Studio Web UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024</span><br></pre></td></tr></table></figure>
<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langgraph/tutorials/get-started/1-build-basic-chatbot/#1-install-packages">构建一个基本聊天机器人 - LangChain 框架</a></p>
<p>官方教程，但是英文<a target="_blank" rel="noopener" href="https://academy.langchain.com/collections">https://academy.langchain.com/collections</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1AY4aeRETY/?spm_id_from=333.1007.top_right_bar_window_history.content.click&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">3.5 小时出证！LangGraph 官方课程 🆓 重磅上线🔥🔥🔥_哔哩哔哩_bilibili</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/07/15/%E5%AD%A6%E4%B9%A0/ai%E6%A1%86%E6%9E%B6/langgraph%E6%9F%A5%E6%BC%8F%E8%A1%A5%E7%BC%BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/07/15/%E5%AD%A6%E4%B9%A0/ai%E6%A1%86%E6%9E%B6/langgraph%E6%9F%A5%E6%BC%8F%E8%A1%A5%E7%BC%BA/" class="post-title-link" itemprop="url">langgraph查漏补缺</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-07-15 00:00:00" itemprop="dateCreated datePublished" datetime="2025-07-15T00:00:00+08:00">2025-07-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-08-04 17:13:58" itemprop="dateModified" datetime="2025-08-04T17:13:58+08:00">2025-08-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ai%E6%A1%86%E6%9E%B6/" itemprop="url" rel="index"><span itemprop="name">ai框架</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ai%E6%A1%86%E6%9E%B6/langgraph/" itemprop="url" rel="index"><span itemprop="name">langgraph</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="注入上下文"><a href="#注入上下文" class="headerlink" title="注入上下文"></a>注入上下文</h3><p>“注入上下文”就是<strong>在运行过程中节点/大模型</strong> 可能需要、但<strong>不会</strong>（也不应该）去改变的<strong>只读信息</strong>的集合。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>场景</th>
<th>注入上下文里可能放什么</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>权限控制</strong></td>
<td><code>user_id</code>, <code>tenant_id</code>（决定能访问哪些数据）</td>
</tr>
<tr>
<td><strong>外部依赖</strong></td>
<td><code>db_connection</code>, <code>api_key</code>, <code>s3_bucket</code>（节点里要用）</td>
</tr>
<tr>
<td><strong>个性化参数</strong></td>
<td><code>language</code>, <code>timezone</code>, <code>model_temperature</code></td>
</tr>
<tr>
<td><strong>会话元信息</strong></td>
<td><code>session_id</code>, <code>channel</code>（Slack / 微信 / Web）</td>
</tr>
</tbody>
</table>
</div>
<h4 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">from dataclasses import dataclass</span><br><span class="line"></span><br><span class="line">from langgraph.graph import StateGraph</span><br><span class="line">from langgraph.runtime import Runtime</span><br><span class="line"></span><br><span class="line">@dataclass</span><br><span class="line">class Context:</span><br><span class="line">    &quot;&quot;&quot;Context schema defined by the developer.&quot;&quot;&quot;    </span><br><span class="line">    user_id: str    </span><br><span class="line">    db_connection: str</span><br><span class="line">    </span><br><span class="line">def node(state: State, runtime: Runtime[Context]):</span><br><span class="line">    # type safe access to context attributes    </span><br><span class="line">    user_id = runtime.context.user_id</span><br><span class="line">    db_conn = runtime.context.db_connection</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">builder = StateGraph(state_schema=State, context_schema=Context)</span><br><span class="line"></span><br><span class="line"># add nodes, edges, compile the graph...</span><br><span class="line"></span><br><span class="line"># top level context arg is typed as Context for autocomplete and type checking</span><br><span class="line">result = graph.invoke(</span><br><span class="line">    &#123;&#x27;input&#x27;: &#x27;abc&#x27;&#125;,</span><br><span class="line">    context=Context(user_id=&#x27;123&#x27;, db_conn=&#x27;conn_mock&#x27;)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="https://langchain-ai.github.io/langgraph/reference/runtime/#runtime"><code>Runtime</code></a> 类提供了一个单一接口，用于访问信息，例如：</p>
<ul>
<li>上下文：在运行开始时传递的静态数据</li>
<li>存储：长期记忆的存储机制</li>
<li>流写入器：用于向图输出流写入的自定义函数</li>
<li>对于功能 API 用户，<code>previous</code> 也可用：给定线程的前一个返回值</li>
</ul>
<p>现在，开发者不再需要将上述所有内容作为单独的参数注入到节点函数中，<br>而是可以通过一个 <code>runtime</code> 参数来访问它们。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/07/14/%E5%AD%A6%E4%B9%A0/ai%E6%A1%86%E6%9E%B6/LangGraphChatBot/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/07/14/%E5%AD%A6%E4%B9%A0/ai%E6%A1%86%E6%9E%B6/LangGraphChatBot/" class="post-title-link" itemprop="url">LangGraphChatBot</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-07-14 00:00:00" itemprop="dateCreated datePublished" datetime="2025-07-14T00:00:00+08:00">2025-07-14</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-07-15 17:54:48" itemprop="dateModified" datetime="2025-07-15T17:54:48+08:00">2025-07-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ai%E6%A1%86%E6%9E%B6/" itemprop="url" rel="index"><span itemprop="name">ai框架</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ai%E6%A1%86%E6%9E%B6/langgraph/" itemprop="url" rel="index"><span itemprop="name">langgraph</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="环境配置">环境配置</h3>
<p>python虚拟环境构建</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m venv .venv</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pip install langgraph==0.2.74                  </span><br><span class="line">pip install langchain-openai==0.3.6            </span><br><span class="line">pip install fastapi==0.115.8                         </span><br><span class="line">pip install uvicorn==0.34.0                          </span><br><span class="line">pip install gradio==5.18.0</span><br></pre></td></tr></table></figure>
<p>查看包<code>pip list</code></p>
<h3 id="构建一个基本的fastapilanggraph应用">构建一个基本的fastapi+langgraph应用</h3>
<h4 id="llm示例的构建利用chatopenai">llm示例的构建（利用ChatOpenAI）</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 创建LLM实例</span><br><span class="line">llm = ChatOpenAI(</span><br><span class="line">    base_url=config[&quot;base_url&quot;],</span><br><span class="line">    api_key=config[&quot;api_key&quot;],</span><br><span class="line">    model=config[&quot;model&quot;],</span><br><span class="line">    temperature=DEFAULT_TEMPERATURE,</span><br><span class="line">    timeout=30,  # 添加超时配置（秒）</span><br><span class="line">    max_retries=2  # 添加重试次数</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h4 id="数据类型的构建">数据类型的构建</h4>
<p>继承于pydantic</p>
<p>规范化 API 请求和响应的数据结构</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义消息类，用于封装API接口返回数据</span></span><br><span class="line"><span class="comment">#基于 Pydantic 的数据模型</span></span><br><span class="line"><span class="comment"># 定义Message类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Message</span>(<span class="title class_ inherited__">BaseModel</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    role (角色): 这是一个字符串，表示消息的发送者。常见的角色包括：</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">- user (用户): 表示用户输入的消息。</span></span><br><span class="line"><span class="string">- assistant (助手): 表示聊天机器人或模型生成的消息。</span></span><br><span class="line"><span class="string">- system (系统): 表示为模型提供上下文或指令的系统消息。</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    role: <span class="built_in">str</span></span><br><span class="line">    content: <span class="built_in">str</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义ChatCompletionRequest类</span></span><br><span class="line"><span class="comment">#聊天 API 请求</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ChatCompletionRequest</span>(<span class="title class_ inherited__">BaseModel</span>):</span><br><span class="line">    messages: <span class="type">List</span>[Message]</span><br><span class="line">    stream: <span class="type">Optional</span>[<span class="built_in">bool</span>] = <span class="literal">False</span><span class="comment">#是否流式方式响应</span></span><br><span class="line">    userId: <span class="type">Optional</span>[<span class="built_in">str</span>] = <span class="literal">None</span><span class="comment">#用于标识发起请求的用户</span></span><br><span class="line">    conversationId: <span class="type">Optional</span>[<span class="built_in">str</span>] = <span class="literal">None</span><span class="comment">#用于标识特定的对话会话，这对于管理对话上下文或历史记录非常有用</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义ChatCompletionResponseChoice类</span></span><br><span class="line"><span class="comment">#聊天完成响应中的一个“选择”或一个生成的回复</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ChatCompletionResponseChoice</span>(<span class="title class_ inherited__">BaseModel</span>):</span><br><span class="line">    index: <span class="built_in">int</span></span><br><span class="line">    message: Message</span><br><span class="line">    finish_reason: <span class="type">Optional</span>[<span class="built_in">str</span>] = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义ChatCompletionResponse类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ChatCompletionResponse</span>(<span class="title class_ inherited__">BaseModel</span>):</span><br><span class="line">    <span class="built_in">id</span>: <span class="built_in">str</span> = Field(default_factory=<span class="keyword">lambda</span>: <span class="string">f&quot;chatcmpl-<span class="subst">&#123;uuid.uuid4().<span class="built_in">hex</span>&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">object</span>: <span class="built_in">str</span> = <span class="string">&quot;chat.completion&quot;</span></span><br><span class="line">    created: <span class="built_in">int</span> = Field(default_factory=<span class="keyword">lambda</span>: <span class="built_in">int</span>(time.time()))</span><br><span class="line">    choices: <span class="type">List</span>[ChatCompletionResponseChoice]<span class="comment">#模型生成的所有可能的回复选项</span></span><br><span class="line">    system_fingerprint: <span class="type">Optional</span>[<span class="built_in">str</span>] = <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<h4 id="定义fastapi应用并管理应用的生命周期">定义fastapi应用并管理应用的生命周期</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义了一个异步函数lifespan，它接收一个FastAPI应用实例app作为参数。这个函数将管理应用的生命周期，包括启动和关闭时的操作</span></span><br><span class="line"><span class="comment"># 函数在应用启动时执行一些初始化操作，如加载上下文数据、以及初始化问题生成器</span></span><br><span class="line"><span class="comment"># 函数在应用关闭时执行一些清理操作</span></span><br><span class="line"><span class="comment"># @asynccontextmanager 装饰器用于创建一个异步上下文管理器，它允许你在 yield 之前和之后执行特定的代码块，分别表示启动和关闭时的操作</span></span><br><span class="line"><span class="meta">@asynccontextmanager</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">lifespan</span>(<span class="params">app: FastAPI</span>):</span><br><span class="line">    <span class="comment"># 启动时执行</span></span><br><span class="line">    <span class="comment"># 申明引用全局变量，在函数中被初始化，并在整个应用中使用</span></span><br><span class="line">    <span class="keyword">global</span> graph</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        logger.info(<span class="string">&quot;正在初始化模型、定义Graph...&quot;</span>)</span><br><span class="line">        <span class="comment">#（1）初始化LLM</span></span><br><span class="line">        llm = get_llm(llm_type)</span><br><span class="line">        <span class="comment">#（2）定义Graph</span></span><br><span class="line">        graph = create_graph(llm)</span><br><span class="line">        <span class="comment">#（3）将Graph可视化图保存</span></span><br><span class="line">        save_graph_visualization(graph)</span><br><span class="line">        logger.info(<span class="string">&quot;初始化完成&quot;</span>)</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        logger.error(<span class="string">f&quot;初始化过程中出错: <span class="subst">&#123;<span class="built_in">str</span>(e)&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="comment"># raise 关键字重新抛出异常，以确保程序不会在错误状态下继续运行</span></span><br><span class="line">        <span class="keyword">raise</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># yield 关键字将控制权交还给FastAPI框架，使应用开始运行</span></span><br><span class="line">    <span class="comment"># 分隔了启动和关闭的逻辑。在yield 之前的代码在应用启动时运行，yield 之后的代码在应用关闭时运行</span></span><br><span class="line">    <span class="keyword">yield</span></span><br><span class="line">    <span class="comment"># 关闭时执行</span></span><br><span class="line">    logger.info(<span class="string">&quot;正在关闭...&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># lifespan参数用于在应用程序生命周期的开始和结束时执行一些初始化或清理工作</span></span><br><span class="line">app = FastAPI(lifespan=lifespan)</span><br></pre></td></tr></table></figure>
<h4 id="langgraph核心逻辑">langgraph核心逻辑</h4>
<p>创建langgraph</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义chatbot的状态</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">State</span>(<span class="title class_ inherited__">TypedDict</span>):</span><br><span class="line">    messages: Annotated[<span class="built_in">list</span>, add_messages]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建和配置chatbot的状态图</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_graph</span>(<span class="params">llm</span>) -&gt; StateGraph:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="comment"># 构建graph</span></span><br><span class="line">        <span class="comment">#创建一个 StateGraph 的实例，并将其配置为使用 State 类作为其状态管理的数据模型</span></span><br><span class="line">        graph_builder = StateGraph(State)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义chatbot的node</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">chatbot</span>(<span class="params">state: State</span>) -&gt; <span class="built_in">dict</span>:</span><br><span class="line">            <span class="comment"># 处理当前状态并返回 LLM 响应</span></span><br><span class="line">            <span class="keyword">return</span> &#123;<span class="string">&quot;messages&quot;</span>: [llm.invoke(state[<span class="string">&quot;messages&quot;</span>])]&#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 配置graph</span></span><br><span class="line">        <span class="comment">#第二个参数 chatbot ：这是一个可调用对象（通常是一个函数或方法），它定义了当执行流程到达这个名为 &quot;chatbot&quot; 的节点时，应该执行什么操作。</span></span><br><span class="line">        graph_builder.add_node(<span class="string">&quot;chatbot&quot;</span>, chatbot)</span><br><span class="line">        graph_builder.add_edge(START, <span class="string">&quot;chatbot&quot;</span>)</span><br><span class="line">        graph_builder.add_edge(<span class="string">&quot;chatbot&quot;</span>, END)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 这里使用内存存储 也可以持久化到数据库</span></span><br><span class="line">        memory = MemorySaver()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 编译生成graph并返回</span></span><br><span class="line">        <span class="comment">#checkpointer 参数将 memory 实例传递给编译过程，使得图能够管理其状态的保存和加载。编译后的图对象被返回，这个对象可以被调用来运行聊天机器人。</span></span><br><span class="line">        <span class="keyword">return</span> graph_builder.<span class="built_in">compile</span>(checkpointer=memory)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        <span class="keyword">raise</span> RuntimeError(<span class="string">f&quot;Failed to create graph: <span class="subst">&#123;<span class="built_in">str</span>(e)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>可视化langgraph节点</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 将构建的graph可视化保存为 PNG 文件</span><br><span class="line">def save_graph_visualization(graph: StateGraph, filename: str = &quot;graph.png&quot;) -&gt; None:</span><br><span class="line">    try:</span><br><span class="line">        with open(filename, &quot;wb&quot;) as f:</span><br><span class="line">            f.write(graph.get_graph().draw_mermaid_png())</span><br><span class="line">        logger.info(f&quot;Graph visualization saved as &#123;filename&#125;&quot;)</span><br><span class="line">    except IOError as e:</span><br><span class="line">        logger.info(f&quot;Warning: Failed to save graph visualization: &#123;str(e)&#125;&quot;)</span><br></pre></td></tr></table></figure>
<h4 id="封装接口">封装接口</h4>
<p>包含流式输出与非流式输出的处理</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 封装POST请求接口，与大模型进行问答</span></span><br><span class="line"><span class="meta">@app.post(<span class="params"><span class="string">&quot;/v1/chat/completions&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">chat_completions</span>(<span class="params">request: ChatCompletionRequest</span>):</span><br><span class="line">    <span class="comment"># 判断初始化是否完成</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> graph:</span><br><span class="line">        logger.error(<span class="string">&quot;服务未初始化&quot;</span>)</span><br><span class="line">        <span class="keyword">raise</span> HTTPException(status_code=<span class="number">500</span>, detail=<span class="string">&quot;服务未初始化&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        logger.info(<span class="string">f&quot;收到聊天完成请求: <span class="subst">&#123;request&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        query_prompt = request.messages[-<span class="number">1</span>].content</span><br><span class="line">        logger.info(<span class="string">f&quot;用户问题是: <span class="subst">&#123;query_prompt&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        config = &#123;<span class="string">&quot;configurable&quot;</span>: &#123;<span class="string">&quot;thread_id&quot;</span>: request.userId+<span class="string">&quot;@@&quot;</span>+request.conversationId&#125;&#125;</span><br><span class="line">        logger.info(<span class="string">f&quot;用户当前会话信息: <span class="subst">&#123;config&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        prompt_template_system = PromptTemplate.from_file(PROMPT_TEMPLATE_TXT_SYS)</span><br><span class="line">        prompt_template_user = PromptTemplate.from_file(PROMPT_TEMPLATE_TXT_USER)</span><br><span class="line">        prompt = [</span><br><span class="line">            &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: prompt_template_system.template&#125;,</span><br><span class="line">            &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: prompt_template_user.template.<span class="built_in">format</span>(query=query_prompt)&#125;</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 处理流式响应</span></span><br><span class="line">        <span class="keyword">if</span> request.stream:</span><br><span class="line">            <span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">generate_stream</span>():</span><br><span class="line">                chunk_id = <span class="string">f&quot;chatcmpl-<span class="subst">&#123;uuid.uuid4().<span class="built_in">hex</span>&#125;</span>&quot;</span></span><br><span class="line">                <span class="keyword">async</span> <span class="keyword">for</span> message_chunk, metadata <span class="keyword">in</span> graph.astream(&#123;<span class="string">&quot;messages&quot;</span>: prompt&#125;, config, stream_mode=<span class="string">&quot;messages&quot;</span>):</span><br><span class="line">                    chunk = message_chunk.content</span><br><span class="line">                    logger.info(<span class="string">f&quot;chunk: <span class="subst">&#123;chunk&#125;</span>&quot;</span>)</span><br><span class="line">                    <span class="comment"># 在处理过程中产生每个块</span></span><br><span class="line">                    <span class="keyword">yield</span> <span class="string">f&quot;data: <span class="subst">&#123;json.dumps(&#123;<span class="string">&#x27;id&#x27;</span>: chunk_id,<span class="string">&#x27;object&#x27;</span>: <span class="string">&#x27;chat.completion.chunk&#x27;</span>,<span class="string">&#x27;created&#x27;</span>: <span class="built_in">int</span>(time.time()),<span class="string">&#x27;choices&#x27;</span>: [&#123;<span class="string">&#x27;index&#x27;</span>: <span class="number">0</span>,<span class="string">&#x27;delta&#x27;</span>: &#123;<span class="string">&#x27;content&#x27;</span>: chunk&#125;</span>,&#x27;finish_reason&#x27;: None&#125;]&#125;)&#125;\n\n&quot;</span></span><br><span class="line">                <span class="comment"># 流结束的最后一块</span></span><br><span class="line">                <span class="keyword">yield</span> <span class="string">f&quot;data: <span class="subst">&#123;json.dumps(&#123;<span class="string">&#x27;id&#x27;</span>: chunk_id,<span class="string">&#x27;object&#x27;</span>: <span class="string">&#x27;chat.completion.chunk&#x27;</span>,<span class="string">&#x27;created&#x27;</span>: <span class="built_in">int</span>(time.time()),<span class="string">&#x27;choices&#x27;</span>: [&#123;<span class="string">&#x27;index&#x27;</span>: <span class="number">0</span>,<span class="string">&#x27;delta&#x27;</span>: &#123;&#125;</span>,&#x27;finish_reason&#x27;: &#x27;stop&#x27;&#125;]&#125;)&#125;\n\n&quot;</span></span><br><span class="line">            <span class="comment"># 返回fastapi.responses中StreamingResponse对象</span></span><br><span class="line">            <span class="keyword">return</span> StreamingResponse(generate_stream(), media_type=<span class="string">&quot;text/event-stream&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 处理非流式响应处理</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                events = graph.stream(&#123;<span class="string">&quot;messages&quot;</span>: prompt&#125;, config)</span><br><span class="line">                <span class="keyword">for</span> event <span class="keyword">in</span> events:</span><br><span class="line">                    <span class="keyword">for</span> value <span class="keyword">in</span> event.values():</span><br><span class="line">                        result = value[<span class="string">&quot;messages&quot;</span>][-<span class="number">1</span>].content</span><br><span class="line">            <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">                logger.info(<span class="string">f&quot;Error processing response: <span class="subst">&#123;<span class="built_in">str</span>(e)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">            formatted_response = <span class="built_in">str</span>(format_response(result))</span><br><span class="line">            logger.info(<span class="string">f&quot;格式化的搜索结果: <span class="subst">&#123;formatted_response&#125;</span>&quot;</span>)</span><br><span class="line">			<span class="comment">#封装响应</span></span><br><span class="line">            response = ChatCompletionResponse(</span><br><span class="line">                choices=[</span><br><span class="line">                    ChatCompletionResponseChoice(</span><br><span class="line">                        index=<span class="number">0</span>,</span><br><span class="line">                        message=Message(role=<span class="string">&quot;assistant&quot;</span>, content=formatted_response),</span><br><span class="line">                        finish_reason=<span class="string">&quot;stop&quot;</span></span><br><span class="line">                    )</span><br><span class="line">                ]</span><br><span class="line">            )</span><br><span class="line">            logger.info(<span class="string">f&quot;发送响应内容: \n<span class="subst">&#123;response&#125;</span>&quot;</span>)</span><br><span class="line">            <span class="comment"># 返回fastapi.responses中JSONResponse对象</span></span><br><span class="line">            <span class="comment"># model_dump()方法通常用于将Pydantic模型实例的内容转换为一个标准的Python字典，以便进行序列化</span></span><br><span class="line">            <span class="keyword">return</span> JSONResponse(content=response.model_dump())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        logger.error(<span class="string">f&quot;处理聊天完成时出错:\n\n <span class="subst">&#123;<span class="built_in">str</span>(e)&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">raise</span> HTTPException(status_code=<span class="number">500</span>, detail=<span class="built_in">str</span>(e))</span><br></pre></td></tr></table></figure>
<h3 id="langgraph的短期记忆与长期记忆">langgraph的短期记忆与长期记忆</h3>
<p>LangGraph支持两种对于构建对话代理至关重要的内存类型：</p>
<ul>
<li><strong><a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langgraph/agents/memory/#short-term-memory">短期内存</a></strong>：通过在会话中维护消息历史来跟踪正在进行的对话。</li>
<li><strong><a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langgraph/agents/memory/#long-term-memory">长期内存</a></strong>：在不同会话之间存储用户特定或应用程序级别的数据。</li>
</ul>
<figure>
<img src="/2025/07/14/%E5%AD%A6%E4%B9%A0/ai%E6%A1%86%E6%9E%B6/LangGraphChatBot/image-20250715094855646.png" alt="image-20250715094855646">
<figcaption aria-hidden="true">image-20250715094855646</figcaption>
</figure>
<p>在LangGraph中</p>
<ul>
<li><em>短期内存</em>也称为<strong>线程级内存</strong>。</li>
<li><em>长期内存</em>也称为<strong>跨线程内存</strong>。</li>
</ul>
<h3 id="教程地址">教程地址</h3>
<p><a target="_blank" rel="noopener" href="https://github.com/NanGePlus/LangGraphChatBot">NanGePlus/LangGraphChatBot:
使用LangGraph+DeepSeek-R1+FastAPI+Gradio实现一个带有记忆功能的流量包推荐智能客服web端用例,同时也支持gpt大模型、国产大模型(OneApi方式)、Ollama本地开源大模型、阿里通义千问大模型</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1m89NYKE2J/?vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">LangGraph+deepseek-r1+FastAPI+Gradio实现拥有记忆的流量包推荐智能客服web端用例,同时也支持gpt、国产大模型、Ollama_哔哩哔哩_bilibili</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/07/13/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/prompt%20Engineering%E4%B8%8Econtext%20Engineering/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/07/13/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/prompt%20Engineering%E4%B8%8Econtext%20Engineering/" class="post-title-link" itemprop="url">prompt Engineering与context Engineering</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2025-07-13 00:00:00 / 修改时间：13:36:33" itemprop="dateCreated datePublished" datetime="2025-07-13T00:00:00+08:00">2025-07-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AE%9E%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">实习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/" itemprop="url" rel="index"><span itemprop="name">晨晟智控</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="prompt-engineering">prompt Engineering</h3>
<p>Prompt
Engineering是与大型语言模型（LLM）交互的基础，其核心在于精心设计输入内容，以引导模型生成期望的输出。</p>
<p>尽管 Prompt Engineering
至关重要，但对于构建稳健、可用于生产环境的系统而言，它存在固有的局限性：</p>
<ul>
<li><p><strong>脆弱性&amp;不可复现性：</strong>
提示中微小的措辞变化可能导致输出结果的巨大差异，使得这一过程更像是一种依赖反复试错的“艺术”，而非可复现的“科学”
。</p></li>
<li><p><strong>扩展性差：</strong>
手动、迭代地优化提示的过程，在面对大量用户、多样化用例和不断出现的边缘情况时，难以有效扩展
。</p></li>
<li><p><strong>用户负担：</strong>
这种方法将精心构建一套详尽指令的负担完全压在了用户身上，对于需要自主运行、或处理高并发请求的系统而言是不切实际的
。</p></li>
<li><p><strong>无状态性：</strong> Prompt Engineering
本质上是为单轮、“一次性”的交互而设计的，难以处理需要记忆和状态管理的长对话或多步骤任务
。</p></li>
</ul>
<h3 id="context-engineering">Context Engineering</h3>
<p><strong>Context
Engineering是一门设计、构建并优化动态自动化系统的学科，旨在为大型语言模型在正确的时间、以正确的格式，提供正确的信息和工具，从而可靠、可扩展地完成复杂任务</strong>
。</p>
<p><strong>prompt 告诉模型如何思考，而 Context
则赋予模型完成工作所需的知识和工具。</strong></p>
<ul>
<li><p>Context Engineering 决定<strong>用什么内容填充 Context
Window</strong> ，</p></li>
<li><p>Prompt Engineering 则负责优化<strong>窗口内的具体指令</strong>
。</p></li>
</ul>
<h3 id="context-engineering-的基石ragretrieval-augmented-generation">Context
Engineering 的基石：RAG（Retrieval-Augmented Generation）</h3>
<p>本部分将阐述检索增强生成（RAG）作为实现 Context Engineering
的主要架构模式。</p>
<h4 id="解决llm的核心弱点">解决LLM的核心弱点</h4>
<p>RAG直接解决了标准LLM在企业应用中存在的固有局限性：</p>
<ul>
<li><p><strong>知识冻结：</strong>
LLM的知识被冻结在<strong>其训练数据的时间点</strong>。RAG通过在推理时注入实时的、最新的信息来解决这个问题
。</p></li>
<li><p><strong>缺乏领域专有知识：</strong>
标准LLM无法访问组织的内部私有数据。RAG则能够将LLM连接到这些内部知识库，如技术手册、政策文件等
。</p></li>
<li><p><strong>幻觉（Hallucination）：</strong> LLM
会不同程度上地编造事实。RAG通过将模型的回答“锚定”在可验证的、检索到的证据上，提高事实的准确性和可信度
。</p></li>
</ul>
<h4 id="rag工作流">RAG工作流</h4>
<ol type="1">
<li><p><strong>索引（离线阶段）：</strong>
在这个阶段，系统会处理外部知识源。文档被加载、分割成更小的
chunks，然后通过Embedding Model
转换为向量表示，并最终存储在专门的向量数据库中以备检索 。</p></li>
<li><p><strong>推理（在线阶段）：</strong>
当用户提出请求时，系统执行以下步骤：</p>
<ol type="1">
<li><strong>检索（Retrieve）：</strong>
将用户的查询同样转换为向量，然后在向量数据库中进行相似性搜索，找出与查询最相关的文档块。</li>
<li><strong>增强（Augment）：</strong>
将检索到的这些文档块与原始的用户查询、系统指令等结合起来，构建一个内容丰富的、增强的最终提示。</li>
<li><strong>生成（Generate）：</strong>
将这个增强后的提示输入给LLM，LLM会基于提供的上下文生成一个有理有据的回答
。</li>
</ol></li>
</ol>
<h3 id="context-工程化如何判断和提取哪些内容应该进入上下文">Context
工程化：如何判断和提取哪些内容应该进入上下文？</h3>
<h4 id="chunking">1.chunking</h4>
<p>文本分块（Chunking）是RAG流程中最关键也最容易被忽视的一步。其目标是创建在语义上自成一体的文本块。</p>
<h4 id="reranking">2.Reranking</h4>
<p>为了平衡检索的速度和准确性，业界普遍采用两阶段检索流程。</p>
<ul>
<li><p><strong>两阶段流程：</strong></p>
<ul>
<li><strong>第一阶段（召回）：</strong>
使用一个快速、高效的检索器（如基于 bi-encoder
的向量搜索或BM25等词法搜索）进行广泛撒网，召回一个较大的候选文档集（例如，前100个）
。</li>
<li><strong>第二阶段（精排/重排序）：</strong>
使用一个更强大但计算成本更高的模型，对这个较小的候选集进行重新评估，以识别出最相关的少数几个文档（例如，前5个）
。</li>
</ul></li>
<li><p><strong>Cross-Encoder：</strong>
交叉编码器之所以在重排序阶段表现优越，是因为它与双编码器的工作方式不同。双编码器独立地为查询和文档生成嵌入向量，然后计算它们的相似度。而交叉编码器则是将查询和文档<strong>同时</strong>作为输入，让模型在内部通过
Attention Mechanism
对二者进行深度交互。这使得模型能够捕捉到更细微的语义关系，从而给出更准确的相关性评分
。</p></li>
<li><p><strong>实际影响：</strong>
重排序显著提高了最终送入LLM的上下文质量，从而产出更准确、幻觉更少的答案。在金融、法律等高风险领域，重排序被认为是必不可少而非可选的步骤
。</p></li>
</ul>
<h4 id="优化上下文窗口压缩与摘要">3.优化上下文窗口：压缩与摘要</h4>
<p>本节详细介绍用于主动管理上下文的技术，确保最有价值的信息被优先呈现。</p>
<ul>
<li><p><strong>上下文压缩的目标：</strong>
缩短检索到的文档列表和/或精简单个文档的内容，只将<strong>最相关的信息传递给LLM</strong>。这能有效降低API调用成本、减少延迟，并缓解
Lost in the Middle 的问题 。</p></li>
<li><p><strong>压缩方法：</strong></p>
<ul>
<li><strong>过滤式压缩：</strong>
这类方法决定是保留还是丢弃整个检索到的文档。
<ul>
<li><strong>LLMChainFilter：</strong>
利用一个LLM对每个文档的相关性做出简单的“是/否”判断 。</li>
<li><strong>EmbeddingsFilter：</strong>
更经济快速的方法，根据文档嵌入与查询嵌入的余弦相似度来过滤文档 。</li>
</ul></li>
<li><strong>内容提取式压缩：</strong> 这类方法会直接修改文档内容。
<ul>
<li><strong>LLMChainExtractor：</strong>
遍历每个文档，并使用LLM从中提取仅与查询相关的句子或陈述 。</li>
</ul></li>
<li><strong>用 top N 代替压缩：</strong>
像LLMListwiseRerank这样的技术，使用LLM对检索到的文档进行重排序，并只返回排名最高的N个，从而起到高质量过滤器的作用
。</li>
</ul></li>
<li><p><strong>作为压缩策略的摘要：</strong>
对于非常长的文档或冗长的对话历史，可以利用LLM生成摘要。这些摘要随后被注入上下文，既保留了关键信息，又大幅减少了
Token 数量。这是在长时程运行的智能体中管理上下文的关键技术 。</p></li>
</ul>
<h3 id="智能体架构中的数据流与工作流编排">智能体架构中的数据流与工作流编排</h3>
<h4 id="工作流workflow-vs.-智能体agent">工作流（Workflow）
vs. 智能体（Agent）</h4>
<ul>
<li><strong>工作流（Workflows）</strong>
<ul>
<li>指的是LLM和工具通过<strong>预定义的代码路径</strong>进行编排的系统。在这种模式下，数据流动的路径是固定的、由开发者明确设计的，类似于上世纪流行的“专家系统”。例如，“第一步：分析用户邮件；第二步：根据分析结果在日历中查找空闲时段；第三步：起草会议邀请邮件”。这种模式确定性高，易于调试和控制，非常适合有明确业务流程的场景（如风控需求高、数据敏感、安全等级要求）。</li>
</ul></li>
<li><strong>智能体（Agents）</strong>
<ul>
<li>指的是LLM<strong>动态地指导</strong>自己的流程和工具使用，自主控制如何完成任务的系统。在这种模式下，数据流动的路径不是预先固定的，而是由LLM在每一步根据当前情况和目标动态决定的。这种模式灵活性高，能处理开放式问题，但可控性和可预测性较低
。</li>
</ul></li>
</ul>
<p>复杂的智能体通常是这两种模式的混合体，在宏观层面遵循一个预定义的工作流，但在某些节点内部，又赋予LLM一定的自主决策权。管理这一切的核心，我们称之为<strong>编排层（Orchestration
Layer）</strong> 。</p>
<h4 id="核心架构预定义数据流的实现"><strong>核心架构：预定义数据流的实现</strong></h4>
<ol type="1">
<li><p><strong>链式工作流（Prompt Chaining）</strong></p></li>
<li><p><strong>路由工作流（Routing)</strong></p></li>
<li><p><strong>编排器-工作者模式（Orchestrator-Workers）</strong></p></li>
</ol>
<h4 id="框架与工具">框架与工具</h4>
<p>上述的架构和机制并非凭空存在，而是通过具体的开发框架实现的。其中，LangGraph作为LangChain的扩展，为构建具有显式数据流的智能体系统提供了强大的工具集。</p>
<p><strong>LangGraph：用图（Graph）定义工作流（Workflow）</strong></p>
<p>LangGraph的核心思想是将智能体应用构建成一个<strong>状态图（State
Graph）</strong>
。这个图由节点和边组成，清晰地定义了数据如何在不同模块间流动</p>
<ul>
<li><strong>状态（State）：</strong>
这是整个图的核心，一个所有节点共享的中央数据对象。
<ul>
<li>你可以把它想象成一个“数据总线”或共享内存。开发者需要预先定义State的结构，每个节点在执行时都可以读取和更新这个State对象
。</li>
</ul></li>
<li><strong>节点（Nodes）：</strong>
代表工作流中的一个计算单元或一个步骤。
<ul>
<li>每个节点通常是一个Python函数，它接收当前的State作为输入，执行特定任务（如调用LLM、执行工具、处理数据），然后返回对State的更新
。</li>
</ul></li>
<li><strong>边（Edges）</strong>：
连接节点，定义了工作流的路径，即数据在State更新后应该流向哪个节点。
<ul>
<li><strong>简单边（Simple Edges）：</strong>
定义了固定的、无条件的流向，用于实现链式工作流 。</li>
<li><strong>条件边（Conditional Edges）：</strong>
用于实现路由逻辑。它会根据一个函数的输出来决定接下来应该走向哪个节点，从而实现流程的分支
。</li>
</ul></li>
<li><strong>检查点（Checkpointer）：</strong>
LangGraph提供了持久化机制，可以在每一步执行后自动保存State的状态。这对于构建需要长期记忆、可中断和恢复、或需要
Human-in-the-Loop 的复杂业务流程至关重要 。</li>
</ul>
<p>复杂业务流程的AI智能体，其核心挑战已从单纯优化信息检索（如RAG）或提示词，转向了对内部<strong>工作流和数据流的精心设计与编排</strong>。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/07/11/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/rag%E8%AF%84%E4%BC%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/07/11/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/rag%E8%AF%84%E4%BC%B0/" class="post-title-link" itemprop="url">rag评估</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-07-11 00:00:00" itemprop="dateCreated datePublished" datetime="2025-07-11T00:00:00+08:00">2025-07-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-07-13 17:07:20" itemprop="dateModified" datetime="2025-07-13T17:07:20+08:00">2025-07-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AE%9E%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">实习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/" itemprop="url" rel="index"><span itemprop="name">晨晟智控</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="rag评估的指标">rag评估的指标</h3>
<h4 id="忠诚度faithfulness">忠诚度Faithfulness</h4>
<p>Faithfulness：衡量生成答案与给定上下文之间的事实一致性。忠实度得分是基于答案和检索到的上下文
计算出来的，答案的评分范围在0到1之间，分数越高越好。</p>
<figure>
<img src="/2025/07/11/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/rag%E8%AF%84%E4%BC%B0/image-20250711154457878.png" alt="image-20250711154457878">
<figcaption aria-hidden="true">image-20250711154457878</figcaption>
</figure>
<p>计算方式：将大模型给出的答案进行切片，检索给出的上下文，计算这些切片是否在上下文中</p>
<figure>
<img src="/2025/07/11/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/rag%E8%AF%84%E4%BC%B0/image-20250711155257239.png" alt="image-20250711155257239">
<figcaption aria-hidden="true">image-20250711155257239</figcaption>
</figure>
<h4 id="答案相关性answerrelevance">答案相关性Answerrelevance</h4>
<p>Answerrelevance：答案相关性的评估指标旨在评估生成的答案与给定提示的相关程度。如果答案不完
整或包含冗余信息，则会被赋予较低的分数。这个指标使用问题和答案来计算，其值介于0到1之间，得
分越高表明答案的相关性越好</p>
<figure>
<img src="/2025/07/11/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/rag%E8%AF%84%E4%BC%B0/image-20250711155128553.png" alt="image-20250711155128553">
<figcaption aria-hidden="true">image-20250711155128553</figcaption>
</figure>
<p>计算方式：根据答案生成多个问题，然后计算生成的答案与原答案的余弦相似度，再取平均</p>
<figure>
<img src="/2025/07/11/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/rag%E8%AF%84%E4%BC%B0/image-20250711155407518.png" alt="image-20250711155407518">
<figcaption aria-hidden="true">image-20250711155407518</figcaption>
</figure>
<h4 id="上下文精确度contextprecision">上下文精确度ContextPrecision</h4>
<p>ContextPrecision：上下文精确度衡量上下文中所有相关的真实信息是否被排在了较高的位置。理想情
况下，所有相关的信息块都应该出现在排名的最前面。这个指标是根据问题和上下文来计算的，数值范
围在0到1之间，分数越高表示精确度越好。 <span class="math display">$$
\text{Context Precision} = \frac{\sum_{k=1}^{K} (\text{rel}(k) \times
\frac{\text{Precision@k}}{\text{Ideal Precision@k}})}{\text{Total
Relevant Documents}}
$$</span></p>
<ul>
<li><code>K</code>：检索返回的文档总数（如 top-5）</li>
<li><code>rel(k)</code>：第 <code>k</code>
个文档是否相关（相关=1，无关=0）</li>
<li><code>Precision@k</code>：前 <code>k</code>
个文档的精确率（相关文档数 / k）</li>
<li><code>Ideal Precision@k</code>：理想情况下前 <code>k</code>
个文档的精确率（假设所有相关文档都排在最前面）</li>
</ul>
<h4 id="上下文召回率contextrecall">上下文召回率ContextRecall</h4>
<p>ContextRecall：用来衡量检索到的上下文与被视为事实真相的标注答案的一致性程度。它根据事实真相
和检索到的上下文来计算，数值范围在0到1之间，数值越高表示性能越好。
为了从事实真相的答案中估计上下文召回率，需要分析答案中的每个句子是否可以归因于检索到的
上下文。在理想情况下，事实真相答案中的所有句子都应该能够对应到检索到的上下文中。
<span class="math display">$$
\text{Context Recall} = \frac{|\{\text{返回的相关文档}\} \cap
\{\text{标准相关文档}\}|}{|\{\text{标准相关文档}\}|}
$$</span> 计算方式：上下文是否包括了标准答案的内容</p>
<h3 id="利用ragas评估rag性能">利用RAGAS评估rag性能</h3>
<p><a target="_blank" rel="noopener" href="https://github.com/zxj-2023/learn-rag-langchain/blob/main/RAGAS-langchian.ipynb">learn-rag-langchain/RAGAS-langchian.ipynb
at main · zxj-2023/learn-rag-langchain</a></p>
<p>检索器 1.Contextprecision(上下文精确度)：评估检索质量。 2.Context
Recall(上下文召回率)：衡量检索的完整性。 生成器
1.Faithfulness(忠实度)：衡量生成答案中的幻觉情况。
2.AnswerRelevance(答案相关性):衡量答案对问题的直接性(紧扣问题的核心)。</p>
<p>最终的RAGAS得分是以上各个指标得分的调和平均值。简而言之，这些指标用来综合评估
-个系统整体的性能。</p>
<h4 id="rag的构建">RAG的构建</h4>
<p>创建RAG文本分割、Embedding model 、 向量库存储Chroma</p>
<p>我们主要使用 <code>RecursiveCharacterTextSplitter</code>
切割文本，通过<code>OpenAIEmbeddings()</code>进行文本编码，存储到
<code>VectorStore</code>。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">from langchain.vectorstores import Chroma</span><br><span class="line">from langchain.embeddings import OpenAIEmbeddings</span><br><span class="line">from langchain.text_splitter import RecursiveCharacterTextSplitter</span><br><span class="line">from langchain_community.embeddings import DashScopeEmbeddings</span><br><span class="line">embeddings_model = DashScopeEmbeddings(</span><br><span class="line">        model=&quot;text-embedding-v2&quot;,</span><br><span class="line">        dashscope_api_key=openai.api_key,</span><br><span class="line">    )</span><br><span class="line">text_splitter = RecursiveCharacterTextSplitter(chunk_size=500)</span><br><span class="line">#进行文本分割，生成更小、更易于处理的文档块</span><br><span class="line">docs = text_splitter.split_documents(paper_docs)</span><br><span class="line"></span><br><span class="line">vectorstore = Chroma.from_documents(docs, embeddings_model)</span><br></pre></td></tr></table></figure>
<p>Chroma
向量数据库默认情况下是内存存储，这意味着数据在程序运行结束后不会保留。
但是，Chroma
也支持持久化存储，您可以指定一个路径将数据保存到磁盘上。这样，即使程序关闭，数据也会被保留，并在下次启动时自动加载。</p>
<h4 id="检索器的构建">检索器的构建</h4>
<p>现在我们可以利用 <code>Chroma</code> 向量库的
<code>.as_retriever()</code> 方式进行检索，需要控制的主要参数为
<code>k</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">base_retriever = vectorstore.as_retriever(search_kwargs=&#123;&quot;k&quot; : 3&#125;)</span><br></pre></td></tr></table></figure>
<ul>
<li>ectorstore.as_retriever() : 这个方法的作用是将一个向量数据库实例（
vectorstore ）转换为 LangChain 中的一个检索器（ Retriever
）对象。检索器是 LangChain
中负责根据用户查询从数据源中获取相关文档的核心组件。</li>
<li>“k” : 这个键表示要检索的“最相似”文档的数量。在这里， “k” : 3
意味着当检索器接收到一个查询时，它将从向量存储中返回与该查询最相似的 3
个文档。这在
RAG（检索增强生成）系统中非常常见，用于限制传递给大型语言模型的上下文信息量，以提高效率和相关性。</li>
</ul>
<p>检索器的作用
检索器（Retriever）是一个核心组件，其主要作用是从一个数据源（如向量数据库、文档加载器等）中根据给定的查询（query）检索出相关的文档或信息。</p>
<h4 id="prompt的构建">prompt的构建</h4>
<p>我们需要利用<code>LLM</code>对<code>Context</code>
生成一系列的问题的<code>answer</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">from langchain import PromptTemplate</span><br><span class="line"></span><br><span class="line">template = &quot;&quot;&quot;You are an assistant for question-answering tasks. </span><br><span class="line">Use the following pieces of retrieved context to answer the question. </span><br><span class="line">If you don&#x27;t know the answer, just say that you don&#x27;t know. </span><br><span class="line"></span><br><span class="line">Question: &#123;question&#125; </span><br><span class="line"></span><br><span class="line">Context: &#123;context&#125; </span><br><span class="line"></span><br><span class="line">Answer:</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">prompt = PromptTemplate(</span><br><span class="line">    template=template, </span><br><span class="line">    input_variables=[&quot;context&quot;,&quot;question&quot;]</span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line">print(prompt)</span><br></pre></td></tr></table></figure>
<h4 id="生成answer利用llm">生成<code>answer</code>,利用LLM</h4>
<p>利用 <code>Runnable</code> 定义一个 <code>chain</code>
实现rag全流程。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">from langchain.schema.runnable import RunnablePassthrough</span><br><span class="line">from langchain.schema.output_parser import StrOutputParser</span><br><span class="line">from langchain_openai import ChatOpenAI</span><br><span class="line">llm = ChatOpenAI(</span><br><span class="line">    model_name=&quot;qwen-plus-2025-04-28&quot;, </span><br><span class="line">    temperature=0,</span><br><span class="line">    api_key=&quot;&quot;,</span><br><span class="line">    base_url=&quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;</span><br><span class="line">    )</span><br><span class="line">#RunnablePassthrough将输入数据原封不动地传递到输出</span><br><span class="line">#StrOutputParser() 它被用作 RAG 链的最后一步，确保最终的答案以字符串形式输出。</span><br><span class="line">rag_chain = (</span><br><span class="line">    &#123;&quot;context&quot;: base_retriever,  &quot;question&quot;: RunnablePassthrough()&#125; </span><br><span class="line">    | prompt </span><br><span class="line">    | llm</span><br><span class="line">    | StrOutputParser() </span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h4 id="创建-ragas-所需的数据">创建 RAGAs 所需的数据</h4>
<p>question Answer contexts ground_truths</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"># Ragas 数据集格式要求  [&#x27;question&#x27;, &#x27;answer&#x27;, &#x27;contexts&#x27;, &#x27;ground_truths&#x27;]</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">&#123;</span><br><span class="line">    &quot;question&quot;: [], &lt;-- 问题基于Context的</span><br><span class="line">    &quot;answer&quot;: [], &lt;-- 答案基于LLM生成的</span><br><span class="line">    &quot;contexts&quot;: [], &lt;-- context</span><br><span class="line">    &quot;ground_truths&quot;: [] &lt;-- 标准答案</span><br><span class="line">&#125;</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line"></span><br><span class="line">from datasets import Dataset</span><br><span class="line">#构建问题与标准答案（黄金数据集）</span><br><span class="line">questions = [&quot;What is faithfulness ?&quot;, </span><br><span class="line">             &quot;How many pages are included in the WikiEval dataset, and which years do they cover information from?&quot;,</span><br><span class="line">             &quot;Why is evaluating Retrieval Augmented Generation (RAG) systems challenging?&quot;,</span><br><span class="line">            ]</span><br><span class="line">ground_truths = [&quot;Faithfulness refers to the idea that the answer should be grounded in the given context.&quot;,</span><br><span class="line">                  &quot; To construct the dataset, we first selected 50 Wikipedia pages covering events that have happened since the start of 2022.&quot;,</span><br><span class="line">                &quot;Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself.&quot;]              </span><br><span class="line">answers = []</span><br><span class="line">contexts = []</span><br><span class="line"></span><br><span class="line"># 生成答案</span><br><span class="line">for query in questions:</span><br><span class="line">    answers.append(rag_chain.invoke(query))</span><br><span class="line">    contexts.append([docs.page_content for docs in base_retriever.get_relevant_documents(query)])</span><br><span class="line"></span><br><span class="line"># 构建数据</span><br><span class="line">data = &#123;</span><br><span class="line">    &quot;user_input&quot;: questions,</span><br><span class="line">    &quot;response&quot;: answers,</span><br><span class="line">    &quot;retrieved_contexts&quot;: contexts,</span><br><span class="line">    &quot;reference&quot;: ground_truths</span><br><span class="line">&#125;</span><br><span class="line">dataset = Dataset.from_dict(data)</span><br></pre></td></tr></table></figure>
<h4 id="使用ragas-进行评估">使用RAGAs 进行评估</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#将评估数据转换成 Ragas 框架专用的格式 。</span><br><span class="line">from ragas import EvaluationDataset</span><br><span class="line">evaluation_dataset = EvaluationDataset.from_list(dataset)</span><br></pre></td></tr></table></figure>
<p>我们可以使用一组常用的RAG评估指标，在收集的数据集上评估我们的RAG系统。您可以选择任何模型作为评估用LLM来进行评估。
ragas默认使用openai的api</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from ragas.llms import LangchainLLMWrapper</span><br><span class="line">evaluator_llm = LangchainLLMWrapper(llm)</span><br></pre></td></tr></table></figure>
<p>调用</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness</span><br><span class="line">from ragas import evaluate</span><br><span class="line">result = evaluate(dataset=evaluation_dataset,metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness()],llm=evaluator_llm)</span><br><span class="line">result</span><br></pre></td></tr></table></figure>
<figure>
<img src="/2025/07/11/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/rag%E8%AF%84%E4%BC%B0/image-20250713164037571.png" alt="image-20250713164037571">
<figcaption aria-hidden="true">image-20250713164037571</figcaption>
</figure>
<h4 id="查看结果">查看结果</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">pd.set_option(&quot;display.max_colwidth&quot;, None)</span><br><span class="line"></span><br><span class="line">df = result.to_pandas()</span><br><span class="line">df</span><br></pre></td></tr></table></figure>
<h3 id="参考资料">参考资料</h3>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/1892529470419736435">RAG系统效果难评？2025年必备的RAG评估框架与工具详解
- 知乎</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Jz421Q7Lw?spm_id_from=333.788.videopod.sections&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">如何利用RAGAs评估RAG系统的好坏_哔哩哔哩_bilibili</a></p>
<p>ragas中文文档<a target="_blank" rel="noopener" href="https://www.aidoczh.com/ragas/getstarted/rag_eval/index.html#want-help-in-improving-your-ai-application-using-evals">Evaluate
a simple RAG - Ragas</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/07/09/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/chunk%E5%88%86%E5%9D%97%E7%AD%96%E7%95%A5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/07/09/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/chunk%E5%88%86%E5%9D%97%E7%AD%96%E7%95%A5/" class="post-title-link" itemprop="url">chunk分块策略</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-07-09 00:00:00" itemprop="dateCreated datePublished" datetime="2025-07-09T00:00:00+08:00">2025-07-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-08-04 15:13:04" itemprop="dateModified" datetime="2025-08-04T15:13:04+08:00">2025-08-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AE%9E%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">实习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/" itemprop="url" rel="index"><span itemprop="name">晨晟智控</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="分块策略"><a href="#分块策略" class="headerlink" title="分块策略"></a>分块策略</h3><p>以下是 RAG 应用程序的典型工作流程：</p>
<p><img src="/2025/07/09/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/chunk%E5%88%86%E5%9D%97%E7%AD%96%E7%95%A5/6878b8fa-5e74-45a1-9a89-5aab92889126_2366x990.gif" alt="6878b8fa-5e74-45a1-9a89-5aab92889126_2366x990"></p>
<p>主流主要有五种分块策略：</p>
<p><img src="https___substack-post-media.s3.amazonaws.com_public_images_92c70184-ba0f-4877-9a55-e4add0e311ad_870x1116.gif" alt="https___substack-post-media.s3.amazonaws.com_public_images_92c70184-ba0f-4877-9a55-e4add0e311ad_870x1116"></p>
<h4 id="Fixed-size-chunking-固定大小的分块"><a href="#Fixed-size-chunking-固定大小的分块" class="headerlink" title="Fixed-size chunking 固定大小的分块"></a>Fixed-size chunking 固定大小的分块</h4><p><img src="/2025/07/09/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/chunk%E5%88%86%E5%9D%97%E7%AD%96%E7%95%A5/98c422a0-f0e2-457c-a256-4476a56a601f_943x232.png" alt="98c422a0-f0e2-457c-a256-4476a56a601f_943x232"></p>
<p>将文本以固定长度分块，overlap为每个块的重合程度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">text = <span class="string">&quot;大家好，我是果粒奶优有果粒，欢迎关注我，让我们一起探索AI。&quot;</span></span><br><span class="line"><span class="keyword">from</span> langchain_text_splitters <span class="keyword">import</span> CharacterTextSplitter</span><br><span class="line"></span><br><span class="line">text_splitter = CharacterTextSplitter(</span><br><span class="line">    separator=<span class="string">&quot;&quot;</span>,<span class="comment">#按字切分</span></span><br><span class="line">    chunk_size=<span class="number">5</span>,</span><br><span class="line">    chunk_overlap=<span class="number">1</span>,</span><br><span class="line">    length_function=<span class="built_in">len</span>,<span class="comment">#以长度计算</span></span><br><span class="line">    is_separator_regex=<span class="literal">False</span>,<span class="comment">#不视为正则表达式</span></span><br><span class="line">)</span><br><span class="line">text_splitter.split_text(text)</span><br></pre></td></tr></table></figure>
<h4 id="Semantic-chunking-语义分块"><a href="#Semantic-chunking-语义分块" class="headerlink" title="Semantic chunking 语义分块"></a>Semantic chunking 语义分块</h4><p><img src="https___substack-post-media.s3.amazonaws.com_public_images_a6ad83a6-2879-4c77-9e49-393f16577aef_1066x288.gif" alt="https___substack-post-media.s3.amazonaws.com_public_images_a6ad83a6-2879-4c77-9e49-393f16577aef_1066x288"></p>
<p>先将文本分段，然后为每个段进行嵌入，若两个段有较高的余弦相似度，则合并成一个块，一直合并到余弦相似度显著下降，再从新的块开始</p>
<p>需要设定阈值来确定余弦相似度是否显著下降，这因文档而异。</p>
<p><img src="/2025/07/09/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/chunk%E5%88%86%E5%9D%97%E7%AD%96%E7%95%A5/image-20250710150106274.png" alt="image-20250710150106274"></p>
<p>具体实现思路：利用滑动窗口，从第一句往后移动滑动窗口，如图，emed1与emed2相差sen3，计算出来的distance决定sen3是否加入chunk1，以此类推</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#利用langchain调用</span></span><br><span class="line"><span class="keyword">from</span> langchain_experimental.text_splitter <span class="keyword">import</span> SemanticChunker</span><br><span class="line"><span class="keyword">from</span> langchain_community.embeddings <span class="keyword">import</span> DashScopeEmbeddings</span><br><span class="line">embeddings_model = DashScopeEmbeddings(</span><br><span class="line">        model=<span class="string">&quot;text-embedding-v2&quot;</span>,</span><br><span class="line">        dashscope_api_key=<span class="string">&quot;&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">semantic_chunk=SemanticChunker(</span><br><span class="line">    embeddings=embeddings_model,<span class="comment">#嵌入模型</span></span><br><span class="line">    breakpoint_threshold_type=<span class="string">&quot;percentile&quot;</span>,<span class="comment">#定义如何计算语义断点阈值</span></span><br><span class="line">    breakpoint_threshold_amount=<span class="number">95</span>,<span class="comment">#设定阈值</span></span><br><span class="line">    <span class="comment">#min_chunk_size=500#限制生成块最小的字符数，避免生成无意义的块</span></span><br><span class="line">    sentence_split_regex=<span class="string">r&#x27;[。！？.\n]&#x27;</span>,<span class="comment">#语句切分</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://developer.aliyun.com/article/1664186">LangChain 搭配 QWen 踩坑-阿里云开发者社区</a></p>
<p>使用OpenAIEmbeddings配置embedding模型，需要设置一个关键参数</p>
<p><code>check_embedding_ctx_length = False</code> 的作用是：</p>
<blockquote>
<p><strong>关闭 langchain_openai 在调用嵌入模型前对输入文本长度的检查与自动截断/分段逻辑。</strong></p>
</blockquote>
<p>但 DashScope 的 <code>text-embedding-v4</code> 接口：</p>
<ul>
<li>对输入格式要求更严格（必须是字符串或字符串列表，不能是分段后的复杂结构）。</li>
<li>不接受 <code>langchain_openai</code> 默认生成的<strong>分段后的列表嵌套结构</strong>。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from langchain_openai import OpenAIEmbeddings,  OpenAI</span><br><span class="line">embeddings = OpenAIEmbeddings(</span><br><span class="line">api_key=&quot;sk-&quot;, </span><br><span class="line">base_url=&quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;,</span><br><span class="line">model=&quot;text-embedding-v4&quot;,</span><br><span class="line">check_embedding_ctx_length = False,</span><br><span class="line">dimensions=1536</span><br><span class="line">)</span><br><span class="line">result=embeddings.embed_query(&quot;Hello, world!&quot;)</span><br><span class="line">print(len(result))</span><br></pre></td></tr></table></figure>
</blockquote>
<p>源代码理解见最后</p>
<h4 id="Recursive-chunking-递归分块"><a href="#Recursive-chunking-递归分块" class="headerlink" title="Recursive chunking 递归分块"></a>Recursive chunking 递归分块</h4><p><img src="/2025/07/09/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/chunk%E5%88%86%E5%9D%97%E7%AD%96%E7%95%A5/f4009caa-34fc-48d6-8102-3d0f6f2c1386_1066x316.gif" alt="f4009caa-34fc-48d6-8102-3d0f6f2c1386_1066x316"></p>
<p>先依据大的段落进行分块，再对每个块进行处理，若符合chunk-size的限制，则不会再分</p>
<p>结果可能如下</p>
<p><img src="/2025/07/09/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/chunk%E5%88%86%E5%9D%97%E7%AD%96%E7%95%A5/b0e40cc1-996f-48f4-9306-781b112536e4_984x428.png" alt="b0e40cc1-996f-48f4-9306-781b112536e4_984x428"></p>
<p>首先，我们定义两个块（紫色的两个段落。接下来，第1段进一步拆分为更小的块。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_text_splitters <span class="keyword">import</span> RecursiveCharacterTextSplitter</span><br><span class="line"></span><br><span class="line">recursive_splitter_chinese = RecursiveCharacterTextSplitter(</span><br><span class="line">    chunk_size=<span class="number">50</span>,</span><br><span class="line">    chunk_overlap=<span class="number">10</span>,</span><br><span class="line">    length_function=<span class="built_in">len</span>,</span><br><span class="line">    separators=[<span class="string">&quot;\n\n&quot;</span>, <span class="string">&quot;。&quot;</span>, <span class="string">&quot;，&quot;</span>, <span class="string">&quot; &quot;</span>, <span class="string">&quot;&quot;</span>]<span class="comment">#中文的分隔符，可以用逗号句号</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h4 id="Document-structure-based-chunking-基于文档结构的分块"><a href="#Document-structure-based-chunking-基于文档结构的分块" class="headerlink" title="Document structure-based chunking 基于文档结构的分块"></a>Document structure-based chunking 基于文档结构的分块</h4><p><img src="/2025/07/09/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/chunk%E5%88%86%E5%9D%97%E7%AD%96%E7%95%A5/e8febecd-ee68-42ff-ab06-41a0a3a43cd3_1102x306.gif" alt="e8febecd-ee68-42ff-ab06-41a0a3a43cd3_1102x306"></p>
<p>根据文档的固有结构进行分块，如markdown的一级标题二级标题等</p>
<p><code>langchain.text_splitter</code>中有两个用于md文档分块的类，<code>MarkdownTextSplitter</code>与<code>MarkdownHeaderTextSplitter</code></p>
<p>二者区别主要在：前者继承于<code>RecursiveCharacterTextSplitter</code>递归分块，它会尝试沿着 Markdown 格式的标题进行分割，但其核心仍然是基于字符的递归分割；后者专注于 基于 Markdown 标题的结构化分割 ，并能将标题信息作为元数据保留，更适合需要保持 Markdown 文档层级结构的应用场景。</p>
<p>需要注意的是<code>MarkdownHeaderTextSplitter</code> 本身不直接提供限制块内容长度的参数，但可以通过与 <code>RecursiveCharacterTextSplitter</code> 等其他文本分割器结合使用来有效控制块的大小。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.text_splitter <span class="keyword">import</span> MarkdownHeaderTextSplitter</span><br><span class="line">headers_to_split_on = [</span><br><span class="line">    (<span class="string">&quot;#&quot;</span>, <span class="string">&quot;Header 1&quot;</span>),</span><br><span class="line">    (<span class="string">&quot;##&quot;</span>, <span class="string">&quot;Header 2&quot;</span>),</span><br><span class="line">    (<span class="string">&quot;###&quot;</span>, <span class="string">&quot;Header 3&quot;</span>),</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on)</span><br><span class="line">md_header_splits = markdown_splitter.split_text(markdown_document)</span><br></pre></td></tr></table></figure>
<p>存储结构类似如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[Document(metadata=&#123;&#x27;Header 1&#x27;: &#x27;Foo&#x27;, &#x27;Header 2&#x27;: &#x27;Bar&#x27;&#125;, page_content=&#x27;Hi this is Jim  \nHi this is Joe&#x27;),</span><br><span class="line"> Document(metadata=&#123;&#x27;Header 1&#x27;: &#x27;Foo&#x27;, &#x27;Header 2&#x27;: &#x27;Bar&#x27;, &#x27;Header 3&#x27;: &#x27;Boo&#x27;&#125;, page_content=&#x27;Hi this is Lance&#x27;),</span><br><span class="line"> Document(metadata=&#123;&#x27;Header 1&#x27;: &#x27;Foo&#x27;, &#x27;Header 2&#x27;: &#x27;Baz&#x27;&#125;, page_content=&#x27;Hi this is Molly&#x27;)]</span><br></pre></td></tr></table></figure>
<h4 id="LLM-based-chunking-基于-LLM-的分块"><a href="#LLM-based-chunking-基于-LLM-的分块" class="headerlink" title="LLM-based chunking 基于 LLM 的分块"></a>LLM-based chunking 基于 LLM 的分块</h4><p><img src="/2025/07/09/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/chunk%E5%88%86%E5%9D%97%E7%AD%96%E7%95%A5/4d1b6d60-8956-4030-8525-d899ee61a9d5_1140x198.gif" alt="4d1b6d60-8956-4030-8525-d899ee61a9d5_1140x198"></p>
<p>利用大模型进行分块</p>
<p>langchain没有提供官方的类实现LLM-based chunking</p>
<p>但是我在找到了别人实现的agentic_chunker<a target="_blank" rel="noopener" href="https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/agentic_chunker.py">RetrievalTutorials/tutorials/LevelsOfTextSplitting/agentic_chunker.py at main · FullStackRetrieval-com/RetrievalTutorials</a>，可供参考</p>
<p>后记：agentic chunk大概的思路为先进行初步分段，按照长度或递归，然后让大模型生成这一段的概要，将段与段合并生成块，但是测试下来，一个文档的内容同质化很严重，基本上都分到一块里了，而且这个主要还是提示词工程，分块并不系统，看个乐吧</p>
<p><a target="_blank" rel="noopener" href="https://github.com/zxj-2023/chunks-strategy-/blob/main/agentic_chunker.py">chunks-strategy-/agentic_chunker.py at main · zxj-2023/chunks-strategy-</a>代码稍作更新，弃用了部分库</p>
<h3 id="embedding"><a href="#embedding" class="headerlink" title="embedding"></a>embedding</h3><p>之前对chunking和embedding的理解不够清晰，chunking是对文本进行分块，由于大多数文本嵌入模型对输入文本长度有严格限制，如果不分块则无法embedding，从而无法更好的进行向量化或者更好地储存在知识库中，提升retriever性能；embedding则是将文本映射到向量空间，为了更好的相似度计算</p>
<h3 id="语义分块的源代码实战"><a href="#语义分块的源代码实战" class="headerlink" title="语义分块的源代码实战"></a>语义分块的源代码实战</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">将文本划分成单句，可以按照标点符号划分</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">single_sentences_list = re.split(<span class="string">r&#x27;(?&lt;=[。！？])&#x27;</span>, essay)</span><br><span class="line"><span class="comment"># 移除可能存在的空字符串</span></span><br><span class="line">single_sentences_list = [s.strip() <span class="keyword">for</span> s <span class="keyword">in</span> single_sentences_list <span class="keyword">if</span> s.strip()]</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">我们需要为单个句子拼接更多的句子，但是 `list` 添加比较困难。因此将其转换为字典列（`List[dict]`）</span></span><br><span class="line"><span class="string">&#123; &#x27;sentence&#x27; : XXX  , &#x27;index&#x27; : 0&#125;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">sentences = [&#123;<span class="string">&#x27;sentence&#x27;</span>: x, <span class="string">&#x27;index&#x27;</span> : i&#125; <span class="keyword">for</span> i, x <span class="keyword">in</span> <span class="built_in">enumerate</span>(single_sentences_list)]</span><br><span class="line"></span><br><span class="line"><span class="comment">#利用滑动窗口分段</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">combine_sentences</span>(<span class="params">sentences, buffer_size=<span class="number">1</span></span>):</span><br><span class="line">    combined_sentences = [</span><br><span class="line">        <span class="string">&#x27; &#x27;</span>.join(sentences[j][<span class="string">&#x27;sentence&#x27;</span>] <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">max</span>(i - buffer_size, <span class="number">0</span>), <span class="built_in">min</span>(i + buffer_size + <span class="number">1</span>, <span class="built_in">len</span>(sentences))))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(sentences))</span><br><span class="line">    ]   </span><br><span class="line">    <span class="comment"># 更新原始字典列表，添加组合后的句子</span></span><br><span class="line">    <span class="keyword">for</span> i, combined_sentence <span class="keyword">in</span> <span class="built_in">enumerate</span>(combined_sentences):</span><br><span class="line">        sentences[i][<span class="string">&#x27;combined_sentence&#x27;</span>] = combined_sentence</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> sentences</span><br><span class="line"></span><br><span class="line">sentences = combine_sentences(sentences)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">接下来使用**embedding model**对**sentences** 进行编码</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">from</span> langchain_community.embeddings <span class="keyword">import</span> DashScopeEmbeddings</span><br><span class="line">embeddings_model = DashScopeEmbeddings(</span><br><span class="line">        model=<span class="string">&quot;text-embedding-v2&quot;</span>,</span><br><span class="line">        dashscope_api_key=<span class="string">&quot;&quot;</span>,</span><br><span class="line"></span><br><span class="line">    )</span><br><span class="line"><span class="comment"># 提取所有组合后的句子用于 embedding</span></span><br><span class="line">combined_sentences_to_embed = [x[<span class="string">&#x27;combined_sentence&#x27;</span>] <span class="keyword">for</span> x <span class="keyword">in</span> sentences]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对句子进行 embedding</span></span><br><span class="line">embeddings = embeddings_model.embed_documents(combined_sentences_to_embed)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;成功对 <span class="subst">&#123;<span class="built_in">len</span>(embeddings)&#125;</span> 个句子进行了 embedding。&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#将embedding添加到sentence中</span></span><br><span class="line"><span class="keyword">for</span> i, sentence <span class="keyword">in</span> <span class="built_in">enumerate</span>(sentences):</span><br><span class="line">    sentence[<span class="string">&#x27;combined_sentence_embedding&#x27;</span>] = embeddings[i]</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">接下来需要根据余弦相似度进行切分</span></span><br><span class="line"><span class="string">通过计算两个向量的夹角余弦值来衡量相似性</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cosine_similarity</span>(<span class="params">vec1, vec2</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Calculate the cosine similarity between two vectors.&quot;&quot;&quot;</span></span><br><span class="line">    dot_product = np.dot(vec1, vec2)</span><br><span class="line">    norm_vec1 = np.linalg.norm(vec1)</span><br><span class="line">    norm_vec2 = np.linalg.norm(vec2)</span><br><span class="line">    <span class="keyword">return</span> dot_product / (norm_vec1 * norm_vec2)</span><br><span class="line"><span class="comment">#遍历，计算余弦相似度</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calculate_cosine_distances</span>(<span class="params">sentences</span>):</span><br><span class="line">    distances = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(sentences) - <span class="number">1</span>):</span><br><span class="line">        embedding_current = sentences[i][<span class="string">&#x27;combined_sentence_embedding&#x27;</span>]</span><br><span class="line">        embedding_next = sentences[i + <span class="number">1</span>][<span class="string">&#x27;combined_sentence_embedding&#x27;</span>]</span><br><span class="line">        <span class="comment"># Calculate cosine similarity</span></span><br><span class="line">        similarity = cosine_similarity(embedding_current, embedding_next)</span><br><span class="line">        <span class="comment"># Convert to cosine distance</span></span><br><span class="line">        distance = <span class="number">1</span> - similarity</span><br><span class="line">        distances.append(distance)</span><br><span class="line">        <span class="comment"># Store distance in the dictionary</span></span><br><span class="line">        sentences[i][<span class="string">&#x27;distance_to_next&#x27;</span>] = distance</span><br><span class="line">    <span class="keyword">return</span> distances, sentences</span><br><span class="line"></span><br><span class="line">distances, sentences = calculate_cosine_distances(sentences)</span><br><span class="line"></span><br><span class="line"><span class="comment">#根据阈值划分</span></span><br><span class="line">breakpoint_percentile_threshold = <span class="number">95</span></span><br><span class="line">breakpoint_distance_threshold = np.percentile(distances, breakpoint_percentile_threshold)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;距离的第95个百分位阈值是:&quot;</span>, breakpoint_distance_threshold)</span><br><span class="line"><span class="comment"># 找到所有距离大于阈值的点的索引，这些索引就是我们的切分点</span></span><br><span class="line">indices_above_thresh = [i <span class="keyword">for</span> i, x <span class="keyword">in</span> <span class="built_in">enumerate</span>(distances) <span class="keyword">if</span> x &gt; breakpoint_distance_threshold]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化块的起始句子索引。我们将根据之前计算出的语义分割点（`indices_above_thresh`）来切分句子列表。</span></span><br><span class="line">start_index = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个列表，用于存储最终组合成的、具有语义连贯性的文本块。</span></span><br><span class="line">chunks = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历所有识别出的语义分割点（这些是句子列表 `sentences` 中的索引）。</span></span><br><span class="line"><span class="keyword">for</span> index <span class="keyword">in</span> indices_above_thresh:</span><br><span class="line">    <span class="comment"># 确定当前文本块的结束点，即当前的分割点索引。</span></span><br><span class="line">    end_index = index</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 从原始句子列表（`sentences`）中切片，提取从上一个分割点到当前分割点之间的所有句子。</span></span><br><span class="line">    <span class="comment"># `end_index + 1` 是为了在切片时包含结束索引指向的那个句子。</span></span><br><span class="line">    group = sentences[start_index:end_index + <span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将切分出的句子组（`group`）中的所有 &#x27;sentence&#x27; 字段的值合并成一个单独的字符串，句子之间用空格隔开。</span></span><br><span class="line">    combined_text = <span class="string">&#x27; &#x27;</span>.join([d[<span class="string">&#x27;sentence&#x27;</span>] <span class="keyword">for</span> d <span class="keyword">in</span> group])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将合并后的文本块添加到 `chunks` 列表中。</span></span><br><span class="line">    chunks.append(combined_text)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 更新下一个文本块的起始索引，设置为当前分割点的下一个位置，为处理下一个块做准备。</span></span><br><span class="line">    start_index = index + <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理最后一个文本块。</span></span><br><span class="line"><span class="comment"># 循环结束后，如果 `start_index` 仍然小于句子总数，说明从最后一个分割点到文本末尾还有剩余的句子。</span></span><br><span class="line"><span class="keyword">if</span> start_index &lt; <span class="built_in">len</span>(sentences):</span><br><span class="line">    <span class="comment"># 将这些剩余的句子合并成最后一个文本块。</span></span><br><span class="line">    combined_text = <span class="string">&#x27; &#x27;</span>.join([d[<span class="string">&#x27;sentence&#x27;</span>] <span class="keyword">for</span> d <span class="keyword">in</span> sentences[start_index:]])</span><br><span class="line">    chunks.append(combined_text)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 此时，`chunks` 列表包含了所有根据语义距离切分和重组后的文本块。</span></span><br><span class="line"><span class="keyword">for</span> i, chunk <span class="keyword">in</span> <span class="built_in">enumerate</span>(chunks):</span><br><span class="line">    buffer = <span class="number">200</span></span><br><span class="line">    <span class="built_in">print</span> (<span class="string">f&quot;Chunk #<span class="subst">&#123;i&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span> (chunk[:buffer].strip())</span><br><span class="line">    <span class="built_in">print</span> (<span class="string">&quot;...&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span> (chunk[-buffer:].strip())</span><br><span class="line">    <span class="built_in">print</span> (<span class="string">&quot;\n&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a target="_blank" rel="noopener" href="https://www.dailydoseofds.com/p/5-chunking-strategies-for-rag/">RAG 的 5 种分块策略 —- 5 Chunking Strategies For RAG</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/wjinjie/article/details/148660229">一文读懂 Qwen3 最新开源的 Embedding 和 Rerank 模型优势！_qwen-rerank-CSDN博客</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1dr421x7Su/?spm_id_from=333.337.search-card.all.click&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">一站帮你选择RAG中的文本切分策略_哔哩哔哩_bilibili</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/The_Thieves/article/details/148747334">LangChain 语义文本拆分指南：基于语义相似度的智能分块技术实战_langchain 语义分割-CSDN博客</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="上一页" aria-label="上一页" href="/page/2/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/13/">13</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/4/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">张熙浚</span>
  </div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="本站访问数 fa fa-user 次"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="本站总访问量 fa fa-eye 次"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script size="400" alpha="0.6" zIndex="-1" src="https://cdnjs.cloudflare.com/ajax/libs/ribbon.js/1.0.2/ribbon.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.1.0/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script src="/js/pjax.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  <script src="/js/third-party/pace.js"></script>


  
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
