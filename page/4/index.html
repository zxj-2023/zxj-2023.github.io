<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha256-XOqroi11tY4EFQMR9ZYwZWKj5ZXiftSx36RRuC3anlA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-bounce.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.20.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="zxj Blogs">
<meta property="og:type" content="website">
<meta property="og:title" content="Zhang XiJun">
<meta property="og:url" content="http://example.com/page/4/index.html">
<meta property="og:site_name" content="Zhang XiJun">
<meta property="og:description" content="zxj Blogs">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="张熙浚">
<meta property="article:tag" content="zxj">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/page/4/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"page/4/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Zhang XiJun</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Zhang XiJun</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">BLOGS</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="张熙浚"
      src="/images/zxjavatar.gif">
  <p class="site-author-name" itemprop="name">张熙浚</p>
  <div class="site-description" itemprop="description">zxj Blogs</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">128</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">49</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">56</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/zxj-2023" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zxj-2023" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="http://wpa.qq.com/msgrd?v=3&uin=2902065320&site=qq&menu=yes" title="QQ → http:&#x2F;&#x2F;wpa.qq.com&#x2F;msgrd?v&#x3D;3&amp;uin&#x3D;2902065320&amp;site&#x3D;qq&amp;menu&#x3D;yes" rel="noopener me" target="_blank"><i class="fab fa-qq fa-fw"></i>QQ</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
    <div class="sidebar-inner sidebar-blogroll">
      <div class="links-of-blogroll animated">
        <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
          链接
        </div>
        <ul class="links-of-blogroll-list">
            <li class="links-of-blogroll-item">
              <a href="https://zxj-2023.github.io/" title="https:&#x2F;&#x2F;zxj-2023.github.io" rel="noopener" target="_blank">Zhang XiJun</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://theme-next.js.org/" title="https:&#x2F;&#x2F;theme-next.js.org" rel="noopener" target="_blank">NexT</a>
            </li>
        </ul>
      </div>
    </div>
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/07/15/%E5%AD%A6%E4%B9%A0/ai%E6%A1%86%E6%9E%B6/langsmith/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/07/15/%E5%AD%A6%E4%B9%A0/ai%E6%A1%86%E6%9E%B6/langsmith/" class="post-title-link" itemprop="url">Langsmith</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2025-07-15 00:00:00 / 修改时间：17:55:00" itemprop="dateCreated datePublished" datetime="2025-07-15T00:00:00+08:00">2025-07-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ai%E6%A1%86%E6%9E%B6/" itemprop="url" rel="index"><span itemprop="name">ai框架</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ai%E6%A1%86%E6%9E%B6/langsmith/" itemprop="url" rel="index"><span itemprop="name">langsmith</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="配置langsmith">配置langsmith</h3>
<h4 id="安装langsmith-sdk">安装LangSmith SDK</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install langsmith</span><br></pre></td></tr></table></figure>
<h4 id="环境变量">环境变量</h4>
<p>获取api<a target="_blank" rel="noopener" href="https://smith.langchain.com/o/56031c2a-c402-41d4-83ed-ed15d0693548/settings/apikeys">LangSmith</a></p>
<p>设置相应的环境变量。这将把跟踪记录到<code>default</code>项目（尽管您可以轻松更改）。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export LANGSMITH_TRACING=true</span><br><span class="line">export LANGSMITH_API_KEY=</span><br><span class="line">export LANGSMITH_PROJECT=default</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">LANGSMITH_TRACING=true</span><br><span class="line">LANGSMITH_ENDPOINT=&quot;https://api.smith.langchain.com&quot;</span><br><span class="line">LANGSMITH_API_KEY=&quot;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&quot;</span><br><span class="line">LANGSMITH_PROJECT=&quot;langgraph&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="资源">资源</h3>
<p>官网<a target="_blank" rel="noopener" href="https://smith.langchain.com/o/56031c2a-c402-41d4-83ed-ed15d0693548/">《LangSmith》
— LangSmith</a></p>
<p>参考文档<a target="_blank" rel="noopener" href="https://langsmith.langchain.ac.cn/">LangSmith 入门 |
🦜️🛠️ LangSmith 文档</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.smith.langchain.com/">Get started with
LangSmith | 🦜️🛠️ LangSmith</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/07/14/%E5%AD%A6%E4%B9%A0/ai%E6%A1%86%E6%9E%B6/LangGraphChatBot/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/07/14/%E5%AD%A6%E4%B9%A0/ai%E6%A1%86%E6%9E%B6/LangGraphChatBot/" class="post-title-link" itemprop="url">LangGraphChatBot</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-07-14 00:00:00" itemprop="dateCreated datePublished" datetime="2025-07-14T00:00:00+08:00">2025-07-14</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-07-15 17:54:48" itemprop="dateModified" datetime="2025-07-15T17:54:48+08:00">2025-07-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ai%E6%A1%86%E6%9E%B6/" itemprop="url" rel="index"><span itemprop="name">ai框架</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ai%E6%A1%86%E6%9E%B6/langgraph/" itemprop="url" rel="index"><span itemprop="name">langgraph</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="环境配置">环境配置</h3>
<p>python虚拟环境构建</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m venv .venv</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pip install langgraph==0.2.74                  </span><br><span class="line">pip install langchain-openai==0.3.6            </span><br><span class="line">pip install fastapi==0.115.8                         </span><br><span class="line">pip install uvicorn==0.34.0                          </span><br><span class="line">pip install gradio==5.18.0</span><br></pre></td></tr></table></figure>
<p>查看包<code>pip list</code></p>
<h3 id="构建一个基本的fastapilanggraph应用">构建一个基本的fastapi+langgraph应用</h3>
<h4 id="llm示例的构建利用chatopenai">llm示例的构建（利用ChatOpenAI）</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 创建LLM实例</span><br><span class="line">llm = ChatOpenAI(</span><br><span class="line">    base_url=config[&quot;base_url&quot;],</span><br><span class="line">    api_key=config[&quot;api_key&quot;],</span><br><span class="line">    model=config[&quot;model&quot;],</span><br><span class="line">    temperature=DEFAULT_TEMPERATURE,</span><br><span class="line">    timeout=30,  # 添加超时配置（秒）</span><br><span class="line">    max_retries=2  # 添加重试次数</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h4 id="数据类型的构建">数据类型的构建</h4>
<p>继承于pydantic</p>
<p>规范化 API 请求和响应的数据结构</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义消息类，用于封装API接口返回数据</span></span><br><span class="line"><span class="comment">#基于 Pydantic 的数据模型</span></span><br><span class="line"><span class="comment"># 定义Message类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Message</span>(<span class="title class_ inherited__">BaseModel</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    role (角色): 这是一个字符串，表示消息的发送者。常见的角色包括：</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">- user (用户): 表示用户输入的消息。</span></span><br><span class="line"><span class="string">- assistant (助手): 表示聊天机器人或模型生成的消息。</span></span><br><span class="line"><span class="string">- system (系统): 表示为模型提供上下文或指令的系统消息。</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    role: <span class="built_in">str</span></span><br><span class="line">    content: <span class="built_in">str</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义ChatCompletionRequest类</span></span><br><span class="line"><span class="comment">#聊天 API 请求</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ChatCompletionRequest</span>(<span class="title class_ inherited__">BaseModel</span>):</span><br><span class="line">    messages: <span class="type">List</span>[Message]</span><br><span class="line">    stream: <span class="type">Optional</span>[<span class="built_in">bool</span>] = <span class="literal">False</span><span class="comment">#是否流式方式响应</span></span><br><span class="line">    userId: <span class="type">Optional</span>[<span class="built_in">str</span>] = <span class="literal">None</span><span class="comment">#用于标识发起请求的用户</span></span><br><span class="line">    conversationId: <span class="type">Optional</span>[<span class="built_in">str</span>] = <span class="literal">None</span><span class="comment">#用于标识特定的对话会话，这对于管理对话上下文或历史记录非常有用</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义ChatCompletionResponseChoice类</span></span><br><span class="line"><span class="comment">#聊天完成响应中的一个“选择”或一个生成的回复</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ChatCompletionResponseChoice</span>(<span class="title class_ inherited__">BaseModel</span>):</span><br><span class="line">    index: <span class="built_in">int</span></span><br><span class="line">    message: Message</span><br><span class="line">    finish_reason: <span class="type">Optional</span>[<span class="built_in">str</span>] = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义ChatCompletionResponse类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ChatCompletionResponse</span>(<span class="title class_ inherited__">BaseModel</span>):</span><br><span class="line">    <span class="built_in">id</span>: <span class="built_in">str</span> = Field(default_factory=<span class="keyword">lambda</span>: <span class="string">f&quot;chatcmpl-<span class="subst">&#123;uuid.uuid4().<span class="built_in">hex</span>&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">object</span>: <span class="built_in">str</span> = <span class="string">&quot;chat.completion&quot;</span></span><br><span class="line">    created: <span class="built_in">int</span> = Field(default_factory=<span class="keyword">lambda</span>: <span class="built_in">int</span>(time.time()))</span><br><span class="line">    choices: <span class="type">List</span>[ChatCompletionResponseChoice]<span class="comment">#模型生成的所有可能的回复选项</span></span><br><span class="line">    system_fingerprint: <span class="type">Optional</span>[<span class="built_in">str</span>] = <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<h4 id="定义fastapi应用并管理应用的生命周期">定义fastapi应用并管理应用的生命周期</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义了一个异步函数lifespan，它接收一个FastAPI应用实例app作为参数。这个函数将管理应用的生命周期，包括启动和关闭时的操作</span></span><br><span class="line"><span class="comment"># 函数在应用启动时执行一些初始化操作，如加载上下文数据、以及初始化问题生成器</span></span><br><span class="line"><span class="comment"># 函数在应用关闭时执行一些清理操作</span></span><br><span class="line"><span class="comment"># @asynccontextmanager 装饰器用于创建一个异步上下文管理器，它允许你在 yield 之前和之后执行特定的代码块，分别表示启动和关闭时的操作</span></span><br><span class="line"><span class="meta">@asynccontextmanager</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">lifespan</span>(<span class="params">app: FastAPI</span>):</span><br><span class="line">    <span class="comment"># 启动时执行</span></span><br><span class="line">    <span class="comment"># 申明引用全局变量，在函数中被初始化，并在整个应用中使用</span></span><br><span class="line">    <span class="keyword">global</span> graph</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        logger.info(<span class="string">&quot;正在初始化模型、定义Graph...&quot;</span>)</span><br><span class="line">        <span class="comment">#（1）初始化LLM</span></span><br><span class="line">        llm = get_llm(llm_type)</span><br><span class="line">        <span class="comment">#（2）定义Graph</span></span><br><span class="line">        graph = create_graph(llm)</span><br><span class="line">        <span class="comment">#（3）将Graph可视化图保存</span></span><br><span class="line">        save_graph_visualization(graph)</span><br><span class="line">        logger.info(<span class="string">&quot;初始化完成&quot;</span>)</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        logger.error(<span class="string">f&quot;初始化过程中出错: <span class="subst">&#123;<span class="built_in">str</span>(e)&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="comment"># raise 关键字重新抛出异常，以确保程序不会在错误状态下继续运行</span></span><br><span class="line">        <span class="keyword">raise</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># yield 关键字将控制权交还给FastAPI框架，使应用开始运行</span></span><br><span class="line">    <span class="comment"># 分隔了启动和关闭的逻辑。在yield 之前的代码在应用启动时运行，yield 之后的代码在应用关闭时运行</span></span><br><span class="line">    <span class="keyword">yield</span></span><br><span class="line">    <span class="comment"># 关闭时执行</span></span><br><span class="line">    logger.info(<span class="string">&quot;正在关闭...&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># lifespan参数用于在应用程序生命周期的开始和结束时执行一些初始化或清理工作</span></span><br><span class="line">app = FastAPI(lifespan=lifespan)</span><br></pre></td></tr></table></figure>
<h4 id="langgraph核心逻辑">langgraph核心逻辑</h4>
<p>创建langgraph</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义chatbot的状态</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">State</span>(<span class="title class_ inherited__">TypedDict</span>):</span><br><span class="line">    messages: Annotated[<span class="built_in">list</span>, add_messages]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建和配置chatbot的状态图</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_graph</span>(<span class="params">llm</span>) -&gt; StateGraph:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="comment"># 构建graph</span></span><br><span class="line">        <span class="comment">#创建一个 StateGraph 的实例，并将其配置为使用 State 类作为其状态管理的数据模型</span></span><br><span class="line">        graph_builder = StateGraph(State)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义chatbot的node</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">chatbot</span>(<span class="params">state: State</span>) -&gt; <span class="built_in">dict</span>:</span><br><span class="line">            <span class="comment"># 处理当前状态并返回 LLM 响应</span></span><br><span class="line">            <span class="keyword">return</span> &#123;<span class="string">&quot;messages&quot;</span>: [llm.invoke(state[<span class="string">&quot;messages&quot;</span>])]&#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 配置graph</span></span><br><span class="line">        <span class="comment">#第二个参数 chatbot ：这是一个可调用对象（通常是一个函数或方法），它定义了当执行流程到达这个名为 &quot;chatbot&quot; 的节点时，应该执行什么操作。</span></span><br><span class="line">        graph_builder.add_node(<span class="string">&quot;chatbot&quot;</span>, chatbot)</span><br><span class="line">        graph_builder.add_edge(START, <span class="string">&quot;chatbot&quot;</span>)</span><br><span class="line">        graph_builder.add_edge(<span class="string">&quot;chatbot&quot;</span>, END)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 这里使用内存存储 也可以持久化到数据库</span></span><br><span class="line">        memory = MemorySaver()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 编译生成graph并返回</span></span><br><span class="line">        <span class="comment">#checkpointer 参数将 memory 实例传递给编译过程，使得图能够管理其状态的保存和加载。编译后的图对象被返回，这个对象可以被调用来运行聊天机器人。</span></span><br><span class="line">        <span class="keyword">return</span> graph_builder.<span class="built_in">compile</span>(checkpointer=memory)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        <span class="keyword">raise</span> RuntimeError(<span class="string">f&quot;Failed to create graph: <span class="subst">&#123;<span class="built_in">str</span>(e)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>可视化langgraph节点</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 将构建的graph可视化保存为 PNG 文件</span><br><span class="line">def save_graph_visualization(graph: StateGraph, filename: str = &quot;graph.png&quot;) -&gt; None:</span><br><span class="line">    try:</span><br><span class="line">        with open(filename, &quot;wb&quot;) as f:</span><br><span class="line">            f.write(graph.get_graph().draw_mermaid_png())</span><br><span class="line">        logger.info(f&quot;Graph visualization saved as &#123;filename&#125;&quot;)</span><br><span class="line">    except IOError as e:</span><br><span class="line">        logger.info(f&quot;Warning: Failed to save graph visualization: &#123;str(e)&#125;&quot;)</span><br></pre></td></tr></table></figure>
<h4 id="封装接口">封装接口</h4>
<p>包含流式输出与非流式输出的处理</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 封装POST请求接口，与大模型进行问答</span></span><br><span class="line"><span class="meta">@app.post(<span class="params"><span class="string">&quot;/v1/chat/completions&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">chat_completions</span>(<span class="params">request: ChatCompletionRequest</span>):</span><br><span class="line">    <span class="comment"># 判断初始化是否完成</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> graph:</span><br><span class="line">        logger.error(<span class="string">&quot;服务未初始化&quot;</span>)</span><br><span class="line">        <span class="keyword">raise</span> HTTPException(status_code=<span class="number">500</span>, detail=<span class="string">&quot;服务未初始化&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        logger.info(<span class="string">f&quot;收到聊天完成请求: <span class="subst">&#123;request&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        query_prompt = request.messages[-<span class="number">1</span>].content</span><br><span class="line">        logger.info(<span class="string">f&quot;用户问题是: <span class="subst">&#123;query_prompt&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        config = &#123;<span class="string">&quot;configurable&quot;</span>: &#123;<span class="string">&quot;thread_id&quot;</span>: request.userId+<span class="string">&quot;@@&quot;</span>+request.conversationId&#125;&#125;</span><br><span class="line">        logger.info(<span class="string">f&quot;用户当前会话信息: <span class="subst">&#123;config&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        prompt_template_system = PromptTemplate.from_file(PROMPT_TEMPLATE_TXT_SYS)</span><br><span class="line">        prompt_template_user = PromptTemplate.from_file(PROMPT_TEMPLATE_TXT_USER)</span><br><span class="line">        prompt = [</span><br><span class="line">            &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: prompt_template_system.template&#125;,</span><br><span class="line">            &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: prompt_template_user.template.<span class="built_in">format</span>(query=query_prompt)&#125;</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 处理流式响应</span></span><br><span class="line">        <span class="keyword">if</span> request.stream:</span><br><span class="line">            <span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">generate_stream</span>():</span><br><span class="line">                chunk_id = <span class="string">f&quot;chatcmpl-<span class="subst">&#123;uuid.uuid4().<span class="built_in">hex</span>&#125;</span>&quot;</span></span><br><span class="line">                <span class="keyword">async</span> <span class="keyword">for</span> message_chunk, metadata <span class="keyword">in</span> graph.astream(&#123;<span class="string">&quot;messages&quot;</span>: prompt&#125;, config, stream_mode=<span class="string">&quot;messages&quot;</span>):</span><br><span class="line">                    chunk = message_chunk.content</span><br><span class="line">                    logger.info(<span class="string">f&quot;chunk: <span class="subst">&#123;chunk&#125;</span>&quot;</span>)</span><br><span class="line">                    <span class="comment"># 在处理过程中产生每个块</span></span><br><span class="line">                    <span class="keyword">yield</span> <span class="string">f&quot;data: <span class="subst">&#123;json.dumps(&#123;<span class="string">&#x27;id&#x27;</span>: chunk_id,<span class="string">&#x27;object&#x27;</span>: <span class="string">&#x27;chat.completion.chunk&#x27;</span>,<span class="string">&#x27;created&#x27;</span>: <span class="built_in">int</span>(time.time()),<span class="string">&#x27;choices&#x27;</span>: [&#123;<span class="string">&#x27;index&#x27;</span>: <span class="number">0</span>,<span class="string">&#x27;delta&#x27;</span>: &#123;<span class="string">&#x27;content&#x27;</span>: chunk&#125;</span>,&#x27;finish_reason&#x27;: None&#125;]&#125;)&#125;\n\n&quot;</span></span><br><span class="line">                <span class="comment"># 流结束的最后一块</span></span><br><span class="line">                <span class="keyword">yield</span> <span class="string">f&quot;data: <span class="subst">&#123;json.dumps(&#123;<span class="string">&#x27;id&#x27;</span>: chunk_id,<span class="string">&#x27;object&#x27;</span>: <span class="string">&#x27;chat.completion.chunk&#x27;</span>,<span class="string">&#x27;created&#x27;</span>: <span class="built_in">int</span>(time.time()),<span class="string">&#x27;choices&#x27;</span>: [&#123;<span class="string">&#x27;index&#x27;</span>: <span class="number">0</span>,<span class="string">&#x27;delta&#x27;</span>: &#123;&#125;</span>,&#x27;finish_reason&#x27;: &#x27;stop&#x27;&#125;]&#125;)&#125;\n\n&quot;</span></span><br><span class="line">            <span class="comment"># 返回fastapi.responses中StreamingResponse对象</span></span><br><span class="line">            <span class="keyword">return</span> StreamingResponse(generate_stream(), media_type=<span class="string">&quot;text/event-stream&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 处理非流式响应处理</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                events = graph.stream(&#123;<span class="string">&quot;messages&quot;</span>: prompt&#125;, config)</span><br><span class="line">                <span class="keyword">for</span> event <span class="keyword">in</span> events:</span><br><span class="line">                    <span class="keyword">for</span> value <span class="keyword">in</span> event.values():</span><br><span class="line">                        result = value[<span class="string">&quot;messages&quot;</span>][-<span class="number">1</span>].content</span><br><span class="line">            <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">                logger.info(<span class="string">f&quot;Error processing response: <span class="subst">&#123;<span class="built_in">str</span>(e)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">            formatted_response = <span class="built_in">str</span>(format_response(result))</span><br><span class="line">            logger.info(<span class="string">f&quot;格式化的搜索结果: <span class="subst">&#123;formatted_response&#125;</span>&quot;</span>)</span><br><span class="line">			<span class="comment">#封装响应</span></span><br><span class="line">            response = ChatCompletionResponse(</span><br><span class="line">                choices=[</span><br><span class="line">                    ChatCompletionResponseChoice(</span><br><span class="line">                        index=<span class="number">0</span>,</span><br><span class="line">                        message=Message(role=<span class="string">&quot;assistant&quot;</span>, content=formatted_response),</span><br><span class="line">                        finish_reason=<span class="string">&quot;stop&quot;</span></span><br><span class="line">                    )</span><br><span class="line">                ]</span><br><span class="line">            )</span><br><span class="line">            logger.info(<span class="string">f&quot;发送响应内容: \n<span class="subst">&#123;response&#125;</span>&quot;</span>)</span><br><span class="line">            <span class="comment"># 返回fastapi.responses中JSONResponse对象</span></span><br><span class="line">            <span class="comment"># model_dump()方法通常用于将Pydantic模型实例的内容转换为一个标准的Python字典，以便进行序列化</span></span><br><span class="line">            <span class="keyword">return</span> JSONResponse(content=response.model_dump())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        logger.error(<span class="string">f&quot;处理聊天完成时出错:\n\n <span class="subst">&#123;<span class="built_in">str</span>(e)&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">raise</span> HTTPException(status_code=<span class="number">500</span>, detail=<span class="built_in">str</span>(e))</span><br></pre></td></tr></table></figure>
<h3 id="langgraph的短期记忆与长期记忆">langgraph的短期记忆与长期记忆</h3>
<p>LangGraph支持两种对于构建对话代理至关重要的内存类型：</p>
<ul>
<li><strong><a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langgraph/agents/memory/#short-term-memory">短期内存</a></strong>：通过在会话中维护消息历史来跟踪正在进行的对话。</li>
<li><strong><a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langgraph/agents/memory/#long-term-memory">长期内存</a></strong>：在不同会话之间存储用户特定或应用程序级别的数据。</li>
</ul>
<figure>
<img src="/2025/07/14/%E5%AD%A6%E4%B9%A0/ai%E6%A1%86%E6%9E%B6/LangGraphChatBot/image-20250715094855646.png" alt="image-20250715094855646">
<figcaption aria-hidden="true">image-20250715094855646</figcaption>
</figure>
<p>在LangGraph中</p>
<ul>
<li><em>短期内存</em>也称为<strong>线程级内存</strong>。</li>
<li><em>长期内存</em>也称为<strong>跨线程内存</strong>。</li>
</ul>
<h3 id="教程地址">教程地址</h3>
<p><a target="_blank" rel="noopener" href="https://github.com/NanGePlus/LangGraphChatBot">NanGePlus/LangGraphChatBot:
使用LangGraph+DeepSeek-R1+FastAPI+Gradio实现一个带有记忆功能的流量包推荐智能客服web端用例,同时也支持gpt大模型、国产大模型(OneApi方式)、Ollama本地开源大模型、阿里通义千问大模型</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1m89NYKE2J/?vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">LangGraph+deepseek-r1+FastAPI+Gradio实现拥有记忆的流量包推荐智能客服web端用例,同时也支持gpt、国产大模型、Ollama_哔哩哔哩_bilibili</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/07/13/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/prompt%20Engineering%E4%B8%8Econtext%20Engineering/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/07/13/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/prompt%20Engineering%E4%B8%8Econtext%20Engineering/" class="post-title-link" itemprop="url">prompt Engineering与context Engineering</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2025-07-13 00:00:00 / 修改时间：13:36:33" itemprop="dateCreated datePublished" datetime="2025-07-13T00:00:00+08:00">2025-07-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AE%9E%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">实习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/" itemprop="url" rel="index"><span itemprop="name">晨晟智控</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="prompt-engineering">prompt Engineering</h3>
<p>Prompt
Engineering是与大型语言模型（LLM）交互的基础，其核心在于精心设计输入内容，以引导模型生成期望的输出。</p>
<p>尽管 Prompt Engineering
至关重要，但对于构建稳健、可用于生产环境的系统而言，它存在固有的局限性：</p>
<ul>
<li><p><strong>脆弱性&amp;不可复现性：</strong>
提示中微小的措辞变化可能导致输出结果的巨大差异，使得这一过程更像是一种依赖反复试错的“艺术”，而非可复现的“科学”
。</p></li>
<li><p><strong>扩展性差：</strong>
手动、迭代地优化提示的过程，在面对大量用户、多样化用例和不断出现的边缘情况时，难以有效扩展
。</p></li>
<li><p><strong>用户负担：</strong>
这种方法将精心构建一套详尽指令的负担完全压在了用户身上，对于需要自主运行、或处理高并发请求的系统而言是不切实际的
。</p></li>
<li><p><strong>无状态性：</strong> Prompt Engineering
本质上是为单轮、“一次性”的交互而设计的，难以处理需要记忆和状态管理的长对话或多步骤任务
。</p></li>
</ul>
<h3 id="context-engineering">Context Engineering</h3>
<p><strong>Context
Engineering是一门设计、构建并优化动态自动化系统的学科，旨在为大型语言模型在正确的时间、以正确的格式，提供正确的信息和工具，从而可靠、可扩展地完成复杂任务</strong>
。</p>
<p><strong>prompt 告诉模型如何思考，而 Context
则赋予模型完成工作所需的知识和工具。</strong></p>
<ul>
<li><p>Context Engineering 决定<strong>用什么内容填充 Context
Window</strong> ，</p></li>
<li><p>Prompt Engineering 则负责优化<strong>窗口内的具体指令</strong>
。</p></li>
</ul>
<h3 id="context-engineering-的基石ragretrieval-augmented-generation">Context
Engineering 的基石：RAG（Retrieval-Augmented Generation）</h3>
<p>本部分将阐述检索增强生成（RAG）作为实现 Context Engineering
的主要架构模式。</p>
<h4 id="解决llm的核心弱点">解决LLM的核心弱点</h4>
<p>RAG直接解决了标准LLM在企业应用中存在的固有局限性：</p>
<ul>
<li><p><strong>知识冻结：</strong>
LLM的知识被冻结在<strong>其训练数据的时间点</strong>。RAG通过在推理时注入实时的、最新的信息来解决这个问题
。</p></li>
<li><p><strong>缺乏领域专有知识：</strong>
标准LLM无法访问组织的内部私有数据。RAG则能够将LLM连接到这些内部知识库，如技术手册、政策文件等
。</p></li>
<li><p><strong>幻觉（Hallucination）：</strong> LLM
会不同程度上地编造事实。RAG通过将模型的回答“锚定”在可验证的、检索到的证据上，提高事实的准确性和可信度
。</p></li>
</ul>
<h4 id="rag工作流">RAG工作流</h4>
<ol type="1">
<li><p><strong>索引（离线阶段）：</strong>
在这个阶段，系统会处理外部知识源。文档被加载、分割成更小的
chunks，然后通过Embedding Model
转换为向量表示，并最终存储在专门的向量数据库中以备检索 。</p></li>
<li><p><strong>推理（在线阶段）：</strong>
当用户提出请求时，系统执行以下步骤：</p>
<ol type="1">
<li><strong>检索（Retrieve）：</strong>
将用户的查询同样转换为向量，然后在向量数据库中进行相似性搜索，找出与查询最相关的文档块。</li>
<li><strong>增强（Augment）：</strong>
将检索到的这些文档块与原始的用户查询、系统指令等结合起来，构建一个内容丰富的、增强的最终提示。</li>
<li><strong>生成（Generate）：</strong>
将这个增强后的提示输入给LLM，LLM会基于提供的上下文生成一个有理有据的回答
。</li>
</ol></li>
</ol>
<h3 id="context-工程化如何判断和提取哪些内容应该进入上下文">Context
工程化：如何判断和提取哪些内容应该进入上下文？</h3>
<h4 id="chunking">1.chunking</h4>
<p>文本分块（Chunking）是RAG流程中最关键也最容易被忽视的一步。其目标是创建在语义上自成一体的文本块。</p>
<h4 id="reranking">2.Reranking</h4>
<p>为了平衡检索的速度和准确性，业界普遍采用两阶段检索流程。</p>
<ul>
<li><p><strong>两阶段流程：</strong></p>
<ul>
<li><strong>第一阶段（召回）：</strong>
使用一个快速、高效的检索器（如基于 bi-encoder
的向量搜索或BM25等词法搜索）进行广泛撒网，召回一个较大的候选文档集（例如，前100个）
。</li>
<li><strong>第二阶段（精排/重排序）：</strong>
使用一个更强大但计算成本更高的模型，对这个较小的候选集进行重新评估，以识别出最相关的少数几个文档（例如，前5个）
。</li>
</ul></li>
<li><p><strong>Cross-Encoder：</strong>
交叉编码器之所以在重排序阶段表现优越，是因为它与双编码器的工作方式不同。双编码器独立地为查询和文档生成嵌入向量，然后计算它们的相似度。而交叉编码器则是将查询和文档<strong>同时</strong>作为输入，让模型在内部通过
Attention Mechanism
对二者进行深度交互。这使得模型能够捕捉到更细微的语义关系，从而给出更准确的相关性评分
。</p></li>
<li><p><strong>实际影响：</strong>
重排序显著提高了最终送入LLM的上下文质量，从而产出更准确、幻觉更少的答案。在金融、法律等高风险领域，重排序被认为是必不可少而非可选的步骤
。</p></li>
</ul>
<h4 id="优化上下文窗口压缩与摘要">3.优化上下文窗口：压缩与摘要</h4>
<p>本节详细介绍用于主动管理上下文的技术，确保最有价值的信息被优先呈现。</p>
<ul>
<li><p><strong>上下文压缩的目标：</strong>
缩短检索到的文档列表和/或精简单个文档的内容，只将<strong>最相关的信息传递给LLM</strong>。这能有效降低API调用成本、减少延迟，并缓解
Lost in the Middle 的问题 。</p></li>
<li><p><strong>压缩方法：</strong></p>
<ul>
<li><strong>过滤式压缩：</strong>
这类方法决定是保留还是丢弃整个检索到的文档。
<ul>
<li><strong>LLMChainFilter：</strong>
利用一个LLM对每个文档的相关性做出简单的“是/否”判断 。</li>
<li><strong>EmbeddingsFilter：</strong>
更经济快速的方法，根据文档嵌入与查询嵌入的余弦相似度来过滤文档 。</li>
</ul></li>
<li><strong>内容提取式压缩：</strong> 这类方法会直接修改文档内容。
<ul>
<li><strong>LLMChainExtractor：</strong>
遍历每个文档，并使用LLM从中提取仅与查询相关的句子或陈述 。</li>
</ul></li>
<li><strong>用 top N 代替压缩：</strong>
像LLMListwiseRerank这样的技术，使用LLM对检索到的文档进行重排序，并只返回排名最高的N个，从而起到高质量过滤器的作用
。</li>
</ul></li>
<li><p><strong>作为压缩策略的摘要：</strong>
对于非常长的文档或冗长的对话历史，可以利用LLM生成摘要。这些摘要随后被注入上下文，既保留了关键信息，又大幅减少了
Token 数量。这是在长时程运行的智能体中管理上下文的关键技术 。</p></li>
</ul>
<h3 id="智能体架构中的数据流与工作流编排">智能体架构中的数据流与工作流编排</h3>
<h4 id="工作流workflow-vs.-智能体agent">工作流（Workflow）
vs. 智能体（Agent）</h4>
<ul>
<li><strong>工作流（Workflows）</strong>
<ul>
<li>指的是LLM和工具通过<strong>预定义的代码路径</strong>进行编排的系统。在这种模式下，数据流动的路径是固定的、由开发者明确设计的，类似于上世纪流行的“专家系统”。例如，“第一步：分析用户邮件；第二步：根据分析结果在日历中查找空闲时段；第三步：起草会议邀请邮件”。这种模式确定性高，易于调试和控制，非常适合有明确业务流程的场景（如风控需求高、数据敏感、安全等级要求）。</li>
</ul></li>
<li><strong>智能体（Agents）</strong>
<ul>
<li>指的是LLM<strong>动态地指导</strong>自己的流程和工具使用，自主控制如何完成任务的系统。在这种模式下，数据流动的路径不是预先固定的，而是由LLM在每一步根据当前情况和目标动态决定的。这种模式灵活性高，能处理开放式问题，但可控性和可预测性较低
。</li>
</ul></li>
</ul>
<p>复杂的智能体通常是这两种模式的混合体，在宏观层面遵循一个预定义的工作流，但在某些节点内部，又赋予LLM一定的自主决策权。管理这一切的核心，我们称之为<strong>编排层（Orchestration
Layer）</strong> 。</p>
<h4 id="核心架构预定义数据流的实现"><strong>核心架构：预定义数据流的实现</strong></h4>
<ol type="1">
<li><p><strong>链式工作流（Prompt Chaining）</strong></p></li>
<li><p><strong>路由工作流（Routing)</strong></p></li>
<li><p><strong>编排器-工作者模式（Orchestrator-Workers）</strong></p></li>
</ol>
<h4 id="框架与工具">框架与工具</h4>
<p>上述的架构和机制并非凭空存在，而是通过具体的开发框架实现的。其中，LangGraph作为LangChain的扩展，为构建具有显式数据流的智能体系统提供了强大的工具集。</p>
<p><strong>LangGraph：用图（Graph）定义工作流（Workflow）</strong></p>
<p>LangGraph的核心思想是将智能体应用构建成一个<strong>状态图（State
Graph）</strong>
。这个图由节点和边组成，清晰地定义了数据如何在不同模块间流动</p>
<ul>
<li><strong>状态（State）：</strong>
这是整个图的核心，一个所有节点共享的中央数据对象。
<ul>
<li>你可以把它想象成一个“数据总线”或共享内存。开发者需要预先定义State的结构，每个节点在执行时都可以读取和更新这个State对象
。</li>
</ul></li>
<li><strong>节点（Nodes）：</strong>
代表工作流中的一个计算单元或一个步骤。
<ul>
<li>每个节点通常是一个Python函数，它接收当前的State作为输入，执行特定任务（如调用LLM、执行工具、处理数据），然后返回对State的更新
。</li>
</ul></li>
<li><strong>边（Edges）</strong>：
连接节点，定义了工作流的路径，即数据在State更新后应该流向哪个节点。
<ul>
<li><strong>简单边（Simple Edges）：</strong>
定义了固定的、无条件的流向，用于实现链式工作流 。</li>
<li><strong>条件边（Conditional Edges）：</strong>
用于实现路由逻辑。它会根据一个函数的输出来决定接下来应该走向哪个节点，从而实现流程的分支
。</li>
</ul></li>
<li><strong>检查点（Checkpointer）：</strong>
LangGraph提供了持久化机制，可以在每一步执行后自动保存State的状态。这对于构建需要长期记忆、可中断和恢复、或需要
Human-in-the-Loop 的复杂业务流程至关重要 。</li>
</ul>
<p>复杂业务流程的AI智能体，其核心挑战已从单纯优化信息检索（如RAG）或提示词，转向了对内部<strong>工作流和数据流的精心设计与编排</strong>。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/07/11/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/rag%E8%AF%84%E4%BC%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/07/11/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/rag%E8%AF%84%E4%BC%B0/" class="post-title-link" itemprop="url">rag与检索评估</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-07-11 00:00:00" itemprop="dateCreated datePublished" datetime="2025-07-11T00:00:00+08:00">2025-07-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-08-18 09:54:42" itemprop="dateModified" datetime="2025-08-18T09:54:42+08:00">2025-08-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AE%9E%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">实习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/" itemprop="url" rel="index"><span itemprop="name">晨晟智控</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="rag评估的指标">rag评估的指标</h3>
<h4 id="忠诚度faithfulness">忠诚度Faithfulness</h4>
<p>Faithfulness：衡量生成答案与给定上下文之间的事实一致性。忠实度得分是基于答案和检索到的上下文
计算出来的，答案的评分范围在0到1之间，分数越高越好。</p>
<figure>
<img src="/2025/07/11/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/rag%E8%AF%84%E4%BC%B0/image-20250711154457878.png" alt="image-20250711154457878">
<figcaption aria-hidden="true">image-20250711154457878</figcaption>
</figure>
<p>计算方式：将大模型给出的答案进行切片，检索给出的上下文，计算这些切片是否在上下文中</p>
<figure>
<img src="/2025/07/11/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/rag%E8%AF%84%E4%BC%B0/image-20250711155257239.png" alt="image-20250711155257239">
<figcaption aria-hidden="true">image-20250711155257239</figcaption>
</figure>
<h4 id="答案相关性answerrelevance">答案相关性Answerrelevance</h4>
<p>Answerrelevance：答案相关性的评估指标旨在评估生成的答案与给定提示的相关程度。如果答案不完
整或包含冗余信息，则会被赋予较低的分数。这个指标使用问题和答案来计算，其值介于0到1之间，得
分越高表明答案的相关性越好</p>
<figure>
<img src="/2025/07/11/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/rag%E8%AF%84%E4%BC%B0/image-20250711155128553.png" alt="image-20250711155128553">
<figcaption aria-hidden="true">image-20250711155128553</figcaption>
</figure>
<p>计算方式：根据答案生成多个问题，然后计算生成的答案与原答案的余弦相似度，再取平均</p>
<figure>
<img src="/2025/07/11/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/rag%E8%AF%84%E4%BC%B0/image-20250711155407518.png" alt="image-20250711155407518">
<figcaption aria-hidden="true">image-20250711155407518</figcaption>
</figure>
<h4 id="上下文精确度contextprecision">上下文精确度ContextPrecision</h4>
<p>ContextPrecision：上下文精确度衡量上下文中所有相关的真实信息是否被排在了较高的位置。理想情
况下，所有相关的信息块都应该出现在排名的最前面。这个指标是根据问题和上下文来计算的，数值范
围在0到1之间，分数越高表示精确度越好。 <span class="math display">$$
\text{Context Precision} = \frac{\sum_{k=1}^{K} (\text{rel}(k) \times
\frac{\text{Precision@k}}{\text{Ideal Precision@k}})}{\text{Total
Relevant Documents}}
$$</span></p>
<ul>
<li><code>K</code>：检索返回的文档总数（如 top-5）</li>
<li><code>rel(k)</code>：第 <code>k</code>
个文档是否相关（相关=1，无关=0）</li>
<li><code>Precision@k</code>：前 <code>k</code>
个文档的精确率（相关文档数 / k）</li>
<li><code>Ideal Precision@k</code>：理想情况下前 <code>k</code>
个文档的精确率（假设所有相关文档都排在最前面）</li>
</ul>
<h4 id="上下文召回率contextrecall">上下文召回率ContextRecall</h4>
<p>ContextRecall：用来衡量检索到的上下文与被视为事实真相的标注答案的一致性程度。它根据事实真相
和检索到的上下文来计算，数值范围在0到1之间，数值越高表示性能越好。
为了从事实真相的答案中估计上下文召回率，需要分析答案中的每个句子是否可以归因于检索到的
上下文。在理想情况下，事实真相答案中的所有句子都应该能够对应到检索到的上下文中。
<span class="math display">$$
\text{Context Recall} = \frac{|\{\text{返回的相关文档}\} \cap
\{\text{标准相关文档}\}|}{|\{\text{标准相关文档}\}|}
$$</span> 计算方式：上下文是否包括了标准答案的内容</p>
<h3 id="检索性能的评估">检索性能的评估</h3>
<h4 id="平均倒数排名mean-reciprocal-rank-mrr">平均倒数排名（Mean
Reciprocal Rank, MRR）</h4>
<p><strong>平均倒数排名（Mean Reciprocal Rank, MRR）</strong>
是一种常用于评估信息检索系统、推荐系统或问答系统性能的评价指标。它特别适用于“每个查询只有一个正确答案”或“我们只关心第一个正确结果”的场景。</p>
<ul>
<li><p><strong>倒数排名（Reciprocal Rank,
RR）</strong>：对于一个查询，如果第一个正确答案出现在排序结果的第 $ k $
位，那么它的倒数排名为： <span class="math display">$$
RR = \frac{1}{k}
$$</span> 如果没有正确答案，则 $ RR = 0 $。</p></li>
<li><p><strong>平均倒数排名（MRR）</strong>：对多个查询的倒数排名取平均值：
<span class="math display">$$
MRR = \frac{1}{|Q|} \sum_{i=1}^{|Q|} \frac{1}{\text{rank}_i}
$$</span> 其中：</p>
<ul>
<li>$ |Q| $ 是查询的总数，</li>
<li>$ _i $ 是第 $ i $ 个查询中第一个正确答案的排名（位置）。</li>
</ul></li>
</ul>
<figure>
<img src="/2025/07/11/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/rag%E8%AF%84%E4%BC%B0/image-20250818093714324.png" alt="image-20250818093714324">
<figcaption aria-hidden="true">image-20250818093714324</figcaption>
</figure>
<h4 id="平均精确率均值mean-average-precision-map">平均精确率均值（Mean
Average Precision, MAP）</h4>
<p><strong>MAP（Mean Average Precision）</strong> 是对多个查询或样本的
<strong>平均精确率（Average Precision, AP）</strong>
取平均，用来衡量排序结果的相关性质量。它综合考虑了：</p>
<ul>
<li>排序中相关结果的数量（召回）</li>
<li>相关结果在排序中的位置（越靠前越好）</li>
</ul>
<p><strong>平均精确率（Average Precision, AP）</strong></p>
<p>AP
是对一个查询而言的，衡量该查询下所有相关文档在排序中的整体表现。</p>
<blockquote>
<p><strong>直观理解</strong>：AP
是“在每个相关文档被检索到时”的精确率的平均值。</p>
</blockquote>
<p>公式定义： <span class="math display">$$
AP = \frac{\sum_{k=1}^{n} (P(k) \times
\text{rel}(k))}{\text{总相关文档数}}
$$</span></p>
<p>其中： - $ P(k) $：在第 $ k $ 个位置的精确率（即前 k
个结果中有多少是相关的） - $ (k) $：第 $ k $ 个文档是否相关（1
表示相关，0 表示不相关）</p>
<blockquote>
<p>也就是说，只在相关文档出现的位置计算并累加精确率，最后除以总相关文档数。</p>
</blockquote>
<p><strong>平均精确率均值（MAP）</strong></p>
<p>将所有查询的 AP 求平均：</p>
<p><span class="math display">$$
MAP = \frac{1}{|Q|} \sum_{i=1}^{|Q|} AP_i
$$</span></p>
<p>其中： - $ |Q| $：查询总数 - $ AP_i $：第 $ i $
个查询的平均精确率</p>
<figure>
<img src="/2025/07/11/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/rag%E8%AF%84%E4%BC%B0/image-20250818094149682.png" alt="image-20250818094149682">
<figcaption aria-hidden="true">image-20250818094149682</figcaption>
</figure>
<h4 id="归一化折损累积增益normalized-discounted-cumulative-gain-ndcg">归一化折损累积增益（Normalized
Discounted Cumulative Gain, nDCG）</h4>
<p>nDCG 的核心思想是： 1. <strong>高相关性的文档更有价值</strong> 2.
<strong>排在前面的结果比排在后面的价值更高</strong>（位置越靠前，权重越大）
3. 将系统的得分与“理想排序”对比，进行归一化，便于跨查询比较</p>
<p>1.<strong>累积增益（Cumulative Gain, CG）</strong></p>
<p>CG 是前 $ k $
个结果的相关性评分之和，<strong>不考虑位置</strong>。</p>
<p><span class="math display">$$
CG@k = \sum_{i=1}^{k} rel_i
$$</span></p>
<p>其中 $ rel_i $ 是第 $ i $ 个文档的相关性评分。</p>
<blockquote>
<p>❌ 缺点：CG 不关心排序顺序。无论相关文档排第1还是第10，CG
都一样。</p>
</blockquote>
<ol start="2" type="1">
<li><strong>折损累积增益（Discounted Cumulative Gain,
DCG）</strong></li>
</ol>
<p>DCG 引入“位置折损”：越靠后的结果，其贡献被“打折”。</p>
<p>常用公式（两种形式，第二种更常见）：</p>
<p><span class="math display">$$
DCG@k = \sum_{i=1}^{k} \frac{rel_i}{\log_2(i+1)} \quad \text{或}
$$</span></p>
<p><span class="math display">$$
DCG@k = rel_1 + \sum_{i=2}^{k} \frac{rel_i}{\log_2(i)} \quad
\text{(更常用)}
$$</span></p>
<blockquote>
<p>💡 解释：第1个位置不打折，第2个位置除以 $ _2(2) = 1 $，第3个位置除以
$ _2(3) $，相当于打了约 63% 的折扣。</p>
</blockquote>
<p>这样，<strong>相关文档越早出现，DCG 越高</strong>。</p>
<ol start="3" type="1">
<li><strong>理想折损累积增益（Ideal DCG, IDCG）</strong></li>
</ol>
<p>IDCG
是在<strong>理想排序下</strong>（所有相关文档按相关性从高到低排列）的
DCG 值。</p>
<p><span class="math display"><em>I</em><em>D</em><em>C</em><em>G</em>@<em>k</em> = 将前
<em>k</em> 个最相关文档按最优顺序排列时的
<em>D</em><em>C</em><em>G</em></span></p>
<p>IDCG 是当前查询下 DCG 的理论最大值。</p>
<ol start="4" type="1">
<li><strong>归一化折损累积增益（nDCG@k）</strong></li>
</ol>
<p><span class="math display">$$
nDCG@k = \frac{DCG@k}{IDCG@k}
$$</span></p>
<blockquote>
<p>✅ nDCG 的取值范围是 <span class="math inline">[0,1]</span>： -
1.0：排序完全理想 - 接近 1：排序质量高 - 接近 0：排序很差</p>
</blockquote>
<figure>
<img src="/2025/07/11/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/rag%E8%AF%84%E4%BC%B0/image-20250818095351579.png" alt="image-20250818095351579">
<figcaption aria-hidden="true">image-20250818095351579</figcaption>
</figure>
<h3 id="利用ragas评估rag性能">利用RAGAS评估rag性能</h3>
<p><a target="_blank" rel="noopener" href="https://github.com/zxj-2023/learn-rag-langchain/blob/main/RAGAS-langchian.ipynb">learn-rag-langchain/RAGAS-langchian.ipynb
at main · zxj-2023/learn-rag-langchain</a></p>
<p>检索器 1.Contextprecision(上下文精确度)：评估检索质量。 2.Context
Recall(上下文召回率)：衡量检索的完整性。 生成器
1.Faithfulness(忠实度)：衡量生成答案中的幻觉情况。
2.AnswerRelevance(答案相关性):衡量答案对问题的直接性(紧扣问题的核心)。</p>
<p>最终的RAGAS得分是以上各个指标得分的调和平均值。简而言之，这些指标用来综合评估
-个系统整体的性能。</p>
<h4 id="rag的构建">RAG的构建</h4>
<p>创建RAG文本分割、Embedding model 、 向量库存储Chroma</p>
<p>我们主要使用 <code>RecursiveCharacterTextSplitter</code>
切割文本，通过<code>OpenAIEmbeddings()</code>进行文本编码，存储到
<code>VectorStore</code>。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">from langchain.vectorstores import Chroma</span><br><span class="line">from langchain.embeddings import OpenAIEmbeddings</span><br><span class="line">from langchain.text_splitter import RecursiveCharacterTextSplitter</span><br><span class="line">from langchain_community.embeddings import DashScopeEmbeddings</span><br><span class="line">embeddings_model = DashScopeEmbeddings(</span><br><span class="line">        model=&quot;text-embedding-v2&quot;,</span><br><span class="line">        dashscope_api_key=openai.api_key,</span><br><span class="line">    )</span><br><span class="line">text_splitter = RecursiveCharacterTextSplitter(chunk_size=500)</span><br><span class="line">#进行文本分割，生成更小、更易于处理的文档块</span><br><span class="line">docs = text_splitter.split_documents(paper_docs)</span><br><span class="line"></span><br><span class="line">vectorstore = Chroma.from_documents(docs, embeddings_model)</span><br></pre></td></tr></table></figure>
<p>Chroma
向量数据库默认情况下是内存存储，这意味着数据在程序运行结束后不会保留。
但是，Chroma
也支持持久化存储，您可以指定一个路径将数据保存到磁盘上。这样，即使程序关闭，数据也会被保留，并在下次启动时自动加载。</p>
<h4 id="检索器的构建">检索器的构建</h4>
<p>现在我们可以利用 <code>Chroma</code> 向量库的
<code>.as_retriever()</code> 方式进行检索，需要控制的主要参数为
<code>k</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">base_retriever = vectorstore.as_retriever(search_kwargs=&#123;&quot;k&quot; : 3&#125;)</span><br></pre></td></tr></table></figure>
<ul>
<li>ectorstore.as_retriever() : 这个方法的作用是将一个向量数据库实例（
vectorstore ）转换为 LangChain 中的一个检索器（ Retriever
）对象。检索器是 LangChain
中负责根据用户查询从数据源中获取相关文档的核心组件。</li>
<li>“k” : 这个键表示要检索的“最相似”文档的数量。在这里， “k” : 3
意味着当检索器接收到一个查询时，它将从向量存储中返回与该查询最相似的 3
个文档。这在
RAG（检索增强生成）系统中非常常见，用于限制传递给大型语言模型的上下文信息量，以提高效率和相关性。</li>
</ul>
<p>检索器的作用
检索器（Retriever）是一个核心组件，其主要作用是从一个数据源（如向量数据库、文档加载器等）中根据给定的查询（query）检索出相关的文档或信息。</p>
<h4 id="prompt的构建">prompt的构建</h4>
<p>我们需要利用<code>LLM</code>对<code>Context</code>
生成一系列的问题的<code>answer</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">from langchain import PromptTemplate</span><br><span class="line"></span><br><span class="line">template = &quot;&quot;&quot;You are an assistant for question-answering tasks. </span><br><span class="line">Use the following pieces of retrieved context to answer the question. </span><br><span class="line">If you don&#x27;t know the answer, just say that you don&#x27;t know. </span><br><span class="line"></span><br><span class="line">Question: &#123;question&#125; </span><br><span class="line"></span><br><span class="line">Context: &#123;context&#125; </span><br><span class="line"></span><br><span class="line">Answer:</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">prompt = PromptTemplate(</span><br><span class="line">    template=template, </span><br><span class="line">    input_variables=[&quot;context&quot;,&quot;question&quot;]</span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line">print(prompt)</span><br></pre></td></tr></table></figure>
<h4 id="生成answer利用llm">生成<code>answer</code>,利用LLM</h4>
<p>利用 <code>Runnable</code> 定义一个 <code>chain</code>
实现rag全流程。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">from langchain.schema.runnable import RunnablePassthrough</span><br><span class="line">from langchain.schema.output_parser import StrOutputParser</span><br><span class="line">from langchain_openai import ChatOpenAI</span><br><span class="line">llm = ChatOpenAI(</span><br><span class="line">    model_name=&quot;qwen-plus-2025-04-28&quot;, </span><br><span class="line">    temperature=0,</span><br><span class="line">    api_key=&quot;&quot;,</span><br><span class="line">    base_url=&quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;</span><br><span class="line">    )</span><br><span class="line">#RunnablePassthrough将输入数据原封不动地传递到输出</span><br><span class="line">#StrOutputParser() 它被用作 RAG 链的最后一步，确保最终的答案以字符串形式输出。</span><br><span class="line">rag_chain = (</span><br><span class="line">    &#123;&quot;context&quot;: base_retriever,  &quot;question&quot;: RunnablePassthrough()&#125; </span><br><span class="line">    | prompt </span><br><span class="line">    | llm</span><br><span class="line">    | StrOutputParser() </span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h4 id="创建-ragas-所需的数据">创建 RAGAs 所需的数据</h4>
<p>question Answer contexts ground_truths</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"># Ragas 数据集格式要求  [&#x27;question&#x27;, &#x27;answer&#x27;, &#x27;contexts&#x27;, &#x27;ground_truths&#x27;]</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">&#123;</span><br><span class="line">    &quot;question&quot;: [], &lt;-- 问题基于Context的</span><br><span class="line">    &quot;answer&quot;: [], &lt;-- 答案基于LLM生成的</span><br><span class="line">    &quot;contexts&quot;: [], &lt;-- context</span><br><span class="line">    &quot;ground_truths&quot;: [] &lt;-- 标准答案</span><br><span class="line">&#125;</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line"></span><br><span class="line">from datasets import Dataset</span><br><span class="line">#构建问题与标准答案（黄金数据集）</span><br><span class="line">questions = [&quot;What is faithfulness ?&quot;, </span><br><span class="line">             &quot;How many pages are included in the WikiEval dataset, and which years do they cover information from?&quot;,</span><br><span class="line">             &quot;Why is evaluating Retrieval Augmented Generation (RAG) systems challenging?&quot;,</span><br><span class="line">            ]</span><br><span class="line">ground_truths = [&quot;Faithfulness refers to the idea that the answer should be grounded in the given context.&quot;,</span><br><span class="line">                  &quot; To construct the dataset, we first selected 50 Wikipedia pages covering events that have happened since the start of 2022.&quot;,</span><br><span class="line">                &quot;Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself.&quot;]              </span><br><span class="line">answers = []</span><br><span class="line">contexts = []</span><br><span class="line"></span><br><span class="line"># 生成答案</span><br><span class="line">for query in questions:</span><br><span class="line">    answers.append(rag_chain.invoke(query))</span><br><span class="line">    contexts.append([docs.page_content for docs in base_retriever.get_relevant_documents(query)])</span><br><span class="line"></span><br><span class="line"># 构建数据</span><br><span class="line">data = &#123;</span><br><span class="line">    &quot;user_input&quot;: questions,</span><br><span class="line">    &quot;response&quot;: answers,</span><br><span class="line">    &quot;retrieved_contexts&quot;: contexts,</span><br><span class="line">    &quot;reference&quot;: ground_truths</span><br><span class="line">&#125;</span><br><span class="line">dataset = Dataset.from_dict(data)</span><br></pre></td></tr></table></figure>
<h4 id="使用ragas-进行评估">使用RAGAs 进行评估</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#将评估数据转换成 Ragas 框架专用的格式 。</span><br><span class="line">from ragas import EvaluationDataset</span><br><span class="line">evaluation_dataset = EvaluationDataset.from_list(dataset)</span><br></pre></td></tr></table></figure>
<p>我们可以使用一组常用的RAG评估指标，在收集的数据集上评估我们的RAG系统。您可以选择任何模型作为评估用LLM来进行评估。
ragas默认使用openai的api</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from ragas.llms import LangchainLLMWrapper</span><br><span class="line">evaluator_llm = LangchainLLMWrapper(llm)</span><br></pre></td></tr></table></figure>
<p>调用</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness</span><br><span class="line">from ragas import evaluate</span><br><span class="line">result = evaluate(dataset=evaluation_dataset,metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness()],llm=evaluator_llm)</span><br><span class="line">result</span><br></pre></td></tr></table></figure>
<figure>
<img src="/2025/07/11/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/rag%E8%AF%84%E4%BC%B0/image-20250713164037571.png" alt="image-20250713164037571">
<figcaption aria-hidden="true">image-20250713164037571</figcaption>
</figure>
<h4 id="查看结果">查看结果</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">pd.set_option(&quot;display.max_colwidth&quot;, None)</span><br><span class="line"></span><br><span class="line">df = result.to_pandas()</span><br><span class="line">df</span><br></pre></td></tr></table></figure>
<h3 id="参考资料">参考资料</h3>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/1892529470419736435">RAG系统效果难评？2025年必备的RAG评估框架与工具详解
- 知乎</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Jz421Q7Lw?spm_id_from=333.788.videopod.sections&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">如何利用RAGAs评估RAG系统的好坏_哔哩哔哩_bilibili</a></p>
<p>ragas中文文档<a target="_blank" rel="noopener" href="https://www.aidoczh.com/ragas/getstarted/rag_eval/index.html#want-help-in-improving-your-ai-application-using-evals">Evaluate
a simple RAG - Ragas</a></p>
<p><a target="_blank" rel="noopener" href="https://segmentfault.com/a/1190000045262257#item-4-5">人工智能 -
RAG系统的7个检索指标：信息检索任务准确性评估指南 - deephub -
SegmentFault 思否</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/07/09/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/chunk%E5%88%86%E5%9D%97%E7%AD%96%E7%95%A5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/07/09/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/chunk%E5%88%86%E5%9D%97%E7%AD%96%E7%95%A5/" class="post-title-link" itemprop="url">chunk分块策略</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-07-09 00:00:00" itemprop="dateCreated datePublished" datetime="2025-07-09T00:00:00+08:00">2025-07-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-08-04 15:13:04" itemprop="dateModified" datetime="2025-08-04T15:13:04+08:00">2025-08-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AE%9E%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">实习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/" itemprop="url" rel="index"><span itemprop="name">晨晟智控</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="分块策略">分块策略</h3>
<p>以下是 RAG 应用程序的典型工作流程：</p>
<figure>
<img src="/2025/07/09/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/chunk%E5%88%86%E5%9D%97%E7%AD%96%E7%95%A5/6878b8fa-5e74-45a1-9a89-5aab92889126_2366x990.gif" alt="6878b8fa-5e74-45a1-9a89-5aab92889126_2366x990">
<figcaption aria-hidden="true">6878b8fa-5e74-45a1-9a89-5aab92889126_2366x990</figcaption>
</figure>
<p>主流主要有五种分块策略：</p>
<figure>
<img src="https___substack-post-media.s3.amazonaws.com_public_images_92c70184-ba0f-4877-9a55-e4add0e311ad_870x1116.gif" alt="https___substack-post-media.s3.amazonaws.com_public_images_92c70184-ba0f-4877-9a55-e4add0e311ad_870x1116">
<figcaption aria-hidden="true">https___substack-post-media.s3.amazonaws.com_public_images_92c70184-ba0f-4877-9a55-e4add0e311ad_870x1116</figcaption>
</figure>
<h4 id="fixed-size-chunking-固定大小的分块">Fixed-size chunking
固定大小的分块</h4>
<figure>
<img src="/2025/07/09/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/chunk%E5%88%86%E5%9D%97%E7%AD%96%E7%95%A5/98c422a0-f0e2-457c-a256-4476a56a601f_943x232.png" alt="98c422a0-f0e2-457c-a256-4476a56a601f_943x232">
<figcaption aria-hidden="true">98c422a0-f0e2-457c-a256-4476a56a601f_943x232</figcaption>
</figure>
<p>将文本以固定长度分块，overlap为每个块的重合程度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">text = <span class="string">&quot;大家好，我是果粒奶优有果粒，欢迎关注我，让我们一起探索AI。&quot;</span></span><br><span class="line"><span class="keyword">from</span> langchain_text_splitters <span class="keyword">import</span> CharacterTextSplitter</span><br><span class="line"></span><br><span class="line">text_splitter = CharacterTextSplitter(</span><br><span class="line">    separator=<span class="string">&quot;&quot;</span>,<span class="comment">#按字切分</span></span><br><span class="line">    chunk_size=<span class="number">5</span>,</span><br><span class="line">    chunk_overlap=<span class="number">1</span>,</span><br><span class="line">    length_function=<span class="built_in">len</span>,<span class="comment">#以长度计算</span></span><br><span class="line">    is_separator_regex=<span class="literal">False</span>,<span class="comment">#不视为正则表达式</span></span><br><span class="line">)</span><br><span class="line">text_splitter.split_text(text)</span><br></pre></td></tr></table></figure>
<h4 id="semantic-chunking-语义分块">Semantic chunking 语义分块</h4>
<figure>
<img src="https___substack-post-media.s3.amazonaws.com_public_images_a6ad83a6-2879-4c77-9e49-393f16577aef_1066x288.gif" alt="https___substack-post-media.s3.amazonaws.com_public_images_a6ad83a6-2879-4c77-9e49-393f16577aef_1066x288">
<figcaption aria-hidden="true">https___substack-post-media.s3.amazonaws.com_public_images_a6ad83a6-2879-4c77-9e49-393f16577aef_1066x288</figcaption>
</figure>
<p>先将文本分段，然后为每个段进行嵌入，若两个段有较高的余弦相似度，则合并成一个块，一直合并到余弦相似度显著下降，再从新的块开始</p>
<p>需要设定阈值来确定余弦相似度是否显著下降，这因文档而异。</p>
<figure>
<img src="/2025/07/09/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/chunk%E5%88%86%E5%9D%97%E7%AD%96%E7%95%A5/image-20250710150106274.png" alt="image-20250710150106274">
<figcaption aria-hidden="true">image-20250710150106274</figcaption>
</figure>
<p>具体实现思路：利用滑动窗口，从第一句往后移动滑动窗口，如图，emed1与emed2相差sen3，计算出来的distance决定sen3是否加入chunk1，以此类推</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#利用langchain调用</span></span><br><span class="line"><span class="keyword">from</span> langchain_experimental.text_splitter <span class="keyword">import</span> SemanticChunker</span><br><span class="line"><span class="keyword">from</span> langchain_community.embeddings <span class="keyword">import</span> DashScopeEmbeddings</span><br><span class="line">embeddings_model = DashScopeEmbeddings(</span><br><span class="line">        model=<span class="string">&quot;text-embedding-v2&quot;</span>,</span><br><span class="line">        dashscope_api_key=<span class="string">&quot;&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">semantic_chunk=SemanticChunker(</span><br><span class="line">    embeddings=embeddings_model,<span class="comment">#嵌入模型</span></span><br><span class="line">    breakpoint_threshold_type=<span class="string">&quot;percentile&quot;</span>,<span class="comment">#定义如何计算语义断点阈值</span></span><br><span class="line">    breakpoint_threshold_amount=<span class="number">95</span>,<span class="comment">#设定阈值</span></span><br><span class="line">    <span class="comment">#min_chunk_size=500#限制生成块最小的字符数，避免生成无意义的块</span></span><br><span class="line">    sentence_split_regex=<span class="string">r&#x27;[。！？.\n]&#x27;</span>,<span class="comment">#语句切分</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://developer.aliyun.com/article/1664186">LangChain 搭配
QWen 踩坑-阿里云开发者社区</a></p>
<p>使用OpenAIEmbeddings配置embedding模型，需要设置一个关键参数</p>
<p><code>check_embedding_ctx_length = False</code> 的作用是：</p>
<blockquote>
<p><strong>关闭 langchain_openai
在调用嵌入模型前对输入文本长度的检查与自动截断/分段逻辑。</strong></p>
</blockquote>
<p>但 DashScope 的 <code>text-embedding-v4</code> 接口：</p>
<ul>
<li>对输入格式要求更严格（必须是字符串或字符串列表，不能是分段后的复杂结构）。</li>
<li>不接受 <code>langchain_openai</code>
默认生成的<strong>分段后的列表嵌套结构</strong>。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from langchain_openai import OpenAIEmbeddings,  OpenAI</span><br><span class="line">embeddings = OpenAIEmbeddings(</span><br><span class="line">api_key=&quot;sk-&quot;, </span><br><span class="line">base_url=&quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;,</span><br><span class="line">model=&quot;text-embedding-v4&quot;,</span><br><span class="line">check_embedding_ctx_length = False,</span><br><span class="line">dimensions=1536</span><br><span class="line">)</span><br><span class="line">result=embeddings.embed_query(&quot;Hello, world!&quot;)</span><br><span class="line">print(len(result))</span><br></pre></td></tr></table></figure>
</blockquote>
<p>源代码理解见最后</p>
<h4 id="recursive-chunking-递归分块">Recursive chunking 递归分块</h4>
<figure>
<img src="/2025/07/09/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/chunk%E5%88%86%E5%9D%97%E7%AD%96%E7%95%A5/f4009caa-34fc-48d6-8102-3d0f6f2c1386_1066x316.gif" alt="f4009caa-34fc-48d6-8102-3d0f6f2c1386_1066x316">
<figcaption aria-hidden="true">f4009caa-34fc-48d6-8102-3d0f6f2c1386_1066x316</figcaption>
</figure>
<p>先依据大的段落进行分块，再对每个块进行处理，若符合chunk-size的限制，则不会再分</p>
<p>结果可能如下</p>
<figure>
<img src="/2025/07/09/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/chunk%E5%88%86%E5%9D%97%E7%AD%96%E7%95%A5/b0e40cc1-996f-48f4-9306-781b112536e4_984x428.png" alt="b0e40cc1-996f-48f4-9306-781b112536e4_984x428">
<figcaption aria-hidden="true">b0e40cc1-996f-48f4-9306-781b112536e4_984x428</figcaption>
</figure>
<p>首先，我们定义两个块（紫色的两个段落。接下来，第1段进一步拆分为更小的块。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_text_splitters <span class="keyword">import</span> RecursiveCharacterTextSplitter</span><br><span class="line"></span><br><span class="line">recursive_splitter_chinese = RecursiveCharacterTextSplitter(</span><br><span class="line">    chunk_size=<span class="number">50</span>,</span><br><span class="line">    chunk_overlap=<span class="number">10</span>,</span><br><span class="line">    length_function=<span class="built_in">len</span>,</span><br><span class="line">    separators=[<span class="string">&quot;\n\n&quot;</span>, <span class="string">&quot;。&quot;</span>, <span class="string">&quot;，&quot;</span>, <span class="string">&quot; &quot;</span>, <span class="string">&quot;&quot;</span>]<span class="comment">#中文的分隔符，可以用逗号句号</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h4 id="document-structure-based-chunking-基于文档结构的分块">Document
structure-based chunking 基于文档结构的分块</h4>
<figure>
<img src="/2025/07/09/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/chunk%E5%88%86%E5%9D%97%E7%AD%96%E7%95%A5/e8febecd-ee68-42ff-ab06-41a0a3a43cd3_1102x306.gif" alt="e8febecd-ee68-42ff-ab06-41a0a3a43cd3_1102x306">
<figcaption aria-hidden="true">e8febecd-ee68-42ff-ab06-41a0a3a43cd3_1102x306</figcaption>
</figure>
<p>根据文档的固有结构进行分块，如markdown的一级标题二级标题等</p>
<p><code>langchain.text_splitter</code>中有两个用于md文档分块的类，<code>MarkdownTextSplitter</code>与<code>MarkdownHeaderTextSplitter</code></p>
<p>二者区别主要在：前者继承于<code>RecursiveCharacterTextSplitter</code>递归分块，它会尝试沿着
Markdown
格式的标题进行分割，但其核心仍然是基于字符的递归分割；后者专注于 基于
Markdown 标题的结构化分割 ，并能将标题信息作为元数据保留，更适合需要保持
Markdown 文档层级结构的应用场景。</p>
<p>需要注意的是<code>MarkdownHeaderTextSplitter</code>
本身不直接提供限制块内容长度的参数，但可以通过与
<code>RecursiveCharacterTextSplitter</code>
等其他文本分割器结合使用来有效控制块的大小。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.text_splitter <span class="keyword">import</span> MarkdownHeaderTextSplitter</span><br><span class="line">headers_to_split_on = [</span><br><span class="line">    (<span class="string">&quot;#&quot;</span>, <span class="string">&quot;Header 1&quot;</span>),</span><br><span class="line">    (<span class="string">&quot;##&quot;</span>, <span class="string">&quot;Header 2&quot;</span>),</span><br><span class="line">    (<span class="string">&quot;###&quot;</span>, <span class="string">&quot;Header 3&quot;</span>),</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on)</span><br><span class="line">md_header_splits = markdown_splitter.split_text(markdown_document)</span><br></pre></td></tr></table></figure>
<p>存储结构类似如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[Document(metadata=&#123;&#x27;Header 1&#x27;: &#x27;Foo&#x27;, &#x27;Header 2&#x27;: &#x27;Bar&#x27;&#125;, page_content=&#x27;Hi this is Jim  \nHi this is Joe&#x27;),</span><br><span class="line"> Document(metadata=&#123;&#x27;Header 1&#x27;: &#x27;Foo&#x27;, &#x27;Header 2&#x27;: &#x27;Bar&#x27;, &#x27;Header 3&#x27;: &#x27;Boo&#x27;&#125;, page_content=&#x27;Hi this is Lance&#x27;),</span><br><span class="line"> Document(metadata=&#123;&#x27;Header 1&#x27;: &#x27;Foo&#x27;, &#x27;Header 2&#x27;: &#x27;Baz&#x27;&#125;, page_content=&#x27;Hi this is Molly&#x27;)]</span><br></pre></td></tr></table></figure>
<h4 id="llm-based-chunking-基于-llm-的分块">LLM-based chunking 基于 LLM
的分块</h4>
<figure>
<img src="/2025/07/09/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/chunk%E5%88%86%E5%9D%97%E7%AD%96%E7%95%A5/4d1b6d60-8956-4030-8525-d899ee61a9d5_1140x198.gif" alt="4d1b6d60-8956-4030-8525-d899ee61a9d5_1140x198">
<figcaption aria-hidden="true">4d1b6d60-8956-4030-8525-d899ee61a9d5_1140x198</figcaption>
</figure>
<p>利用大模型进行分块</p>
<p>langchain没有提供官方的类实现LLM-based chunking</p>
<p>但是我在找到了别人实现的agentic_chunker<a target="_blank" rel="noopener" href="https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/agentic_chunker.py">RetrievalTutorials/tutorials/LevelsOfTextSplitting/agentic_chunker.py
at main · FullStackRetrieval-com/RetrievalTutorials</a>，可供参考</p>
<p>后记：agentic
chunk大概的思路为先进行初步分段，按照长度或递归，然后让大模型生成这一段的概要，将段与段合并生成块，但是测试下来，一个文档的内容同质化很严重，基本上都分到一块里了，而且这个主要还是提示词工程，分块并不系统，看个乐吧</p>
<p><a target="_blank" rel="noopener" href="https://github.com/zxj-2023/chunks-strategy-/blob/main/agentic_chunker.py">chunks-strategy-/agentic_chunker.py
at main · zxj-2023/chunks-strategy-</a>代码稍作更新，弃用了部分库</p>
<h3 id="embedding">embedding</h3>
<p>之前对chunking和embedding的理解不够清晰，chunking是对文本进行分块，由于大多数文本嵌入模型对输入文本长度有严格限制，如果不分块则无法embedding，从而无法更好的进行向量化或者更好地储存在知识库中，提升retriever性能；embedding则是将文本映射到向量空间，为了更好的相似度计算</p>
<h3 id="语义分块的源代码实战">语义分块的源代码实战</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">将文本划分成单句，可以按照标点符号划分</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">single_sentences_list = re.split(<span class="string">r&#x27;(?&lt;=[。！？])&#x27;</span>, essay)</span><br><span class="line"><span class="comment"># 移除可能存在的空字符串</span></span><br><span class="line">single_sentences_list = [s.strip() <span class="keyword">for</span> s <span class="keyword">in</span> single_sentences_list <span class="keyword">if</span> s.strip()]</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">我们需要为单个句子拼接更多的句子，但是 `list` 添加比较困难。因此将其转换为字典列（`List[dict]`）</span></span><br><span class="line"><span class="string">&#123; &#x27;sentence&#x27; : XXX  , &#x27;index&#x27; : 0&#125;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">sentences = [&#123;<span class="string">&#x27;sentence&#x27;</span>: x, <span class="string">&#x27;index&#x27;</span> : i&#125; <span class="keyword">for</span> i, x <span class="keyword">in</span> <span class="built_in">enumerate</span>(single_sentences_list)]</span><br><span class="line"></span><br><span class="line"><span class="comment">#利用滑动窗口分段</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">combine_sentences</span>(<span class="params">sentences, buffer_size=<span class="number">1</span></span>):</span><br><span class="line">    combined_sentences = [</span><br><span class="line">        <span class="string">&#x27; &#x27;</span>.join(sentences[j][<span class="string">&#x27;sentence&#x27;</span>] <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">max</span>(i - buffer_size, <span class="number">0</span>), <span class="built_in">min</span>(i + buffer_size + <span class="number">1</span>, <span class="built_in">len</span>(sentences))))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(sentences))</span><br><span class="line">    ]   </span><br><span class="line">    <span class="comment"># 更新原始字典列表，添加组合后的句子</span></span><br><span class="line">    <span class="keyword">for</span> i, combined_sentence <span class="keyword">in</span> <span class="built_in">enumerate</span>(combined_sentences):</span><br><span class="line">        sentences[i][<span class="string">&#x27;combined_sentence&#x27;</span>] = combined_sentence</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> sentences</span><br><span class="line"></span><br><span class="line">sentences = combine_sentences(sentences)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">接下来使用**embedding model**对**sentences** 进行编码</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">from</span> langchain_community.embeddings <span class="keyword">import</span> DashScopeEmbeddings</span><br><span class="line">embeddings_model = DashScopeEmbeddings(</span><br><span class="line">        model=<span class="string">&quot;text-embedding-v2&quot;</span>,</span><br><span class="line">        dashscope_api_key=<span class="string">&quot;&quot;</span>,</span><br><span class="line"></span><br><span class="line">    )</span><br><span class="line"><span class="comment"># 提取所有组合后的句子用于 embedding</span></span><br><span class="line">combined_sentences_to_embed = [x[<span class="string">&#x27;combined_sentence&#x27;</span>] <span class="keyword">for</span> x <span class="keyword">in</span> sentences]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对句子进行 embedding</span></span><br><span class="line">embeddings = embeddings_model.embed_documents(combined_sentences_to_embed)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;成功对 <span class="subst">&#123;<span class="built_in">len</span>(embeddings)&#125;</span> 个句子进行了 embedding。&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#将embedding添加到sentence中</span></span><br><span class="line"><span class="keyword">for</span> i, sentence <span class="keyword">in</span> <span class="built_in">enumerate</span>(sentences):</span><br><span class="line">    sentence[<span class="string">&#x27;combined_sentence_embedding&#x27;</span>] = embeddings[i]</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">接下来需要根据余弦相似度进行切分</span></span><br><span class="line"><span class="string">通过计算两个向量的夹角余弦值来衡量相似性</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cosine_similarity</span>(<span class="params">vec1, vec2</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Calculate the cosine similarity between two vectors.&quot;&quot;&quot;</span></span><br><span class="line">    dot_product = np.dot(vec1, vec2)</span><br><span class="line">    norm_vec1 = np.linalg.norm(vec1)</span><br><span class="line">    norm_vec2 = np.linalg.norm(vec2)</span><br><span class="line">    <span class="keyword">return</span> dot_product / (norm_vec1 * norm_vec2)</span><br><span class="line"><span class="comment">#遍历，计算余弦相似度</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calculate_cosine_distances</span>(<span class="params">sentences</span>):</span><br><span class="line">    distances = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(sentences) - <span class="number">1</span>):</span><br><span class="line">        embedding_current = sentences[i][<span class="string">&#x27;combined_sentence_embedding&#x27;</span>]</span><br><span class="line">        embedding_next = sentences[i + <span class="number">1</span>][<span class="string">&#x27;combined_sentence_embedding&#x27;</span>]</span><br><span class="line">        <span class="comment"># Calculate cosine similarity</span></span><br><span class="line">        similarity = cosine_similarity(embedding_current, embedding_next)</span><br><span class="line">        <span class="comment"># Convert to cosine distance</span></span><br><span class="line">        distance = <span class="number">1</span> - similarity</span><br><span class="line">        distances.append(distance)</span><br><span class="line">        <span class="comment"># Store distance in the dictionary</span></span><br><span class="line">        sentences[i][<span class="string">&#x27;distance_to_next&#x27;</span>] = distance</span><br><span class="line">    <span class="keyword">return</span> distances, sentences</span><br><span class="line"></span><br><span class="line">distances, sentences = calculate_cosine_distances(sentences)</span><br><span class="line"></span><br><span class="line"><span class="comment">#根据阈值划分</span></span><br><span class="line">breakpoint_percentile_threshold = <span class="number">95</span></span><br><span class="line">breakpoint_distance_threshold = np.percentile(distances, breakpoint_percentile_threshold)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;距离的第95个百分位阈值是:&quot;</span>, breakpoint_distance_threshold)</span><br><span class="line"><span class="comment"># 找到所有距离大于阈值的点的索引，这些索引就是我们的切分点</span></span><br><span class="line">indices_above_thresh = [i <span class="keyword">for</span> i, x <span class="keyword">in</span> <span class="built_in">enumerate</span>(distances) <span class="keyword">if</span> x &gt; breakpoint_distance_threshold]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化块的起始句子索引。我们将根据之前计算出的语义分割点（`indices_above_thresh`）来切分句子列表。</span></span><br><span class="line">start_index = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个列表，用于存储最终组合成的、具有语义连贯性的文本块。</span></span><br><span class="line">chunks = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历所有识别出的语义分割点（这些是句子列表 `sentences` 中的索引）。</span></span><br><span class="line"><span class="keyword">for</span> index <span class="keyword">in</span> indices_above_thresh:</span><br><span class="line">    <span class="comment"># 确定当前文本块的结束点，即当前的分割点索引。</span></span><br><span class="line">    end_index = index</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 从原始句子列表（`sentences`）中切片，提取从上一个分割点到当前分割点之间的所有句子。</span></span><br><span class="line">    <span class="comment"># `end_index + 1` 是为了在切片时包含结束索引指向的那个句子。</span></span><br><span class="line">    group = sentences[start_index:end_index + <span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将切分出的句子组（`group`）中的所有 &#x27;sentence&#x27; 字段的值合并成一个单独的字符串，句子之间用空格隔开。</span></span><br><span class="line">    combined_text = <span class="string">&#x27; &#x27;</span>.join([d[<span class="string">&#x27;sentence&#x27;</span>] <span class="keyword">for</span> d <span class="keyword">in</span> group])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将合并后的文本块添加到 `chunks` 列表中。</span></span><br><span class="line">    chunks.append(combined_text)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 更新下一个文本块的起始索引，设置为当前分割点的下一个位置，为处理下一个块做准备。</span></span><br><span class="line">    start_index = index + <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理最后一个文本块。</span></span><br><span class="line"><span class="comment"># 循环结束后，如果 `start_index` 仍然小于句子总数，说明从最后一个分割点到文本末尾还有剩余的句子。</span></span><br><span class="line"><span class="keyword">if</span> start_index &lt; <span class="built_in">len</span>(sentences):</span><br><span class="line">    <span class="comment"># 将这些剩余的句子合并成最后一个文本块。</span></span><br><span class="line">    combined_text = <span class="string">&#x27; &#x27;</span>.join([d[<span class="string">&#x27;sentence&#x27;</span>] <span class="keyword">for</span> d <span class="keyword">in</span> sentences[start_index:]])</span><br><span class="line">    chunks.append(combined_text)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 此时，`chunks` 列表包含了所有根据语义距离切分和重组后的文本块。</span></span><br><span class="line"><span class="keyword">for</span> i, chunk <span class="keyword">in</span> <span class="built_in">enumerate</span>(chunks):</span><br><span class="line">    buffer = <span class="number">200</span></span><br><span class="line">    <span class="built_in">print</span> (<span class="string">f&quot;Chunk #<span class="subst">&#123;i&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span> (chunk[:buffer].strip())</span><br><span class="line">    <span class="built_in">print</span> (<span class="string">&quot;...&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span> (chunk[-buffer:].strip())</span><br><span class="line">    <span class="built_in">print</span> (<span class="string">&quot;\n&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="参考资料">参考资料</h3>
<p><a target="_blank" rel="noopener" href="https://www.dailydoseofds.com/p/5-chunking-strategies-for-rag/">RAG
的 5 种分块策略 — 5 Chunking Strategies For RAG</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/wjinjie/article/details/148660229">一文读懂
Qwen3 最新开源的 Embedding 和 Rerank
模型优势！_qwen-rerank-CSDN博客</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1dr421x7Su/?spm_id_from=333.337.search-card.all.click&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">一站帮你选择RAG中的文本切分策略_哔哩哔哩_bilibili</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/The_Thieves/article/details/148747334">LangChain
语义文本拆分指南：基于语义相似度的智能分块技术实战_langchain
语义分割-CSDN博客</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/docker/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/docker/" class="post-title-link" itemprop="url">docker</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-07-08 00:00:00" itemprop="dateCreated datePublished" datetime="2025-07-08T00:00:00+08:00">2025-07-08</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-07-24 16:00:56" itemprop="dateModified" datetime="2025-07-24T16:00:56+08:00">2025-07-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AE%9E%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">实习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/" itemprop="url" rel="index"><span itemprop="name">晨晟智控</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="镜像image">镜像（Image）</h2>
<p>镜像可以被看作是一个轻量级、可执行的独立软件包，包含了运行某个应用所需的所有代码、库、环境变量和配置文件。它是容器的<strong>静态模板</strong>，在创建容器时用作基础。</p>
<p><strong>只读</strong>：镜像本身是只读的，无法修改。</p>
<p><strong>可重用</strong>：镜像是可以多次重用的，你可以基于相同的镜像创建多个容器。</p>
<h2 id="容器container">容器（Container）</h2>
<p>与虚拟机通过操作系统实现隔离不同，容器技术<strong>只隔离应用程序的运行时环境但容器之间可以共享同一个操作系统</strong>，这里的运行时环境指的是程序运行依赖的各种库以及配置。</p>
<p>容器更加的<strong>轻量级且占用的资源更少</strong>，与操作系统动辄几G的内存占用相比，容器技术只需数M空间，因此我们可以在同样规格的硬件上<strong>大量部署容器</strong>，这是虚拟机所不能比拟的，而且不同于操作系统数分钟的启动时间容器几乎瞬时启动，容器技术为<strong>打包服务栈</strong>提供了一种更加高效的方式</p>
<h2 id="镜像与容器的关系">镜像与容器的关系</h2>
<p><strong>镜像是静态的</strong>：它只包含应用和运行环境，不能进行任何运行时的操作。你可以把它看作是软件的<strong>安装包</strong>。</p>
<p><strong>容器是动态的</strong>：它是在镜像的基础上创建的，可以运行、执行代码、修改文件系统等。你可以把它看作是镜像的<strong>运行实例</strong>。</p>
<h2 id="docker">docker</h2>
<p>docker将程序以及程序所有的依赖都打包到<a target="_blank" rel="noopener" href="https://zhida.zhihu.com/search?content_id=129800958&amp;content_type=Article&amp;match_order=1&amp;q=docker+container&amp;zhida_source=entity">docker
container</a>，这样你的程序可以在任何环境都会有一致的表现</p>
<p>此外docker的另一个好处就是<strong>快速部署</strong>，这是当前互联网公司最常见的一个应用场景，一个原因在于容器启动速度非常快，另一个原因在于只要确保一个容器中的程序正确运行，那么你就能确信无论在生产环境部署多少都能正确运行。</p>
<p>每一种容器都是一个完整的运行环境，容器之间互相隔离。</p>
<p>简单来说，docker将程序打包部署，方便了软件的部署，避免了环境冲突等问题</p>
<h2 id="常用命令">常用命令</h2>
<p>查看所有容器（包括停止的容器）：<code>docker ps -a</code></p>
<p>在Docker中运行容器：<code>docker run [OPTIONS] IMAGE [COMMAND] [ARG...]</code></p>
<ul>
<li>•
<code>[OPTIONS]</code>：可选参数，用于配置容器的各种选项，如端口映射、容器名称等。</li>
<li>• <code>IMAGE</code>：要运行的镜像名称或ID。</li>
<li>•
<code>[COMMAND] [ARG...]</code>：可选的命令和参数，用于在容器内执行特定的命令。</li>
</ul>
<p>停止正在运行的容器：<code>docker stop [OPTIONS] CONTAINER [CONTAINER...]</code></p>
<p>启动已停止的容器：<code>docker start [OPTIONS] CONTAINER [CONTAINER...]</code></p>
<p>删除已停止的容器或镜像：<code>docker rm [OPTIONS] CONTAINER [CONTAINER...]   docker rmi [OPTIONS] IMAGE [IMAGE...]</code></p>
<ul>
<li>• <code>docker rm</code>：删除容器的命令。</li>
<li>• <code>docker rmi</code>：删除镜像的命令。</li>
</ul>
<p>从Docker仓库中拉取现有的镜像：<code>docker pull [OPTIONS] NAME[:TAG|@DIGEST]</code></p>
<ul>
<li>• <code>docker pull</code>：拉取镜像的命令。</li>
<li>•
<code>[OPTIONS]</code>：可选参数，用于配置拉取过程，如认证信息等。</li>
<li>•
<code>NAME[:TAG|@DIGEST]</code>：要拉取的镜像名称、标签或摘要。</li>
</ul>
<h2 id="docker部分指令">docker部分指令</h2>
<p><strong>linux安装docker</strong>：<code>sudo apt-get update &amp;&amp; sudo apt-get install docker.io</code></p>
<p><strong>查看 Docker
版本信息</strong>：<code>docker version</code></p>
<p><strong>查看镜像</strong>：<code>docker images</code></p>
<p><strong>查看所有的容器</strong>：<code>docker ps -a</code></p>
<blockquote>
<p><code>systemctl</code> 是 <strong>systemd</strong>
系统和服务管理器的核心工具，用于管理系统和服务的状态及配置。</p>
</blockquote>
<p><code>mysql-client</code> 是 MySQL
数据库的命令行客户端工具。它允许你通过命令行连接和操作 MySQL
数据库服务器，比如执行 SQL 查询、管理数据库和用户等。</p>
<p>常用命令格式如下：<code>mysql -h 主机地址 -P 端口号 -u 用户名 -p</code></p>
<p>你可以在终端输入以下命令来检查是否已安装
<code>mysql-client</code>：<code>mysql --version</code></p>
<p>可以使用以下命令安装：<code>sudo apt-get update  sudo apt-get install mysql-client</code></p>
<blockquote>
<p><code>sudo apt-get update</code>
这个命令的作用是<strong>更新本地软件包列表</strong>。</p>
</blockquote>
<p><strong>停止并删除容器</strong>：<code>docker stop fastapi  docker rm fastapi</code></p>
<p><strong>Linux修改镜像源</strong>：<a target="_blank" rel="noopener" href="https://blog.csdn.net/couragehope/article/details/137777158">如何查看docker配置的镜像仓库_查看docker镜像地址-CSDN博客</a></p>
<h2 id="常见参数">常见参数</h2>
<p>基础参数：</p>
<table>
<thead>
<tr class="header">
<th><code>-d</code>或<code>--detach</code></th>
<th>后台运行容器（detached mode）</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>--name &lt;name&gt;</code></td>
<td>为容器指定名称（如<code>--name my_container</code>）</td>
</tr>
<tr class="even">
<td><code>--rm</code></td>
<td>容器停止后自动删除（适用于临时容器）</td>
</tr>
</tbody>
</table>
<p>端口映射：</p>
<table>
<colgroup>
<col style="width: 45%">
<col style="width: 54%">
</colgroup>
<thead>
<tr class="header">
<th><code>-p &lt;主机端口&gt;:&lt;容器端口&gt;</code></th>
<th>映射主机端口到容器端口（如<code>-p 80:80</code>）</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>-p &lt;主机IP&gt;:&lt;主机端口&gt;:&lt;容器端口&gt;</code></td>
<td>指定主机IP绑定（如<code>-p 127.0.0.1:8080:80</code>）</td>
</tr>
<tr class="even">
<td><code>-P</code>或<code>--publish-all</code></td>
<td>自动映射所有暴露的端口（随机分配主机端口）</td>
</tr>
</tbody>
</table>
<p>卷挂载：</p>
<table>
<thead>
<tr class="header">
<th><code>-v &lt;主机路径&gt;:&lt;容器路径&gt;</code></th>
<th>挂载主机目录到容器（如<code>-v //app</code>）</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>-v &lt;卷名&gt;:&lt;容器路径&gt;</code></td>
<td>使用命名卷（如<code>-v my_volume:/data</code>）</td>
</tr>
</tbody>
</table>
<p>环境变量：</p>
<table>
<thead>
<tr class="header">
<th><code>-e &lt;KEY=VALUE&gt;</code></th>
<th>设置环境变量（如<code>-e DEBUG=true</code>）</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>--env-file &lt;文件名&gt;</code></td>
<td>从文件加载环境变量（每行<code>KEY=VALUE</code>）</td>
</tr>
</tbody>
</table>
<p>网络配置：</p>
<table>
<colgroup>
<col style="width: 27%">
<col style="width: 72%">
</colgroup>
<thead>
<tr class="header">
<th><code>--network &lt;网络名&gt;</code></th>
<th>指定容器使用的网络（如<code>--network bridge</code>或自定义网络）</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>--network host</code></td>
<td>使用主机网络（共享主机网络命名空间）</td>
</tr>
</tbody>
</table>
<h2 id="拯救被wsl占用的内存">拯救被wsl占用的内存</h2>
<p>以笔者的情况来说，我的wsl中只有一些必备的开发环境，项目源代码 和
docker。前两者显然没啥可操作的空间，所以只有一个靶子 —— docker。</p>
<p>首先，我们可以进入wsl，通过以下命令，看看 Docker
的磁盘使用情况和资源总量。</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker system df </span><br></pre></td></tr></table></figure>
<p>大家都知道，docker运行一段时间后，可能会产生一些无用的镜像文件。要清理无用的
Docker 镜像，则可以运行以下命令：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker image prune </span><br></pre></td></tr></table></figure>
<p>该命令可以删除所有未被任何容器使用的镜像。如果想清理所有已停止的容器和未使用的镜像：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker system prune -a</span><br></pre></td></tr></table></figure>
<p>执行完后咱们可以再运行第一个命令查看磁盘使用情况，大概率能看到释放了一部分磁盘空间。如果确实长时间为清理过，很大可能可释放几十G。</p>
<p>然而这时候我们退出wsl回到win10,
你可能会看到磁盘空间几乎没啥变化。这是因为wsl还需要我们手动释放这部分空间，即压缩磁盘。</p>
<h2 id="修改docker存储镜像位置">修改docker存储镜像位置</h2>
<p>windows</p>
<figure>
<img src="/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/docker/image-20250709095041821.png" alt="image-20250709095041821">
<figcaption aria-hidden="true">image-20250709095041821</figcaption>
</figure>
<figure>
<img src="/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/docker/image-20250709092539404.png" alt="image-20250709092539404">
<figcaption aria-hidden="true">image-20250709092539404</figcaption>
</figure>
<figure>
<img src="/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/docker/image-20250709092605183.png" alt="image-20250709092605183">
<figcaption aria-hidden="true">image-20250709092605183</figcaption>
</figure>
<p>Linux</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_43412762/article/details/134571411">修改Docker默认镜像和容器存储位置（超详细！！！）_docker更改存储位置-CSDN博客</a></p>
<h2 id="修改镜像源">修改镜像源</h2>
<p>查看可用的镜像源<a target="_blank" rel="noopener" href="https://tools.opsnote.top/registry-mirrors/">DockerHub加速器可用性监控</a></p>
<h3 id="如何使用vscode进入远程服务器的docker容器内部调试代码">如何使用vscode进入远程服务器的docker容器内部调试代码</h3>
<p>安装一下插件</p>
<figure>
<img src="/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/docker/9a3be11a07b5a0866d09d1bcbbaae4dc.png" alt="9a3be11a07b5a0866d09d1bcbbaae4dc">
<figcaption aria-hidden="true">9a3be11a07b5a0866d09d1bcbbaae4dc</figcaption>
</figure>
<figure>
<img src="/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/docker/image-20250716165555700.png" alt="image-20250716165555700">
<figcaption aria-hidden="true">image-20250716165555700</figcaption>
</figure>
<h2 id="buildpull与run">build，pull与run</h2>
<p><strong><code>docker build</code>：从源代码构建镜像</strong></p>
<ul>
<li><strong>作用</strong>：根据你提供的
<code>Dockerfile</code>（一个包含构建镜像所需指令的文本文件）以及上下文（通常是包含
<code>Dockerfile</code>
的目录及其子目录），<strong>创建</strong>一个新的 Docker 镜像。</li>
</ul>
<p><strong><code>docker pull</code>：从注册中心下载镜像</strong></p>
<ul>
<li><strong>作用</strong>：从 Docker 注册中心（默认是 Docker
Hub，也可以是私有注册中心如 Harbor, GitLab Registry, AWS ECR
等）<strong>下载</strong>一个已经构建好的 Docker
镜像到你的本地机器。</li>
</ul>
<p><strong><code>docker run</code>：创建并启动容器</strong></p>
<ul>
<li><strong>作用</strong>：基于一个<strong>本地已有的镜像</strong>（无论这个镜像是你刚
<code>build</code> 出来的，还是 <code>pull</code>
下来的，或是之前就存在的），<strong>创建</strong>一个新的容器实例，并按照指定的命令（或镜像默认的命令）<strong>启动</strong>它。</li>
</ul>
<h2 id="docker的自定义网络">docker的自定义网络</h2>
<p>创建自定义桥接网络</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker network create mynet</span><br></pre></td></tr></table></figure>
<p>启动服务容器（不映射宿主机端口也能被同网络容器访问）</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --name api --network demo-net fastapi-svc</span><br></pre></td></tr></table></figure>
<p>列出所有网络（包括自定义网络）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker network ls</span><br><span class="line"></span><br><span class="line">#只列出自定义网络（过滤掉默认网络）</span><br><span class="line">docker network ls --filter type=custom</span><br></pre></td></tr></table></figure>
<p>查看某个自定义网络的详细信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker network inspect network_test </span><br></pre></td></tr></table></figure>
<h2 id="参考文献">参考文献</h2>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1ai421S7zj/?spm_id_from=333.337.search-card.all.click&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">改变软件行业的技术！程序员、软件爱好者必须掌握的Docker，到底是什么？_哔哩哔哩_bilibili</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/187505981">什么是Docker？看这一篇干货文章就够了！
- 知乎</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/Python_0011/article/details/140313812">Docker常用命令大全（非常详细）零基础入门到精通，收藏这一篇就够了-CSDN博客</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1THKyzBER6/?spm_id_from=333.337.search-card.all.click&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">40分钟的Docker实战攻略，一期视频精通Docker_哔哩哔哩_bilibili</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/mineru/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/mineru/" class="post-title-link" itemprop="url">MinerU</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-07-08 00:00:00" itemprop="dateCreated datePublished" datetime="2025-07-08T00:00:00+08:00">2025-07-08</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-08-06 11:04:39" itemprop="dateModified" datetime="2025-08-06T11:04:39+08:00">2025-08-06</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AE%9E%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">实习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/" itemprop="url" rel="index"><span itemprop="name">晨晟智控</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="docker部署">docker部署</h3>
<p>使用dockerfile构建镜像：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget https://gcore.jsdelivr.net/gh/opendatalab/MinerU@master/docker/china/Dockerfile</span><br><span class="line">docker build -t mineru-sglang:latest -f Dockerfile .</span><br></pre></td></tr></table></figure>
<p>使用<code>wget https://gcore.jsdelivr.net/gh/opendatalab/MinerU @master/docker/china/Dockerfile -O Dockerfile</code>将指定的
Dockerfile 下载到本地</p>
<p>Dockerfile：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># 使用官方的 sglang 镜像作为基础镜像</span><br><span class="line">FROM lmsysorg/sglang:v0.4.9-cu126</span><br><span class="line"></span><br><span class="line"># 安装 OpenCV 依赖库</span><br><span class="line">RUN apt-get update &amp;&amp; apt-get install -y libgl1 &amp;&amp; apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/*</span><br><span class="line"></span><br><span class="line"># 安装 mineru Python 包</span><br><span class="line">RUN python3 -m pip install -U &#x27;mineru[core]&#x27; -i https://mirrors.aliyun.com/pypi/simple --break-system-packages</span><br><span class="line"></span><br><span class="line"># 下载模型并配置</span><br><span class="line">RUN /bin/bash -c &quot;mineru-models-download -s modelscope -m all&quot;</span><br><span class="line"></span><br><span class="line"># 设置容器入口命令</span><br><span class="line">ENTRYPOINT [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;export MINERU_MODEL_SOURCE=local &amp;&amp; exec \&quot;$@\&quot;&quot;, &quot;--&quot;]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>SGLang（全称可能为 <strong>Serving Large Language Models with
Golang</strong>
）是由斯坦福大学研究团队开发的一个<strong>高效的大语言模型（LLM）推理服务框架</strong>
，旨在通过优化模型推理过程，显著提升生成式AI服务的吞吐量和响应速度。</p>
<ul>
<li><strong>SGlang 版本</strong> ：<code>v0.4.8.post1</code>（SGlang
是一个用于大语言模型（LLM）推理和服务的高性能框架）。</li>
<li><strong>CUDA 版本</strong> ：<code>cu126</code> 表示使用
<strong>CUDA 12.6</strong> ，适用于 <strong>Turing/Ampere/Ada
Lovelace/Hopper 架构的 GPU</strong> （如 RTX 30/40
系列、A100、H100）。</li>
</ul>
</blockquote>
<h4 id="报错排查">报错排查</h4>
<p>之前由于默认dockerfile内容为<code>FROM lmsysorg/sglang:v0.4.8.post1-cu126</code>报错</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lmsysorg/sglang:v0.4.8.post1-cu126: failed to resolve source metadata for docker.io/lmsysorg/sglang:v0.4.8.post1-cu126: unexpected status from HEAD request to https://yaj2teeh.mirror.aliyuncs.com/v2/lmsysorg/sglang/manifests/v0.4.8.post1-cu126?ns=docker.io: 403 Forbidden</span><br></pre></td></tr></table></figure>
<p>之前以为是sglang版本问题，然后去dockerhub上查找，并通过<code>docker pull sglang:v0.4.8.post1-cu126</code>测试，是可以拉取的，最后认为原因还是网络问题</p>
<p>解决方法，更换了镜像源</p>
<h4 id="镜像源配置">镜像源配置</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&quot;registry-mirrors&quot;: [</span><br><span class="line">  &quot;https://registry.docker-cn.com&quot;,</span><br><span class="line">  &quot;http://hub-mirror.c.163.com&quot;,</span><br><span class="line">  &quot;https://dockerhub.azk8s.cn&quot;,</span><br><span class="line">  &quot;https://mirror.ccs.tencentyun.com&quot;,</span><br><span class="line">  &quot;https://registry.cn-hangzhou.aliyuncs.com&quot;,</span><br><span class="line">  &quot;https://docker.mirrors.ustc.edu.cn&quot;,</span><br><span class="line">  &quot;https://docker.m.daocloud.io&quot;,  </span><br><span class="line">  &quot;https://noohub.ru&quot;, </span><br><span class="line">  &quot;https://huecker.io&quot;,</span><br><span class="line">  &quot;https://dockerhub.timeweb.cloud&quot; </span><br><span class="line"> ]</span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1xHA3euEcn/?spm_id_from=333.337.search-card.all.click&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">保姆级Docker安装+镜像加速
计算机系必备技能_哔哩哔哩_bilibili</a></p>
<h4 id="为什么要指定基础镜像">为什么要指定基础镜像</h4>
<ul>
<li><strong>提供操作系统和依赖</strong> 基础镜像包含操作系统（如
Ubuntu、Alpine）、运行时环境（如 Python、Node.js）或框架（如
TensorFlow、PyTorch）等核心组件，后续所有操作（如安装依赖、拷贝文件）都基于此环境。
<ul>
<li>例如：<code>FROM python:3.9</code> 提供了 Python 3.9
的运行环境，后续可以直接用 <code>pip install</code> 安装 Python
包。</li>
</ul></li>
<li><strong>避免重复造轮子</strong>
如果直接从空镜像（<code>scratch</code>）开始，需要手动安装所有依赖，效率低下且容易出错。使用现有基础镜像可以复用已验证的环境配置。</li>
</ul>
<h4 id="确认支持的cuda版本">确认支持的cuda版本</h4>
<p>命令<code>nvidia-smi</code></p>
<figure>
<img src="/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/mineru/image-20250708160948192.png" alt="image-20250708160948192">
<figcaption aria-hidden="true">image-20250708160948192</figcaption>
</figure>
<p><strong>CUDA Version</strong> 显示当前驱动支持的最高 CUDA 版本</p>
<h4 id="问题使用dockerfile直接部署始终出现网络问题">问题：使用dockerfile直接部署，始终出现网络问题</h4>
<p>解决方案</p>
<p>先修改了一下docker储存镜像的位置，太大了</p>
<p>先拉取基础镜像<code>docker pull lmsysorg/sglang:v0.4.8.post1-cu126</code></p>
<p>再使用<code>docker build -t mineru-sglang:latest -f Dockerfile .</code>，可以直接跳过基础镜像的拉取</p>
<h3 id="启动">启动</h3>
<p>官方启动命令</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">docker run -d \</span><br><span class="line">  --name sglang-server \          # 容器命名（便于管理）</span><br><span class="line">  --gpus all \                   # 启用所有GPU</span><br><span class="line">  --shm-size 32g \               # 共享内存大小</span><br><span class="line">  -p 30000:30000 \               # 端口映射（主机端口:容器端口）</span><br><span class="line">  --ipc=host \                   # 共享主机IPC命名空间</span><br><span class="line">  mineru-sglang:latest \</span><br><span class="line">  mineru-sglang-server --host 0.0.0.0 --port 30000</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --name sglang-server --gpus all --shm-size 32g -p 30000:30000 --ipc=host mineru-sglang:latest mineru-sglang-server --host 0.0.0.0 --port 30000</span><br></pre></td></tr></table></figure>
<blockquote>
<p>将mineru-sglang-server暴露到30000端口的作用</p>
<p>为了支持 vlm-sglang-client
后端模式，使得MinerU客户端可以通过网络连接到这个服务器，实现多个客户端可以同时连接到同一个服务器</p>
</blockquote>
<p>使用<code>docker exec -it sglang-server bash</code>命令进入容器</p>
<p>或</p>
<p>使用docker desk</p>
<figure>
<img src="/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/mineru/image-20250709152627120.png" alt="image-20250709152627120">
<figcaption aria-hidden="true">image-20250709152627120</figcaption>
</figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --name mineru-server --gpus all --shm-size 32g -p 30000:30000 -p 7860:7860 -p 8000:8000 --ipc=host \</span><br><span class="line">-v &quot;F:/project python/实习/mineru/demo/pdfs:/pdfs&quot; \</span><br><span class="line">-v &quot;F:/project python/实习/mineru/output:/output&quot; \</span><br><span class="line">mineru-sglang:latest \</span><br><span class="line">mineru-sglang-server --host 0.0.0.0 --port 30000</span><br></pre></td></tr></table></figure>
<p>使用挂载卷启动</p>
<ul>
<li>将本地的PDF文件目录挂载到容器内的 /pdfs 目录</li>
<li>将本地的输出目录挂载到容器内的 /output 目录</li>
<li>把8000，和7860端口暴露，方便调用fastapi与gradio webui 可视化</li>
</ul>
<figure>
<img src="/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/mineru/image-20250710100047464.png" alt="image-20250710100047464">
<figcaption aria-hidden="true">image-20250710100047464</figcaption>
</figure>
<figure>
<img src="/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/mineru/image-20250710103505453.png" alt="image-20250710103505453">
<figcaption aria-hidden="true">image-20250710103505453</figcaption>
</figure>
<h3 id="调用">调用</h3>
<h4 id="命令行调用sglang-serverclient-模式">命令行调用sglang-server/client
模式</h4>
<p><code>docker exec mineru-server mineru -p /pdfs/demo1.pdf -o /output -b vlm-sglang-client -u http://localhost:30000</code></p>
<p>这条命令在名为 mineru-server 的容器内执行 mineru 工具，处理 /pdfs
目录下的 demo1.pdf 文件，输出结果到 /output 目录，使用 vlm-sglang-client
后端，并连接到 http://localhost:30000 的SGLang服务器。</p>
<figure>
<img src="/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/mineru/image-20250710103615272.png" alt="image-20250710103615272">
<figcaption aria-hidden="true">image-20250710103615272</figcaption>
</figure>
<h4 id="fastapi调用与gradio-webui-可视化">fastapi调用与gradio webui
可视化</h4>
<p>在完成docker的端口映射之后，运行<code>mineru-api --host 0.0.0.0 --port 8080</code>启动fastapi服务，</p>
<blockquote>
<p>FastAPI服务的使用场景</p>
<p>FastAPI服务提供了一个<code>/file_parse</code>端点，用于处理PDF和图像文件的解析请求</p>
<p>微服务架构部署，FastAPI服务可以独立部署</p>
<p>服务提供了标准的HTTP API接口，允许客户端通过网络请求进行文档解析</p>
</blockquote>
<p>运行<code>mineru-gradio --server-name 0.0.0.0 --server-port 7860</code>启动gradio
webui服务</p>
<p>或<code>mineru-gradio --server-name 0.0.0.0 --server-port 7860 --enable-sglang-engine true</code></p>
<blockquote>
<p>注意，模型下载需要配置环境变量</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 在容器内设置环境变量</span><br><span class="line">export MINERU_MODEL_SOURCE=local</span><br><span class="line"># 验证环境变量是否设置成功</span><br><span class="line">echo $MINERU_MODEL_SOURCE</span><br></pre></td></tr></table></figure>
</blockquote>
<figure>
<img src="/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/mineru/115678648318f55f1fe3a5baaeac2aaf.png" alt="115678648318f55f1fe3a5baaeac2aaf">
<figcaption aria-hidden="true">115678648318f55f1fe3a5baaeac2aaf</figcaption>
</figure>
<p><strong>在调用过程中关于端口的问题与思考</strong></p>
<p>调用过程中发现，在容器中使用<code>mineru-api --host 127.0.0.1 --port 8000</code>，宿主机无法访问<code>http://127.0.0.1:8000/docs/</code>，经过查询ai，命令改为<code>mineru-api --host 0.0.0.0 --port 8000</code>就可以正常访问，那么关键在于对这两个地址的理解</p>
<blockquote>
<p>查看端口<code>netstat -ano | findstr LISTENING</code></p>
</blockquote>
<p>127.0.0.1与0.0.0.0</p>
<ul>
<li>127.0.0.1 (localhost) ：仅表示本机回环地址，只能在 同一设备内
访问</li>
<li>0.0.0.0 ：表示监听所有可用的网络接口，允许 来自任何地址 的连接</li>
</ul>
<p>当您在Docker容器内运行服务时：</p>
<ol type="1">
<li><p>使用127.0.0.1作为绑定地址 ：</p>
<ul>
<li>服务只接受来自容器内部的连接</li>
<li>即使您映射了端口，宿主机也无法访问该服务</li>
<li>只有容器内的应用程序可以通过 127.0.0.1:端口 访问</li>
</ul></li>
<li><p>使用0.0.0.0作为绑定地址 ：</p>
<ul>
<li>服务接受来自任何网络接口的连接请求</li>
<li>允许从容器外部（包括宿主机）访问该服务</li>
<li>当您映射端口时（如 -p 8000:8000 ），宿主机可以通过 localhost:8000 或
127.0.0.1:8000 访问</li>
</ul></li>
</ol>
<p><strong>为什么需要在容器内使用0.0.0.0</strong></p>
<p>在Docker环境中，容器有自己独立的网络命名空间，这意味着容器内的
127.0.0.1 与宿主机的 127.0.0.1 是完全不同的两个环境。因此：</p>
<ul>
<li>当您在容器内使用 –host 0.0.0.0
启动服务时，该服务会监听容器的所有网络接口</li>
<li>当您在宿主机上访问 127.0.0.1:映射端口
时，Docker会将请求转发到容器内监听在 0.0.0.0:容器端口 的服务</li>
</ul>
<h3 id="mineru相关知识">mineru相关知识</h3>
<h4 id="参数">参数</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Usage: mineru [OPTIONS]</span><br><span class="line"></span><br><span class="line">Options:</span><br><span class="line">  -v, --version                   显示版本并退出</span><br><span class="line">  -p, --path PATH                 输入文件路径或目录（必填）</span><br><span class="line">  -o, --output PATH               输出目录（必填）</span><br><span class="line">  -m, --method [auto|txt|ocr]     解析方法：auto（默认）、txt、ocr（仅用于 pipeline 后端）</span><br><span class="line">  -b, --backend [pipeline|vlm-transformers|vlm-sglang-engine|vlm-sglang-client]</span><br><span class="line">                                  解析后端（默认为 pipeline）</span><br><span class="line">  -l, --lang [ch|ch_server|ch_lite|en|korean|japan|chinese_cht|ta|te|ka|latin|arabic|east_slavic|cyrillic|devanagari]</span><br><span class="line">                                  指定文档语言（可提升 OCR 准确率，仅用于 pipeline 后端）</span><br><span class="line">  -u, --url TEXT                  当使用 sglang-client 时，需指定服务地址</span><br><span class="line">  -s, --start INTEGER             开始解析的页码（从 0 开始）</span><br><span class="line">  -e, --end INTEGER               结束解析的页码（从 0 开始）</span><br><span class="line">  -f, --formula BOOLEAN           是否启用公式解析（默认开启）</span><br><span class="line">  -t, --table BOOLEAN             是否启用表格解析（默认开启）</span><br><span class="line">  -d, --device TEXT               推理设备（如 cpu/cuda/cuda:0/npu/mps，仅 pipeline 后端）</span><br><span class="line">  --vram INTEGER                  单进程最大 GPU 显存占用(GB)（仅 pipeline 后端）</span><br><span class="line">  --source [huggingface|modelscope|local]</span><br><span class="line">                                  模型来源，默认 huggingface</span><br><span class="line">  --help                          显示帮助信息</span><br></pre></td></tr></table></figure>
<h4 id="后端的区别">后端的区别</h4>
<p>pipeline (默认后端) :</p>
<ul>
<li>含义 : 这是 MinerU 的默认后端，它使用本地安装的 mineru
库来执行文档解析任务。它通常不依赖于外部的
VLM（视觉语言模型）服务，而是直接在本地处理 PDF 文件。</li>
</ul>
<figure>
<img src="/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/mineru/10d6d1a7f702c5806e29ee7b1c51283.png" alt="10d6d1a7f702c5806e29ee7b1c51283">
<figcaption aria-hidden="true">10d6d1a7f702c5806e29ee7b1c51283</figcaption>
</figure>
<p>vlm-transformers :</p>
<ul>
<li>含义 : 这个后端利用 Hugging Face transformers 库中提供的 VLM
模型进行文档分析。它会在本地加载并运行一个基于 transformers 的 VLM
模型来处理 PDF 中的视觉信息和文本内容。</li>
</ul>
<blockquote>
<p>VLM（Vision-Language
Model，视觉语言模型）是一种结合计算机视觉和自然语言处理能力的多模态人工智能模型。</p>
<p>OCR 是 Optical Character
Recognition（光学字符识别）的缩写。它是一种技术，用于将图像中的手写、打印或打字文本转换为机器编码的文本，使其可以被计算机编辑、搜索、存储和处理。</p>
</blockquote>
<p>vlm-sglang-engine :</p>
<ul>
<li>含义 : 这个后端表示 MinerU 将直接集成并使用 SGLang 引擎进行 VLM
推理。SGLang 是一个高性能的推理引擎，旨在优化大型语言模型（LLM）和 VLM
的推理速度和效率。在这种模式下，SGLang 引擎作为 MinerU
进程的一部分运行。</li>
</ul>
<p>vlm-sglang-client :</p>
<ul>
<li>含义 : 这个后端表示 MinerU 作为 SGLang
服务器的客户端。在这种模式下，MinerU 不会直接运行 VLM 模型，而是将 PDF
处理请求发送到一个独立的 SGLang 服务器（通过 -u 参数指定的 URL，例如
http://localhost:30000 ）。SGLang 服务器负责执行实际的 VLM
推理，并将结果返回给 MinerU 客户端。</li>
</ul>
<blockquote>
<p>用场景 : 这是我们之前讨论的 Docker
容器部署场景中推荐的模式。它非常适合以下情况： - 资源隔离 : 将 VLM
推理的计算密集型任务从 MinerU 主进程中分离出来，允许独立扩展和管理
SGLang 服务器。 - 集中管理 : 可以在一个或多个 SGLang 服务器上集中管理
VLM 模型，供多个 MinerU 客户端共享使用。 - 性能优化 : SGLang
服务器可以针对 VLM 推理进行专门优化，提供更好的吞吐量和延迟。 - 灵活部署
: SGLang
服务器可以部署在不同的机器上，甚至作为微服务运行，提供更大的部署灵活性。</p>
</blockquote>
<h4 id="关于吞吐量的相关知识">关于吞吐量的相关知识</h4>
<p><strong>“吞吐量”（throughput）*<em>指的是系统在单位时间内能处理的
**Token 数量**，单位通常是
**tokens/秒**。这个指标衡量的是整体系统*</em>处理并发请求的能力</strong>，而不仅仅是单个请求的速度。</p>
<p>SGLang 支持两种主要并行方式来提升吞吐量：</p>
<table>
<colgroup>
<col style="width: 16%">
<col style="width: 43%">
<col style="width: 39%">
</colgroup>
<thead>
<tr class="header">
<th>并行类型</th>
<th>作用</th>
<th>对吞吐量的影响</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>张量并行（TP）</strong></td>
<td>把模型权重切分到多张卡上，<strong>减少单卡负载</strong></td>
<td>提升单请求处理能力，<strong>但通信开销大</strong></td>
</tr>
<tr class="even">
<td><strong>数据并行（DP）</strong></td>
<td>把不同请求分发到不同卡上，<strong>并行处理多个请求</strong></td>
<td>直接提升并发吞吐量，<strong>尤其适合高并发场景</strong></td>
</tr>
</tbody>
</table>
<h3 id="mineru的并发测试和吞吐量测试">mineru的并发测试和吞吐量测试</h3>
<p>并发能力是测试mineru同时处理多个请求的能力，吞吐量是测试mineru处理文件时的tokens</p>
<p>明确要的是哪个吞吐量：一个是<strong>MinerU 内部推理引擎（如
vLLM/SGLang）的 token/s 输出</strong>，即
<strong>生成阶段（decode）的吞吐量</strong>，一个是你压测
<code>/file_parse</code> 接口时的 <strong>端到端 tokens/s</strong>。</p>
<p>是否有缓存（kvcache）</p>
<p><strong>测试场景</strong>：10页的pdf，50用户并发</p>
<p><strong>工具</strong>：locust</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">locust \</span><br><span class="line">  -f locustfile.py \</span><br><span class="line">  --headless \</span><br><span class="line">  -u 50 \               # 并发用户数</span><br><span class="line">  -r 5 \                # 每秒启动用户数</span><br><span class="line">  --host=http://mineru-server:30000 \</span><br><span class="line">  --html=report.html \  # 自动生成 HTML 报告</span><br><span class="line">  --csv=result          # 同时保存 csv（result_stats.csv / result_failures.csv）</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">locust -f locustfile.py --headless -u 100 -r 5 --host=http://mineru-server:30000 --html=report.html --csv=result</span><br></pre></td></tr></table></figure>
<p><strong>测试结果</strong></p>
<figure>
<img src="/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/mineru/image-20250805152017255.png" alt="image-20250805152017255">
<figcaption aria-hidden="true">image-20250805152017255</figcaption>
</figure>
<p>对于推理模型的吞吐量，在3个gpu开启数据并行的情况下，平均每秒单个gpu处理tokens为1500左右</p>
<figure>
<img src="/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/mineru/image-20250805152149822.png" alt="image-20250805152149822">
<figcaption aria-hidden="true">image-20250805152149822</figcaption>
</figure>
<p>gpu状态如上:<strong>显存几乎打满 85–87 %</strong>,<strong>GPU 利用率
59–63 %</strong>,<strong>功耗 170–188 W / 350 W</strong></p>
<figure>
<img src="/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/mineru/image-20250805153516235.png" alt="image-20250805153516235">
<figcaption aria-hidden="true">image-20250805153516235</figcaption>
</figure>
<p>完整压测结果如上</p>
<p>重要指标：</p>
<table>
<colgroup>
<col style="width: 18%">
<col style="width: 28%">
<col style="width: 52%">
</colgroup>
<thead>
<tr class="header">
<th>指标</th>
<th>数值</th>
<th>通俗解释</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>平均响应时间</strong></td>
<td><strong>241 秒</strong> ≈ <strong>4 分钟</strong></td>
<td>上传一个 PDF → 拿到解析结果，平均要等 4 分钟。</td>
</tr>
<tr class="even">
<td><strong>中位数</strong></td>
<td><strong>215 秒</strong> ≈ <strong>3.6 分钟</strong></td>
<td>一半请求在 3.6 分钟内完成。</td>
</tr>
<tr class="odd">
<td><strong>95% 用户</strong></td>
<td><strong>361 秒</strong> ≈ <strong>6 分钟</strong></td>
<td>最慢的 5% 要等 6 分钟以上。</td>
</tr>
<tr class="even">
<td><strong>吞吐量</strong></td>
<td><strong>0.18 req/s</strong></td>
<td>这台 MinerU <strong>每分钟只能处理约11 个 PDF</strong>。</td>
</tr>
</tbody>
</table>
<figure>
<img src="/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/mineru/image-20250806104340935.png" alt="image-20250806104340935">
<figcaption aria-hidden="true">image-20250806104340935</figcaption>
</figure>
<figure>
<img src="/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/mineru/image-20250806110438932.png" alt="image-20250806110438932">
<figcaption aria-hidden="true">image-20250806110438932</figcaption>
</figure>
<p>参考资料</p>
<p><a target="_blank" rel="noopener" href="https://github.com/opendatalab/MinerU/blob/be4f3de32b58ccf81c6a6dcb9d3e4998424cee6a/projects/multi_gpu_v2/README_zh.md">MinerU/projects/multi_gpu_v2/README_zh.md
at be4f3de32b58ccf81c6a6dcb9d3e4998424cee6a · opendatalab/MinerU</a></p>
<h3 id="部署服务器并运行">部署服务器并运行</h3>
<p>load镜像</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker load -i mineru-sglang-latest.tar</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">free -h</span><br><span class="line">nvidia-smi</span><br></pre></td></tr></table></figure>
<figure>
<img src="/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/mineru/image-20250716092152823.png" alt="image-20250716092152823">
<figcaption aria-hidden="true">image-20250716092152823</figcaption>
</figure>
<p>每秒刷新<code>watch -n1 nvidia-smi          # 每秒刷新</code></p>
<p>查看Linux路径</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pwd </span><br></pre></td></tr></table></figure>
<p>启动容器</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --name mineru-server --gpus all --shm-size 32g -p 30000:30000 -p 7860:7860 -p 8000:8000 --ipc=host \</span><br><span class="line">-v &quot;/aisys/repo_dev/xizhang/pdfs:/pdfs&quot; \</span><br><span class="line">-v &quot;/aisys/repo_dev/xizhang/outputs:/output&quot; \</span><br><span class="line">mineru-sglang:latest \</span><br><span class="line">mineru-sglang-server --host 0.0.0.0 --port 30000</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --name mineru-server --gpus all --shm-size 32g -p 30000:30000 --ipc=host -v &quot;/aisys/repo_dev/xizhang/pdfs:/pdfs&quot; -v &quot;/aisys/repo_dev/xizhang/outputs:/output&quot; mineru-sglang:latest mineru-sglang-server --host 0.0.0.0 --port 30000</span><br></pre></td></tr></table></figure>
<p>调用</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker exec mineru-server mineru -p /pdfs/demo1.pdf -o /output -b vlm-sglang-client -u http://localhost:30000`</span><br></pre></td></tr></table></figure>
<p>进入容器</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker exec -it mineru-server /bin/bash</span><br></pre></td></tr></table></figure>
<p>使用pipline解析后端模式</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mineru -p /pdfs/demo1.pdf -o /output --source local</span><br></pre></td></tr></table></figure>
<p>使用sglang加速推理</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=1,2,3 mineru -p /pdfs/small_ocr.pdf -o /output -b vlm-sglang-engine --source local</span><br></pre></td></tr></table></figure>
<blockquote>
<p>vlm模式同样可以处理扫描件</p>
</blockquote>
<p>使用ocr解析扫描件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mineru -p /pdfs/small_ocr.pdf -o /output --source local -m ocr</span><br></pre></td></tr></table></figure>
<p>增加推理设备</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mineru -p /pdfs/small_ocr.pdf -o /output --source local -m ocr -d cuda</span><br></pre></td></tr></table></figure>
<p>通过在命令行的开头添加<code>CUDA_VISIBLE_DEVICES</code>
环境变量来指定可见的 GPU 设备。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=1,2,3 mineru -p /pdfs/small_ocr.pdf -o /output --source local -m ocr</span><br></pre></td></tr></table></figure>
<p>使用sglang加速模式的多GPU并行</p>
<p>数据并行（dp-size）和张量并行（tp-size）</p>
<p>MinerU支持通过sglang的多GPU并行模式来提升推理速度。您可以使用以下参数：</p>
<ul>
<li><code>--dp-size</code>:
数据并行，通过多卡同时处理多个输入来增加吞吐量</li>
<li><code>--tp-size</code>:
张量并行，将模型分布到多张GPU上以扩展可用显存</li>
</ul>
<blockquote>
<p>如果您已经可以正常使用sglang对vlm模型进行加速推理，但仍然希望进一步提升推理速度，可以尝试以下参数：</p>
<ul>
<li>如果您有超过多张显卡，可以使用sglang的多卡并行模式来增加吞吐量：<code>--dp-size 2</code></li>
<li>同时您可以启用<code>torch.compile</code>来将推理速度加速约15%：<code>--enable-torch-compile</code></li>
</ul>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=1,2,3 mineru -p /pdfs -o /output -b vlm-sglang-engine --source local --dp-size 3 --enable-torch-compile</span><br></pre></td></tr></table></figure>
<p>将python文件上传docker并运行</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker cp demo.py mineru-server:/demo.py</span><br><span class="line"></span><br><span class="line">docker exec mineru-server python /demo.py</span><br><span class="line"></span><br><span class="line">#删除</span><br><span class="line">rm -i demo.py</span><br></pre></td></tr></table></figure>
<p>添加自定义网络，修改挂载卷</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --name mineru-server --gpus all --shm-size 32g -p 30000:30000 --ipc=host -v /aisys/:/aisys/ --network network_test mineru-sglang:latest mineru-sglang-server --host 0.0.0.0 --port 30000</span><br></pre></td></tr></table></figure>
<p>后面才知道，上面这个命令会自动启动sglang-server服务</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --name mineru-server --gpus all --shm-size 32g -p 30000:30000 --ipc=host -v /aisys/:/aisys/ --network network_test mineru-sglang:latest tail -f /dev/null</span><br><span class="line">docker start mineru-server</span><br></pre></td></tr></table></figure>
<p>使用上面这个命令启动容器，但不启动sglang-server服务，使用下面指令手动启动</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker exec -it mineru-server /bin/bash</span><br><span class="line">MINERU_MODEL_SOURCE=local CUDA_VISIBLE_DEVICES=1,2,3 mineru-sglang-server --port 30000 --dp-size 3 --enable-torch-compile</span><br></pre></td></tr></table></figure>
<p>启动服务后在另一个容器尝试访问</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl http://mineru-server:30000/get_model_info</span><br></pre></td></tr></table></figure>
<p>在另一个容器使用服务</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mineru -p /test -o / -b vlm-sglang-client -u http://mineru-server:30000</span><br></pre></td></tr></table></figure>
<hr>
<p>启动fastapi服务</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MINERU_MODEL_SOURCE=local CUDA_VISIBLE_DEVICES=1,2,3 mineru-api --host 0.0.0.0 --port 30000 --dp-size 3 --enable-torch-compile</span><br></pre></td></tr></table></figure>
<p>在另一个容器验证</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl http://mineru-server:30000/openapi.json</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 在容器内设置环境变量</span><br><span class="line">export CUDA_VISIBLE_DEVICES=1,2,3</span><br><span class="line"># 验证环境变量是否设置成功</span><br><span class="line">echo $CUDA_VISIBLE_DEVICES</span><br><span class="line"></span><br><span class="line"># 在容器内设置环境变量</span><br><span class="line">export MINERU_MODEL_SOURCE=local</span><br><span class="line"># 验证环境变量是否设置成功</span><br><span class="line">echo $MINERU_MODEL_SOURCE</span><br></pre></td></tr></table></figure>
<h3 id="资料">资料</h3>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/liuzhenghua66/article/details/148980203">MinerU
2.0部署-CSDN博客</a></p>
<p>https://github.com/opendatalab/MinerU?tab=readme-ov-file#local-deployment</p>
<p>https://deepwiki.com/opendatalab/MinerU</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/%E6%97%A5%E5%BF%97/7.8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/%E6%97%A5%E5%BF%97/7.8/" class="post-title-link" itemprop="url">实习日志</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-07-08 00:00:00" itemprop="dateCreated datePublished" datetime="2025-07-08T00:00:00+08:00">2025-07-08</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-07-30 09:11:25" itemprop="dateModified" datetime="2025-07-30T09:11:25+08:00">2025-07-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AE%9E%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">实习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/" itemprop="url" rel="index"><span itemprop="name">晨晟智控</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="日志">日志</h3>
<p>7.16 10：46</p>
<p>已经将mineru部署至服务器，完成pdf和扫描件的提取测试，但是mineru是没办法直接提取doc与docx的，能不能直接对doc与docx进行文本分块，或者先转换成pdf再提取（官方给出的解决方案：通过独立部署的LibreOffice服务先行转换为PDF格式，再进行后续解析操作。）</p>
<p>7.17 11：04</p>
<p>完成mineru脚本的编写，仅输出提取的md与image；实现调用多块gpu；实现数据并行，通过多卡同时处理多个输入来增加吞吐量；使用sglang框架</p>
<p>7.18 14：37</p>
<p>发现mineru提取的markdown文档是不带多级标题的，只有一级标题，所以不考虑文档结构分块；语义分块要调用embedding模型，而且文档里有很多表格，我感觉效果不一定会好；后续我想法是使用递归分块，表格在md文档中以html表格格式存储的，大量冗余信息，我想先对其进行预处理，转换成md表格的形式吧（用“
|”存储），然后再分块，或许效果会好一些，正在进行</p>
<p>还有一个就是libreoffice是部署在哪里，我没有找到诶</p>
<p>7.19 10：26</p>
<p>完成表格预处理的脚本编写，完成对md文档的预处理；完成递归分块，后续完成存入es数据库</p>
<p>7.21</p>
<p>完成简单地将分块结果存入elasticsearch（mapping只有content字段，使用http请求存的，没有用langchain-elasticsearch）后面要试试langchain-elasticsearch，去看看字段的处理</p>
<p>7.22</p>
<p>完成langchain-elasticsearch的bm25的检索测试，接下来尝试使用服务器的embedding模型进行测试，阅读项目文件，理清思路，具体内容见<a target="_blank" rel="noopener" href="https://icnrn6ghqrgx.feishu.cn/wiki/MWZ9wVYRxixABqkMhjQcQPqTnfe">多模态
-
PDF表格图片&amp;扫描件</a>，存入es与文件处理字段处理部分已经理清了，成功存入服务器的es</p>
<p>7.23</p>
<p>今天在把之前做的所有工作进行整合，编写一个完整的代码，实现生产的流程，遇到的主要问题有两个：1.我还是没有很看懂之前对字段的存储，和字段的结构2.我没有太理解pdf分块的部分在哪里，是没有吗，我看doc的rewrite_word是有文本分块的</p>
<p>感觉需要你给我讲一下，不然我后面不大知道该怎么处理，那我明天去把语义分块和其他的检索方案试一下吧，整合代码先稍微延后</p>
<p>7.24</p>
<p>使用docker的自定义网络，实现容器之间的通信，从而可以在repo容器中调用mineru，其他容器都在network_test下，我把mineru也启动到里面</p>
<p>成功在repo容器访问到mineru容器，完成pdf分块数据的批量写入elasticsearch并可以成功检索，后续完成调用mineru的fastapi接口（还没编完），继续完成代码整合（已经完成大部分，主要差mineru的部分，和一些衔接的代码）</p>
<p>7.25</p>
<p>完成通过接口使用mineru批量处理pdf文件</p>
<p>7.28</p>
<p>完成了代码整合，可以完成pdf整个流程的处理</p>
<p>7.29</p>
<p>补充了libreoffice的代码，实现了将doc，docx转换成pdf，统一处理流程；完成了数据标记web；后续计划：根据问题检索url，处理后存入zxj_test
es数据库，再进行进一步检索产生数据集</p>
<p>7.30</p>
<p>完成扫描件的测试，文字公式图片皆可正常识别</p>
<h3 id="其他">其他</h3>
<h4 id="vscode连接远程服务器">vscode连接远程服务器</h4>
<ol type="1">
<li>输入 ssh root@10.117.128.50</li>
<li>输入密码 think123@</li>
</ol>
<h5 id="使用旧版remotessh">使用旧版remotessh</h5>
<figure>
<img src="/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/%E6%97%A5%E5%BF%97/7.8/image-20250708104452761.png" alt="image-20250708104452761">
<figcaption aria-hidden="true">image-20250708104452761</figcaption>
</figure>
<blockquote>
<p>原因：可能因为内网，服务器那边没有进行更新，所以新版的remotessh无法连接</p>
<p>其他问题：可能由于上述原因，trae也无法连接，并且由于trae的远程连接插件无法更改版本，因此无法使用</p>
</blockquote>
<h5 id="虚拟环境创建">虚拟环境创建</h5>
<figure>
<img src="/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/%E6%97%A5%E5%BF%97/7.8/image-20250708104254347.png" alt="image-20250708104254347">
<figcaption aria-hidden="true">image-20250708104254347</figcaption>
</figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">python3 -m venv .venv</span><br><span class="line"></span><br><span class="line">which python</span><br><span class="line"></span><br><span class="line">#激活虚拟环境</span><br><span class="line"> .venv\Scripts\activate</span><br><span class="line"> source .venv/bin/activate</span><br><span class="line"> </span><br><span class="line"> #停掉虚拟环境</span><br><span class="line"> deactivate</span><br></pre></td></tr></table></figure>
<h4 id="git连接远程仓库">git连接远程仓库</h4>
<p>初始化仓库：<code>git init</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 添加所有文件到暂存区</span><br><span class="line">git add .</span><br><span class="line"></span><br><span class="line"># 提交更改</span><br><span class="line">git commit -m &quot;Initial commit&quot;</span><br><span class="line"></span><br><span class="line">#上传远程库</span><br><span class="line">git push -u origin main</span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr class="header">
<th>目标</th>
<th>命令</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>查看远程仓库</td>
<td><code>git remote -v</code></td>
</tr>
<tr class="even">
<td>修改远程仓库地址</td>
<td><code>git remote set-url origin &lt;新地址&gt;</code></td>
</tr>
<tr class="odd">
<td>添加新远程仓库（不同名）</td>
<td><code>git remote add upstream &lt;新地址&gt;</code></td>
</tr>
<tr class="even">
<td>删除远程仓库</td>
<td><code>git remote remove origin</code></td>
</tr>
<tr class="odd">
<td>添加第一个远程仓库</td>
<td><code>git remote add origin</code></td>
</tr>
</tbody>
</table>
<blockquote>
<figure>
<img src="/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/%E6%97%A5%E5%BF%97/7.8/image-20250708151320868.png" alt="image-20250708151320868">
<figcaption aria-hidden="true">image-20250708151320868</figcaption>
</figure>
<p>当前存在连接超时问题，可能是服务器连接的原因</p>
</blockquote>
<h4 id="mineru部署情况">mineru部署情况</h4>
<figure>
<img src="/2025/07/08/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/%E6%97%A5%E5%BF%97/7.8/1a9954d6-31d7-404a-9419-cd9a87c9ee09.png" alt="1a9954d6-31d7-404a-9419-cd9a87c9ee09">
<figcaption aria-hidden="true">1a9954d6-31d7-404a-9419-cd9a87c9ee09</figcaption>
</figure>
<p>通过调整docker镜像源，可以拉取基础镜像了，但是遇到<code>RUN apt-get update &amp;&amp; apt-get install -y libgl1 &amp;&amp; apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/*</code>第二部命令再次出现网络问题</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/06/24/diary/%E5%A4%A9%E4%B8%8B%E8%8B%B1%E9%9B%84%E5%A6%82%E8%BF%87%E6%B1%9F%E4%B9%8B%E9%B2%AB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/06/24/diary/%E5%A4%A9%E4%B8%8B%E8%8B%B1%E9%9B%84%E5%A6%82%E8%BF%87%E6%B1%9F%E4%B9%8B%E9%B2%AB/" class="post-title-link" itemprop="url">天下英雄如过江之鲫</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2025-06-24 00:00:00 / 修改时间：11:37:06" itemprop="dateCreated datePublished" datetime="2025-06-24T00:00:00+08:00">2025-06-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E9%9A%8F%E7%AC%94/" itemprop="url" rel="index"><span itemprop="name">随笔</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>当你来到双非，你会艳羡211的牌子，当你拼死考上211，你会发现985的头衔会处处卡死你。当你侥幸考上末流
985，你就会发现华五c9的光芒压的你喘不过气。若你真问鼎华五，抬头看，京城中双日凌空。或许你是真正的天才，清北中的佼佼者，他们告诉你，世界不止中国。当你最终成为这一世最不折不扣的天才，你会发现欧拉，黎曼，还有7岁因为想快点放学而创造求和公式的高斯，
早已在山顶等候多时。</p>
<p>有时候想想，这学上到多高才算高啊，大专上面有本科，本科上面有硕博，好不容易毕业了吧，副高，正高，青基，博导，在上面还有杰青，院士。唉，天下英雄，如过江之鲫，无穷尽也。之前没有感觉，自从上了大学，这种感受就像一团乌云一直萦绕在我心头，不禁反思人这一生究竟在追求什么。</p>
<p>大多数人追求的东西，无非三者：权钱学。</p>
<p>有的人梦想升官，但官外有官，权外有权，科级处级厅级，省部级已经算是人中龙凤，但上面还有副国级正国级，大多数人忙忙碌碌一生也就当个副处级，他们真的甘心吗，那种拼劲全力也无法跨过的鸿沟，最后只剩下无奈，遗憾和释然。</p>
<p>有人的渴望财富，赚到了十万就想赚百万，赚到了百万又开始想办法，想赚千万，亿，觉得自己有能力了，开始创业投资，拿着钱去炒股炒币最后赔了个精光，更有甚者权钱勾结，做些不法勾当，不都是为了满足自己的贪婪，但欲望无穷无尽，何时才能填满这个无底洞，更可怕的是，多少人的欲望和他的能力并不符合，自身没有那么大的能力却渴望一切，最后只会反噬，自食其果。</p>
<p>有的人钻研学识，中国的大多数人都是通过高考这一途径踏进学识的殿堂，那些在高中自命不凡的天才们，进入了高校才发现，自己只是芸芸众生的普通一员，以前的光辉也变的暗淡无光，即便是清北级别，已经是很多人可望而不可即的存在，在面对越来越难的知识，在面对更聪明的身边人，在发现自己再努力也无法达成目标时，也会学习释然这一门课。</p>
<p>我想起来看过的一个清华物理系同学的采访，有一句话我印象很深刻，古人会说少壮不努力，老大徒伤悲，但不会说少壮不成功，老大徒伤悲，或许我上面说的这些，都太注重结果了，以结果的好坏判定了过程的意义，这是不对的，在追求这些目标的过程中，我们努力了，拼搏了，奋斗了，其实那就足够了，不应该把结果失利的压力强加在自己身上。</p>
<p>唉，这些说起来容易，真正能做到不为结果所动哪里容易呢，只跟自己比较，不与他人攀比，处之泰然地面对任何困难与挑战。这就是我所追求的心境吧，我什么时候能做到这种地步，可能才是真正的长大了吧。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/06/18/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%80%E4%BC%98%E5%8C%96/%E6%9C%80%E4%BC%98%E5%8C%96%E6%9C%9F%E6%9C%AB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/06/18/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%80%E4%BC%98%E5%8C%96/%E6%9C%80%E4%BC%98%E5%8C%96%E6%9C%9F%E6%9C%AB/" class="post-title-link" itemprop="url">最优化方法——期末复习</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-06-18 00:00:00" itemprop="dateCreated datePublished" datetime="2025-06-18T00:00:00+08:00">2025-06-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-08-02 22:47:35" itemprop="dateModified" datetime="2025-08-02T22:47:35+08:00">2025-08-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%A4%A7%E4%BA%8C%E4%B8%8B/" itemprop="url" rel="index"><span itemprop="name">大二下</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%80%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">最优化方法</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1uP411K7Hf?spm_id_from=333.788.videopod.sections&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">最优化理论与方法-对偶线性规划（例题分析）_哔哩哔哩_bilibili</a></p>
<h3 id="复习笔记">复习笔记</h3>
<figure>
<img src="/2025/06/18/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%80%E4%BC%98%E5%8C%96/%E6%9C%80%E4%BC%98%E5%8C%96%E6%9C%9F%E6%9C%AB/IMG_20250624_120458.jpg" alt="IMG_20250624_120458">
<figcaption aria-hidden="true">IMG_20250624_120458</figcaption>
</figure>
<figure>
<img src="/2025/06/18/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%80%E4%BC%98%E5%8C%96/%E6%9C%80%E4%BC%98%E5%8C%96%E6%9C%9F%E6%9C%AB/IMG_20250624_120516.jpg" alt="IMG_20250624_120516">
<figcaption aria-hidden="true">IMG_20250624_120516</figcaption>
</figure>
<figure>
<img src="/2025/06/18/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%80%E4%BC%98%E5%8C%96/%E6%9C%80%E4%BC%98%E5%8C%96%E6%9C%9F%E6%9C%AB/IMG_20250624_120523.jpg" alt="IMG_20250624_120523">
<figcaption aria-hidden="true">IMG_20250624_120523</figcaption>
</figure>
<figure>
<img src="/2025/06/18/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%80%E4%BC%98%E5%8C%96/%E6%9C%80%E4%BC%98%E5%8C%96%E6%9C%9F%E6%9C%AB/IMG_20250624_120527.jpg" alt="IMG_20250624_120527">
<figcaption aria-hidden="true">IMG_20250624_120527</figcaption>
</figure>
<figure>
<img src="/2025/06/18/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%80%E4%BC%98%E5%8C%96/%E6%9C%80%E4%BC%98%E5%8C%96%E6%9C%9F%E6%9C%AB/IMG_20250624_120534.jpg" alt="IMG_20250624_120534">
<figcaption aria-hidden="true">IMG_20250624_120534</figcaption>
</figure>
<figure>
<img src="/2025/06/18/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%80%E4%BC%98%E5%8C%96/%E6%9C%80%E4%BC%98%E5%8C%96%E6%9C%9F%E6%9C%AB/IMG_20250624_120539.jpg" alt="IMG_20250624_120539">
<figcaption aria-hidden="true">IMG_20250624_120539</figcaption>
</figure>
<figure>
<img src="/2025/06/18/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%80%E4%BC%98%E5%8C%96/%E6%9C%80%E4%BC%98%E5%8C%96%E6%9C%9F%E6%9C%AB/IMG_20250624_120546.jpg" alt="IMG_20250624_120546">
<figcaption aria-hidden="true">IMG_20250624_120546</figcaption>
</figure>
<figure>
<img src="/2025/06/18/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%80%E4%BC%98%E5%8C%96/%E6%9C%80%E4%BC%98%E5%8C%96%E6%9C%9F%E6%9C%AB/IMG_20250624_120550.jpg" alt="IMG_20250624_120550">
<figcaption aria-hidden="true">IMG_20250624_120550</figcaption>
</figure>
<figure>
<img src="/2025/06/18/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%80%E4%BC%98%E5%8C%96/%E6%9C%80%E4%BC%98%E5%8C%96%E6%9C%9F%E6%9C%AB/IMG_20250624_120557.jpg" alt="IMG_20250624_120557">
<figcaption aria-hidden="true">IMG_20250624_120557</figcaption>
</figure>
<figure>
<img src="/2025/06/18/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%80%E4%BC%98%E5%8C%96/%E6%9C%80%E4%BC%98%E5%8C%96%E6%9C%9F%E6%9C%AB/IMG_20250624_120600.jpg" alt="IMG_20250624_120600">
<figcaption aria-hidden="true">IMG_20250624_120600</figcaption>
</figure>
<figure>
<img src="/2025/06/18/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%80%E4%BC%98%E5%8C%96/%E6%9C%80%E4%BC%98%E5%8C%96%E6%9C%9F%E6%9C%AB/IMG_20250624_120611.jpg" alt="IMG_20250624_120611">
<figcaption aria-hidden="true">IMG_20250624_120611</figcaption>
</figure>
<figure>
<img src="/2025/06/18/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%80%E4%BC%98%E5%8C%96/%E6%9C%80%E4%BC%98%E5%8C%96%E6%9C%9F%E6%9C%AB/IMG_20250624_120607.jpg" alt="IMG_20250624_120607">
<figcaption aria-hidden="true">IMG_20250624_120607</figcaption>
</figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="上一页" aria-label="上一页" href="/page/3/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><span class="space">&hellip;</span><a class="page-number" href="/page/13/">13</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/5/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">张熙浚</span>
  </div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="本站访问数 fa fa-user 次"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="本站总访问量 fa fa-eye 次"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script size="400" alpha="0.6" zIndex="-1" src="https://cdnjs.cloudflare.com/ajax/libs/ribbon.js/1.0.2/ribbon.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.1.0/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script src="/js/pjax.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  <script src="/js/third-party/pace.js"></script>


  
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
