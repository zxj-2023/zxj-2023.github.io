<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha256-XOqroi11tY4EFQMR9ZYwZWKj5ZXiftSx36RRuC3anlA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-bounce.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.20.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="zxj Blogs">
<meta property="og:type" content="website">
<meta property="og:title" content="Zhang XiJun">
<meta property="og:url" content="http://example.com/page/4/index.html">
<meta property="og:site_name" content="Zhang XiJun">
<meta property="og:description" content="zxj Blogs">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="张熙浚">
<meta property="article:tag" content="zxj">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/page/4/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"page/4/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Zhang XiJun</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Zhang XiJun</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">BLOGS</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="张熙浚"
      src="/images/zxjavatar.gif">
  <p class="site-author-name" itemprop="name">张熙浚</p>
  <div class="site-description" itemprop="description">zxj Blogs</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">155</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">60</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">61</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/zxj-2023" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zxj-2023" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="http://wpa.qq.com/msgrd?v=3&uin=2902065320&site=qq&menu=yes" title="QQ → http:&#x2F;&#x2F;wpa.qq.com&#x2F;msgrd?v&#x3D;3&amp;uin&#x3D;2902065320&amp;site&#x3D;qq&amp;menu&#x3D;yes" rel="noopener me" target="_blank"><i class="fab fa-qq fa-fw"></i>QQ</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
    <div class="sidebar-inner sidebar-blogroll">
      <div class="links-of-blogroll animated">
        <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
          链接
        </div>
        <ul class="links-of-blogroll-list">
            <li class="links-of-blogroll-item">
              <a href="https://zxj-2023.github.io/" title="https:&#x2F;&#x2F;zxj-2023.github.io" rel="noopener" target="_blank">Zhang XiJun</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://theme-next.js.org/" title="https:&#x2F;&#x2F;theme-next.js.org" rel="noopener" target="_blank">NexT</a>
            </li>
        </ul>
      </div>
    </div>
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/08/12/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83qwen3-32b/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/08/12/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83qwen3-32b/" class="post-title-link" itemprop="url">分布式训练qwen3-32b</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-08-12 00:00:00" itemprop="dateCreated datePublished" datetime="2025-08-12T00:00:00+08:00">2025-08-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-08-28 14:58:38" itemprop="dateModified" datetime="2025-08-28T14:58:38+08:00">2025-08-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%A8%A1%E5%9E%8B/" itemprop="url" rel="index"><span itemprop="name">模型</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%A8%A1%E5%9E%8B/%E5%BE%AE%E8%B0%83/" itemprop="url" rel="index"><span itemprop="name">微调</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="训练框架-LLaMA-Factor"><a href="#训练框架-LLaMA-Factor" class="headerlink" title="训练框架-LLaMA-Factor"></a>训练框架-<a target="_blank" rel="noopener" href="https://github.com/hiyouga/LLaMA-Factory">LLaMA-Factor</a></h3><p><a target="_blank" rel="noopener" href="https://llamafactory.readthedocs.io/zh-cn/latest/getting_started/installation.html">安装 - LLaMA Factory</a></p>
<p><strong>docker部署镜像</strong>，以便后续传入内网</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git</span><br><span class="line">cd LLaMA-Factory</span><br><span class="line"></span><br><span class="line">docker build -f ./docker/docker-cuda/Dockerfile \</span><br><span class="line">    --build-arg PIP_INDEX=https://pypi.org/simple \</span><br><span class="line">    --build-arg EXTRAS=metrics \</span><br><span class="line">    -t llamafactory:latest .</span><br><span class="line"></span><br><span class="line">docker run -dit --ipc=host --gpus=all \</span><br><span class="line">    -p 7860:7860 \</span><br><span class="line">    -p 8001:8000 \    # 主机 8001 → 容器 8000，主机8000端口被占用了</span><br><span class="line">    --name llamafactory \</span><br><span class="line">    -v /aisys/:/aisys/ \</span><br><span class="line">    docker.1ms.run/hiyouga/llamafactory</span><br><span class="line"></span><br><span class="line">docker run -dit --ipc=host --gpus=all -p 7860:7860 -p 8001:8000 -v /aisys/:/aisys/ --name llamafactory docker.1ms.run/hiyouga/llamafactory</span><br><span class="line"></span><br><span class="line">docker exec -it llamafactory bash</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker pull docker.1ms.run/hiyouga/llamafactory                                    </span><br><span class="line"></span><br><span class="line">docker save docker.1ms.run/hiyouga/llamafactory:latest -o llamafactory-image.tar</span><br><span class="line"></span><br><span class="line">docker load -i llamafactory-image.tar</span><br></pre></td></tr></table></figure>
<p><strong>LLaMA Board 可视化微调（由 <a target="_blank" rel="noopener" href="https://github.com/gradio-app/gradio">Gradio</a> 驱动）</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">llamafactory-cli webui</span><br></pre></td></tr></table></figure>
<ul>
<li>Web UI 访问：<code>http://localhost:7860</code></li>
<li>API 服务访问：<code>http://localhost:8001</code></li>
</ul>
<h3 id="数据集-easy-dataset"><a href="#数据集-easy-dataset" class="headerlink" title="数据集-easy-dataset"></a>数据集-<a target="_blank" rel="noopener" href="https://github.com/ConardLi/easy-dataset">easy-dataset</a></h3><p><strong>docker部署镜像</strong>，以便后续传入内网</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/ConardLi/easy-dataset.git</span><br><span class="line">cd easy-dataset</span><br><span class="line"></span><br><span class="line">docker build -t easy-dataset .</span><br><span class="line"></span><br><span class="line">docker load -i easy-dataset.tar</span><br><span class="line"></span><br><span class="line">docker run -d \</span><br><span class="line">  -p 1717:1717 \</span><br><span class="line">  -v /aisys/repo_dev/xizhang/lora_database:/app/local-db \</span><br><span class="line">  -v /aisys/repo_dev/xizhang/lora_databse_prisma:/app/prisma \</span><br><span class="line">  --name easy-dataset \</span><br><span class="line">  easy-dataset</span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">docker exec -it easy-dataset sh</span><br><span class="line"></span><br><span class="line">docker stop easy-dataset</span><br><span class="line">docker rm easy-dataset</span><br><span class="line"></span><br><span class="line">#实时跟踪</span><br><span class="line"> docker logs -f easy-dataset</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>注意：</strong> 请将 <code>&#123;YOUR_LOCAL_DB_PATH&#125;</code>、<code>&#123;LOCAL_PRISMA_PATH&#125;</code> 替换为你希望存储本地数据库的实际路径，建议直接使用当前代码仓库目录下的 <code>local-db</code> 和 <code>prisma</code> 文件夹，这样可以和 NPM 启动时的数据库路径保持一致。</p>
<p><strong>注意：</strong> 如果需要挂载数据库文件（PRISMA），需要提前执行 <code>npm run db:push</code> 初始化数据库文件。</p>
</blockquote>
<p>使用开源项目制作数据集</p>
<p>打开浏览器，访问 <code>http://localhost:1717</code></p>
<h3 id="上传内网"><a href="#上传内网" class="headerlink" title="上传内网"></a>上传内网</h3><p>使用scp</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp -r &quot;F:\project python\实习\微调\universal-llm_latest.tar&quot; root@10.117.128.50:/aisys/repo_dev/xizhang/images</span><br></pre></td></tr></table></figure>
<p><strong>SCP</strong> 全称是 <strong>Secure Copy Protocol</strong>（安全复制协议），是一种用于在计算机之间<strong>安全地复制文件</strong>的网络协议。</p>
<p>它基于 <strong>SSH</strong>（Secure Shell）协议工作，因此所有传输的数据都是<strong>加密的</strong>，可以防止被窃听或篡改，非常适合在不安全的网络（如互联网）中使用。</p>
<h3 id="模型部署与调用"><a href="#模型部署与调用" class="headerlink" title="模型部署与调用"></a>模型部署与调用</h3><h4 id="制作模型运行镜像"><a href="#制作模型运行镜像" class="headerlink" title="制作模型运行镜像"></a>制作模型运行镜像</h4><p>qwen3部署版本要求如下</p>
<p>使用 Python 3.10 或以上版本， PyTorch 2.6 或以上版本</p>
<p><code>transformers&gt;=4.51.0</code> 版本</p>
<p>使用 <code>sglang&gt;=0.4.6.post1</code> 或 <code>vllm&gt;=0.8.5</code> 来创建一个与 OpenAI 兼容的 API 端点</p>
<h4 id="镜像信息"><a href="#镜像信息" class="headerlink" title="镜像信息"></a>镜像信息</h4><div class="table-container">
<table>
<thead>
<tr>
<th>类别</th>
<th>组件</th>
<th>版本 / 来源</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>OS</strong></td>
<td>Ubuntu</td>
<td>22.04 LTS (Jammy)</td>
<td>上游镜像继承</td>
</tr>
<tr>
<td><strong>Python</strong></td>
<td>CPython</td>
<td>3.11</td>
<td>镜像自带</td>
</tr>
<tr>
<td><strong>PyTorch</strong></td>
<td>PyTorch</td>
<td>2.6.0+cu126</td>
<td>官方 wheel，CUDA 12.6</td>
</tr>
<tr>
<td><strong>CUDA</strong></td>
<td>Runtime</td>
<td>12.6.3</td>
<td>与宿主机 535 驱动兼容</td>
</tr>
<tr>
<td><strong>cuDNN</strong></td>
<td>cuDNN</td>
<td>9</td>
<td>包含在镜像</td>
</tr>
<tr>
<td><strong>核心库</strong></td>
<td>transformers</td>
<td>≥4.51.0</td>
<td>官方最新</td>
</tr>
<tr>
<td></td>
<td>tokenizers</td>
<td>≥0.21</td>
<td>transformers 依赖</td>
</tr>
<tr>
<td></td>
<td>accelerate</td>
<td>≥1.0.0</td>
<td>训练 / 推理加速</td>
</tr>
<tr>
<td></td>
<td>sentencepiece</td>
<td>≥0.2.0</td>
<td>Qwen3 分词器必需</td>
</tr>
<tr>
<td></td>
<td>protobuf</td>
<td>≥5.28.0</td>
<td>序列化 / 模型加载</td>
</tr>
<tr>
<td></td>
<td>tiktoken</td>
<td>≥0.8.0</td>
<td>OpenAI 格式分词</td>
</tr>
<tr>
<td><strong>推理框架</strong></td>
<td>vLLM</td>
<td>≥0.8.5</td>
<td>支持 tensor-parallel、PagedAttention</td>
</tr>
<tr>
<td></td>
<td>SGLang</td>
<td>≥0.4.6.post1</td>
<td>支持 outline 解码、MoE 优化</td>
</tr>
<tr>
<td><strong>可选加速</strong></td>
<td>flash-attn</td>
<td>≥2.7</td>
<td>长上下文 / 大 batch 推理</td>
</tr>
<tr>
<td><strong>权重下载</strong></td>
<td>modelscope</td>
<td>最新</td>
<td>国内镜像加速</td>
</tr>
<tr>
<td><strong>工具链</strong></td>
<td>git / git-lfs</td>
<td>最新</td>
<td>拉取 HuggingFace 权重</td>
</tr>
<tr>
<td></td>
<td>curl / jq / vim</td>
<td>最新</td>
<td>调试 &amp; 健康检查</td>
</tr>
</tbody>
</table>
</div>
<p><strong>基础镜像</strong><code>pytorch/pytorch:2.6.0-cuda12.6-cudnn9-devel</code> 是 <strong>PyTorch 官方在 Docker Hub 上提供的“全家桶”开发镜像</strong>，发布日期 2025-01-29，镜像大小约 13 GB，定位是 <strong>“开箱即用”的 GPU 训练 / 推理 / 调试环境</strong></p>
<h4 id="dockerfile"><a href="#dockerfile" class="headerlink" title="dockerfile"></a>dockerfile</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"># ---------- 1. 基础镜像 ----------</span><br><span class="line">FROM pytorch/pytorch:2.6.0-cuda12.6-cudnn9-devel</span><br><span class="line"></span><br><span class="line"># ---------- 2. 国内镜像源 ----------</span><br><span class="line">RUN sed -i &#x27;s|http://archive.ubuntu.com|https://mirrors.tuna.tsinghua.edu.cn|g&#x27; /etc/apt/sources.list &amp;&amp; \</span><br><span class="line">    sed -i &#x27;s|http://security.ubuntu.com|https://mirrors.tuna.tsinghua.edu.cn|g&#x27; /etc/apt/sources.list &amp;&amp; \</span><br><span class="line">    pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple &amp;&amp; \</span><br><span class="line">    pip config set global.trusted-host pypi.tuna.tsinghua.edu.cn</span><br><span class="line"></span><br><span class="line"># ---------- 3. 系统依赖 ----------</span><br><span class="line">RUN apt-get update &amp;&amp; \</span><br><span class="line">    DEBIAN_FRONTEND=noninteractive apt-get install -y \</span><br><span class="line">    git git-lfs build-essential ninja-build curl wget vim jq &amp;&amp; \</span><br><span class="line">    rm -rf /var/lib/apt/lists/*</span><br><span class="line"></span><br><span class="line"># ---------- 4. Python 依赖 ----------</span><br><span class="line">RUN pip install --no-cache-dir --upgrade pip setuptools wheel &amp;&amp; \</span><br><span class="line">    pip install --no-cache-dir \</span><br><span class="line">    &quot;torch==2.6.0+cu126&quot; \</span><br><span class="line">    &quot;transformers&gt;=4.51.0&quot; \</span><br><span class="line">    &quot;tokenizers&gt;=0.21&quot; \</span><br><span class="line">    &quot;accelerate&gt;=1.0.0&quot; \</span><br><span class="line">    &quot;sentencepiece&gt;=0.2.0&quot; \</span><br><span class="line">    &quot;protobuf&gt;=5.28.0&quot; \</span><br><span class="line">    &quot;tiktoken&gt;=0.8.0&quot; \</span><br><span class="line">    &quot;vllm&gt;=0.8.5&quot; \</span><br><span class="line">    &quot;sglang[all]&gt;=0.4.6.post1&quot; \</span><br><span class="line">    &quot;modelscope&quot; \</span><br><span class="line">    &quot;fastapi&quot; &quot;uvicorn[standard]&quot; &quot;pydantic&quot;</span><br><span class="line"></span><br><span class="line"># ---------- 5. 可选性能加速 ----------</span><br><span class="line">RUN pip install --no-cache-dir &quot;flash-attn&gt;=2.7&quot; --no-build-isolation || true</span><br><span class="line"></span><br><span class="line"># ---------- 6. 国内 HuggingFace 镜像 ----------</span><br><span class="line">ENV HF_ENDPOINT=https://hf-mirror.com</span><br><span class="line"></span><br><span class="line"># ---------- 7. 工作目录 ----------</span><br><span class="line">WORKDIR /app</span><br><span class="line">EXPOSE 4000 4001 4002</span><br><span class="line"></span><br><span class="line"># ---------- 8. 默认命令 ----------</span><br><span class="line">CMD [&quot;/bin/bash&quot;]</span><br></pre></td></tr></table></figure>
<h4 id="运行容器"><a href="#运行容器" class="headerlink" title="运行容器"></a>运行容器</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">docker run -it \</span><br><span class="line">  --name llm-service \</span><br><span class="line">  --gpus all \</span><br><span class="line">  -p 4000:4000 \</span><br><span class="line">  -p 4001:4001 \</span><br><span class="line">  -p 4002:4002 \</span><br><span class="line">  -v /aisys/repo_dev/xizhang/models:/app/models \</span><br><span class="line">  -v /aisys/repo_dev/xizhang/models/cache:/app/models/.cache \</span><br><span class="line">  --shm-size=8g \</span><br><span class="line">  universal-llm:latest bash</span><br></pre></td></tr></table></figure>
<h4 id="vllm部署qwen3"><a href="#vllm部署qwen3" class="headerlink" title="vllm部署qwen3"></a>vllm部署qwen3</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">vllm serve /app/models/qwen3-32b-lora-new \</span><br><span class="line">    --port 4001 \</span><br><span class="line">    --tensor-parallel-size 4 \</span><br><span class="line">    --max-model-len 1024 \</span><br><span class="line">    --reasoning-parser qwen3 \</span><br><span class="line">    --gpu-memory-utilization 0.8 \</span><br><span class="line">    --max-num-seqs 8 \</span><br><span class="line">    --host 0.0.0.0</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>含义</th>
<th>推荐/注意</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>--port 8000</code></td>
<td>服务监听端口</td>
<td>与 <code>-p 8000:8000</code> 保持一致；如需多实例，可改 8001/8002 …</td>
</tr>
<tr>
<td><code>--tensor-parallel-size 4</code></td>
<td>把模型权重切成 4 份，跨 4 张 GPU 并行计算</td>
<td>必须 ≤ 实际 GPU 数量；Qwen3-32B 在 4×L20 上显存刚好够，<strong>不可再大</strong></td>
</tr>
<tr>
<td><code>--max-model-len 1024</code></td>
<td>单次推理最大 token 数（含 prompt + 生成）</td>
<td>若场景需要 4k/8k/32k，可调到 4096/8192；显存占用 ∝ 长度²</td>
</tr>
<tr>
<td><code>--reasoning-parser qwen3</code></td>
<td>vLLM ≥0.8.5 新增开关，解析 Qwen3 的 <code>&lt;think&gt;…&lt;/think&gt;</code> 标签，把推理过程单独返回</td>
<td>仅在 <strong>Qwen3</strong> 系列模型有效，其他模型请去掉</td>
</tr>
<tr>
<td><code>--gpu-memory-utilization 0.8</code></td>
<td>显存使用上限 80 %；剩余 20 % 留给 CUDA kernel、KV cache 膨胀</td>
<td>若出现 OOM，可降到 0.7；若想多并发，可尝试 0.85（风险 OOM）</td>
</tr>
<tr>
<td><code>--max-num-seqs 8</code></td>
<td>同一时刻最多并发处理的 <strong>请求条数</strong></td>
<td>与 <code>--max-model-len</code> 和显存同时决定；若长度 ↑，此值需 ↓</td>
</tr>
<tr>
<td><code>--host 0.0.0.0</code></td>
<td>监听所有网卡，使容器外可访问</td>
<td>生产环境可改为内网 IP 或 127.0.0.1 提高安全性</td>
</tr>
</tbody>
</table>
</div>
<h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">curl http://localhost:4001/v1/chat/completions \</span><br><span class="line">   -H &quot;Content-Type: application/json&quot; \</span><br><span class="line">   -d &#x27;&#123;</span><br><span class="line">       &quot;model&quot;: &quot;/app/models/qwen3-32b-lora-new&quot;,</span><br><span class="line">       &quot;messages&quot;: [</span><br><span class="line">           &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;请用中文介绍一下你自己&quot;&#125;</span><br><span class="line">       ],</span><br><span class="line">       &quot;temperature&quot;: 0.7,</span><br><span class="line">       &quot;max_tokens&quot;: 512</span><br><span class="line">   &#125;&#x27;</span><br></pre></td></tr></table></figure>
<h4 id="调用"><a href="#调用" class="headerlink" title="调用"></a>调用</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">from openai import OpenAI</span><br><span class="line"></span><br><span class="line"># 指向本地 vLLM</span><br><span class="line">client = OpenAI(</span><br><span class="line">    base_url=&quot;http://localhost:8000/v1&quot;,</span><br><span class="line">    api_key=&quot;dummy&quot;          # vLLM 不做鉴权，随便填</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">resp = client.chat.completions.create(</span><br><span class="line">    model=&quot;qwen3-32b&quot;,       # 必须和 vLLM 启动路径或 --served-model-name 保持一致</span><br><span class="line">    messages=[</span><br><span class="line">        &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;9.9 和 9.11 哪个大？&quot;&#125;</span><br><span class="line">    ],</span><br><span class="line">    max_tokens=512,</span><br><span class="line">    temperature=0.7,</span><br><span class="line">    stream=False             # True 可开流式</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">print(resp.choices[0].message.content)</span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="https://qwen.readthedocs.io/en/latest/getting_started/quickstart.html">快速入门 - Qwen —- Quickstart - Qwen</a></p>
<p><a target="_blank" rel="noopener" href="https://www.modelscope.cn/models/Qwen/Qwen3-32B">通义千问3-32B · 模型库</a></p>
<h3 id="微调数据集"><a href="#微调数据集" class="headerlink" title="微调数据集"></a>微调数据集</h3><h4 id="alpaca和sharegpt的区别"><a href="#alpaca和sharegpt的区别" class="headerlink" title="alpaca和sharegpt的区别"></a>alpaca和sharegpt的区别</h4><p>▶ Alpaca 典型字段</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;instruction&quot;</span><span class="punctuation">:</span> <span class="string">&quot;把下面句子翻译成英文&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;input&quot;</span><span class="punctuation">:</span> <span class="string">&quot;今天天气真好&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;output&quot;</span><span class="punctuation">:</span> <span class="string">&quot;The weather is nice today.&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;system&quot;</span><span class="punctuation">:</span> <span class="string">&quot;你是一个翻译助手&quot;</span><span class="punctuation">,</span>   <span class="comment">// 可选</span></span><br><span class="line">  <span class="attr">&quot;history&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">]</span>                 <span class="comment">// 可选，放前几轮</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<ul>
<li>一条数据 = 一次独立任务</li>
<li>字段固定：<code>instruction / input / output</code> 三板斧</li>
</ul>
<p>▶ ShareGPT 典型字段</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;conversations&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">    <span class="punctuation">&#123;</span><span class="attr">&quot;from&quot;</span><span class="punctuation">:</span> <span class="string">&quot;human&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;value&quot;</span><span class="punctuation">:</span> <span class="string">&quot;我今天心情不好&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span><span class="attr">&quot;from&quot;</span><span class="punctuation">:</span> <span class="string">&quot;gpt&quot;</span><span class="punctuation">,</span>   <span class="attr">&quot;value&quot;</span><span class="punctuation">:</span> <span class="string">&quot;怎么啦？想聊聊吗&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span><span class="attr">&quot;from&quot;</span><span class="punctuation">:</span> <span class="string">&quot;human&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;value&quot;</span><span class="punctuation">:</span> <span class="string">&quot;论文又被拒了&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span><span class="attr">&quot;from&quot;</span><span class="punctuation">:</span> <span class="string">&quot;gpt&quot;</span><span class="punctuation">,</span>   <span class="attr">&quot;value&quot;</span><span class="punctuation">:</span> <span class="string">&quot;理解你的挫败感…&quot;</span><span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;system&quot;</span><span class="punctuation">:</span> <span class="string">&quot;你是贴心聊天机器人&quot;</span><span class="punctuation">,</span>   <span class="comment">// 可选</span></span><br><span class="line">  <span class="attr">&quot;tools&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span>...<span class="punctuation">]</span>                   <span class="comment">// 可选，放函数描述</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<ul>
<li>一条数据 = 一段完整的多轮对话</li>
<li>角色交替：<code>human / gpt / function / observation</code> 等</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>维度</th>
<th>Alpaca</th>
<th>ShareGPT</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>来源</strong></td>
<td>斯坦福 Alpaca 项目，为了低成本做指令微调</td>
<td>ShareGPT 网站爬取的真实 ChatGPT 对话</td>
</tr>
<tr>
<td><strong>目标</strong></td>
<td>让模型学会“看到指令+输入→给出答案”</td>
<td>让模型学会“像 ChatGPT 一样多轮对话”</td>
</tr>
</tbody>
</table>
</div>
<h4 id="详解"><a href="#详解" class="headerlink" title="详解"></a>详解</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;instruction&quot;: &quot;电阻率测定法的环境温湿度控制对检测结果的可信度有何影响？&quot;,</span><br><span class="line">    &quot;input&quot;: &quot;- \&quot;检测依据：DL<span class="operator">/</span>T <span class="number">421</span><span class="number">-2009</span> 电力用油体积电阻率测定法\&quot;\n- \&quot;检测地点及环境条件：油品检测室 温度：<span class="number">16</span>℃ 湿度：<span class="number">57</span><span class="operator">%</span>RH\&quot;\n- \&quot;电阻率（\&quot;&quot;,</span><br><span class="line">    &quot;output&quot;: &quot;&lt;think&gt;### \n首先，理解问题的核心：电阻率测定法中，环境温湿度控制如何影响检测结果的可信度，这涉及到测量过程的准确性和可靠性。\n\n然后，分析关键信息：检测依据是DL/T 421-2009标准，它规定了电阻率测定的方法；环境条件被记录为温度16℃和湿度57%RH；检测结果显示电阻率为1.04×10^10 Ω·cm，符合DL/T 571-2014标准的要求（≥6×10^9 Ω·cm）。\n\n接着，推理温湿度控制的影响：环境温湿度是测量过程中的关键变量，控制这些条件确保&quot;,</span><br><span class="line">    &quot;system&quot;: &quot;作为电力能源报告解读专家，我在生成答案时，将严格遵循以下格式：\n根据“信息来源”，“信息来源”是原文中可直接支撑结论的句子、数据或图表编号给出“结论与推理”——用上述逐条复现的信息为唯一依据，推导出最终答案。&quot;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>instruction为问题；input为上下文；output包含思维链与答案；system为系统提示词</p>
<h3 id="微调参数设置"><a href="#微调参数设置" class="headerlink" title="微调参数设置"></a>微调参数设置</h3><h4 id="DeepSpeed-stage（DeepSpeed-阶段）"><a href="#DeepSpeed-stage（DeepSpeed-阶段）" class="headerlink" title="DeepSpeed stage（DeepSpeed 阶段）"></a><strong>DeepSpeed stage（DeepSpeed 阶段）</strong></h4><p><strong>deepSpeed 的 ZeRO 分布式优化阶段</strong>，用于在多 GPU 上高效训练大模型。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Stage</th>
<th>功能</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Stage 0</strong></td>
<td>不做任何优化</td>
<td>基础分布式训练（DDP），显存占用高</td>
</tr>
<tr>
<td><strong>Stage 1</strong></td>
<td>梯度分片（Gradient Sharding）</td>
<td>将梯度切分到不同 GPU，减少显存</td>
</tr>
<tr>
<td><strong>Stage 2</strong></td>
<td>参数 + 梯度分片</td>
<td>进一步降低显存，但需通信同步</td>
</tr>
<tr>
<td><strong>Stage 3</strong></td>
<td>✅ <strong>参数 + 梯度 + 优化器状态分片</strong></td>
<td>最强显存优化，支持超大模型</td>
</tr>
</tbody>
</table>
</div>
<h4 id="使用-DeepSpeed-offload（使用-offload）"><a href="#使用-DeepSpeed-offload（使用-offload）" class="headerlink" title="使用 DeepSpeed offload（使用 offload）"></a>使用 DeepSpeed offload（使用 offload）</h4><p>将 <strong>部分或全部模型参数、优化器状态卸载到 CPU 内存</strong>，进一步释放 GPU 显存。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">llamafactory-cli train \</span><br><span class="line">    --stage sft \</span><br><span class="line">    --do_train True \</span><br><span class="line">    --model_name_or_path /aisys/repo_dev/xizhang/models/qwen3-32b-lora-new \</span><br><span class="line">    --preprocessing_num_workers 16 \</span><br><span class="line">    --finetuning_type lora \</span><br><span class="line">    --template qwen3 \</span><br><span class="line">    --flash_attn auto \</span><br><span class="line">    --dataset_dir /aisys/repo_dev/xizhang/lora_database/P9er76jCWCFW \</span><br><span class="line">    --dataset [Easy Dataset] [P9er76jCWCFW] Alpaca \</span><br><span class="line">    --cutoff_len 4096 \</span><br><span class="line">    --learning_rate 5e-05 \</span><br><span class="line">    --num_train_epochs 3.0 \</span><br><span class="line">    --max_samples 100000 \</span><br><span class="line">    --per_device_train_batch_size 2 \</span><br><span class="line">    --gradient_accumulation_steps 1 \</span><br><span class="line">    --lr_scheduler_type cosine \</span><br><span class="line">    --max_grad_norm 1.0 \</span><br><span class="line">    --logging_steps 5 \</span><br><span class="line">    --save_steps 200 \</span><br><span class="line">    --warmup_steps 0 \</span><br><span class="line">    --packing False \</span><br><span class="line">    --enable_thinking True \</span><br><span class="line">    --report_to none \</span><br><span class="line">    --output_dir saves/Qwen3-32B-Thinking/lora/train_2025-08-28-03-04-52 \</span><br><span class="line">    --bf16 True \</span><br><span class="line">    --plot_loss True \</span><br><span class="line">    --trust_remote_code True \</span><br><span class="line">    --ddp_timeout 180000000 \</span><br><span class="line">    --include_num_input_tokens_seen True \</span><br><span class="line">    --optim adamw_torch \</span><br><span class="line">    --lora_rank 8 \</span><br><span class="line">    --lora_alpha 16 \</span><br><span class="line">    --lora_dropout 0 \</span><br><span class="line">    --lora_target all \</span><br><span class="line">    --val_size 0.15 \</span><br><span class="line">    --eval_strategy steps \</span><br><span class="line">    --eval_steps 200 \</span><br><span class="line">    --per_device_eval_batch_size 2 \</span><br><span class="line">    --deepspeed cache/ds_z3_config.json</span><br></pre></td></tr></table></figure>
<h3 id="训练结果"><a href="#训练结果" class="headerlink" title="训练结果"></a>训练结果</h3><p><img src="/2025/08/12/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83qwen3-32b/image-20250827091214108.png" alt="image-20250827091214108"></p>
<h3 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h3><p>不知道为什么使用llamafactory的评估会爆显存，我怀疑是因为那个webui评估可能不支持多卡，就进行一下人工评估吧</p>
<p>输入</p>
<p><img src="/2025/08/12/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83qwen3-32b/image-20250828144300339.png" alt="image-20250828144300339"></p>
<p>微调模型</p>
<p><img src="/2025/08/12/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83qwen3-32b/image-20250828145106050.png" alt="image-20250828145106050"></p>
<p>初始模型</p>
<p><img src="/2025/08/12/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83qwen3-32b/image-20250828145750115.png" alt="image-20250828145750115"></p>
<h3 id="在内网计算节点访问SwanLab-Cloud"><a href="#在内网计算节点访问SwanLab-Cloud" class="headerlink" title="在内网计算节点访问SwanLab Cloud"></a>在内网计算节点访问SwanLab Cloud</h3><p><a target="_blank" rel="noopener" href="https://docs.swanlab.cn/guide_cloud/experiment_track/ssh-portforwarding.html">在内网计算节点访问SwanLab Cloud | SwanLab官方文档</a></p>
<h3 id="如何计算训练步数"><a href="#如何计算训练步数" class="headerlink" title="如何计算训练步数"></a>如何计算训练步数</h3><h4 id="1-训练集样本量"><a href="#1-训练集样本量" class="headerlink" title="1. 训练集样本量"></a>1. 训练集样本量</h4><p><strong>公式</strong><br>训练集样本量 = 总数据量 × (1 − 验证集比例)</p>
<p><strong>示例</strong><br>总数据 2876 条，验证集占 15%<br>2876 × (1 − 0.15) = 2876 × 0.85 = <strong>2446 条</strong></p>
<h4 id="2-每次参数更新处理的样本数（effective-batch-size）"><a href="#2-每次参数更新处理的样本数（effective-batch-size）" class="headerlink" title="2. 每次参数更新处理的样本数（effective batch size）"></a>2. 每次参数更新处理的样本数（effective batch size）</h4><p><strong>公式</strong><br>每次更新样本数 = 单设备批次大小 × GPU 数 × 梯度累积步数</p>
<p><strong>示例</strong></p>
<ul>
<li>per_device_train_batch_size = 1</li>
<li>GPU 数 = 2</li>
<li>gradient_accumulation_steps = 8</li>
</ul>
<p>1 × 2 × 8 = <strong>16 条</strong></p>
<blockquote>
<p>通俗理解：<br>GPU 一次只能看 1 条 → 2 卡并行就是 2 条 → 累积 8 次才更新一次参数，所以一次更新真正看了 16 条数据。</p>
</blockquote>
<h4 id="3-每轮（epoch）的训练步数"><a href="#3-每轮（epoch）的训练步数" class="headerlink" title="3. 每轮（epoch）的训练步数"></a>3. 每轮（epoch）的训练步数</h4><p><strong>公式</strong><br>每轮步数 = ⌊ 训练集样本量 ÷ 每次更新样本数 ⌋<br>（⌊ ⌋ 表示向下取整）</p>
<p><strong>示例</strong><br>2446 ÷ 16 = 152.875 → <strong>152 步</strong></p>
<h4 id="4-总训练步数"><a href="#4-总训练步数" class="headerlink" title="4. 总训练步数"></a>4. 总训练步数</h4><p><strong>公式</strong><br>总步数 = 每轮步数 × 训练轮数 (epochs)</p>
<p><strong>示例</strong><br>152 × 3 = <strong>456 步</strong></p>
<h3 id="如何计算一个模型占用的显存"><a href="#如何计算一个模型占用的显存" class="headerlink" title="如何计算一个模型占用的显存"></a>如何计算一个模型占用的显存</h3><h4 id="基础模型的权重"><a href="#基础模型的权重" class="headerlink" title="基础模型的权重"></a>基础模型的权重</h4><ul>
<li>定义：预训练模型的参数矩阵，即选择的预训练模型所占用显存的大小。</li>
<li><strong>计算公式</strong>：<br><strong>显存占用 = 模型参数数量 × 单个参数的字节数</strong></li>
</ul>
<h4 id="常见模型精度下的单个参数显存占用："><a href="#常见模型精度下的单个参数显存占用：" class="headerlink" title="常见模型精度下的单个参数显存占用："></a>常见模型精度下的单个参数显存占用：</h4><p>表格</p>
<p>复制</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">精度类型</th>
<th style="text-align:left">二进制位数</th>
<th style="text-align:left">字节数</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">FP32</td>
<td style="text-align:left">32位</td>
<td style="text-align:left">4字节</td>
</tr>
<tr>
<td style="text-align:left">FP16</td>
<td style="text-align:left">16位</td>
<td style="text-align:left">2字节</td>
</tr>
<tr>
<td style="text-align:left">BF16</td>
<td style="text-align:left">16位</td>
<td style="text-align:left">2字节（指数位同FP32）</td>
</tr>
<tr>
<td style="text-align:left">INT8</td>
<td style="text-align:left">8位</td>
<td style="text-align:left">1字节</td>
</tr>
<tr>
<td style="text-align:left">INT4</td>
<td style="text-align:left">4位</td>
<td style="text-align:left">0.5字节</td>
</tr>
<tr>
<td style="text-align:left">INT2</td>
<td style="text-align:left">2位</td>
<td style="text-align:left">0.25字节</td>
</tr>
</tbody>
</table>
</div>
<p>例如</p>
<ul>
<li><strong>模型选择</strong>：Qwen2.5-7B-Instruct</li>
<li><strong>参数规模</strong>：70亿（7B）</li>
<li><strong>计算精度</strong>：BF16（2字节/参数）</li>
<li><strong>预估显存占用</strong>：<br><strong>70亿 × 2字节 = 140亿字节 = 14GB</strong></li>
</ul>
<h4 id="框架开销（Framework-Overhead）"><a href="#框架开销（Framework-Overhead）" class="headerlink" title="框架开销（Framework Overhead）"></a>框架开销（Framework Overhead）</h4><ul>
<li><strong>定义</strong>：LLaMAFactory 底层使用的深度学习框架（如 PyTorch）本身的显存占用。</li>
<li><strong>包含内容</strong>：<ul>
<li>张量缓存</li>
<li>线程资源</li>
<li>内核调度开销</li>
<li>自动微分图结构等</li>
</ul>
</li>
<li><strong>计算方法</strong>：难以精确计算</li>
<li><strong>估算方法</strong>：通常占用不大，默认估算为 <strong>1 GB</strong></li>
</ul>
<h4 id="LoRA-适配器（LoRA-Adapters）"><a href="#LoRA-适配器（LoRA-Adapters）" class="headerlink" title="LoRA 适配器（LoRA Adapters）"></a>LoRA 适配器（LoRA Adapters）</h4><ul>
<li><p><strong>定义</strong>：在 LoRA 微调中，不直接修改原始模型的庞大权重，而是插入轻量级的“LoRA适配器模块”来学习微调所需的变化。</p>
</li>
<li><p><strong>计算方法</strong>：</p>
<p>显存占用=LoRA层数×秩（Rank）×(输入维度+输出维度)×2<em>B</em></p>
</li>
<li><p><strong>估算方法</strong>：</p>
<ul>
<li>与 LoRA 的秩（Rank）大小相关</li>
<li>一般占用不大，常规配置下通常不超过 <strong>0.5 GB</strong>，保守估计为 <strong>0.5 GB</strong></li>
</ul>
</li>
</ul>
<h4 id="激活值（Activations）"><a href="#激活值（Activations）" class="headerlink" title="激活值（Activations）"></a>激活值（Activations）</h4><ul>
<li><p><strong>定义</strong>：前向传播过程中各层的输出张量（如隐藏层状态、注意力矩阵等），即模型“处理数据时产生的所有中间结果”。</p>
</li>
<li><p><strong>计算方法</strong>：</p>
<p>显存占用=批量大小×序列长度×隐藏层维度×模型层数×单个元素字节数</p>
</li>
<li><p><strong>估算方法</strong>：</p>
<ul>
<li>单次处理的 Token 量每增加 <strong>1K</strong>，显存约增加 <strong>2.5 GB</strong></li>
<li>与单 GPU 的批量大小和数据集的截断长度（序列长度）正相关</li>
<li>在固定其他配置（基础模型权重、框架开销、LoRA适配器）后，剩余显存即为激活值占用</li>
</ul>
</li>
</ul>
<h3 id="加速方式"><a href="#加速方式" class="headerlink" title="加速方式"></a>加速方式</h3><div class="table-container">
<table>
<thead>
<tr>
<th>加速方式</th>
<th>全称 / 来源</th>
<th>核心原理与特点</th>
<th>适用场景与注意事项</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>auto</strong></td>
<td>自动选择</td>
<td>由框架（如 transformers、LLaMA-Factory、DeepSpeed 等）根据当前硬件、驱动、CUDA 版本自动挑选最快的可用算子或路径。 优点：零配置、开箱即用；缺点：不一定能启用最新、最快的内核。</td>
<td>初次实验、不想手动调参时首选。</td>
</tr>
<tr>
<td><strong>flashattn2</strong></td>
<td>FlashAttention-2</td>
<td>通过 IO-Aware 的算法和 GPU Tensor Core 优化，将标准 Multi-Head Attention 的显存访问次数大幅降低，从而显著加快训练/推理速度（通常 2-4×），并减少显存占用。 需要 A100、H100、RTX 30/40 系列等 Ampere/Lovelace 架构；依赖 CUDA≥11.8、PyTorch≥2.0 且需安装 <code>flash-attn</code> wheel。</td>
<td>训练/微调 LLM 时首选；序列越长收益越大。若编译失败可退回 xformers 或原生实现。</td>
</tr>
<tr>
<td><strong>unsloth</strong></td>
<td>Unsloth 开源库</td>
<td>针对 Llama、Mistral、Qwen 等架构，使用动态量化、手工 fused-kernel 和梯度检查点优化，使 LoRA 微调在消费级 GPU 上也能跑更大 batch/更长序列。官方宣称速度提升 2-5×，显存节省 50-70%。 安装简单：<code>pip install unsloth</code>（会自动替换部分 PyTorch 层）。</td>
<td>单卡 4090/3090 上 LoRA 微调 7B-13B 模型效果最佳；目前仅支持有限模型。</td>
</tr>
<tr>
<td><strong>liger_kernel</strong></td>
<td>Liger-Kernel（微软开源）</td>
<td>以 Triton 编写的高性能 fused-kernel 合集（SwiGLU、RMSNorm、CrossEntropy、RoPE 等），在保持数值精度的同时减少 kernel launch 和显存写回，训练吞吐量可提升 10-20%。 纯 Python/Triton 实现，无需额外 CUDA 编译。</td>
<td>对训练框架侵入性小，可与 FlashAttention 并存；适合想“无痛”提速 10-20% 的场景。</td>
</tr>
</tbody>
</table>
</div>
<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a target="_blank" rel="noopener" href="https://llamafactory.readthedocs.io/zh-cn/latest/">LLaMA Factory</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/08/11/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/LoRA%E5%85%B6%E4%BB%96%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E6%96%B9%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/08/11/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/LoRA%E5%85%B6%E4%BB%96%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E6%96%B9%E6%B3%95/" class="post-title-link" itemprop="url">LoRA其他的模型微调方法</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-08-11 00:00:00" itemprop="dateCreated datePublished" datetime="2025-08-11T00:00:00+08:00">2025-08-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-08-14 10:58:54" itemprop="dateModified" datetime="2025-08-14T10:58:54+08:00">2025-08-14</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%A8%A1%E5%9E%8B/" itemprop="url" rel="index"><span itemprop="name">模型</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%A8%A1%E5%9E%8B/%E5%BE%AE%E8%B0%83/" itemprop="url" rel="index"><span itemprop="name">微调</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><img src="/2025/08/11/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/LoRA%E5%85%B6%E4%BB%96%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E6%96%B9%E6%B3%95/image-20250814105851771.png" alt="image-20250814105851771"></p>
<p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzkzODI1NzQyNA==&amp;mid=2247494667&amp;idx=1&amp;sn=c3af7d2472de61752ef8b8df28746f2e&amp;poc_token=HCyNmWijps0ViWD6wPgqFiDYUZVRSs7xUDRfowWE">大模型微调技巧：LoRA 与 QLoRA讲解</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/javatiange/article/details/149964743?fromshare=blogdetail&amp;sharetype=blogdetail&amp;sharerId=149964743&amp;sharerefer=PC&amp;sharesource=2501_91530961&amp;sharefrom=from_link">一文详解：8种常见的大模型微调方法，看这篇就够了！-CSDN博客</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/682082440">大模型微调技术 - 知乎</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/08/11/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/08/11/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B/" class="post-title-link" itemprop="url">大模型训练流程</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2025-08-11 00:00:00 / 修改时间：09:55:56" itemprop="dateCreated datePublished" datetime="2025-08-11T00:00:00+08:00">2025-08-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%A8%A1%E5%9E%8B/" itemprop="url" rel="index"><span itemprop="name">模型</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%A8%A1%E5%9E%8B/%E5%BE%AE%E8%B0%83/" itemprop="url" rel="index"><span itemprop="name">微调</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="什么是大模型"><a href="#什么是大模型" class="headerlink" title="什么是大模型"></a>什么是大模型</h3><p>随着2022年底 ChatGPT 再一次刷新 NLP 的能力上限，大<strong>语言模型（Large Language Model，LLM）开始接替传统的预训练语言模型（Pre-trained Language Model，PLM）</strong> 成为 NLP 的主流方向，基于 LLM 的全新研究范式也正在刷新被 BERT 发扬光大的<strong>预训练-微调范式</strong>，NLP 由此迎来又一次翻天覆地的变化。</p>
<p>LLM，即 Large Language Model，中文名为大语言模型或大型语言模型，是一种相<strong>较传统语言模型参数量更多、在更大规模语料上进行预训练的语言模型</strong>。</p>
<p>一般来说，LLM 指包含<strong>数百亿（或更多）参数的语言模型</strong>，它们往往在<strong>数 T token 语料上</strong>通过多卡分布式集群进行预训练，具备远超出传统预训练模型的文本理解与生成能力。不过，随着 LLM 研究的不断深入，多种参数尺寸的 LLM 逐渐丰富，广义的 LLM 一般覆盖了从<strong>十亿参数</strong>（如 Qwen-1.5B）到<strong>千亿参数</strong>（如 Grok-314B）的所有大型语言模型。只要模型展现出<strong>涌现能力</strong>，即在一系列复杂任务上表现出远超传统预训练模型（如 BERT、T5）的能力与潜力，都可以称之为 LLM。</p>
<p>一般认为，GPT-3（1750亿参数）是 LLM 的开端，基于 GPT-3 通过 <strong>预训练（Pretraining）、监督微调（Supervised Fine-Tuning，SFT）、强化学习与人类反馈（Reinforcement Learning with Human Feedback，RLHF）</strong>三阶段训练得到的 ChatGPT 更是主导了 LLM 时代的到来。</p>
<blockquote>
<p>区分 LLM 与传统 PLM 最显著的特征即是 LLM 具备 <code>涌现能力</code> 。涌现能力是指同样的模型架构与预训练任务下，某些能力在小型模型中不明显，但在大型模型中特别突出。</p>
</blockquote>
<h3 id="训练流程"><a href="#训练流程" class="headerlink" title="训练流程"></a>训练流程</h3><p><img src="/2025/08/11/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B/image-20250811092459843.png" alt="image-20250811092459843"></p>
<p>一般而言，训练一个完整的 LLM 需要经过图1中的三个阶段——Pretrain、SFT 和 RLHF。</p>
<h3 id="Pretrain"><a href="#Pretrain" class="headerlink" title="Pretrain"></a>Pretrain</h3><p>Pretrain，即预训练，是训练 LLM 最核心也是工程量最大的第一步。</p>
<h4 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h4><div class="table-container">
<table>
<thead>
<tr>
<th>模型</th>
<th>hidden_layers</th>
<th>hidden_size</th>
<th>heads</th>
<th>整体参数量</th>
<th>预训练数据量</th>
</tr>
</thead>
<tbody>
<tr>
<td>BERT-base</td>
<td>12</td>
<td>768</td>
<td>12</td>
<td>0.1B</td>
<td>3B</td>
</tr>
<tr>
<td>BERT-large</td>
<td>24</td>
<td>1024</td>
<td>16</td>
<td>0.3B</td>
<td>3B</td>
</tr>
<tr>
<td>Qwen-1.8B</td>
<td>24</td>
<td>2048</td>
<td>16</td>
<td>1.8B</td>
<td>2.2T</td>
</tr>
<tr>
<td>LLaMA-7B</td>
<td>32</td>
<td>4096</td>
<td>32</td>
<td>7B</td>
<td>1T</td>
</tr>
<tr>
<td>GPT-3</td>
<td>96</td>
<td>12288</td>
<td>96</td>
<td>175B</td>
<td>300B</td>
</tr>
</tbody>
</table>
</div>
<p>根据定义，LLM 的核心特点即在于其具有<strong>远超传统预训练模型的参数量</strong>，<strong>同时在更海量的语料上进行预训练</strong>。传统预训练模型如 BERT，有 base 和 large 两个版本。BERT-base 模型由 12个 Encoder 层组成，其 hidden_size 为 768，使用 12个头作为多头注意力层，整体参数量为 1亿（110M）；而 BERT-large 模型由 24个 Encoder 层组成，hidden_size 为 1024，有 16个头，整体参数量为 3亿（340M）。同时，BERT 预训练使用了 33亿（3B）token 的语料，在 64块 TPU 上训练了 4天。事实上，相对于传统的深度学习模型，3亿参数量、33亿训练数据的 BERT 已经是一个能力超群、资源消耗巨大的庞然大物。</p>
<p>但是，前面我们提到，<strong>一般而言的 LLM 通常具有数百亿甚至上千亿参数</strong>，即使是广义上最小的 LLM，一般也有十亿（1B）以上的参数量。例如以开山之作 GPT-3 为例，其有 96个 Decoder 层，12288 的 hidden_size 和 96个头，<strong>共有 1750亿（175B）参数，比 BERT 大出快 3个数量级</strong>。即使是目前流行的小型 LLM 如 Qwen-1.8B，其也有 24个 Decoder 层、2048的 hidden_size 和 16个注意力头，整体参数量达到 18亿（1.8B）。</p>
<h4 id="分布式训练"><a href="#分布式训练" class="headerlink" title="分布式训练"></a>分布式训练</h4><p>也正因如此，<strong>分布式训练框架也成为 LLM 训练必不可少的组成部分</strong>。分布式训练框架的核心思路是<strong>数据并行和模型并行</strong>。所谓数据并行，是指训练模型的尺寸可以被单个 GPU 内存容纳，但是由于增大训练的 batch_size 会增大显存开销，无法使用较大的 batch_size 进行训练；同时，训练数据量非常大，使用单张 GPU 训练时长难以接受。</p>
<h4 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h4><p><strong>训练数据本身也是预训练 LLM 的一个重大挑战</strong>。训练一个 LLM，至少需要数百 B 甚至上 T 的预训练语料。根据研究，LLM 所掌握的知识绝大部分都是在预训练过程中学会的，因此，为了使训练出的 LLM 能够覆盖尽可能广的知识面，预训练语料需要组织多种来源的数据，并以一定比例进行混合。目前，主要的开源预训练语料包括 CommonCrawl、C4、Github、Wikipedia 等。<strong>不同的 LLM 往往会在开源预训练语料基础上，加入部分私有高质量语料，再基于自己实验得到的最佳配比来构造预训练数据集</strong>。事实上，<strong>数据配比</strong>向来是预训练 LLM 的“核心秘籍”，不同的配比往往会相当大程度影响最终模型训练出来的性能。</p>
<p>训练一个中文 LLM，训练数据的难度会更大。目前，高质量语料还是大部分集中在英文范畴，例如上表的 Wikipedia、Arxiv 等，均是英文数据集；而 C4 等多语言数据集中，英文语料也占据主要地位。目前开源的中文 LLM 如 ChatGLM、Baichuan 等模型均未开放其预训练数据集，开源的中文预训练数据集目前仅有昆仑天工开源的<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/Skywork/SkyPile-150B">SkyPile</a>（150B）、中科闻歌开源的<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/wenge-research/yayi2_pretrain_data">yayi2</a>（100B）等，相较于英文开源数据集有明显差距。</p>
<h4 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h4><p><strong>预训练数据的处理与清洗</strong>也是 LLM 预训练的一个重要环节。诸多研究证明，预训练数据的质量往往比体量更加重要。预训练数据处理一般包括以下流程：</p>
<ol>
<li>文档准备。由于海量预训练语料往往是从互联网上获得，一般需要从爬取的网站来获得自然语言文档。文档准备主要包括 URL 过滤（根据网页 URL 过滤掉有害内容）、文档提取（从 HTML 中提取纯文本）、语言选择（确定提取的文本的语种）等。</li>
<li>语料过滤。语料过滤的核心目的是去除低质量、无意义、有毒有害的内容，例如乱码、广告等。语料过滤一般有两种方法：基于模型的方法，即通过高质量语料库训练一个文本分类器进行过滤；基于启发式的方法，一般通过人工定义 web 内容的质量指标，计算语料的指标值来进行过滤。</li>
<li>语料去重。实验表示，大量重复文本会显著影响模型的泛化能力，因此，语料去重即删除训练语料中相似度非常高的文档，也是必不可少的一个步骤。去重一般基于 hash 算法计算数据集内部或跨数据集的文档相似性，将相似性大于指定阈值的文档去除；也可以基于子串在序列级进行精确匹配去重。</li>
</ol>
<h3 id="SFT-指令微调"><a href="#SFT-指令微调" class="headerlink" title="SFT 指令微调"></a>SFT 指令微调</h3><p>预训练赋予了 LLM 能力，却还需要第二步将其激发出来。经过预训练的 LLM 好像一个博览群书但又不求甚解的书生，对什么样的偏怪问题，都可以流畅地接出下文，但他偏偏又<strong>不知道问题本身的含义</strong>，只会“死板背书”。这一现象的本质是因为，LLM 的预训练任务就是经典的 <strong>CLM</strong>，也就是训<strong>练其预测下一个 token 的能力</strong>，在没有进一步微调之前，其无法与其他下游任务或是用户指令适配。</p>
<p>因此，我们还需要第二步来教这个博览群书的学生如何去使用它的知识，也就是 <strong>SFT（Supervised Fine-Tuning，有监督微调）</strong>。</p>
<p>面对能力强大的 LLM，我们往往不再是在指定下游任务上构造有监督数据进行微调，而是选择训练模型的“通用指令遵循能力”，也就是一般<strong>通过<code>指令微调</code>的方式来进行 SFT</strong>。</p>
<p>所谓指令微调，即我们训练的输入是各种类型的用户指令，而需要模型拟合的输出则是我们希望模型在收到该指令后做出的回复。例如，我们的一条训练样本可以是：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">input:告诉我今天的天气预报？</span><br><span class="line">output:根据天气预报，今天天气是晴转多云，最高温度26摄氏度，最低温度9摄氏度，昼夜温差大，请注意保暖哦</span><br></pre></td></tr></table></figure>
<p>也就是说，SFT 的主要目标是让模型从多种类型、多种风格的指令中获得泛化的指令遵循能力，也就是能够理解并回复用户的指令。</p>
<h3 id="RLHF"><a href="#RLHF" class="headerlink" title="RLHF"></a>RLHF</h3><p>RLHF，全称是 <strong>Reinforcement Learning from Human Feedback，即人类反馈强化学习</strong>，是利用强化学习来训练 LLM 的关键步骤。相较于在 GPT-3 就已经初见雏形的 SFT，RLHF 往往被认为是 ChatGPT 相较于 GPT-3 的最核心突破。事实上，从功能上出发，我们可以将 LLM 的训练过程分成<strong>预训练与对齐（alignment）两个阶段</strong>。预训练的核心作用是赋予模型海量的知识，而所谓对齐，其实就是让模型与人类价值观一致，从而输出人类希望其输出的内容。在这个过程中，SFT 是让 LLM 和人类的指令对齐，从而具有指令遵循能力；而 RLHF 则是从更深层次令 LLM 和人类价值观对齐，令其达到安全、有用、无害的核心标准。</p>
<p>RLHF 分为两个步骤：<strong>训练 RM 和 PPO 训练</strong>。</p>
<p><strong>RM，Reward Model，即奖励模型</strong>。RM 是用于拟合人类偏好，来给 LLM 做出反馈的。在强化学习的训练中，对于 LLM 的每一个回复，RM 会进行打分，这个打分反映了生成回复符合人类偏好的程度。然后 LLM 会根据强化学习的原理，基于 RM 的打分来进行优化训练。</p>
<p>在完成 RM 训练之后，就可以使用 PPO 算法来进行强化学习训练。<strong>PPO，Proximal Policy Optimization，近端策略优化算法</strong>，是一种经典的 RL 算法。事实上，强化学习训练时也可以使用其他的强化学习算法，但目前 PPO 算法因为成熟、成本较低，还是最适合 RLHF 的算法。</p>
<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a target="_blank" rel="noopener" href="https://datawhalechina.github.io/happy-llm/#/./chapter4/第四章 大语言模型">第四章 大语言模型</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/08/11/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E2%80%94%E2%80%94LoRA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/08/11/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E2%80%94%E2%80%94LoRA/" class="post-title-link" itemprop="url">模型微调——LoRA</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-08-11 00:00:00" itemprop="dateCreated datePublished" datetime="2025-08-11T00:00:00+08:00">2025-08-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-08-12 19:15:16" itemprop="dateModified" datetime="2025-08-12T19:15:16+08:00">2025-08-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%A8%A1%E5%9E%8B/" itemprop="url" rel="index"><span itemprop="name">模型</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%A8%A1%E5%9E%8B/%E5%BE%AE%E8%B0%83/" itemprop="url" rel="index"><span itemprop="name">微调</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="为什么要微调"><a href="#为什么要微调" class="headerlink" title="为什么要微调"></a>为什么要微调</h3><p>预训练大模型在海量通用语料上学到的知识，在垂直场景（医疗、法律、零售客服等）里往往“泛而浅”。</p>
<p>从零训练一个同等规模的大模型成本极高（千卡周级别），而微调只需在已有权重上做小步调整，算力/数据量都指数级下降。</p>
<h3 id="什么是全量微调"><a href="#什么是全量微调" class="headerlink" title="什么是全量微调"></a>什么是全量微调</h3><p><img src="/2025/08/11/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E2%80%94%E2%80%94LoRA/image-20250811104846104.png" alt="image-20250811104846104"></p>
<p>全量微调（full fine-tuning）通俗来说，对于参数的每一个权重，都要学习一个新的值（或者偏移量），更新所有 Transformer 层里的权重矩阵（包括 embedding、attention、FFN），这样的开销是很大的。</p>
<h3 id="什么是LoRA"><a href="#什么是LoRA" class="headerlink" title="什么是LoRA"></a>什么是LoRA</h3><p>LoRA（Low-Rank Adaptation，低秩适配）是一种<strong>参数高效微调（PEFT）</strong>技术，核心目的：<br><strong>“冻结大模型 99 % 以上原始权重，只额外训练极少量低秩矩阵，就能让模型在下游任务上达到近似全量微调的效果。”</strong></p>
<p><img src="/2025/08/11/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E2%80%94%E2%80%94LoRA/image-20250811105145633.png" alt="image-20250811105145633"></p>
<p>通俗来说，通过学习两个低秩的矩阵，来近似于完整的矩阵，如图，W=A*B，矩阵相乘</p>
<p>在实际应用中，<strong>LoRA可以直接和transformer的FFN层（线性层）对齐</strong></p>
<p>Transformer 模型的核心是注意力机制，其中涉及到 Query, Key, Value 的计算，这些都是线性变换。</p>
<p>在标准的注意力机制中，计算公式为：</p>
<script type="math/tex; mode=display">
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V</script><p>其中 $Q$, $K$, $V$ 的计算为：</p>
<script type="math/tex; mode=display">
Q = X_Q W_Q, \quad K = X_K W_K, \quad V = X_V W_V</script><p> $X_Q$, $X_K$, $X_V$ 的输入可以相同，也可以不同。例如，在 Cross-Attention 中，解码器的隐藏状态作为 $X_Q$，编码器的输出作为 $X_K$ 和 $X_V$。</p>
<p><strong>LoRA 可以应用到 $W_Q$, $W_K$, $W_V$ 上，采用与线性层类似的方式</strong>。</p>
<h3 id="为什么要用lora"><a href="#为什么要用lora" class="headerlink" title="为什么要用lora"></a>为什么要用lora</h3><p>首先要理解低秩：秩可以理解成一个矩阵所代表的信息，低秩矩阵，便是带有少量信息的矩阵，当然这样的矩阵计算效率是更高的，</p>
<p>在全量微调中，由于训练一个完整的矩阵开销是非常大的；在lora中就通过训练低秩矩阵，来近似<strong>模型权重更新</strong>的效果</p>
<blockquote>
<p>若模型参数比较小，使用冻结部分参数或全量微调的方式，往往更好</p>
</blockquote>
<p>初学者不禁会思考，这样难道不会损失信息导致大模型的性能变差吗？但是，实验下来效果还是不错的，通过牺牲一点性能，来换取开销的大幅度减少</p>
<blockquote>
<p> LoRA 原文实验<br>在 GPT-3 175 B 上，仅用 rank 4 的 LoRA 就能在全量微调 99 % 参数量的情况下，保持 97 % 的下游指标。</p>
</blockquote>
<h3 id="什么是QLoRA"><a href="#什么是QLoRA" class="headerlink" title="什么是QLoRA"></a>什么是QLoRA</h3><p>QLoRA（Quantized Low-Rank Adaptation，量化低秩适应）是 <strong>LoRA 的“极致省内存”版本</strong>。它把 LoRA 的“低秩增量”思路再往前推一步：<strong>先把整个底座模型权重压到 4-bit，再在上面做 LoRA 微调</strong>。</p>
<p>QLoRA 是另一个热门术语，它与 LoRA 之间的唯一区别在于首字母“Q”，代表“量化（quantized）”。“量化”一词指的是用来减少存储神经元权重的比特数。</p>
<p>例如，神经网络的权重通常以浮点数表示，每个权重需要 32 位。量化的思想是将神经网络的权重压缩为更低的精度，而不会显著损失模型性能或产生重大影响。因此，不再使用 32 位，而是可以舍弃部分比特，例如只用 16 位。</p>
<p><img src="/2025/08/11/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E2%80%94%E2%80%94LoRA/image-20250811142432782.png" alt="image-20250811142432782"></p>
<h3 id="微调工具的介绍"><a href="#微调工具的介绍" class="headerlink" title="微调工具的介绍"></a>微调工具的介绍</h3><h4 id="unsloth"><a href="#unsloth" class="headerlink" title="unsloth"></a>unsloth</h4><p><a target="_blank" rel="noopener" href="https://github.com/unslothai/unsloth?tab=readme-ov-file">unslothai/unsloth: Fine-tuning &amp; Reinforcement Learning for LLMs. 🦥 Train OpenAI gpt-oss, Qwen3, Llama 4, DeepSeek-R1, Gemma 3, TTS 2x faster with 70% less VRAM.</a></p>
<p>unsloth是一个专为大型语言模型（LLM）设计的动态量化与微调框架，旨在提高微调效率并减少显存占用，因此主要用于单机单卡的模型微调。</p>
<p>值得一提的是，Unsloth动态量化模型：<a target="_blank" rel="noopener" href="https://unsloth.ai/blog/dynamic-v2">https://unsloth.ai/blog/dynamic-v2</a></p>
<p>Unsloth的动态量化方法，特别是其最新的Dynamic 2.0版本，旨在在尽量减少性能损失的同时显著压缩大型语言模型（LLMs）的体积。对于Qwen3模型，尤其是4-bit动态量化版本，现有的评测显示其性能下降非常有限，甚至在某些任务上与原始模型相当。</p>
<blockquote>
<p>Unsloth 的「动态量化」可以一句话概括为：<br><strong>“按层、按敏感度自动决定每块权重到底用 2.5 / 3.5 / 4 / 6 / 8 / 32 bit 的精细化量化策略，而不是一股脑全量化到 4 bit。”</strong></p>
</blockquote>
<p>这也使得Unsloth的动态量化模型成为<strong>个人配置</strong>下的最佳微调工具。</p>
<p>不过需要注意的是，动态量化由利也有弊，其<strong>好处在于可以极大程度压缩模型运行所需占用的显存大小，同时几乎不损失性能</strong>，但问题在于动态量化的模型，无论是推理还是微调，<strong>只能单卡运行</strong>，这就使得其吞吐量有限，无法在一台物理机上实现多GPU并行从而扩大吞吐量。</p>
<h4 id="LLaMA-Factory"><a href="#LLaMA-Factory" class="headerlink" title="LLaMA Factory"></a><strong>LLaMA Factory</strong></h4><p><a target="_blank" rel="noopener" href="https://github.com/hiyouga/LLaMA-Factory/tree/main">hiyouga/LLaMA-Factory: Unified Efficient Fine-Tuning of 100+ LLMs &amp; VLMs (ACL 2024)</a></p>
<p>LLaMA Factory 是一个简单易用且高效的大型语言模型训练与微调平台。通过它，用户可以在无需编写任何代码的前提下，在本地完成上百种预训练模型的微调。</p>
<p>LLaMA Factory 提供了API Server 和一站式 WebUI Board，方便企业进行模型的管理和部署。适合不会写代码或代码基础比较弱的同学快速上手进行微调。</p>
<h4 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h4><p>ms-SWIFT GitHub项目主页：<a target="_blank" rel="noopener" href="https://github.com/modelscope/swift">https://github.com/modelscope/swift</a></p>
<p>ColossalAI GitHub项目主页：<a target="_blank" rel="noopener" href="https://github.com/hpcaitech/ColossalAI">https://github.com/hpcaitech/ColossalAI</a></p>
<p>除此之外，也可以借助更加底层的库，如peft、LoRA、transformer等实现高效微调。</p>
<h3 id="模型性能评估框架"><a href="#模型性能评估框架" class="headerlink" title="模型性能评估框架"></a>模型性能评估框架</h3><h4 id="EvalScope"><a href="#EvalScope" class="headerlink" title="EvalScope"></a>EvalScope</h4><p>项目地址： <a target="_blank" rel="noopener" href="https://github.com/modelscope/evalscope">https://github.com/modelscope/evalscope</a></p>
<p>EvalScope 是由阿里巴巴魔搭社区（ModelScope）推出的一款开源模型评估框架，旨在为大语言 模型（LLM）和多模态模型提供统一、系统化的性能评估方案。该框架具备高度的自动化和可扩展性， 适用于研究机构、工业界以及模型开发者在模型验证与性能对比场景中的广泛需求。</p>
<h3 id="可视化框架"><a href="#可视化框架" class="headerlink" title="可视化框架"></a>可视化框架</h3><h4 id="wandb"><a href="#wandb" class="headerlink" title="wandb"></a>wandb</h4><p><strong>Weights &amp; Biases（简称 wandb）</strong> 是一个专为机器学习 / 深度学习设计的 <strong>云端实验管理、可视化与协作平台</strong>。它帮你把“训练过程中发生了什么”全部自动化地记录下来，并以网页仪表盘的形式实时展示，省去你手动保存日志、画图、整理表格的麻烦。</p>
<p>wandb官网： <a target="_blank" rel="noopener" href="https://wandb.ai/site">https://wandb.ai/site</a></p>
<h4 id="swanlab"><a href="#swanlab" class="headerlink" title="swanlab"></a>swanlab</h4><p>SwanLab 是一款<strong>开源、轻量</strong>的 AI 模型训练跟踪与可视化工具，提供了一个<strong>跟踪、记录、比较、和协作实验</strong>的平台。</p>
<p>SwanLab 面向人工智能研究者，设计了友好的Python API 和漂亮的UI界面，并提供<strong>训练可视化、自动日志记录、超参数记录、实验对比、多人协同等功能</strong>。在SwanLab上，研究者能基于直观的可视化图表发现训练问题，对比多个实验找到研究灵感，并通过<strong>在线网页</strong>的分享与基于组织的<strong>多人协同训练</strong>，打破团队沟通的壁垒，提高组织训练效率。</p>
<p><a target="_blank" rel="noopener" href="https://docs.swanlab.cn/">SwanLab官方文档 | 先进的AI团队协作与模型创新引擎</a></p>
<h3 id="构造微调数据集"><a href="#构造微调数据集" class="headerlink" title="构造微调数据集"></a>构造微调数据集</h3><h4 id="为什么要构造微调数据集"><a href="#为什么要构造微调数据集" class="headerlink" title="为什么要构造微调数据集"></a>为什么要构造微调数据集</h4><p><img src="/2025/08/11/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E2%80%94%E2%80%94LoRA/image-20250811162229104.png" alt="image-20250811162229104"></p>
<p>其中 &lt;∣im_start∣&gt; 代表文本开始,而user则代表消息身份,用于构建多轮对话,而<lim_end>则代表文本结束,即用户输入结束,而<lim_start>代表新一段文本开始,assistant代表接下来由模型创建消息,而<lim_end>同样代表模型创建消息的结束。</lim_end></lim_start></lim_end></p>
<p>而模型其实是通过这样一组<strong>特殊字符标记</strong>来规范自己的行为,<strong>判断当前消息类型,以及通过输出特殊标记来确定停止时间</strong>。对于绝大多数模型,我们可以在模型的<strong>tokenizer_config.json中看到完整的特殊标记符</strong>(以及系统提示词模板):</p>
<p><img src="/2025/08/11/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E2%80%94%E2%80%94LoRA/image-20250811163120092.png" alt="image-20250811163120092"></p>
<p>而在实际微调过程中,我们都知道需要<strong>有监督的数据集</strong>、也就是需要输入QA对来进行微调。以著名的<strong>alpaca_zh中文微调数据集</strong>来说,其基本格式如下:</p>
<p><img src="/2025/08/11/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E2%80%94%E2%80%94LoRA/image-20250811163232521.png" alt="image-20250811163232521"></p>
<p>就可以表示为下列json格式数据集:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">json&#123;  &quot;instruction&quot;: &quot;&quot;,  &quot;input&quot;: &quot;输入:你好。&quot;,  &quot;output&quot;: &quot;输出:你好,有什么可以帮到你的?&quot;&#125;</span><br></pre></td></tr></table></figure>
<p>而在真实的微调过程中,如果是针对Qwen3进行微调,微调脚本会将这条数据集(无论什么格式)转化为如下格式:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xml&lt;im_start|&gt;user\n你好&lt;im_end|&gt;\n&lt;im_start|&gt;assistant\n你好,有什么可以帮到你的?&lt;im_end|&gt;</span><br></pre></td></tr></table></figure>
<p>而在实际训练过程中,模型就会根据assistant前的内容,学习assistant后面的输出内容。</p>
<p><strong>因此我们要在下载数据集后，进行微调前，对数据集进行预处理</strong>，接下来引出构造数据集的几种场景</p>
<h4 id="带有系统提示微调数据集格式"><a href="#带有系统提示微调数据集格式" class="headerlink" title="带有系统提示微调数据集格式"></a>带有系统提示微调数据集格式</h4><p>在很多场景下,我们还会发现一些<strong>带有instruction字段的微调数据集</strong>,那instruction字段是如何带入到微调过程中的呢?</p>
<p><img src="/2025/08/11/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E2%80%94%E2%80%94LoRA/image-20250811163232521.png" alt="image-20250811163232521"></p>
<p>答案非常简单,还是依靠特殊字符。例如有一个对话内容如下:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">- 系统提示词(instruction):你是一名助人为乐的助手。</span><br><span class="line">- 用户输入(input):你好,好久不见。</span><br><span class="line">- 助手回复(output):是的呀,好久不见,最近有什么有趣的事情要和我分享么?</span><br></pre></td></tr></table></figure>
<p>此时模型的输入和输出如下:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;lim_start|&gt;system你是一名助人为乐的助手。&lt;/im_end&gt;</span><br><span class="line">&lt;lim_start|&gt;user 你好,好久不见。&lt;/lim_end&gt;</span><br><span class="line">&lt;lim_start|&gt;assistant 是的呀,好久不见,最近有什么有趣的事情要和我分享么?&lt;/lim_end&gt;</span><br></pre></td></tr></table></figure>
<p>即会通过<lim_start|>system…<lim_end|>来标记系统提示词。实际进行微调时,模型会根据assistant为界,学习assistant之前的文本输入情况下应该如何输出。</lim_end|></lim_start|></p>
<h4 id="带Function-calling微调数据集格式"><a href="#带Function-calling微调数据集格式" class="headerlink" title="带Function calling微调数据集格式"></a>带Function calling微调数据集格式</h4><p>更进一步的,如果对话过程中带入了<strong>Function calling</strong>,此时首先模型会读取提前准备好的tool schema(也可能是自动生成的,例如MCP即可自动创建tool schema):</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;tool_schema&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;name&quot;: &quot;get_weather&quot;,</span><br><span class="line">      &quot;description&quot;: &quot;查询指定城市的天气信息&quot;,</span><br><span class="line">      &quot;parameters&quot;: &#123;</span><br><span class="line">        &quot;type&quot;: &quot;object&quot;,</span><br><span class="line">        &quot;properties&quot;: &#123;</span><br><span class="line">          &quot;location&quot;: &#123;</span><br><span class="line">            &quot;type&quot;: &quot;string&quot;,</span><br><span class="line">            &quot;description&quot;: &quot;要查询天气的城市名称&quot;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;required&quot;: [&quot;location&quot;]</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>而假设我们的对话内容如下:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">- 系统提示词(instruction):你是一名助人为乐的助手。当用户查询天气的时候,请调用get_weather函数进行天气信息查询。</span><br><span class="line">- 用户输入(input):你好,请帮我查询下北京天气。</span><br><span class="line">- 助手回复(output):&#123;&quot;name&quot;: &quot;get_weather&quot;, &quot;arguments&quot;: &#123;&quot;location&quot;: &quot;北京&quot;&#125;&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>此时回复内容就是一条Function call message</p>
</blockquote>
<p>而此时模型真实的输入和输出内容如下:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&lt;|im_start|&gt;system</span><br><span class="line">你是天气助手，当用户查询天气时请调用 get_weather 函数。</span><br><span class="line"># Tools</span><br><span class="line">You may call one or more functions to assist with the user query.</span><br><span class="line">You are provided with function signatures within &lt;tools&gt;&lt;/tools&gt; XML tags:</span><br><span class="line">&lt;tools&gt;</span><br><span class="line">[&#123;&quot;name&quot;:&quot;get_weather&quot;,&quot;description&quot;:&quot;查询指定城市的天气信息&quot;,&quot;parameters&quot;:&#123;&quot;type&quot;:&quot;object&quot;,&quot;properties&quot;:&#123;&quot;location&quot;:&#123;&quot;type&quot;:&quot;string&quot;,&quot;description&quot;:&quot;要查询天气的城市名称&quot;&#125;&#125;,&quot;required&quot;:[&quot;location&quot;]&#125;&#125;]</span><br><span class="line">&lt;/tools&gt;</span><br><span class="line">&lt;tool_call&gt;</span><br><span class="line"> &#123;&quot;name&quot;: &lt;function-name&gt;, &quot;arguments&quot;: &lt;args-json-object&gt;&#125;</span><br><span class="line">&lt;/tool_call&gt;.</span><br><span class="line">&lt;|im_end|&gt;</span><br><span class="line">&lt;|im_start|&gt;user</span><br><span class="line">北京天气如何？</span><br><span class="line">&lt;|im_end|&gt;</span><br><span class="line">&lt;|im_start|&gt;assistant</span><br><span class="line">&lt;tool_call&gt;&#123;&quot;name&quot;:&quot;get_weather&quot;,&quot;arguments&quot;:&#123;&quot;location&quot;:&quot;北京&quot;&#125;&#125;&lt;/tool_call&gt;</span><br><span class="line">&lt;|im_end|&gt;</span><br></pre></td></tr></table></figure>
<p>接下来在进行训练时,模型同样根据assistant前的内容,学习assistant后面的输出内容。不过需要注意的是,由于高效微调调整的参数量较少,因此只能优化模型的Function calling能力,并不能从无到有让模型学会Function calling。</p>
<h4 id="带有思考过程的微调数据集结构"><a href="#带有思考过程的微调数据集结构" class="headerlink" title="带有思考过程的微调数据集结构"></a>带有思考过程的微调数据集结构</h4><p>而如果是带有思考链,则一个简单的问答数据如下:</p>
<p><img src="/2025/08/11/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E2%80%94%E2%80%94LoRA/image-20250811165802090.png" alt="image-20250811165802090"></p>
<ul>
<li>系统提示词(instruction):你是一名助人为乐的助手。</li>
<li>用户输入(input):你好,好久不见。</li>
<li>助手回复(output):好的,用户发来“你好,好久不见!”,我需要回应。首先,用户可能希望得到亲切的回应,所以应该用友好的语气。/n是的呀,好久不见,最近有什么有趣的事情要和我分享么?</li>
</ul>
<p>此时模型真实的内部输入和输出结果如下:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;lim_start|&gt;system</span><br><span class="line">你是一名助人为乐的助手。&lt;lim_end|&gt;</span><br><span class="line">&lt;lim_start|&gt;user</span><br><span class="line">你好,好久不见。&lt;lim_end|&gt;</span><br><span class="line">&lt;lim_start|&gt;assistant</span><br><span class="line"></span><br><span class="line">&lt;think&gt;  好的,用户发来“你好,好久不见!”,我需要回应。首先,用户可能希望得到亲切的回应,所以应该用友好的语气。&lt;/think&gt;</span><br><span class="line"></span><br><span class="line">是的呀,好久不见,最近有什么有趣的事情要和我分享么?&lt;/lim_end|&gt;</span><br></pre></td></tr></table></figure>
<p>模型同样根据assistant前的内容,学习assistant后面的输出内容。也就是说,所谓的思考过程,本质上其实是一种文本响应格式,通过模型训练而来。</p>
<h4 id="混合推理模型构造微调数据集基本方法"><a href="#混合推理模型构造微调数据集基本方法" class="headerlink" title="混合推理模型构造微调数据集基本方法"></a>混合推理模型构造微调数据集基本方法</h4><p>在了解了微调数据集结构背后的基本原理后,接下来的问题是应该如何构造微调数据集呢?</p>
<p>一般来说我们可以在huggingface、ModelScope或llama- factory中挑选合适的数据集,并根据实际情况进行组装。</p>
<p>例如围绕Qwen3模型的高效微调,为了确保其仍然<strong>保留混合推理能力,</strong>我们可以考虑在微调数据集中加入如普<strong>通对话数据集</strong><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/mlabonne/FineTome-100k">FineTome</a>,以及<strong>带有推理字段的数学类数据集</strong><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/nvidia/OpenMathReasoning">OpenMathReasoning</a>,<strong>并围绕这两个数据集进行拼接</strong>,从而在确保能提升模型的数学能力的同时,保留非推理的功能。</p>
<p>同时还需要在持续微调训练过程中<strong>不断调整COT数学数据集和普通文本问答数据集之间的配比</strong>,以确保模型能够在提升数学能力的同时,保留混合推理的性能。</p>
<blockquote>
<p>Qwen3 的「混合推理能力」= <strong>在同一个模型里内置两套“大脑”</strong>：<br>• <strong>快思考（非思考模式）</strong>：轻量算力、秒级响应，适合简单问答；<br>• <strong>慢思考（思考模式）</strong>：多步链式推理、深度推敲，适合复杂逻辑、数学、代码。<br>系统会自动或按用户指令在两种模式之间 <strong>动态切换</strong>，从而 <strong>既省算力又保证难题精度</strong>。</p>
</blockquote>
<h3 id="微调的基本流程"><a href="#微调的基本流程" class="headerlink" title="微调的基本流程"></a>微调的基本流程</h3><p><img src="/2025/08/11/%E5%AD%A6%E4%B9%A0/%E6%A8%A1%E5%9E%8B/%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E2%80%94%E2%80%94LoRA/image-20250812105535330.png" alt="image-20250812105535330"></p>
<h3 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h3><p><strong>安装Unsloth</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install --upgrade --force-reinstall --no-cache-dir unsloth unsloth_zoo</span><br></pre></td></tr></table></figure>
<p><strong>安装Qwen3-8B-unsloth-bnb-4bit</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">modelscope download --model unsloth/Qwen3-8B-unsloth-bnb-4bit --local_dir /workspace/qwen3-8b</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#模型下载</span><br><span class="line">from modelscope import snapshot_download</span><br><span class="line">model_dir = snapshot_download(&#x27;unsloth/Qwen3-8B-unsloth-bnb-4bit&#x27;)</span><br></pre></td></tr></table></figure>
<blockquote>
<p> <strong>unsloth/Qwen3-8B-unsloth-bnb-4bit</strong> 这个模型它是 <strong>专门为Unsloth微调框架优化过的4bit量化版本</strong></p>
<p>原始 Qwen3-8B（FP16）需要约 <strong>22GB 显存</strong>，而 4bit 量化后仅需 <strong>6GB 左右</strong></p>
<p><strong>只要显存允许，原始 FP16/BF16 模型也可以用 Unsloth 做 4-bit LoRA（即 QLoRA）微调；官方预量化 4-bit 模型只是帮你把“量化”这一步提前做完了，二者本质相同。</strong></p>
<p><strong>Unsloth 的两种用法示例</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">场景</th>
<th style="text-align:left">代码片段</th>
<th style="text-align:left">备注</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">A. 用官方已量化好的 4-bit 权重</td>
<td style="text-align:left"><code>model_name=&quot;unsloth/Qwen3-8B-bnb-4bit&quot;</code></td>
<td style="text-align:left">显卡 6 GB 就能跑，省去自己量化</td>
</tr>
<tr>
<td style="text-align:left">B. 用原始 FP16 权重并现场 4-bit 量化</td>
<td style="text-align:left"><code>model_name=&quot;Qwen/Qwen3-8B&quot;</code> + <code>load_in_4bit=True</code></td>
<td style="text-align:left">显卡仍需 6 GB，显存占用与 A 相同</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> unsloth <span class="keyword">import</span> FastLanguageModel</span><br><span class="line"></span><br><span class="line"><span class="comment"># 两种写法效果等价</span></span><br><span class="line">model, tokenizer = FastLanguageModel.from_pretrained(</span><br><span class="line">    model_name=<span class="string">&quot;Qwen/Qwen3-8B&quot;</span>,   <span class="comment"># 原始权重</span></span><br><span class="line">    load_in_4bit=<span class="literal">True</span>,            <span class="comment"># 现场量化到 4-bit</span></span><br><span class="line">    max_seq_length=<span class="number">2048</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
</blockquote>
<p><strong>安装EvalScope</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">pip install evalscope                </span><br><span class="line"># 安装 Native backend (默认)</span><br><span class="line"> # 额外选项</span><br><span class="line">pip install &#x27;evalscope[opencompass]&#x27;   # 安装 OpenCompass backend</span><br><span class="line"> pip install &#x27;evalscope[vlmeval]&#x27;       </span><br><span class="line"># 安装 VLMEvalKit backend</span><br><span class="line"> pip install &#x27;evalscope[rag]&#x27;           </span><br><span class="line">pip install &#x27;evalscope[perf]&#x27;          </span><br><span class="line">pip install &#x27;evalscope[app]&#x27;           </span><br><span class="line"># 或可以直接输入all，安装全部模块</span><br><span class="line"># pip install &#x27;evalscope[all]&#x27;           </span><br><span class="line"># 安装 RAGEval backend</span><br><span class="line"> # 安装 模型压测模块 依赖</span><br><span class="line"># 安装 可视化 相关依赖</span><br><span class="line"># 安装所有 backends (Native, OpenCompass, </span><br><span class="line">VLMEvalKit, RAGEval)</span><br></pre></td></tr></table></figure>
<p><strong>安装wandb</strong></p>
<p>wandb官网： <a target="_blank" rel="noopener" href="https://wandb.ai/site">https://wandb.ai/site</a></p>
<p>安装wandb： </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install wandb</span><br></pre></td></tr></table></figure>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://github.com/SwanHubX/SwanLab?tab=readme-ov-file#-快速开始">SwanHubX/SwanLab: ⚡️SwanLab - an open-source, modern-design AI training tracking and visualization tool. Supports Cloud / Self-hosted use. Integrated with PyTorch / Transformers / LLaMA Factory / veRL/ Swift / Ultralytics / MMEngine / Keras etc.</a></p>
<p>与其类似，一个开源、现代化设计的深度学习训练跟踪与可视化工具</p>
</blockquote>
<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV13BKozLEXE/?spm_id_from=333.337.search-card.all.click&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">DIY你的AI梦中情人？Qwen3微调手把手教你！_哔哩哔哩_bilibili</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1tthPeFEWb/?spm_id_from=333.337.search-card.all.click&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">通俗易懂理解全量微调和LoRA微调_哔哩哔哩_bilibili</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1DT421r7Et?spm_id_from=333.788.videopod.sections&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">通俗易懂理解大模型预训练和微调_哔哩哔哩_bilibili</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1YLE1zyEvX?spm_id_from=333.788.videopod.episodes&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11&amp;p=3">3.四大微调框架及微调硬件环境介绍_哔哩哔哩_bilibili</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1s2AUe2EBq/?spm_id_from=333.788.videopod.sections&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">如何把你的 DeePseek-R1 微调为某个领域的专家？（实战篇）_哔哩哔哩_bilibili</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/javatiange/article/details/149964743?fromshare=blogdetail&amp;sharetype=blogdetail&amp;sharerId=149964743&amp;sharerefer=PC&amp;sharesource=2501_91530961&amp;sharefrom=from_link">一文详解：8种常见的大模型微调方法，看这篇就够了！-CSDN博客</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/08/08/%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AE%97%E6%B3%95/tokenizer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/08/08/%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AE%97%E6%B3%95/tokenizer/" class="post-title-link" itemprop="url">tokenizer</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2025-08-08 00:00:00 / 修改时间：16:21:20" itemprop="dateCreated datePublished" datetime="2025-08-08T00:00:00+08:00">2025-08-08</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">大模型算法</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AE%97%E6%B3%95/tokenizer/" itemprop="url" rel="index"><span itemprop="name">tokenizer</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="什么是-Tokenizer？"><a href="#什么是-Tokenizer？" class="headerlink" title="什么是 Tokenizer？"></a>什么是 Tokenizer？</h3><p><strong>Tokenizer</strong>（分词器）可以将原始文本（raw text）转换为模型能够理解的数字序列，在模型输入和输出的两个主要阶段中发挥重要作用：</p>
<h4 id="模型输入（编码-Encode）阶段"><a href="#模型输入（编码-Encode）阶段" class="headerlink" title="模型输入（编码 Encode）阶段"></a>模型输入（编码 Encode）阶段</h4><ol>
<li><p><strong>分词（Tokenize）</strong></p>
<p>将文本拆分为词元（Token），常见的分词方式包括字级、词级、子词级（如 BPE、WordPiece）、空格分词等。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">输入: &quot;你好&quot;</span><br><span class="line">分词: [&quot;你&quot;, &quot;好&quot;]</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>映射（Mapping）</strong></p>
<p>将每个词元映射为词汇表中的唯一 ID，生成的数字序列即为模型的输入。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">分词: [&quot;你&quot;, &quot;好&quot;]</span><br><span class="line">映射: [<span class="number">1001</span>, <span class="number">1002</span>]</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h4 id="模型输出（解码-Decode）阶段"><a href="#模型输出（解码-Decode）阶段" class="headerlink" title="模型输出（解码 Decode）阶段"></a>模型输出（解码 Decode）阶段</h4><ol>
<li><p><strong>反映射（De-mapping）</strong></p>
<p>模型输出的数字序列通过词汇表映射回对应的词元，二者是一一对应的关系。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">输出: [<span class="number">1001</span>, <span class="number">1002</span>]</span><br><span class="line">反映射: [&quot;你&quot;, &quot;好&quot;]</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>文本重组</strong></p>
<p>将解码后的词元以某种规则重新拼接为完整文本。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">反映射: [&quot;你&quot;, &quot;好&quot;]</span><br><span class="line">重组: &quot;你好&quot;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h4 id="直观感受"><a href="#直观感受" class="headerlink" title="直观感受"></a>直观感受</h4><p>访问 <a target="_blank" rel="noopener" href="https://tiktokenizer.vercel.app">Tiktokenizer</a>，通过右上角选取不同的 Tokenizer 进行尝试</p>
<h3 id="词汇表"><a href="#词汇表" class="headerlink" title="词汇表"></a>词汇表</h3><p>两种常见的构建词汇表的方法：</p>
<ul>
<li><strong>BPE（Byte-Pair Encoding）</strong>：用于 GPT、GPT-2、RoBERTa、BART 和 DeBERTa 等模型。</li>
<li><strong>WordPiece</strong>：用于 DistilBERT、MobileBERT、Funnel Transformers 和 MPNET 等模型。</li>
</ul>
<h4 id="BPE"><a href="#BPE" class="headerlink" title="BPE"></a>BPE</h4><p>BPE（Byte Pair Encoding，字节对编码）在 NLP 里是一种<strong>贪心式的子词（subword）分词算法</strong>。<br>理解：从“字符”开始，反复把<strong>出现次数最多的相邻字符对</strong>合并成新的符号，并加入词汇表，直到达到预设的词汇表大小。</p>
<blockquote>
<p>为什么可以处理 OOV（Out-Of-Vocabulary）情况</p>
<p>因为所有词汇都是由字符或词根组成的，通过对单个字符的学习，可以组成oov的词汇</p>
<p>为什么需要词汇表</p>
<p>编码时，从文本到模型：需要将文本分词为 Tokens，再通过词汇表将 Tokens 转换为 Token IDs，再传给transformer</p>
<p>解码时，从模型到文本：需要通过词汇表Token IDs 转换为 Tokens，再把Tokens 拼接为文本</p>
</blockquote>
<h5 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h5><ol>
<li><strong>初始化词汇表 $V$</strong>：<ul>
<li>$V$ 包含语料库中的所有唯一字符，即单词字符的集合。</li>
</ul>
</li>
<li><strong>统计字符对的频次</strong>：<ul>
<li>对于每个单词的字符序列，统计相邻字符对的出现频次。</li>
</ul>
</li>
<li><strong>找到频次（Score）最高的字符对并合并</strong>：<ul>
<li>选择出现频率最高的字符对 $(x, y)$，将其合并为新符号 $xy$。</li>
</ul>
</li>
<li><strong>更新词汇表并重复步骤 2 到 4</strong>：<ul>
<li>将新符号添加到词汇表 $V = V \cup {xy}$。</li>
<li>更新语料库中的单词表示，重复统计和合并过程，直到满足停止条件（例如，词汇表达到预定大小）。</li>
</ul>
</li>
</ol>
<p><strong>示例</strong></p>
<p>我们需要将语料库（corpus）的文本拆分为单词，假设当前语料库包含的单词和对应频次如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(&quot;low&quot;, 5), (&quot;lower&quot;, 2), (&quot;newest&quot;, 6), (&quot;widest&quot;, 3)</span><br></pre></td></tr></table></figure>
<p><strong>步骤 1：初始化词汇表</strong></p>
<p><strong>将单词拆分为字符序列</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(&quot;l&quot;, &quot;o&quot;, &quot;w&quot;), 5  </span><br><span class="line">(&quot;l&quot;, &quot;o&quot;, &quot;w&quot;, &quot;e&quot;, &quot;r&quot;), 2  </span><br><span class="line">(&quot;n&quot;, &quot;e&quot;, &quot;w&quot;, &quot;e&quot;, &quot;s&quot;, &quot;t&quot;), 6  </span><br><span class="line">(&quot;w&quot;, &quot;i&quot;, &quot;d&quot;, &quot;e&quot;, &quot;s&quot;, &quot;t&quot;), 3</span><br></pre></td></tr></table></figure>
<p><strong>词汇表 V</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;l&#x27;, &#x27;o&#x27;, &#x27;w&#x27;, &#x27;e&#x27;, &#x27;r&#x27;, &#x27;n&#x27;, &#x27;s&#x27;, &#x27;t&#x27;, &#x27;i&#x27;, &#x27;d&#x27;&#125;</span><br></pre></td></tr></table></figure>
<p><strong>步骤 2：统计字符对的频次</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">字符对频次统计结果:</span><br><span class="line">(&#x27;l&#x27;, &#x27;o&#x27;): 7        # 5 (low) + 2 (lower)</span><br><span class="line">(&#x27;o&#x27;, &#x27;w&#x27;): 7        # 5 (low) + 2 (lower)</span><br><span class="line">(&#x27;w&#x27;, &#x27;e&#x27;): 8        # 2 (lower) + 6 (newest)</span><br><span class="line">(&#x27;e&#x27;, &#x27;r&#x27;): 2</span><br><span class="line">(&#x27;n&#x27;, &#x27;e&#x27;): 6</span><br><span class="line">(&#x27;e&#x27;, &#x27;w&#x27;): 6</span><br><span class="line">(&#x27;e&#x27;, &#x27;s&#x27;): 9        # 6 (newest) + 3 (widest)</span><br><span class="line">(&#x27;s&#x27;, &#x27;t&#x27;): 9        # 6 (newest) + 3 (widest)</span><br><span class="line">(&#x27;w&#x27;, &#x27;i&#x27;): 3</span><br><span class="line">(&#x27;i&#x27;, &#x27;d&#x27;): 3</span><br><span class="line">(&#x27;d&#x27;, &#x27;e&#x27;): 3</span><br></pre></td></tr></table></figure>
<p><strong>步骤 3：找到频次最高的字符对并合并</strong></p>
<p><strong>选择频次最高的字符对</strong>：</p>
<ul>
<li><code>(&quot;e&quot;, &quot;s&quot;)</code> 和 <code>(&quot;s&quot;, &quot;t&quot;)</code>，频次均为 9。可以任选其一进行合并，假设选择排序第一的： <code>(&quot;e&quot;, &quot;s&quot;)</code>。</li>
</ul>
<p><strong>合并 <code>(&quot;e&quot;, &quot;s&quot;)</code> 为新符号 <code>es</code></strong>。</p>
<p><strong>记录合并操作</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Merge 1: (&quot;e&quot;, &quot;s&quot;) -&gt; &quot;es&quot;</span><br></pre></td></tr></table></figure>
<p><strong>步骤 4：更新词汇表并重复</strong></p>
<p><strong>更新单词序列</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(&quot;l&quot;, &quot;o&quot;, &quot;w&quot;), 5  </span><br><span class="line">(&quot;l&quot;, &quot;o&quot;, &quot;w&quot;, &quot;e&quot;, &quot;r&quot;), 2  </span><br><span class="line">(&quot;n&quot;, &quot;e&quot;, &quot;w&quot;, &quot;es&quot;, &quot;t&quot;), 6  </span><br><span class="line">(&quot;w&quot;, &quot;i&quot;, &quot;d&quot;, &quot;es&quot;, &quot;t&quot;), 3</span><br></pre></td></tr></table></figure>
<p><strong>更新词汇表 V</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;l&#x27;, &#x27;o&#x27;, &#x27;w&#x27;, &#x27;e&#x27;, &#x27;r&#x27;, &#x27;n&#x27;, &#x27;s&#x27;, &#x27;t&#x27;, &#x27;i&#x27;, &#x27;d&#x27;, &#x27;es&#x27;&#125;</span><br></pre></td></tr></table></figure>
<p><strong>重复步骤 2 到 4，直到达到预定的词汇表大小</strong>。</p>
<h4 id="WordPiece"><a href="#WordPiece" class="headerlink" title="WordPiece"></a>WordPiece</h4><p>WordPiece 是 Google 在 2016 年为语音识别与 BERT 提出的<strong>子词（subword）分词算法</strong>，可看作 BPE 的“似然改进版”。理解：“<strong>用概率贪心而不是频次贪心，从字符开始逐步合并子词</strong>。”</p>
<p>与 BPE 不同，WordPiece 的 Score 由字符对频次与其组成部分频次的比值决定，定义 Score：</p>
<script type="math/tex; mode=display">
\text{Score}_{\text{WordPiece}}(x, y) = \frac{\text{freq}(xy)}{\text{freq}(x) \times \text{freq}(y)}</script><p>其中, $\text{freq}(x)$, $\text{freq}(y)$ 和 $\text{freq}(xy)$ 分别表示符号 $x$, $y$ 和它们合并后的符号 $xy$ 的频次。</p>
<h5 id="步骤-1"><a href="#步骤-1" class="headerlink" title="步骤"></a>步骤</h5><ol>
<li><strong>初始化词汇表 $V$</strong>：<ul>
<li>与 BPE 相同, $V$ 包含语料库中的所有唯一字符，但处理方式略有不同：<strong>对于每个单词，除了首个字符外，其他字符前都加上 <code>##</code> 前缀。</strong></li>
</ul>
</li>
<li><strong>统计字符对的频次及 Score</strong>：<ul>
<li>对于每个可能的字符对 $(x, y)$，计算 $\text{freq}(x)$, $\text{freq}(y)$, $\text{freq}(xy)$，并计算 Score。</li>
</ul>
</li>
<li><strong>找到 Score 最高的字符对并合并</strong>：<ul>
<li>选择 Score 最高的字符对 $(x, y)$，将其合并为新符号 $xy$，注意：<ul>
<li>如果第二个符号以 <code>##</code> 开头，合并时去掉 <code>##</code> 前缀再进行连接。</li>
<li>新符号是否以 <code>##</code> 开头，取决于第一个符号是否以 <code>##</code> 开头。</li>
</ul>
</li>
</ul>
</li>
<li><strong>更新词汇表并重复步骤 2 到 4</strong>：<ul>
<li>将新符号添加到词汇表 $V = V \cup {xy}$。</li>
<li>更新语料库中的单词表示，重复统计和合并过程，直到满足停止条件。</li>
</ul>
</li>
</ol>
<h3 id="映射（Mapping）"><a href="#映射（Mapping）" class="headerlink" title="映射（Mapping）"></a>映射（Mapping）</h3><p>以 BPE 为例，最终词汇表 $V$ 中的 Token 和对应的频次分别为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">vocab = &#123;</span><br><span class="line">    &#x27;lo&#x27;: 7,</span><br><span class="line">    &#x27;w&#x27;: 16,</span><br><span class="line">    &#x27;e&#x27;: 8,</span><br><span class="line">    &#x27;r&#x27;: 2,</span><br><span class="line">    &#x27;n&#x27;: 6,</span><br><span class="line">    &#x27;est&#x27;: 9,</span><br><span class="line">    &#x27;i&#x27;: 3,</span><br><span class="line">    &#x27;d&#x27;: 3</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>输出</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Token to ID: &#123;&#x27;lo&#x27;: 0, &#x27;w&#x27;: 1, &#x27;e&#x27;: 2, &#x27;r&#x27;: 3, &#x27;n&#x27;: 4, &#x27;est&#x27;: 5, &#x27;i&#x27;: 6, &#x27;d&#x27;: 7&#125;</span><br><span class="line">ID to Token: &#123;0: &#x27;lo&#x27;, 1: &#x27;w&#x27;, 2: &#x27;e&#x27;, 3: &#x27;r&#x27;, 4: &#x27;n&#x27;, 5: &#x27;est&#x27;, 6: &#x27;i&#x27;, 7: &#x27;d&#x27;&#125;</span><br></pre></td></tr></table></figure>
<p>当然，也可以根据频次或者其他规则进行特殊处理。</p>
<p>以上是编码部分的概述，实际上在文本预处理的时候还会增加特殊标记，但这些以及后续的解码部分大多是一些文本处理的规则，这里就不过多赘述了，Tokenizer 之间的核心差异在于使用的分割方法和词汇表的构建策略。</p>
<h3 id="transformer中的分词"><a href="#transformer中的分词" class="headerlink" title="transformer中的分词"></a>transformer中的分词</h3><p>在 Transformers 中，<strong>分词（tokenization）</strong> 实际上包含以下几个步骤：</p>
<ol>
<li><strong>标准化（Normalization）</strong>：对文本进行必要的清理操作，例如删除多余空格或重音符号、进行 Unicode 标准化等。</li>
<li><strong>预分词（Pre-tokenization）</strong>：将输入拆分为单词。</li>
<li><strong>通过模型处理输入（Running the input through the model）</strong>：使用预分词后的单词生成一系列词元（tokens）。</li>
<li><strong>后处理（Post-processing）</strong>：添加分词器的特殊标记，生成注意力掩码（attention mask）和词元类型 ID（token type IDs）。</li>
</ol>
<p>流程图如下</p>
<p><img src="/2025/08/08/%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AE%97%E6%B3%95/tokenizer/image-20250808100051423.png" alt="image-20250808100051423"></p>
<h4 id="注意力掩码（Attention-Mask）和词元类型-ID-（Token-Type-IDs）是什么？"><a href="#注意力掩码（Attention-Mask）和词元类型-ID-（Token-Type-IDs）是什么？" class="headerlink" title="注意力掩码（Attention Mask）和词元类型 ID （Token Type IDs）是什么？"></a>注意力掩码（Attention Mask）和词元类型 ID （Token Type IDs）是什么？</h4><p><img src="/2025/08/08/%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AE%97%E6%B3%95/tokenizer/image-20250808100813881.png" alt="image-20250808100813881"></p>
<p>1️⃣ 注意力掩码（Attention Mask）<br>• 目的：告诉模型“哪些位置可以被看到”，其余位置直接屏蔽。<br>• 典型场景：<br>– <strong>自注意力里做 padding 掩码</strong>：把 <code>&lt;pad&gt;</code> 对应的位置设为 −∞，softmax 后权重=0。<br>– <strong>解码器自回归掩码</strong>：生成任务用下三角掩码，避免第 i 个 token 看到未来 token。</p>
<p>2️⃣ 词元类型 ID（Token Type IDs，也叫 Segment IDs）<br>• 目的：区分<strong>同一次输入里不同句子或段落</strong>，让模型知道“这段属于 A，那段属于 B”。<br>• 典型场景：<br>– BERT 做句子对分类（NSP）：<code>[CLS] 句子A [SEP] 句子B [SEP]</code> → TypeID = 0 0 0 0 1 1 1。<br>– RoBERTa、GPT 等单句模型则<strong>不需要</strong> Token Type IDs。</p>
<p><strong>注意力掩码</strong>确保模型只关注实际的词元，忽略填充部分，从而避免无效的计算：</p>
<ul>
<li><strong>1</strong>：表示模型应关注的词元（Tokens）</li>
<li><strong>0</strong>：表示模型应忽略的词元（通常是填充 <code>padding</code> 的部分）。</li>
</ul>
<p><strong>词元类型 ID</strong> 用于区分输入中的不同句子或段落：</p>
<ul>
<li><strong>0</strong>：表示第一个句子的词元。</li>
<li><strong>1</strong>：表示第二个句子的词元。</li>
</ul>
<blockquote>
<p>CLS，SEP，PAD都是什么意思</p>
<p><code>[CLS]</code>（Classification），作用：对应位置的隐藏状态被当作<strong>整句/句对的“整体表示”</strong>，用来接分类头做句子级任务（情感分类、NLI 等）。</p>
<p><code>[SEP]</code>（Separator），作用：让模型知道<strong>分段 / 句子边界</strong>，配合 Token Type IDs 区分句子 A 和句子 B。</p>
<p><code>[PAD]</code>（padding token）的作用是 <strong>批量训练时把不同长度的序列补齐到同一长度</strong>，让张量可以堆叠成规整的矩阵；模型在计算注意力时通过 Attention Mask 把 <code>[PAD]</code> 对应的位置屏蔽掉，不让它们影响有效 token 的表示。</p>
</blockquote>
<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a target="_blank" rel="noopener" href="https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/Guide/21. BPE vs WordPiece：理解 Tokenizer 的工作原理与子词分割方法.md">AI-Guide-and-Demos-zh_CN/Guide/21. BPE vs WordPiece：理解 Tokenizer 的工作原理与子词分割方法.md at master · Hoper-J/AI-Guide-and-Demos-zh_CN</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/08/06/%E5%AD%A6%E4%B9%A0/agent%E5%AE%9E%E6%88%98/redis%E5%AD%98%E5%82%A8%E7%8A%B6%E6%80%81/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/08/06/%E5%AD%A6%E4%B9%A0/agent%E5%AE%9E%E6%88%98/redis%E5%AD%98%E5%82%A8%E7%8A%B6%E6%80%81/" class="post-title-link" itemprop="url">redis存储状态</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-08-06 00:00:00" itemprop="dateCreated datePublished" datetime="2025-08-06T00:00:00+08:00">2025-08-06</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-10-09 20:20:06" itemprop="dateModified" datetime="2025-10-09T20:20:06+08:00">2025-10-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/agent%E5%AE%9E%E6%88%98/" itemprop="url" rel="index"><span itemprop="name">agent实战</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="为什么用redis"><a href="#为什么用redis" class="headerlink" title="为什么用redis"></a>为什么用redis</h3><p>Redis通过 RedisSessionManager 类来管理用户会话，存储结构如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">session:&#123;user_id&#125; -&gt; &#123;</span><br><span class="line">  &quot;session_id&quot;: &quot;会话ID&quot;,</span><br><span class="line">  &quot;status&quot;: &quot;idle|running|interrupted|completed|error&quot;,</span><br><span class="line">  &quot;last_response&quot;: &quot;上次智能体响应&quot;,</span><br><span class="line">  &quot;last_query&quot;: &quot;用户上次查询&quot;,</span><br><span class="line">  &quot;last_updated&quot;: &quot;最后更新时间戳&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><img src="/2025/08/06/%E5%AD%A6%E4%B9%A0/agent%E5%AE%9E%E6%88%98/redis%E5%AD%98%E5%82%A8%E7%8A%B6%E6%80%81/image-20250809232853613.png" alt="image-20250809232853613"></p>
<p>主要功能</p>
<ul>
<li>会话创建与维护 ：为每个用户创建唯一会话，支持会话超时自动清理</li>
<li>状态跟踪 ：实时跟踪智能体执行状态（空闲、运行中、中断、完成、错误）</li>
<li>中断恢复支持 ：当智能体需要人工干预时，Redis保存中断状态，支持后续恢复执行</li>
<li>用户管理 ：统计活跃用户数量，管理多用户并发访问</li>
</ul>
<p>与PostgreSQL的分工</p>
<ul>
<li>Redis ：负责临时会话状态和实时数据（快速读写）</li>
<li>PostgreSQL ：负责智能体的长期记忆存储（通过LangGraph的checkpointer）</li>
</ul>
<blockquote>
<p>为什么不使用pgsql完成对状态的存储</p>
<p>频繁读写 ：会话状态需要频繁更新（每次请求都要更新状态），PostgreSQL的磁盘I/O比Redis内存操作慢很多4</p>
<p>短期记忆（PostgreSQL + LangGraph Checkpointer）</p>
<p>临时状态记忆（Redis）</p>
</blockquote>
<h3 id="redis实现状态存储业务逻辑总览图"><a href="#redis实现状态存储业务逻辑总览图" class="headerlink" title="redis实现状态存储业务逻辑总览图"></a>redis实现状态存储业务逻辑总览图</h3><p><img src="/2025/08/06/%E5%AD%A6%E4%B9%A0/agent%E5%AE%9E%E6%88%98/redis%E5%AD%98%E5%82%A8%E7%8A%B6%E6%80%81/image-20250806164348663.png" alt="image-20250806164348663"></p>
<p>使用redis的根本逻辑：存储对话的状态，当出现由工具调用或者客户端崩溃导致的中断时，可以存储状态在redis，在开始对话时，通过session_id获取redis的状态，并根据状态判断是要恢复中断还是正常对话</p>
<p>存储的redis（调用invoke<em>agent接口）：开始（创建）对话时要根据会话user<em>id获取或创建redis；再调用agent后，根据响应是否存在<strong>status</strong>字段是否是”__interrupt</em></em>“，判断是否有终端，最后更新redis状态</p>
<p>恢复的redis（调用resume_agent接口）：获取redis状态，并根据请求的恢复内容，使用Command命令恢复agent，最后更新redis</p>
<p><img src="/2025/08/06/%E5%AD%A6%E4%B9%A0/agent%E5%AE%9E%E6%88%98/redis%E5%AD%98%E5%82%A8%E7%8A%B6%E6%80%81/image-20250809233037563.png" alt="image-20250809233037563"></p>
<h3 id="redis类"><a href="#redis类" class="headerlink" title="redis类"></a>redis类</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># 初始化异步 Redis 连接和会话配置</span><br><span class="line">def __init__(self, redis_host, redis_port, redis_db, session_timeout):</span><br><span class="line">    self.redis_client = redis.Redis(</span><br><span class="line">        host=redis_host,</span><br><span class="line">        port=redis_port,</span><br><span class="line">        db=redis_db,</span><br><span class="line">        decode_responses=True</span><br><span class="line">    )</span><br><span class="line">    self.session_timeout = session_timeout  # 会话过期时间（秒）</span><br><span class="line"></span><br><span class="line"># 关闭 Redis 连接</span><br><span class="line">async def close(self):</span><br><span class="line">    await self.redis_client.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th>方法名</th>
<th>作用</th>
<th>输入参数</th>
<th>返回值</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>__init__</code></td>
<td>建立与 Redis 的异步连接并设置会话超时</td>
<td><code>redis_host</code>, <code>redis_port</code>, <code>redis_db</code>, <code>session_timeout</code></td>
<td>-</td>
<td><code>decode_responses=True</code> 使 Redis 返回字符串而非字节</td>
</tr>
<tr>
<td><code>close</code></td>
<td>优雅关闭 Redis 连接</td>
<td>-</td>
<td>-</td>
<td>异步方法，需 <code>await</code></td>
</tr>
<tr>
<td><code>create_session</code></td>
<td>为指定用户新建（或覆盖）会话记录</td>
<td><code>user_id</code>, 可选 <code>session_id</code>, <code>status</code>, <code>last_query</code>, <code>last_response</code>, <code>last_updated</code></td>
<td><code>str</code>：生成的 <code>session_id</code></td>
<td>会话键格式：<code>session:&#123;user_id&#125;</code>；过期时间为 <code>session_timeout</code></td>
</tr>
<tr>
<td><code>get_session</code></td>
<td>读取指定用户的完整会话字典</td>
<td><code>user_id</code></td>
<td><code>dict</code> 或 <code>None</code></td>
<td>自动将 JSON 里的 <code>last_response</code> 反序列化为 <code>AgentResponse</code> 对象</td>
</tr>
<tr>
<td><code>update_session</code></td>
<td>增量更新已有会话的字段</td>
<td><code>user_id</code>, 可选 <code>status</code>, <code>last_query</code>, <code>last_response</code>, <code>last_updated</code></td>
<td><code>bool</code>：<code>True</code> 更新成功，<code>False</code> 用户不存在</td>
<td>更新后刷新过期时间</td>
</tr>
<tr>
<td><code>delete_session</code></td>
<td>删除单个用户的会话</td>
<td><code>user_id</code></td>
<td><code>bool</code>：<code>True</code> 删除成功</td>
<td>直接删除 <code>session:&#123;user_id&#125;</code></td>
</tr>
<tr>
<td><code>get_session_count</code></td>
<td>计算当前活跃会话总数</td>
<td>-</td>
<td><code>int</code></td>
<td>使用异步扫描 <code>session:*</code> 键空间</td>
</tr>
<tr>
<td><code>get_all_user_ids</code></td>
<td>取出所有已创建会话的 <code>user_id</code></td>
<td>-</td>
<td><code>List[str]</code></td>
<td>同样基于 <code>session:*</code> 扫描</td>
</tr>
<tr>
<td><code>user_id_exists</code></td>
<td>快速判断某用户是否已有会话</td>
<td><code>user_id</code></td>
<td><code>bool</code></td>
<td>利用 <code>EXISTS</code> 命令</td>
</tr>
</tbody>
</table>
</div>
<h3 id="安装redis"><a href="#安装redis" class="headerlink" title="安装redis"></a>安装redis</h3><h4 id="linux系统"><a href="#linux系统" class="headerlink" title="linux系统"></a>linux系统</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sudo apt update</span><br><span class="line">sudo apt install -y redis-server</span><br><span class="line"># 启动 Redis 服务</span><br><span class="line">sudo service redis-server start</span><br><span class="line"># 检查 Redis 服务状态</span><br><span class="line">sudo service redis-server status</span><br></pre></td></tr></table></figure>
<h4 id="docker"><a href="#docker" class="headerlink" title="docker"></a>docker</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"># Docker Compose 配置文件，用于启动 Redis 服务</span><br><span class="line"># 该配置为 FastAPI 应用提供 Redis 后端，支持分布式会话管理</span><br><span class="line">version: &#x27;3.8&#x27;</span><br><span class="line"></span><br><span class="line">services:</span><br><span class="line">  redis:</span><br><span class="line">    # 使用官方 Redis 镜像</span><br><span class="line">    image: redis:latest</span><br><span class="line">    # 服务名称</span><br><span class="line">    container_name: redis</span><br><span class="line">    # 映射 Redis 默认端口到主机</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;6379:6379&quot;</span><br><span class="line">    # 持久化存储配置（可选）</span><br><span class="line">    volumes:</span><br><span class="line">      - redis-data:/data</span><br><span class="line">    # 确保容器在重启时自动启动</span><br><span class="line">    restart: unless-stopped</span><br><span class="line">    # 健康检查：验证 Redis 服务是否正常运行</span><br><span class="line">    healthcheck:</span><br><span class="line">      test: [&quot;CMD&quot;, &quot;redis-cli&quot;, &quot;ping&quot;]</span><br><span class="line">      interval: 30s</span><br><span class="line">      timeout: 10s</span><br><span class="line">      retries: 3</span><br><span class="line">      start_period: 10s</span><br><span class="line">    # 网络配置</span><br><span class="line">    networks:</span><br><span class="line">      - app-network</span><br><span class="line"></span><br><span class="line"># 定义持久化存储卷</span><br><span class="line">volumes:</span><br><span class="line">  redis-data:</span><br><span class="line">    name: redis-data</span><br><span class="line"></span><br><span class="line"># 定义网络</span><br><span class="line">networks:</span><br><span class="line">  app-network:</span><br><span class="line">    driver: bridge</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d  --name redis -p 6379:6379  -v redis-data:/data redis:latest</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/08/05/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/%E6%97%A5%E5%BF%97/%E4%B8%83%E6%9C%88%E5%A4%8D%E7%9B%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/08/05/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/%E6%97%A5%E5%BF%97/%E4%B8%83%E6%9C%88%E5%A4%8D%E7%9B%98/" class="post-title-link" itemprop="url">实习七月复盘</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2025-08-05 00:00:00 / 修改时间：15:46:50" itemprop="dateCreated datePublished" datetime="2025-08-05T00:00:00+08:00">2025-08-05</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AE%9E%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">实习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/" itemprop="url" rel="index"><span itemprop="name">晨晟智控</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="文件处理阶段"><a href="#文件处理阶段" class="headerlink" title="文件处理阶段"></a>文件处理阶段</h3><ol>
<li>使用libreoffice将doc，docx文件处理成pdf文件，方便后续使用mineru进行提取</li>
<li>完成mineru的docker本地部署；搭建fastapi服务，与项目容器构建自定义网络，方便后续服务调用；使用locust完成对mineru的并发性能测试，和吞吐量测试</li>
<li>对mineru提取的html格式的表格进行预处理工作，将其转化成md格式，方便后续分块，节省tokens</li>
</ol>
<h4 id="部分技术细节"><a href="#部分技术细节" class="headerlink" title="部分技术细节"></a>部分技术细节</h4><p><strong>mineru提取效果说明</strong></p>
<p>可以完整提取表格与图片，将图片以相对链接形式储存在images文件夹下；可以完成pdf与扫描件的提取，可以实现对图片中文字的识别；输出符合人类阅读顺序的文本，适用于单栏、多栏及复杂排版；删除页眉、页脚、脚注、页码等元素，确保语义连贯</p>
<p>目前问题：仍无法实现对多级标题的识别</p>
<p><strong>mineru的fastapi启动指令</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MINERU_MODEL_SOURCE=local CUDA_VISIBLE_DEVICES=1,2,3 mineru-api --host 0.0.0.0 --port 30000 --dp-size 3 --enable-torch-compile</span><br></pre></td></tr></table></figure>
<blockquote>
<p>MinerU支持通过sglang的多GPU并行模式来提升推理速度。</p>
<ul>
<li>如果您有超过多张显卡，可以使用sglang的多卡并行模式来增加吞吐量：<code>--dp-size 2</code></li>
<li>同时您可以启用<code>torch.compile</code>来将推理速度加速约15%：<code>--enable-torch-compile</code></li>
</ul>
<p>注意设置环境变量<code>MINERU_MODEL_SOURCE=local CUDA_VISIBLE_DEVICES=1,2,3</code>保证模型本地加载与调用指定gpu</p>
</blockquote>
<p><strong>mineru容器启动指令</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --name mineru-server --gpus all --shm-size 32g -p 30000:30000 --ipc=host -v /aisys/:/aisys/ --network network_test mineru-sglang:latest tail -f /dev/null</span><br><span class="line">docker start mineru-server</span><br></pre></td></tr></table></figure>
<p><strong>mineru三种后端模式测试</strong></p>
<p>pipeline (默认后端) ，vlm-sglang-engine，vlm-sglang-client</p>
<p>项目中使用的是vlm-sglang-engine，原因如下，pipeline应用场景更多是仅能cpu推理，解析速度大大落后与vlm模式，而我们gpu资源充足，自然不考虑；vlm-sglang-client应用场景更多是有SGLang服务器，这样客户端既可以不用安装sglang，同样不符合我们的条件</p>
<p><strong>mineru并发与吞吐量测试</strong></p>
<p><strong>测试场景</strong>：10页的pdf，50用户并发</p>
<p><strong>工具</strong>：locust</p>
<p><strong>测试结果</strong></p>
<p>对于推理模型的吞吐量，在3个gpu开启数据并行的情况下，平均每秒单个gpu处理tokens为1500左右</p>
<p>gpu状态如上:<strong>显存几乎打满 85–87 %</strong>,<strong>GPU 利用率 59–63 %</strong>,<strong>功耗 170–188 W / 350 W</strong></p>
<p>压测结果如下，选取部分指标</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>指标</th>
<th>数值</th>
<th>通俗解释</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>平均响应时间</strong></td>
<td><strong>241 秒</strong> ≈ <strong>4 分钟</strong></td>
<td>上传一个 PDF → 拿到解析结果，平均要等 4 分钟。</td>
</tr>
<tr>
<td><strong>中位数</strong></td>
<td><strong>215 秒</strong> ≈ <strong>3.6 分钟</strong></td>
<td>一半请求在 3.6 分钟内完成。</td>
</tr>
<tr>
<td><strong>95% 用户</strong></td>
<td><strong>361 秒</strong> ≈ <strong>6 分钟</strong></td>
<td>最慢的 5% 要等 6 分钟以上。</td>
</tr>
<tr>
<td><strong>吞吐量</strong></td>
<td><strong>0.18 req/s</strong></td>
<td>这台 MinerU <strong>每分钟只能处理约11 个 PDF</strong>。</td>
</tr>
</tbody>
</table>
</div>
<h3 id="分块阶段"><a href="#分块阶段" class="headerlink" title="分块阶段"></a>分块阶段</h3><p>当前主流的分块方式共五种：固定长度分块，语义分块，递归分块，文档结构分块，llm分块。</p>
<p>最后项目我选择了递归分块，原因如下：</p>
<ol>
<li>mineru无法正确提取md文档结构，因此我舍弃了文档结构分块</li>
<li>测试了agentic chunk（其主要思想是，先进行初步分段，按照长度或递归，然后让大模型生成这一段的概要，将段与段合并生成块），但是测试下来，我们这个一个文档的内容同质化很严重，基本上都分到一块里了，我猜测语义分块也是这种效果，因此舍弃</li>
<li>我们的文档中存在大量表格，我在预处理阶段增加了对表格的首尾标记，使用递归分块可以更好的保留这些结构</li>
</ol>
<h3 id="检索阶段"><a href="#检索阶段" class="headerlink" title="检索阶段"></a>检索阶段</h3><p>基于langchain_elasticsearch完成了向量搜索，bm25，混合检索，模糊检索的检索函数的编写。</p>
<p>结果如下：</p>
<ol>
<li>混合检索elasticsearch需要付费使用</li>
<li>bm25的多字段搜索有三种模式且字段的权重可以调整，后续评估时调整进行测试</li>
<li>检索的效果需要后续进行rag评估时判定</li>
</ol>
<h3 id="elasticsearch相关"><a href="#elasticsearch相关" class="headerlink" title="elasticsearch相关"></a>elasticsearch相关</h3><p>完成对项目es模块的熟悉阅读；实现对elasticsearch的连接与字段的构建与存入。</p>
<p>关于字段的存储，我选取了report_name，report_url，page_content</p>
<h4 id="相关细节"><a href="#相关细节" class="headerlink" title="相关细节"></a>相关细节</h4><h5 id="阅读elasticsearch代码相关记录"><a href="#阅读elasticsearch代码相关记录" class="headerlink" title="阅读elasticsearch代码相关记录:"></a><strong>阅读elasticsearch代码相关记录:</strong></h5><ol>
<li><strong>embedding_model</strong>.select_embeddings_model:根据指定的模型名称加载</li>
<li><strong>make_es_vector_store</strong>：<ol>
<li>docs_url = pd.read_excel(‘docs_new.xlsx’)，从excel加载url并进行数据处理，保存筛选后的ur</li>
<li>完成文件加载测试：xizhang/retrival/docfile_test/test.py；</li>
<li>初始化es，使用elastic_search.load_es_index加载存储索引</li>
<li>完成测试elasticsearch连接与索引构建（索引名zxj_test）：/aisys/repo_dev/xizhang/retrival/elasticsearch_test/test_es_connect.py，/aisys/repo_dev/xizhang/retrival/elasticsearch_test/test_add_es.py</li>
<li>顺序批量处理文件：共16000多份，每十份为一批进行处理，使用download_pdf.py进行文件下载，使用，使用vector_base.rewrite_file对文件进行处理，这里可以修改代码，增加对mineru处理pdf的markdown文件的处理，返回Document对象列表；</li>
</ol>
</li>
<li><strong>elastic_search</strong></li>
</ol>
<p>重写了检索策略的函数，包括BM25，KNN，混合搜索</p>
<ol>
<li><p>elastic_retriever创建Elasticsearch检索器：根据搜索类型选择对应的查询函数，创建Elasticsearch检索器ElasticsearchRetriever.from_es_params</p>
</li>
<li><p><strong>retrievers</strong></p>
<ol>
<li>定义函数select_retriever，根据指定的名称选择并返回相应的检索器，目前只有bm25</li>
</ol>
</li>
</ol>
<h5 id="文档结构"><a href="#文档结构" class="headerlink" title="文档结构"></a>文档结构</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">doc = Document(</span><br><span class="line">            page_content=text_content,</span><br><span class="line">            metadata=&#123;</span><br><span class="line">                &quot;report_name&quot;: folder_name,</span><br><span class="line">                &quot;report_url&quot;: report_url,</span><br><span class="line">                &quot;chunk_id&quot;: chunk_id</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<h3 id="rag评估"><a href="#rag评估" class="headerlink" title="rag评估"></a>rag评估</h3><p>待补充</p>
<h3 id="后续优化思考"><a href="#后续优化思考" class="headerlink" title="后续优化思考"></a>后续优化思考</h3><ol>
<li>重排序部分我没有做过，不知道怎么做，也不知道效果会怎样（我感觉在我们这个场景应该提升有限，听你说也是这样）</li>
<li>如何存入数据库的部分，可能也是优化的点，比如可以尝试agentic rag这种，在存入数据库前再进行一步处理</li>
<li>还有一个点我比较好奇，我们项目在召回后是如何处理的，就是上下文拼接吗</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/08/02/%E5%AD%A6%E4%B9%A0/python%E7%9B%B8%E5%85%B3/python-uv%E5%8C%85%E7%AE%A1%E7%90%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/08/02/%E5%AD%A6%E4%B9%A0/python%E7%9B%B8%E5%85%B3/python-uv%E5%8C%85%E7%AE%A1%E7%90%86/" class="post-title-link" itemprop="url">python-uv包管理</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-08-02 00:00:00" itemprop="dateCreated datePublished" datetime="2025-08-02T00:00:00+08:00">2025-08-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-10-18 20:46:36" itemprop="dateModified" datetime="2025-10-18T20:46:36+08:00">2025-10-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AD%A6%E4%B9%A0/python%E7%9B%B8%E5%85%B3/" itemprop="url" rel="index"><span itemprop="name">python相关</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="什么是uv"><a href="#什么是uv" class="headerlink" title="什么是uv"></a>什么是uv</h3><p><code>uv</code> 是由 <strong>Astral</strong> 团队开发的一个<strong>超高速 Python 包管理器</strong>，用 <strong>Rust</strong> 编写，目标是替代 <code>pip</code>、<code>venv</code>、<code>pip-tools</code>、<code>poetry</code> 等多个工具。</p>
<h3 id="uv常用命令"><a href="#uv常用命令" class="headerlink" title="uv常用命令"></a>uv常用命令</h3><p>uv init myproj    创建新项目</p>
<p>source .venv/bin/activate（Linux/macOS）激活虚拟环境</p>
<p>uv add requests    安装依赖并写入 pyproject.toml</p>
<p>uv remove requests    移除依赖</p>
<p>uv sync    同步依赖到虚拟环境</p>
<p>uv export    导出 lock 文件为 requirements.txt 等格式</p>
<p>uv build    构建源码包和 wheel</p>
<p>uv publish    发布到 PyPI</p>
<h3 id="uvx是什么"><a href="#uvx是什么" class="headerlink" title="uvx是什么"></a>uvx是什么</h3><p><strong><code>uvx</code></strong> 是：</p>
<blockquote>
<p><strong>uv tool run</strong> 的<strong>快捷别名</strong>（alias），用于<strong>无需安装即可运行 Python 包提供的命令行工具</strong>。</p>
</blockquote>
<p><code>uvx</code> 就像 Python 世界的 <strong><code>npx</code></strong> 或 <strong><code>pipx run</code></strong> ——<br><strong>临时拉取、构建隔离环境、运行工具，用完即走，不留痕迹</strong>。</p>
<h3 id="uv管理命令行工具"><a href="#uv管理命令行工具" class="headerlink" title="uv管理命令行工具"></a>uv管理命令行工具</h3><p>使用<code>uv tool</code></p>
<ul>
<li><strong>用途</strong>：安装、管理、运行<strong>全局可用的 Python 命令行工具</strong>。</li>
<li><strong>安装位置</strong>：默认安装到 <code>~/.local/bin</code>（Windows: <code>C:\Users\&lt;USER&gt;\.local\bin</code>）。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">uv tool install pytest</span><br></pre></td></tr></table></figure>
<p>安装后可以直接使用<code>pytest</code>而不用<code>uv run pytest</code></p>
<h3 id="uv-sync和uv-pip-install-e-的区别"><a href="#uv-sync和uv-pip-install-e-的区别" class="headerlink" title="uv sync和uv pip install -e .的区别"></a>uv sync和uv pip install -e .的区别</h3><p>✅ <code>uv pip install -e .</code></p>
<ul>
<li><strong>作用</strong>：将当前项目以<strong>可编辑模式</strong>安装到当前 Python 环境。</li>
<li><strong>行为</strong>：<ul>
<li>读取 <code>pyproject.toml</code> 中的 <code>[project]</code> 元数据。</li>
<li>构建并安装你的<strong>主包</strong>（如 <code>my_package</code>），使其可被 <code>import</code>。</li>
<li><strong>不会自动安装依赖</strong>（除非你显式加上 <code>--deps</code>，但通常不这么做）。</li>
</ul>
</li>
<li><strong>典型用途</strong>：开发自己的包时，让本地代码可导入。</li>
</ul>
<p>✅ <code>uv sync</code></p>
<ul>
<li><strong>作用</strong>：<strong>根据锁定文件（如 <code>uv.lock</code>）精确同步整个项目的依赖环境</strong>。</li>
<li><strong>行为</strong>：<ul>
<li>读取 <code>uv.lock</code>（由 <code>uv lock</code> 生成）或 <code>pyproject.toml</code>。</li>
<li>安装<strong>所有依赖项</strong>（包括直接依赖和传递依赖）到当前环境。</li>
<li><strong>默认也会以可编辑模式安装当前项目</strong>（如果 <code>pyproject.toml</code> 中定义了项目）。</li>
<li>确保环境状态与锁定文件<strong>完全一致</strong>（版本、哈希、来源等）。</li>
</ul>
</li>
<li><strong>前提</strong>：通常需要先运行 <code>uv lock</code> 生成 <code>uv.lock</code>。</li>
</ul>
<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1ajJ7zPEa5/?spm_id_from=333.1387.upload.video_card.click&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">【uv】Python迄今最好的项目管理+环境管理工具（吧？）_哔哩哔哩_bilibili</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV13WGHz8EEz?spm_id_from=333.788.videopod.sections&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">从pip到uv：一口气梳理现代Python项目管理全流程！_哔哩哔哩_bilibili</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/08/02/%E5%AD%A6%E4%B9%A0/python%E7%9B%B8%E5%85%B3/python-logging%E6%A8%A1%E5%9D%97/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/08/02/%E5%AD%A6%E4%B9%A0/python%E7%9B%B8%E5%85%B3/python-logging%E6%A8%A1%E5%9D%97/" class="post-title-link" itemprop="url">python-logging模块</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-08-02 00:00:00" itemprop="dateCreated datePublished" datetime="2025-08-02T00:00:00+08:00">2025-08-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-10-19 14:58:53" itemprop="dateModified" datetime="2025-10-19T14:58:53+08:00">2025-10-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">学习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AD%A6%E4%B9%A0/python%E7%9B%B8%E5%85%B3/" itemprop="url" rel="index"><span itemprop="name">python相关</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="日志级别"><a href="#日志级别" class="headerlink" title="日志级别"></a>日志级别</h3><div class="table-container">
<table>
<thead>
<tr>
<th>级别</th>
<th>方法</th>
<th>用途</th>
</tr>
</thead>
<tbody>
<tr>
<td>DEBUG</td>
<td><code>logging.debug()</code></td>
<td>调试信息</td>
</tr>
<tr>
<td>INFO</td>
<td><code>logging.info()</code></td>
<td>普通信息</td>
</tr>
<tr>
<td>WARNING</td>
<td><code>logging.warning()</code></td>
<td>警告信息</td>
</tr>
<tr>
<td>ERROR</td>
<td><code>logging.error()</code></td>
<td>错误信息</td>
</tr>
<tr>
<td>CRITICAL</td>
<td><code>logging.critical()</code></td>
<td>严重错误</td>
</tr>
</tbody>
</table>
</div>
<p>python默认只会打印warning以上级别的日志，可通过<code>basicConfig</code>进行设置，如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 基础配置</span><br><span class="line">logging.basicConfig(level=logging.DEBUG)</span><br><span class="line"></span><br><span class="line"># 记录不同级别的日志</span><br><span class="line">logging.debug(&quot;这是一个DEBUG级别的日志&quot;)</span><br><span class="line">logging.info(&quot;这是一个INFO级别的日志&quot;)</span><br><span class="line">logging.warning(&quot;这是一个WARNING级别的日志&quot;)</span><br><span class="line">logging.error(&quot;这是一个ERROR级别的日志&quot;)</span><br><span class="line">logging.critical(&quot;这是一个CRITICAL级别的日志&quot;)</span><br></pre></td></tr></table></figure>
<h3 id="格式化log并输出"><a href="#格式化log并输出" class="headerlink" title="格式化log并输出"></a>格式化log并输出</h3><p>我们可以使用全局配置，完成log的格式化和输出成文件，如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">logging.basicConfig(level=logging.DEBUG,format=&#x27;%(asctime)s - %(levelname)s - %(message)s&#x27;,filename=&#x27;basic.log&#x27;,filemode=&#x27;w&#x27;)</span><br></pre></td></tr></table></figure>
<p>同样，我们可以自定义logger</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"># 创建自定义logger</span><br><span class="line">logger = logging.getLogger(&#x27;my_app&#x27;)</span><br><span class="line">logger.setLevel(logging.DEBUG)</span><br><span class="line"></span><br><span class="line"># 清除之前的处理器</span><br><span class="line">logger.handlers.clear()</span><br><span class="line"></span><br><span class="line"># 创建文件处理器</span><br><span class="line">file_handler = logging.FileHandler(&#x27;logs/my_app.log&#x27;, encoding=&#x27;utf-8&#x27;)</span><br><span class="line">file_handler.setLevel(logging.DEBUG)</span><br><span class="line"></span><br><span class="line"># 创建控制台处理器</span><br><span class="line">console_handler = logging.StreamHandler()</span><br><span class="line">console_handler.setLevel(logging.WARNING)</span><br><span class="line"></span><br><span class="line"># 创建不同的格式器</span><br><span class="line">file_formatter = logging.Formatter(</span><br><span class="line">    &#x27;%(asctime)s | %(name)s | %(levelname)s | %(funcName)s:%(lineno)d | %(message)s&#x27;</span><br><span class="line">)</span><br><span class="line">console_formatter = logging.Formatter(</span><br><span class="line">    &#x27;🚨 %(levelname)s: %(message)s&#x27;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">file_handler.setFormatter(file_formatter)</span><br><span class="line">console_handler.setFormatter(console_formatter)</span><br><span class="line"></span><br><span class="line"># 添加处理器</span><br><span class="line">logger.addHandler(file_handler)</span><br><span class="line">logger.addHandler(console_handler)</span><br><span class="line"></span><br><span class="line"># 测试不同级别的日志</span><br><span class="line">logger.debug(&quot;调试信息 - 只写入文件&quot;)</span><br><span class="line">logger.info(&quot;普通信息 - 只写入文件&quot;)</span><br><span class="line">logger.warning(&quot;警告信息 - 控制台和文件都有&quot;)</span><br><span class="line">logger.error(&quot;错误信息 - 控制台和文件都有&quot;)</span><br></pre></td></tr></table></figure>
<h3 id="异常捕获"><a href="#异常捕获" class="headerlink" title="异常捕获"></a>异常捕获</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">try:</span><br><span class="line">    result = divide(10, 0)</span><br><span class="line">except ZeroDivisionError as exc:</span><br><span class="line">    # 方式 1：记录异常对象</span><br><span class="line">    logger.error(&quot;除零异常发生: &#123;&#125;&quot;, exc)</span><br><span class="line"></span><br><span class="line">    # 方式 2：记录完整 traceback（推荐）</span><br><span class="line">    logger.exception(&quot;捕获到异常，详情如下&quot;)</span><br></pre></td></tr></table></figure>
<h3 id="loguru的常用使用方法"><a href="#loguru的常用使用方法" class="headerlink" title="loguru的常用使用方法"></a>loguru的常用使用方法</h3><p>基础用法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from loguru import logger</span><br><span class="line"></span><br><span class="line">logger.debug(&quot;这是 debug&quot;)</span><br><span class="line">logger.info(&quot;这是 info&quot;)</span><br><span class="line">logger.warning(&quot;这是 warning&quot;)</span><br><span class="line">logger.error(&quot;这是 error&quot;)</span><br><span class="line">logger.critical(&quot;这是 critical&quot;)</span><br></pre></td></tr></table></figure>
<p>输出到文件    logger.add(“app.log”)</p>
<p>过滤级别    logger.add(“app.log”, level=”WARNING”)</p>
<p>移除默认控制台输出       logger.remove()</p>
<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1rJv8eNE1x?spm_id_from=333.788.videopod.sections&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">[Python] logging模块怎么用_哔哩哔哩_bilibili</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1VnW7edEq7?spm_id_from=333.788.videopod.sections&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">[Python] 打印log神器 —— loguru_哔哩哔哩_bilibili</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/08/01/%E5%AD%A6%E4%B9%A0/ai%E7%9B%B8%E5%85%B3/mcp%E5%AD%A6%E4%B9%A0/langgraph%E5%AE%9E%E6%88%98mcp/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/08/01/%E5%AD%A6%E4%B9%A0/ai%E7%9B%B8%E5%85%B3/mcp%E5%AD%A6%E4%B9%A0/langgraph%E5%AE%9E%E6%88%98mcp/" class="post-title-link" itemprop="url">langgraph实战mcp</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-08-01 00:00:00" itemprop="dateCreated datePublished" datetime="2025-08-01T00:00:00+08:00">2025-08-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-08-02 11:47:16" itemprop="dateModified" datetime="2025-08-02T11:47:16+08:00">2025-08-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/mcp/" itemprop="url" rel="index"><span itemprop="name">mcp</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install langchain-mcp-adapters</span><br></pre></td></tr></table></figure>
<h3 id="使用langgraph调用mcp"><a href="#使用langgraph调用mcp" class="headerlink" title="使用langgraph调用mcp"></a>使用langgraph调用mcp</h3><p>要点主要是利用MultiServerMCPClient构建服务，获取tool</p>
<p>利用预设的create_react_agent构建ReAct架构的智能体并调用工具</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">import asyncio # 需要导入 asyncio 来运行异步函数</span><br><span class="line"># 从langchain_mcp_adapters.client模块导入MultiServerMCPClient类</span><br><span class="line"># 从langgraph.prebuilt模块导入create_react_agent函数</span><br><span class="line">from langchain_mcp_adapters.client import MultiServerMCPClient</span><br><span class="line">from langgraph.prebuilt import create_react_agent</span><br><span class="line"></span><br><span class="line"># 导入 LLM 相关库</span><br><span class="line">from langchain_openai import ChatOpenAI</span><br><span class="line"></span><br><span class="line"># 将主要逻辑封装在一个异步函数中</span><br><span class="line">async def main():</span><br><span class="line">    # 创建MultiServerMCPClient实例，配置两个不同的服务</span><br><span class="line">    client = MultiServerMCPClient(</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;math&quot;: &#123;  # 数学计算服务</span><br><span class="line">                &quot;command&quot;: &quot;python&quot;,  # 使用python命令启动</span><br><span class="line">                # 替换为你的math_server.py文件的绝对路径</span><br><span class="line">                &quot;args&quot;: [&quot;/workspace/langgraph-mcp/math_server.py&quot;],</span><br><span class="line">                &quot;transport&quot;: &quot;stdio&quot;,  # 使用标准输入输出传输</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;weather&quot;: &#123;  # 天气服务</span><br><span class="line">                # 确保你的天气服务器在8000端口运行</span><br><span class="line">                # *** 确保这个 URL 是正确的，并且服务器正在运行 ***</span><br><span class="line">                &quot;url&quot;: &quot;http://localhost:8000/mcp&quot;,</span><br><span class="line">                &quot;transport&quot;: &quot;streamable_http&quot;,  # 使用可流式HTTP传输</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    tools = []</span><br><span class="line">    try:</span><br><span class="line">        # 在异步函数内部正确使用 await</span><br><span class="line">        tools = await client.get_tools()</span><br><span class="line">        print(f&quot;成功获取到 &#123;len(tools)&#125; 个MCP工具。&quot;)</span><br><span class="line">        for tool_item in tools:</span><br><span class="line">            print(f&quot;  - &#123;tool_item.name&#125;: &#123;tool_item.description&#125;&quot;)</span><br><span class="line">    except Exception as e:</span><br><span class="line">        print(f&quot;获取MCP工具失败: &#123;e&#125;&quot;)</span><br><span class="line">        print(&quot;请确保MCP服务URL有效且可访问，或者您已正确配置了认证信息。&quot;)</span><br><span class="line">        # 在函数内部，如果出错可以选择返回或继续处理</span><br><span class="line">        # return # 这里可以 return，但会结束 main 函数</span><br><span class="line"></span><br><span class="line">    if not tools:</span><br><span class="line">        print(&quot;没有获取到工具，无法创建代理。&quot;)</span><br><span class="line">        return</span><br><span class="line"></span><br><span class="line">    # 创建ReAct代理</span><br><span class="line">    llm = ChatOpenAI(</span><br><span class="line">        model=&quot;qwen3-235b-a22b-thinking-2507&quot;,</span><br><span class="line">        api_key=&quot;sk-a8ef27c47ea84224ac6eed6d4bba1bab&quot;,</span><br><span class="line">        base_url=&quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot; # 修正了末尾多余的空格</span><br><span class="line">    )</span><br><span class="line">    agent = create_react_agent(llm, tools)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    # 异步调用代理来解决数学问题</span><br><span class="line">    # 确保在异步函数内部使用 await</span><br><span class="line">    math_response = await agent.ainvoke(</span><br><span class="line">        &#123;&quot;messages&quot;: [&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;what&#x27;s (3 + 5) x 12?&quot;&#125;]&#125;</span><br><span class="line">    )</span><br><span class="line">    print(&quot;\n--- 数学问题回答 ---&quot;)</span><br><span class="line">    print(math_response[&quot;messages&quot;][-1].content) # 打印最后一条消息（LLM的回答）</span><br><span class="line"></span><br><span class="line">    # 异步调用代理来查询天气</span><br><span class="line">    weather_response = await agent.ainvoke(</span><br><span class="line">        &#123;&quot;messages&quot;: [&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;what is the weather in nyc?&quot;&#125;]&#125;</span><br><span class="line">    )</span><br><span class="line">    print(&quot;\n--- 天气问题回答 ---&quot;)</span><br><span class="line">    print(weather_response[&quot;messages&quot;][-1].content) # 打印最后一条消息（LLM的回答）</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># --- 这是脚本的入口点 ---</span><br><span class="line"># 使用 asyncio.run() 来运行你的主异步函数</span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    asyncio.run(main())</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langgraph/agents/mcp/">使用 MCP - LangChain 框架</a></p>
<h3 id="框架流程"><a href="#框架流程" class="headerlink" title="框架流程"></a>框架流程</h3><p><img src="/2025/08/01/%E5%AD%A6%E4%B9%A0/ai%E7%9B%B8%E5%85%B3/mcp%E5%AD%A6%E4%B9%A0/langgraph%E5%AE%9E%E6%88%98mcp/image-20250801164905578.png" alt="image-20250801164905578"></p>
<p>✅ 三个角色（系统组件）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>角色</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Client</strong></td>
<td>前端或用户界面，发起请求</td>
</tr>
<tr>
<td><strong>Auth Provider</strong></td>
<td>认证服务（如 OAuth、JWT 提供者），负责登录和签发 token</td>
</tr>
<tr>
<td><strong>LangGraph Backend</strong></td>
<td>应用的后端服务，处理业务逻辑</td>
</tr>
<tr>
<td><strong>Secret Store</strong></td>
<td>存放用户敏感信息（如 token、密钥等）</td>
</tr>
<tr>
<td><strong>MCP Server</strong></td>
<td>后端工具服务，提供具体的工具或资源接口</td>
</tr>
</tbody>
</table>
</div>
<p>✅ 流程详解（12步）</p>
<p>🔐 阶段一：用户登录 &amp; 获取 Token（1~6）</p>
<ol>
<li><p><strong>用户登录</strong><br>Client 提交用户名和密码给 Auth Provider。</p>
</li>
<li><p><strong>返回 Token</strong><br>Auth Provider 验证成功后，返回一个访问令牌（token）。</p>
</li>
<li><p><strong>携带 Token 请求</strong><br>Client 将 token 附加在请求头中，发给 LangGraph Backend。</p>
</li>
<li><p><strong>验证 Token</strong><br>LangGraph Backend 使用 <code>@auth.authenticate</code> 中间件验证 token 是否有效。</p>
</li>
<li><p><strong>获取用户信息</strong><br>验证通过后，LangGraph Backend 从 Auth Provider 拉取用户详细信息。</p>
</li>
<li><p><strong>确认有效性</strong><br>后端确认用户信息无误，流程继续。</p>
</li>
</ol>
<p>🔑 阶段二：获取用户权限 Token（6a~6b）</p>
<p>6a. <strong>拉取用户权限 Token</strong><br>   LangGraph Backend 从 Secret Store 获取该用户对应的权限 token（可能是 MCP 所需的访问凭证）。</p>
<p>6b. <strong>返回权限 Token</strong><br>   Secret Store 返回该 token。</p>
<p>🛠️ 阶段三：调用工具 &amp; 返回结果（7~12）</p>
<ol>
<li><p><strong>权限控制检查</strong><br>LangGraph Backend 使用 <code>@auth.on.*</code> 权限控制逻辑，确认用户是否有权调用该工具。</p>
</li>
<li><p><strong>构建 MCP Client</strong><br>后端用用户的权限 token 构建一个 MCP 客户端。</p>
</li>
<li><p><strong>调用 MCP 工具</strong><br>MCP Client 发起请求，调用某个具体工具，携带 token（通常放在请求头中）。</p>
</li>
<li><p><strong>MCP 验证并执行</strong><br>MCP Server 验证 token 是否有效，确认无误后执行工具逻辑。</p>
</li>
<li><p><strong>工具返回结果</strong><br>MCP Server 返回工具执行结果或资源数据。</p>
</li>
<li><p><strong>返回给前端</strong><br>LangGraph Backend 将结果返回给 Client，完成整个链路。</p>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="上一页" aria-label="上一页" href="/page/3/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><span class="space">&hellip;</span><a class="page-number" href="/page/16/">16</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/5/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">张熙浚</span>
  </div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="本站访问数 fa fa-user 次"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="本站总访问量 fa fa-eye 次"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script size="400" alpha="0.6" zIndex="-1" src="https://cdnjs.cloudflare.com/ajax/libs/ribbon.js/1.0.2/ribbon.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.1.0/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script src="/js/pjax.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  <script src="/js/third-party/pace.js"></script>


  
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
