<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha256-XOqroi11tY4EFQMR9ZYwZWKj5ZXiftSx36RRuC3anlA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-bounce.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.20.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="zxj Blogs">
<meta property="og:type" content="website">
<meta property="og:title" content="Zhang XiJun">
<meta property="og:url" content="http://example.com/page/6/index.html">
<meta property="og:site_name" content="Zhang XiJun">
<meta property="og:description" content="zxj Blogs">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="张熙浚">
<meta property="article:tag" content="zxj">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/page/6/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"page/6/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Zhang XiJun</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Zhang XiJun</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">BLOGS</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="张熙浚"
      src="/images/zxjavatar.gif">
  <p class="site-author-name" itemprop="name">张熙浚</p>
  <div class="site-description" itemprop="description">zxj Blogs</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">155</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">60</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">61</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/zxj-2023" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zxj-2023" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="http://wpa.qq.com/msgrd?v=3&uin=2902065320&site=qq&menu=yes" title="QQ → http:&#x2F;&#x2F;wpa.qq.com&#x2F;msgrd?v&#x3D;3&amp;uin&#x3D;2902065320&amp;site&#x3D;qq&amp;menu&#x3D;yes" rel="noopener me" target="_blank"><i class="fab fa-qq fa-fw"></i>QQ</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
    <div class="sidebar-inner sidebar-blogroll">
      <div class="links-of-blogroll animated">
        <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
          链接
        </div>
        <ul class="links-of-blogroll-list">
            <li class="links-of-blogroll-item">
              <a href="https://zxj-2023.github.io/" title="https:&#x2F;&#x2F;zxj-2023.github.io" rel="noopener" target="_blank">Zhang XiJun</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://theme-next.js.org/" title="https:&#x2F;&#x2F;theme-next.js.org" rel="noopener" target="_blank">NexT</a>
            </li>
        </ul>
      </div>
    </div>
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/07/18/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/libreoffice/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/07/18/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/libreoffice/" class="post-title-link" itemprop="url">Libreoffice</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-07-18 00:00:00" itemprop="dateCreated datePublished" datetime="2025-07-18T00:00:00+08:00">2025-07-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-08-20 16:31:32" itemprop="dateModified" datetime="2025-08-20T16:31:32+08:00">2025-08-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AE%9E%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">实习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/" itemprop="url" rel="index"><span itemprop="name">晨晟智控</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="libreoffice部署"><a href="#libreoffice部署" class="headerlink" title="libreoffice部署"></a>libreoffice部署</h3><h4 id="查看Linux发行版"><a href="#查看Linux发行版" class="headerlink" title="查看Linux发行版"></a>查看Linux发行版</h4><p><img src="/2025/07/18/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/libreoffice/image-20250718095931232.png" alt="image-20250718095931232"></p>
<p>系统是 <strong>银河麒麟高级服务器操作系统 V10（Kylin Linux Advanced Server V10）</strong>，属于 <strong>中国国产、兼容 CentOS/RHEL 生态</strong> 的 Linux 发行版。</p>
<p>因此它使用 <strong>RPM 包管理</strong>（<code>dnf</code>/<code>yum</code>），而不是 <code>.deb</code>。</p>
<h4 id="查看CPU-处理器架构"><a href="#查看CPU-处理器架构" class="headerlink" title="查看CPU 处理器架构"></a>查看<strong>CPU 处理器架构</strong></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">uname -m</span><br></pre></td></tr></table></figure>
<p>是<strong>x86_64</strong></p>
<h4 id="不用部署了，镜像里有，直接用了"><a href="#不用部署了，镜像里有，直接用了" class="headerlink" title="不用部署了，镜像里有，直接用了"></a>不用部署了，镜像里有，直接用了</h4><h4 id="使用libreoffice处理doc文件，转成pdf"><a href="#使用libreoffice处理doc文件，转成pdf" class="headerlink" title="使用libreoffice处理doc文件，转成pdf"></a>使用libreoffice处理doc文件，转成pdf</h4><p>将当前目录下所有 <code>.doc</code> 和 <code>.docx</code> 转为 PDF：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">libreoffice --headless --convert-to pdf *.doc *.docx --outdir ./pdf_output/</span><br><span class="line"></span><br><span class="line"># 检查是否有残留进程</span><br><span class="line">ps aux | grep libreoffice</span><br><span class="line"></span><br><span class="line"># 如果有残留进程，杀死它们</span><br><span class="line">killall soffice.bin 2&gt;/dev/null</span><br></pre></td></tr></table></figure>
<h4 id="python"><a href="#python" class="headerlink" title="python"></a>python</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import subprocess</span><br><span class="line">import argparse</span><br><span class="line">import glob</span><br><span class="line">from pathlib import Path</span><br><span class="line">def batch_convert_documents(input_path, output_dir):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    批量转换文档的函数版本</span><br><span class="line">    </span><br><span class="line">    Args:</span><br><span class="line">        input_path (str): 输入路径（文件、目录或通配符）</span><br><span class="line">        output_dir (str): 输出目录</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">        bool: 转换是否成功</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    # 确保输出目录存在</span><br><span class="line">    Path(output_dir).mkdir(parents=True, exist_ok=True)</span><br><span class="line">    </span><br><span class="line">    # 收集所有要转换的文件</span><br><span class="line">    files_to_convert = []</span><br><span class="line">    </span><br><span class="line">    if os.path.isfile(input_path):</span><br><span class="line">        # 单个文件</span><br><span class="line">        if input_path.lower().endswith((&#x27;.doc&#x27;, &#x27;.docx&#x27;)):</span><br><span class="line">            files_to_convert.append(input_path)</span><br><span class="line">    elif os.path.isdir(input_path):</span><br><span class="line">        # 目录中的所有doc/docx文件</span><br><span class="line">        for pattern in [&#x27;*.doc&#x27;, &#x27;*.docx&#x27;]:</span><br><span class="line">            files_to_convert.extend(glob.glob(os.path.join(input_path, pattern)))</span><br><span class="line">    else:</span><br><span class="line">        # 通配符模式</span><br><span class="line">        files_to_convert = glob.glob(input_path)</span><br><span class="line">        # 过滤出doc和docx文件</span><br><span class="line">        files_to_convert = [f for f in files_to_convert if f.lower().endswith((&#x27;.doc&#x27;, &#x27;.docx&#x27;))]</span><br><span class="line">    </span><br><span class="line">    if not files_to_convert:</span><br><span class="line">        print(&quot;未找到要转换的文档文件&quot;)</span><br><span class="line">        return False</span><br><span class="line">    </span><br><span class="line">    print(f&quot;找到 &#123;len(files_to_convert)&#125; 个文件需要转换&quot;)</span><br><span class="line">    </span><br><span class="line">    # 构建命令</span><br><span class="line">    cmd = [</span><br><span class="line">        &#x27;libreoffice&#x27;,</span><br><span class="line">        &#x27;--headless&#x27;,</span><br><span class="line">        &#x27;--convert-to&#x27;, &#x27;pdf&#x27;,</span><br><span class="line">        &#x27;--outdir&#x27;, output_dir</span><br><span class="line">    ] + files_to_convert</span><br><span class="line">    </span><br><span class="line">    try:</span><br><span class="line">        print(&quot;正在转换文件...&quot;)</span><br><span class="line">        result = subprocess.run(cmd, capture_output=True, text=True, timeout=600)</span><br><span class="line">        </span><br><span class="line">        if result.returncode == 0:</span><br><span class="line">            print(f&quot;成功转换 &#123;len(files_to_convert)&#125; 个文件&quot;)</span><br><span class="line">            return True</span><br><span class="line">        else:</span><br><span class="line">            print(f&quot;转换失败: &#123;result.stderr&#125;&quot;)</span><br><span class="line">            return False</span><br><span class="line">            </span><br><span class="line">    except Exception as e:</span><br><span class="line">        print(f&quot;转换过程中发生错误: &#123;e&#125;&quot;)</span><br><span class="line">        return False</span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    success = batch_convert_documents(&quot;./docs&quot;, &quot;./pdf_output&quot;)</span><br></pre></td></tr></table></figure>
<h3 id="Linux扫盲"><a href="#Linux扫盲" class="headerlink" title="Linux扫盲"></a>Linux扫盲</h3><h4 id="发行版"><a href="#发行版" class="headerlink" title="发行版"></a>发行版</h4><p>像Ubuntu，CentOS都属于Linux的发行版，就像Windows11属于Windows的关系</p>
<p>常见发行版分类：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>系列</th>
<th>代表发行版</th>
<th>包格式</th>
<th>特点</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Debian 系</strong></td>
<td>Debian、Ubuntu、Kali、Linux Mint</td>
<td><code>.deb</code></td>
<td>包多、社区大、教程多</td>
</tr>
<tr>
<td><strong>Red Hat 系</strong></td>
<td>CentOS、RHEL、Rocky、Alma、Fedora</td>
<td><code>.rpm</code></td>
<td>企业级稳定、官方支持长</td>
</tr>
<tr>
<td><strong>SUSE 系</strong></td>
<td>openSUSE Leap / Tumbleweed</td>
<td><code>.rpm</code></td>
<td>YaST 管理工具、欧洲流行</td>
</tr>
<tr>
<td><strong>Arch 系</strong></td>
<td>Arch Linux、Manjaro</td>
<td><code>.pkg.tar.zst</code></td>
<td>滚动更新、极客向</td>
</tr>
<tr>
<td><strong>轻量/最小</strong></td>
<td>Alpine、Debian netinst、CentOS Stream Minimal</td>
<td>任意</td>
<td>镜像小、资源占用低</td>
</tr>
</tbody>
</table>
</div>
<p>如何查看Linux发行版</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /etc/os-release</span><br></pre></td></tr></table></figure>
<h4 id="deb和rpm"><a href="#deb和rpm" class="headerlink" title="deb和rpm"></a>deb和rpm</h4><p> <code>.deb</code> 和 <code>.rpm</code> 想象成 <strong>“Linux 世界里的安装程序”</strong>，就像 Windows 的 <code>.exe</code> / <code>.msi</code></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>格式</th>
<th>适用系统</th>
<th>安装命令</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong><code>.deb</code></strong></td>
<td>Debian、Ubuntu、Linux Mint、Kali 等</td>
<td><code>sudo dpkg -i xxx.deb</code> 或 <code>sudo apt install ./xxx.deb</code></td>
</tr>
<tr>
<td><strong><code>.rpm</code></strong></td>
<td>CentOS、RHEL、Fedora、openSUSE、Rocky、Alma 等</td>
<td><code>sudo rpm -ivh xxx.rpm</code> 或 <code>sudo dnf/yum install xxx.rpm</code></td>
</tr>
</tbody>
</table>
</div>
<h4 id="cpu处理器架构"><a href="#cpu处理器架构" class="headerlink" title="cpu处理器架构"></a>cpu处理器架构</h4><div class="table-container">
<table>
<thead>
<tr>
<th>目录名</th>
<th>代表架构</th>
<th>适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>x86_64</strong></td>
<td><strong>Intel/AMD 64 位</strong></td>
<td>绝大多数台式机、服务器（如 Xeon、EPYC、Core、Ryzen）</td>
</tr>
<tr>
<td><strong>aarch64</strong></td>
<td><strong>ARM 64 位</strong></td>
<td>树莓派 4/5、苹果 M 系列（Asahi Linux）、鲲鹏、飞腾、Ampere ARM 服务器等</td>
</tr>
</tbody>
</table>
</div>
<p>查看处理器架构：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">uname -m</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/07/18/%E5%AD%A6%E4%B9%A0/ai%E6%A1%86%E6%9E%B6/langgraph-agent/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/07/18/%E5%AD%A6%E4%B9%A0/ai%E6%A1%86%E6%9E%B6/langgraph-agent/" class="post-title-link" itemprop="url">LangGraph学习——agent——上</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-07-18 00:00:00" itemprop="dateCreated datePublished" datetime="2025-07-18T00:00:00+08:00">2025-07-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-08-13 09:25:22" itemprop="dateModified" datetime="2025-08-13T09:25:22+08:00">2025-08-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ai%E6%A1%86%E6%9E%B6/" itemprop="url" rel="index"><span itemprop="name">ai框架</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ai%E6%A1%86%E6%9E%B6/langgraph/" itemprop="url" rel="index"><span itemprop="name">langgraph</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>本教程为langchain官方教程的学习记录</p>
<p><a target="_blank" rel="noopener" href="https://academy.langchain.com/enrollments">academy.langchain.com/enrollments</a></p>
<p>代码见<a target="_blank" rel="noopener" href="https://github.com/zxj-2023/learn-rag-langchain"><a target="_blank" rel="noopener" href="https://github.com/zxj-2023/learn-rag-langchain/tree/main/academy-langgraph">learn-rag-langchain/academy-langgraph at main · zxj-2023/learn-rag-langchain</a></a></p>
<h3 id="module-1"><a href="#module-1" class="headerlink" title="module-1"></a>module-1</h3><h4 id="route路由"><a href="#route路由" class="headerlink" title="route路由"></a>route路由</h4><p>在 LangGraph 中，<strong>route（路由）\</strong>的核心作用是*<em>根据当前状态动态决定“下一步应该执行哪个节点”*</em></p>
<h5 id="定义工具"><a href="#定义工具" class="headerlink" title="定义工具"></a>定义工具</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def multiply(a: int, b: int) -&gt; int:</span><br><span class="line">    &quot;&quot;&quot;Multiply a and b.</span><br><span class="line"></span><br><span class="line">    Args:</span><br><span class="line">        a: first int</span><br><span class="line">        b: second int</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    return a * b</span><br><span class="line">    </span><br><span class="line">llm = ChatOpenAI(</span><br><span class="line"></span><br><span class="line">)</span><br><span class="line">llm_with_tools = llm.bind_tools([multiply])</span><br></pre></td></tr></table></figure>
<blockquote>
<p>三引号字符串叫 <strong>docstring</strong>，它会被 LangChain 拿来做两件事：</p>
<ol>
<li><strong>生成工具的 description（给大模型看的“说明书”）</strong><br>没有它时，LangChain 只能退而求其次，把函数名 <code>multiply</code> 拼成一句 “multiply tool” 之类的默认描述。大模型拿到的工具列表里，这个工具就只有一个干巴巴的名字和参数列表，它可能猜不到这个工具到底是干什么的</li>
<li><strong>给人类开发者自己看</strong><br>IDE、文档生成器、静态检查工具都会读取这段文字，方便后期维护。</li>
</ol>
</blockquote>
<h5 id="构建条件边"><a href="#构建条件边" class="headerlink" title="构建条件边"></a>构建条件边</h5><p><code>tool_calling_llm</code> 是一个<strong>普通的计算节点</strong>（node），负责把当前对话状态交给大模型，让大模型决定要不要调用工具；</p>
<p>真正完成“路由”动作的是 <strong><code>tools_condition</code></strong> 这个函数——它才是 LangGraph 里的 <strong>route（条件边）</strong>。</p>
<p><code>tools_condition</code> 是 作为<strong>LangGraph 预置的“默认路由函数”</strong>，功能就是，如果大模型的最新回复中包含工具调用，就调用工具</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># Node</span><br><span class="line">def tool_calling_llm(state: MessagesState):</span><br><span class="line">	#调用大模型后将最新的消息返回</span><br><span class="line">    return &#123;&quot;messages&quot;: [llm_with_tools.invoke(state[&quot;messages&quot;])]&#125;</span><br><span class="line"></span><br><span class="line"># Build graph</span><br><span class="line">#MessagesState 是 LangGraph 官方预置 的一种 状态（State）定义</span><br><span class="line">#这个状态维护了一个消息list，有新的消息就加进这个消息list</span><br><span class="line">builder = StateGraph(MessagesState)</span><br><span class="line">builder.add_node(&quot;tool_calling_llm&quot;, tool_calling_llm)</span><br><span class="line">builder.add_node(&quot;tools&quot;, ToolNode([multiply]))</span><br><span class="line">builder.add_edge(START, &quot;tool_calling_llm&quot;)</span><br><span class="line">builder.add_conditional_edges(</span><br><span class="line">    &quot;tool_calling_llm&quot;,</span><br><span class="line">    # 如果助手（结果）的最新消息是工具调用 -&gt; tools_condition 路由到工具</span><br><span class="line">    # 如果助手（结果）的最新消息不是工具调用 -&gt; tools_condition 路由到 END</span><br><span class="line">    tools_condition,</span><br><span class="line">)</span><br><span class="line">builder.add_edge(&quot;tools&quot;, END)</span><br><span class="line">graph = builder.compile()</span><br><span class="line"></span><br><span class="line"># View</span><br><span class="line">display(Image(graph.get_graph().draw_mermaid_png()))</span><br></pre></td></tr></table></figure>
<p><img src="/2025/07/18/%E5%AD%A6%E4%B9%A0/ai%E6%A1%86%E6%9E%B6/langgraph-agent/image-20250719142543838.png" alt="image-20250719142543838"></p>
<h5 id="调用"><a href="#调用" class="headerlink" title="调用"></a>调用</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from langchain_core.messages import HumanMessage</span><br><span class="line">messages = [HumanMessage(content=&quot;你好，2乘2是多少&quot;)]</span><br><span class="line">messages = graph.invoke(&#123;&quot;messages&quot;: messages&#125;)</span><br><span class="line">for m in messages[&#x27;messages&#x27;]:</span><br><span class="line">    m.pretty_print()</span><br></pre></td></tr></table></figure>
<p>输出</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">================================[1m Human Message [0m=================================</span><br><span class="line"></span><br><span class="line">你好，2乘2是多少</span><br><span class="line">==================================[1m Ai Message [0m==================================</span><br><span class="line">Tool Calls:</span><br><span class="line">  multiply (call_e026ceb409e247748786ad)</span><br><span class="line"> Call ID: call_e026ceb409e247748786ad</span><br><span class="line">  Args:</span><br><span class="line">    a: 2</span><br><span class="line">    b: 2</span><br><span class="line">=================================[1m Tool Message [0m=================================</span><br><span class="line">Name: multiply</span><br><span class="line"></span><br><span class="line">4</span><br></pre></td></tr></table></figure>
<h4 id="agent代理"><a href="#agent代理" class="headerlink" title="agent代理"></a>agent代理</h4><p>在 LangGraph 中，<strong>代理（Agent）</strong> 被明确定义为<strong>“一个由大语言模型（LLM）驱动的、能够循环决策并调用外部工具来完成任务的节点或子图”</strong>。</p>
<p><strong>Agent = LLM + 工具集合 + 提示模板</strong>，三者在 LangGraph 的状态化图结构里循环运行，直到满足停止条件。</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2210.03629">ReAct</a> 是一种流行的通用智能体架构，它结合了这些扩展，并整合了三个核心概念。</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langgraph/concepts/agentic_concepts/#tool-calling">工具调用</a>：允许LLM根据需要选择和使用各种工具。</li>
<li><a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langgraph/concepts/agentic_concepts/#memory">记忆</a>：使智能体能够保留和使用之前步骤的信息。</li>
<li><a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langgraph/concepts/agentic_concepts/#planning">规划</a>：使LLM能够创建并遵循多步计划以实现目标。</li>
</ol>
<p>即</p>
<p><code>act</code>- 让模型调用特定工具</p>
<p><code>observe</code> - 将工具输出传递回模型  </p>
<p> <code>reason</code> - 让模型对工具输出进行推理，以决定下一步操作（例如，调用另一个工具或直接响应）</p>
</blockquote>
<h5 id="定义工具-1"><a href="#定义工具-1" class="headerlink" title="定义工具"></a>定义工具</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tools = [add, multiply, divide]#工具函数具体内容省略</span><br><span class="line">llm = ChatOpenAI()</span><br><span class="line"></span><br><span class="line"># 在这个 ipynb 文件中，我们将并行工具调用（parallel tool calling）设置为 false，因为数学计算通常是按顺序执行的，并且这次我们有3个可以进行数学计算的工具。</span><br><span class="line"># OpenAI 模型为了效率，默认进行并行工具调用，详情请参阅 `https://python.langchain.com/docs/how_to/tool_calling_parallel/`</span><br><span class="line"># 不妨尝试一下，看看模型在处理数学方程式时的表现！</span><br><span class="line">llm_with_tools = llm.bind_tools(tools, parallel_tool_calls=False)</span><br></pre></td></tr></table></figure>
<h5 id="创建代理"><a href="#创建代理" class="headerlink" title="创建代理"></a>创建代理</h5><p>定义节点</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from langgraph.graph import MessagesState</span><br><span class="line">from langchain_core.messages import HumanMessage, SystemMessage</span><br><span class="line"></span><br><span class="line"># System message</span><br><span class="line">sys_msg = SystemMessage(content=&quot;你是一个乐于助人的助手，负责对一组输入执行算术运算。&quot;)</span><br><span class="line"></span><br><span class="line"># Node</span><br><span class="line">def assistant(state: MessagesState):</span><br><span class="line">   return &#123;&quot;messages&quot;: [llm_with_tools.invoke([sys_msg] + state[&quot;messages&quot;])]&#125;</span><br></pre></td></tr></table></figure>
<p>这一步相当于定义了系统提示词，然后在 assistant 这个节点里，通过 [sys_msg] + state[“messages”] 这部分代码，这个系统提示词被添加到了整个对话历史的最前面，然后一起发送给模型。这样一来，模型在生成回复时就会遵循这个系统提示词的指示。</p>
<p>与上一个不同的是，我们将 <code>Tools</code> 节点 <strong>回环</strong> 连接到 <code>Assistant</code>，从而形成一个回路。</p>
<p>在 assistant节点执行后，<code>tools_condition</code>检查模型的输出是否为工具调用。</p>
<p>如果是工具调用，则流程被导向至 <code>tools</code> 节点。  </p>
<p><code>tools</code>节点重新连接到<code>assistant</code><strong>。</strong> 只要模型决定调用工具，此循环就会继续。  </p>
<p>如果模型的响应不是工具调用，则流程被导向至结束，终止该过程。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># Graph</span><br><span class="line">builder = StateGraph(MessagesState)</span><br><span class="line"></span><br><span class="line"># Define nodes: these do the work</span><br><span class="line">builder.add_node(&quot;assistant&quot;, assistant)</span><br><span class="line">builder.add_node(&quot;tools&quot;, ToolNode(tools))</span><br><span class="line"></span><br><span class="line"># Define edges: these determine how the control flow moves</span><br><span class="line">builder.add_edge(START, &quot;assistant&quot;)</span><br><span class="line">builder.add_conditional_edges(</span><br><span class="line">    &quot;assistant&quot;,</span><br><span class="line">    # If the latest message (result) from assistant is a tool call -&gt; tools_condition routes to tools</span><br><span class="line">    # If the latest message (result) from assistant is a not a tool call -&gt; tools_condition routes to END</span><br><span class="line">    tools_condition,</span><br><span class="line">)</span><br><span class="line">builder.add_edge(&quot;tools&quot;, &quot;assistant&quot;)</span><br><span class="line">react_graph = builder.compile()</span><br><span class="line"></span><br><span class="line"># Show</span><br><span class="line">display(Image(react_graph.get_graph(xray=True).draw_mermaid_png()))</span><br></pre></td></tr></table></figure>
<p><img src="/2025/07/18/%E5%AD%A6%E4%B9%A0/ai%E6%A1%86%E6%9E%B6/langgraph-agent/image-20250719145948088.png" alt="image-20250719145948088"></p>
<h5 id="调用-1"><a href="#调用-1" class="headerlink" title="调用"></a>调用</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">messages = [HumanMessage(content=&quot;将3和4相加。将结果乘以2。再将结果除以5。&quot;)]</span><br><span class="line">messages = react_graph.invoke(&#123;&quot;messages&quot;: messages&#125;)</span><br><span class="line"></span><br><span class="line">for m in messages[&#x27;messages&#x27;]:</span><br><span class="line">    m.pretty_print()</span><br></pre></td></tr></table></figure>
<p>parallel_tool_calls=False的输出</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">================================[1m Human Message [0m=================================</span><br><span class="line"></span><br><span class="line">将3和4相加。将结果乘以2。再将结果除以5。</span><br><span class="line">==================================[1m Ai Message [0m==================================</span><br><span class="line">Tool Calls:</span><br><span class="line">  add (call_6c69898dba0342bfbb889e)</span><br><span class="line"> Call ID: call_6c69898dba0342bfbb889e</span><br><span class="line">  Args:</span><br><span class="line">    a: 3</span><br><span class="line">    b: 4</span><br><span class="line">=================================[1m Tool Message [0m=================================</span><br><span class="line">Name: add</span><br><span class="line"></span><br><span class="line">7</span><br><span class="line">==================================[1m Ai Message [0m==================================</span><br><span class="line">Tool Calls:</span><br><span class="line">  multiply (call_9940e7603ecf4a13a5f2fb)</span><br><span class="line"> Call ID: call_9940e7603ecf4a13a5f2fb</span><br><span class="line">  Args:</span><br><span class="line">    a: 7</span><br><span class="line">    b: 2</span><br><span class="line">=================================[1m Tool Message [0m=================================</span><br><span class="line">Name: multiply</span><br><span class="line"></span><br><span class="line">14</span><br><span class="line">==================================[1m Ai Message [0m==================================</span><br><span class="line">Tool Calls:</span><br><span class="line">  divide (call_d48fbbe205a14dfbaa3500)</span><br><span class="line"> Call ID: call_d48fbbe205a14dfbaa3500</span><br><span class="line">  Args:</span><br><span class="line">    a: 14</span><br><span class="line">    b: 5</span><br><span class="line">=================================[1m Tool Message [0m=================================</span><br><span class="line">Name: divide</span><br><span class="line"></span><br><span class="line">2.8</span><br><span class="line">==================================[1m Ai Message [0m==================================</span><br><span class="line"></span><br><span class="line">最终结果是2.8。</span><br></pre></td></tr></table></figure>
<p>parallel_tool_calls=True的输出</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">================================[1m Human Message [0m=================================</span><br><span class="line"></span><br><span class="line">将3和4相加。将结果乘以2。再将结果除以5。</span><br><span class="line">==================================[1m Ai Message [0m==================================</span><br><span class="line">Tool Calls:</span><br><span class="line">  add (call_e0c7d8e65f2c49e8aecd3e)</span><br><span class="line"> Call ID: call_e0c7d8e65f2c49e8aecd3e</span><br><span class="line">  Args:</span><br><span class="line">    a: 3</span><br><span class="line">    b: 4</span><br><span class="line">  multiply (call_5bf824058e64489aaace91)</span><br><span class="line"> Call ID: call_5bf824058e64489aaace91</span><br><span class="line">  Args:</span><br><span class="line">    a: 7</span><br><span class="line">    b: 2</span><br><span class="line">  divide (call_36c34f69f6574028b28847)</span><br><span class="line"> Call ID: call_36c34f69f6574028b28847</span><br><span class="line">  Args:</span><br><span class="line">    a: 14</span><br><span class="line">    b: 5</span><br><span class="line">=================================[1m Tool Message [0m=================================</span><br><span class="line">Name: add</span><br><span class="line"></span><br><span class="line">7</span><br><span class="line">=================================[1m Tool Message [0m=================================</span><br><span class="line">Name: multiply</span><br><span class="line"></span><br><span class="line">14</span><br><span class="line">=================================[1m Tool Message [0m=================================</span><br><span class="line">Name: divide</span><br><span class="line"></span><br><span class="line">2.8</span><br><span class="line">==================================[1m Ai Message [0m==================================</span><br><span class="line"></span><br><span class="line">最终结果是 **2.8**。</span><br></pre></td></tr></table></figure>
<h4 id="Agent-memory代理记忆"><a href="#Agent-memory代理记忆" class="headerlink" title="Agent memory代理记忆"></a>Agent memory代理记忆</h4><p>使用chekpointer检查点的功能，最简单的检查点之一是 <code>MemorySaver</code>，这是一个用于图形状态的内存键值存储。</p>
<p>这个检查点就相当于把<strong>图的每一次“状态快照”持久化到外部存储</strong>的机制。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from langgraph.checkpoint.memory import MemorySaver</span><br><span class="line">memory = MemorySaver()</span><br><span class="line">react_graph_memory = builder.compile(checkpointer=memory)</span><br></pre></td></tr></table></figure>
<p>我们可以使用 <a target="_blank" rel="noopener" href="https://langchain-ai.github.io/langgraph/how-tos/persistence/">记忆功能</a> 来解决这个问题！LangGraph 可以使用检查点工具在每一步之后自动保存图的状态。这一内置的持久化层为我们提供了内存功能，使 LangGraph 能够从最后一次状态更新处继续。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># Specify a thread</span><br><span class="line">config = &#123;&quot;configurable&quot;: &#123;&quot;thread_id&quot;: &quot;1&quot;&#125;&#125;</span><br><span class="line"></span><br><span class="line"># Specify an input</span><br><span class="line">messages = [HumanMessage(content=&quot;Add 3 and 4.&quot;)]</span><br><span class="line"></span><br><span class="line"># Run</span><br><span class="line">messages = react_graph_memory.invoke(&#123;&quot;messages&quot;: messages&#125;,config)</span><br><span class="line">for m in messages[&#x27;messages&#x27;]:</span><br><span class="line">    m.pretty_print()</span><br></pre></td></tr></table></figure>
<p>当我们使用内存时，我们需要指定一个 <code>thread_id</code>。这 <code>thread_id</code> 将存储我们的图形状态集合。</p>
<p>如下图，检查点在图的每一步写入状态，这些检查点保存在一个线程中 ，我们可以使用 <code>thread_id</code> 在未来访问该线程</p>
<p><img src="/2025/07/18/%E5%AD%A6%E4%B9%A0/ai%E6%A1%86%E6%9E%B6/langgraph-agent/66e0e9f526b41a4ed9e2d28b_agent-memory2.png" alt="state.jpg"></p>
<h3 id="module-2"><a href="#module-2" class="headerlink" title="module-2"></a>module-2</h3><h4 id="state-scheme状态模式"><a href="#state-scheme状态模式" class="headerlink" title="state-scheme状态模式"></a>state-scheme状态模式</h4><p>LangGraph 的 <strong>state-scheme（状态模式）</strong> 就是“一张<strong>蓝图</strong>”，它告诉框架：“在整个图的生命周期里，状态对象应该长什么样、每个字段怎样被更新、以及节点之间如何共享或隔离数据。”</p>
<p>state-scheme 用 <strong>TypedDict</strong> 或 <strong>Pydantic BaseModel</strong> 来声明，定义了：</p>
<ul>
<li>状态里有哪些字段（key）</li>
<li>每个字段的 Python 类型</li>
<li><strong>可选</strong> 该字段的 <strong>reducer</strong>（更新规则）</li>
</ul>
<h5 id="TypedDict"><a href="#TypedDict" class="headerlink" title="TypedDict"></a><strong>TypedDict</strong></h5><p><strong>基本定义</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from typing_extensions import TypedDict</span><br><span class="line"></span><br><span class="line">class TypedDictState(TypedDict):</span><br><span class="line">    foo: str</span><br><span class="line">    bar: str</span><br></pre></td></tr></table></figure>
<p><strong>可增加像 <code>Literal</code> 这样的类型提示，使其更有价值</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from typing import Literal</span><br><span class="line"></span><br><span class="line">class TypedDictState(TypedDict):</span><br><span class="line">    name: str</span><br><span class="line">    mood: Literal[&quot;happy&quot;,&quot;sad&quot;]</span><br></pre></td></tr></table></figure>
<p>在这里，<code>mood</code> 只能是 “happy” 或 “sad”。</p>
<p><strong>加 reducer：让更新“可追加”而不覆盖</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">class MathState(TypedDict):</span><br><span class="line">    question: str</span><br><span class="line">    scratchpad: Annotated[list[str], add_message]  # 新元素自动追加</span><br><span class="line">    answer: int</span><br></pre></td></tr></table></figure>
<p><code>Annotated[list[str], add]</code> 告诉 LangGraph：当节点返回 <code>&#123;&quot;scratchpad&quot;: [&quot;新步骤&quot;]&#125;</code> 时，<strong>追加</strong>到现有列表，而不是替换</p>
<p><strong>示例</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">import random</span><br><span class="line">from IPython.display import Image, display</span><br><span class="line">from langgraph.graph import StateGraph, START, END</span><br><span class="line">#定义节点</span><br><span class="line">def node_1(state):</span><br><span class="line">    print(&quot;---Node 1---&quot;)</span><br><span class="line">    return &#123;&quot;name&quot;: state[&#x27;name&#x27;] + &quot; is ... &quot;&#125;</span><br><span class="line"></span><br><span class="line">def node_2(state):</span><br><span class="line">    print(&quot;---Node 2---&quot;)</span><br><span class="line">    return &#123;&quot;mood&quot;: &quot;happy&quot;&#125;</span><br><span class="line"></span><br><span class="line">def node_3(state):</span><br><span class="line">    print(&quot;---Node 3---&quot;)</span><br><span class="line">    return &#123;&quot;mood&quot;: &quot;sad&quot;&#125;</span><br><span class="line">#路由函数</span><br><span class="line">def decide_mood(state) -&gt; Literal[&quot;node_2&quot;, &quot;node_3&quot;]:</span><br><span class="line">        </span><br><span class="line">    # Here, let&#x27;s just do a 50 / 50 split between nodes 2, 3</span><br><span class="line">    if random.random() &lt; 0.5:</span><br><span class="line"></span><br><span class="line">        # 50% of the time, we return Node 2</span><br><span class="line">        return &quot;node_2&quot;</span><br><span class="line">    </span><br><span class="line">    # 50% of the time, we return Node 3</span><br><span class="line">    return &quot;node_3&quot;</span><br><span class="line"></span><br><span class="line"># Build graph</span><br><span class="line">builder = StateGraph(TypedDictState)</span><br><span class="line">builder.add_node(&quot;node_1&quot;, node_1)</span><br><span class="line">builder.add_node(&quot;node_2&quot;, node_2)</span><br><span class="line">builder.add_node(&quot;node_3&quot;, node_3)</span><br><span class="line"></span><br><span class="line"># Logic</span><br><span class="line">builder.add_edge(START, &quot;node_1&quot;)</span><br><span class="line">builder.add_conditional_edges(&quot;node_1&quot;, decide_mood)</span><br><span class="line">builder.add_edge(&quot;node_2&quot;, END)</span><br><span class="line">builder.add_edge(&quot;node_3&quot;, END)</span><br><span class="line"></span><br><span class="line"># Add</span><br><span class="line">graph = builder.compile()</span><br><span class="line"></span><br><span class="line"># View</span><br><span class="line">display(Image(graph.get_graph().draw_mermaid_png()))</span><br></pre></td></tr></table></figure>
<p><img src="/2025/07/18/%E5%AD%A6%E4%B9%A0/ai%E6%A1%86%E6%9E%B6/langgraph-agent/image-20250719154439415.png" alt="image-20250719154439415"></p>
<p>因为我们的状态是一个字典，我们只需用一个字典调用图，以设置状态中 <code>name</code> 键的初始值。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">graph.invoke(&#123;&quot;name&quot;:&quot;Lance&quot;&#125;)</span><br></pre></td></tr></table></figure>
<h5 id="Dataclass数据类"><a href="#Dataclass数据类" class="headerlink" title="Dataclass数据类"></a><strong>Dataclass数据类</strong></h5><p>python的dataclasses库提供了一种简洁的语法，用于创建主要用于存储数据的类。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from dataclasses import dataclass</span><br><span class="line"></span><br><span class="line">@dataclass</span><br><span class="line">class DataclassState:</span><br><span class="line">    name: str</span><br><span class="line">    mood: Literal[&quot;happy&quot;,&quot;sad&quot;]</span><br></pre></td></tr></table></figure>
<p><strong>示例</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">def node_1(state):</span><br><span class="line">    print(&quot;---Node 1---&quot;)</span><br><span class="line">    return &#123;&quot;name&quot;: state.name + &quot; is ... &quot;&#125;</span><br><span class="line"></span><br><span class="line"># Build graph</span><br><span class="line">builder = StateGraph(DataclassState)</span><br><span class="line">builder.add_node(&quot;node_1&quot;, node_1)</span><br><span class="line">builder.add_node(&quot;node_2&quot;, node_2)</span><br><span class="line">builder.add_node(&quot;node_3&quot;, node_3)</span><br><span class="line"></span><br><span class="line"># Logic</span><br><span class="line">builder.add_edge(START, &quot;node_1&quot;)</span><br><span class="line">builder.add_conditional_edges(&quot;node_1&quot;, decide_mood)</span><br><span class="line">builder.add_edge(&quot;node_2&quot;, END)</span><br><span class="line">builder.add_edge(&quot;node_3&quot;, END)</span><br><span class="line"></span><br><span class="line"># Add</span><br><span class="line">graph = builder.compile()</span><br><span class="line"></span><br><span class="line"># View</span><br><span class="line">display(Image(graph.get_graph().draw_mermaid_png()))</span><br></pre></td></tr></table></figure>
<p>要访问 <code>dataclass</code> 的键，我们只需修改在 <code>node_1</code> 中使用的下标即可：</p>
<p>我们使用 <code>state.name</code> 来表示 <code>dataclass</code> 状态，而不是使用 <code>state[&quot;name&quot;]</code> 来表示上面的 <code>TypedDict</code>。</p>
<p>你会注意到一个有点奇怪的地方：在每个节点中，我们仍然返回一个字典来执行状态更新。</p>
<p><strong>Dataclass 只是“描述”状态的形状，而真正在 LangGraph 的节点之间流动的依旧是「字典」</strong>，这是框架设计层面的约定</p>
<p>在这种情况下，<code>dataclass</code> 拥有键 <code>name</code>，因此我们可以通过从节点传递一个字典来更新它，就像在状态为 <code>TypedDict</code> 时所做的那样。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">graph.invoke(DataclassState(name=&quot;Lance&quot;,mood=&quot;sad&quot;))</span><br></pre></td></tr></table></figure>
<p>我们通过 <code>dataclass</code> 来设置状态中每个键/通道的初始值！</p>
<h4 id="State-Reducers状态更新函数"><a href="#State-Reducers状态更新函数" class="headerlink" title="State Reducers状态更新函数"></a><strong>State Reducers</strong>状态更新函数</h4><p><a target="_blank" rel="noopener" href="https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers">Reducers</a> 为我们指定了如何执行更新。它接收 <strong>旧状态</strong> 与 <strong>一次变更指令（action / 增量字段）</strong>，<strong>返回全新的状态对象</strong>，整个过程中<strong>不能修改原有数据</strong>。</p>
<p>我们可以使用 <code>Annotated</code> 类型来指定一个 reducer 函数。在这种情况下，让我们将每个节点返回的值附加到结果中，而不是覆盖它们。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from operator import add</span><br><span class="line">from typing import Annotated</span><br><span class="line"></span><br><span class="line">class State(TypedDict):</span><br><span class="line">    foo: Annotated[list[int], add]</span><br></pre></td></tr></table></figure>
<p>我们只需要一个可以执行此操作的缩减器：<code>operator.add</code> 是 Python 内置 operator 模块中的一个函数。当 <code>operator.add</code> 应用于列表时，它执行列表连接。</p>
<h5 id="Custom-Reducers-自定义-Reducers"><a href="#Custom-Reducers-自定义-Reducers" class="headerlink" title="Custom Reducers 自定义 Reducers"></a><strong>Custom Reducers 自定义 Reducers</strong></h5><p>我们同样可以自定义reducers函数，解决一些特殊情况，比如，如下可以解决传入参数为none的情况</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">def reduce_list(left: list | None, right: list | None) -&gt; list:</span><br><span class="line">    &quot;&quot;&quot;安全地合并两个列表，处理其中一个或两个输入可能为 None 的情况。</span><br><span class="line"></span><br><span class="line">    参数：</span><br><span class="line">        left (list | None): 要合并的第一个列表，或 None。</span><br><span class="line">        right (list | None): 要合并的第二个列表，或 None。</span><br><span class="line"></span><br><span class="line">    返回：</span><br><span class="line">        list: 一个包含两个输入列表所有元素的新列表。</span><br><span class="line">               如果输入为 None，则将其视为空列表。</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    if not left:</span><br><span class="line">        left = []</span><br><span class="line">    if not right:</span><br><span class="line">        right = []</span><br><span class="line">    return left + right</span><br><span class="line"></span><br><span class="line">class DefaultState(TypedDict):</span><br><span class="line">    foo: Annotated[list[int], add]</span><br><span class="line"></span><br><span class="line">class CustomReducerState(TypedDict):</span><br><span class="line">    foo: Annotated[list[int], reduce_list]</span><br></pre></td></tr></table></figure>
<h5 id="MessagesState"><a href="#MessagesState" class="headerlink" title="MessagesState"></a>MessagesState</h5><p>我可以使用内置的 reducer <code>add_messages</code> 来处理状态中的消息</p>
<p>而<em><code>MessagesState</code></em> <em>内置了一个</em> <em><code>messages</code></em> 键 它还为该键内置了一个 <code>add_messages</code> 合并器，这两个是等价的。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">from typing import Annotated</span><br><span class="line">from langgraph.graph import MessagesState</span><br><span class="line">from langchain_core.messages import AnyMessage</span><br><span class="line">from langgraph.graph.message import add_messages</span><br><span class="line"></span><br><span class="line"># 定义一个自定义的 TypedDict，其中包含一个带有 add_messages reducer 的消息列表。</span><br><span class="line">class CustomMessagesState(TypedDict):</span><br><span class="line">    messages: Annotated[list[AnyMessage], add_messages]</span><br><span class="line">    added_key_1: str</span><br><span class="line">    added_key_2: str</span><br><span class="line">    # etc</span><br><span class="line"></span><br><span class="line"># 使用 MessagesState ，它包含带有 add_messages reducer 的 messages 键。</span><br><span class="line">class ExtendedMessagesState(MessagesState):</span><br><span class="line">    # 添加除 messages 之外所需的任何键， messages 是预构建的。</span><br><span class="line">    added_key_1: str</span><br><span class="line">    added_key_2: str</span><br><span class="line">    # etc</span><br></pre></td></tr></table></figure>
<p>在使用 <code>add_messages</code> reducer 时，让我们展示一些有用的技巧。</p>
<p><strong>重写（Re-writing）</strong></p>
<p>如果我们传递的消息与 <code>messages</code> 列表中已有的消息具有相同的 ID，则该消息将被覆盖！</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># Initial state</span><br><span class="line">initial_messages = [AIMessage(content=&quot;Hello! How can I assist you?&quot;, name=&quot;Model&quot;, id=&quot;1&quot;),</span><br><span class="line">                    HumanMessage(content=&quot;I&#x27;m looking for information on marine biology.&quot;, name=&quot;Lance&quot;, id=&quot;2&quot;)</span><br><span class="line">                   ]</span><br><span class="line"></span><br><span class="line"># New message to add</span><br><span class="line">new_message = HumanMessage(content=&quot;I&#x27;m looking for information on whales, specifically&quot;, name=&quot;Lance&quot;, id=&quot;2&quot;)</span><br><span class="line"></span><br><span class="line"># Test</span><br><span class="line">add_messages(initial_messages , new_message)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[AIMessage(content=&#x27;Hello! How can I assist you?&#x27;, name=&#x27;Model&#x27;, id=&#x27;1&#x27;),</span><br><span class="line"> HumanMessage(content=&quot;I&#x27;m looking for information on whales, specifically&quot;, name=&#x27;Lance&#x27;, id=&#x27;2&#x27;)]</span><br></pre></td></tr></table></figure>
<p><strong>删除（Removal）</strong></p>
<p><code>add_messages</code> 也 <a target="_blank" rel="noopener" href="https://langchain-ai.github.io/langgraph/how-tos/memory/delete-messages/">同样支持删除</a>。为此，我们简单地使用 <a target="_blank" rel="noopener" href="https://api.python.langchain.com/en/latest/messages/langchain_core.messages.modifier.RemoveMessage.html">RemoveMessage</a> 来自 <code>langchain_core</code>。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from langchain_core.messages import RemoveMessage</span><br><span class="line"></span><br><span class="line"># Message list</span><br><span class="line">messages = [AIMessage(&quot;Hi.&quot;, name=&quot;Bot&quot;, id=&quot;1&quot;)]</span><br><span class="line">messages.append(HumanMessage(&quot;Hi.&quot;, name=&quot;Lance&quot;, id=&quot;2&quot;))</span><br><span class="line">messages.append(AIMessage(&quot;So you said you were researching ocean mammals?&quot;, name=&quot;Bot&quot;, id=&quot;3&quot;))</span><br><span class="line">messages.append(HumanMessage(&quot;Yes, I know about whales. But what others should I learn about?&quot;, name=&quot;Lance&quot;, id=&quot;4&quot;))</span><br><span class="line"></span><br><span class="line"># Isolate messages to delete</span><br><span class="line">delete_messages = [RemoveMessage(id=m.id) for m in messages[:-2]]</span><br><span class="line">print(delete_messages)</span><br></pre></td></tr></table></figure>
<h4 id="Multiple-Schemas-多种状态"><a href="#Multiple-Schemas-多种状态" class="headerlink" title="Multiple Schemas 多种状态"></a><strong>Multiple Schemas</strong> 多种状态</h4><h5 id="Private-State-私有状态"><a href="#Private-State-私有状态" class="headerlink" title="Private State 私有状态"></a><strong>Private State</strong> 私有状态</h5><p>首先，让我们讨论在节点之间传递 <a target="_blank" rel="noopener" href="https://langchain-ai.github.io/langgraph/how-tos/pass_private_state/">private state</a> 的情况。这对于图的中间计算逻辑中需要的任何内容都很有用，但与图的整体输入或输出无关。</p>
<p>示例</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">from typing_extensions import TypedDict</span><br><span class="line">from IPython.display import Image, display</span><br><span class="line">from langgraph.graph import StateGraph, START, END</span><br><span class="line"></span><br><span class="line">class OverallState(TypedDict):</span><br><span class="line">    foo: int</span><br><span class="line"></span><br><span class="line">class PrivateState(TypedDict):</span><br><span class="line">    baz: int</span><br><span class="line"></span><br><span class="line">def node_1(state: OverallState) -&gt; PrivateState:</span><br><span class="line">    print(&quot;---Node 1---&quot;)</span><br><span class="line">    return &#123;&quot;baz&quot;: state[&#x27;foo&#x27;] + 1&#125;</span><br><span class="line"></span><br><span class="line">def node_2(state: PrivateState) -&gt; OverallState:</span><br><span class="line">    print(&quot;---Node 2---&quot;)</span><br><span class="line">    return &#123;&quot;foo&quot;: state[&#x27;baz&#x27;] + 1&#125;</span><br><span class="line"></span><br><span class="line"># Build graph</span><br><span class="line">builder = StateGraph(OverallState)</span><br><span class="line">builder.add_node(&quot;node_1&quot;, node_1)</span><br><span class="line">builder.add_node(&quot;node_2&quot;, node_2)</span><br><span class="line"></span><br><span class="line"># Logic</span><br><span class="line">builder.add_edge(START, &quot;node_1&quot;)</span><br><span class="line">builder.add_edge(&quot;node_1&quot;, &quot;node_2&quot;)</span><br><span class="line">builder.add_edge(&quot;node_2&quot;, END)</span><br><span class="line"></span><br><span class="line"># Add</span><br><span class="line">graph = builder.compile()</span><br><span class="line"></span><br><span class="line"># View</span><br><span class="line">display(Image(graph.get_graph().draw_mermaid_png()))</span><br></pre></td></tr></table></figure>
<p><img src="/2025/07/18/%E5%AD%A6%E4%B9%A0/ai%E6%A1%86%E6%9E%B6/langgraph-agent/image-20250719171509917.png" alt="image-20250719171509917"></p>
<p>我们将定义一个 <code>OverallState</code> 和一个 <code>PrivateState</code>。<code>node_2</code> 使用 <code>PrivateState</code> 作为输入，但输出写入到 <code>OverallState</code>。</p>
<p><code>baz</code> 仅包含在 <code>PrivateState</code> 中。因此，我们可以看到 <code>baz</code> 被排除在图形输出之外，因为它不在 <code>OverallState</code> 中。</p>
<h5 id="Input-Output-Schema-输入-输出模式"><a href="#Input-Output-Schema-输入-输出模式" class="headerlink" title="Input / Output Schema 输入/输出模式"></a><strong>Input / Output Schema </strong>输入/输出模式</h5><p>在 LangGraph 中，<strong>Input / Output Schema</strong> 就是“<strong>图的对外接口协议</strong>”：<strong>调用者只能按 Input Schema 传参；图运行完后，只吐出 Output Schema 规定的字段。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">from langgraph.graph import StateGraph, START, END</span><br><span class="line">from typing_extensions import TypedDict</span><br><span class="line"></span><br><span class="line"># 定义输入的模式</span><br><span class="line">class InputState(TypedDict):</span><br><span class="line">    question: str</span><br><span class="line"></span><br><span class="line"># 定义输出的模式</span><br><span class="line">class OutputState(TypedDict):</span><br><span class="line">    answer: str</span><br><span class="line"></span><br><span class="line"># 定义整体模式，结合输入和输出</span><br><span class="line">class OverallState(InputState, OutputState):</span><br><span class="line">    pass</span><br><span class="line"></span><br><span class="line"># 定义处理输入并生成答案的节点</span><br><span class="line">def answer_node(state: InputState):</span><br><span class="line">    # 示例答案和额外键</span><br><span class="line">    return &#123;&quot;answer&quot;: &quot;bye&quot;, &quot;question&quot;: state[&quot;question&quot;]&#125;</span><br><span class="line"></span><br><span class="line"># 构建图，并指定输入和输出模式</span><br><span class="line">builder = StateGraph(OverallState, input_schema=InputState, output_schema=OutputState)</span><br><span class="line">builder.add_node(answer_node)  # 添加答案节点</span><br><span class="line">builder.add_edge(START, &quot;answer_node&quot;)  # 定义起始边</span><br><span class="line">builder.add_edge(&quot;answer_node&quot;, END)  # 定义结束边</span><br><span class="line">graph = builder.compile()  # 编译图</span><br><span class="line"></span><br><span class="line"># 使用输入调用图并打印结果</span><br><span class="line">print(graph.invoke(&#123;&quot;question&quot;: &quot;hi&quot;&#125;))</span><br></pre></td></tr></table></figure>
<p>输出</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;answer&#x27;: &#x27;bye Lance&#x27;&#125;</span><br></pre></td></tr></table></figure>
<p>在这里，<code>input</code> / <code>output</code> 模式对图的输入和输出上允许的键进行 <strong>过滤</strong>。可以看到 <code>output</code> 模式将输出限制为仅包含 <code>answer</code> 键。</p>
<h4 id="Filtering-and-trimming-messages筛选和精简消息"><a href="#Filtering-and-trimming-messages筛选和精简消息" class="headerlink" title="Filtering and trimming messages筛选和精简消息"></a><strong>Filtering and trimming messages</strong>筛选和精简消息</h4><p>如果我们在处理长时间对话时不够小心，会导致高令牌使用量和延迟，因为我们传递给模型的是一系列不断增加的消息。所以要进行筛选和精简消息。</p>
<h5 id="简化器（Reducer）"><a href="#简化器（Reducer）" class="headerlink" title="简化器（Reducer）"></a><strong>简化器（Reducer）</strong></h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">from langchain_core.messages import RemoveMessage</span><br><span class="line"></span><br><span class="line"># Nodes</span><br><span class="line">def filter_messages(state: MessagesState):</span><br><span class="line">    # 删除除最近两条消息外的所有消息</span><br><span class="line">    delete_messages = [RemoveMessage(id=m.id) for m in state[&quot;messages&quot;][:-2]]</span><br><span class="line">    return &#123;&quot;messages&quot;: delete_messages&#125;</span><br><span class="line"></span><br><span class="line">def chat_model_node(state: MessagesState):    </span><br><span class="line">    return &#123;&quot;messages&quot;: [llm.invoke(state[&quot;messages&quot;])]&#125;</span><br><span class="line"></span><br><span class="line"># Build graph</span><br><span class="line">builder = StateGraph(MessagesState)</span><br><span class="line">builder.add_node(&quot;filter&quot;, filter_messages)</span><br><span class="line">builder.add_node(&quot;chat_model&quot;, chat_model_node)</span><br><span class="line">builder.add_edge(START, &quot;filter&quot;)</span><br><span class="line">builder.add_edge(&quot;filter&quot;, &quot;chat_model&quot;)</span><br><span class="line">builder.add_edge(&quot;chat_model&quot;, END)</span><br><span class="line">graph = builder.compile()</span><br><span class="line"></span><br><span class="line"># View</span><br><span class="line">display(Image(graph.get_graph().draw_mermaid_png()))</span><br></pre></td></tr></table></figure>
<p><img src="/2025/07/18/%E5%AD%A6%E4%B9%A0/ai%E6%A1%86%E6%9E%B6/langgraph-agent/image-20250719203204234.png" alt="image-20250719203204234"></p>
<h5 id="筛选消息（Filtering-messages）"><a href="#筛选消息（Filtering-messages）" class="headerlink" title="筛选消息（Filtering messages）"></a><strong>筛选消息（Filtering messages）</strong></h5><p>如果你不需要或不希望修改图状态，可以直接过滤传递给聊天模型的消息。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># Node</span><br><span class="line">def chat_model_node(state: MessagesState):</span><br><span class="line">    return &#123;&quot;messages&quot;: [llm.invoke(state[&quot;messages&quot;][-1:])]&#125;</span><br><span class="line"></span><br><span class="line"># Build graph</span><br><span class="line">builder = StateGraph(MessagesState)</span><br><span class="line">builder.add_node(&quot;chat_model&quot;, chat_model_node)</span><br><span class="line">builder.add_edge(START, &quot;chat_model&quot;)</span><br><span class="line">builder.add_edge(&quot;chat_model&quot;, END)</span><br><span class="line">graph = builder.compile()</span><br><span class="line"></span><br><span class="line"># View</span><br><span class="line">display(Image(graph.get_graph().draw_mermaid_png()))</span><br></pre></td></tr></table></figure>
<p>例如，只需传递一个过滤后的列表：<code>llm.invoke(messages[-1:])</code> 给模型。</p>
<p>状态包含了所有消息。但这里模型调用仅使用最后一条消息</p>
<h5 id="裁剪消息（Trim-messages）"><a href="#裁剪消息（Trim-messages）" class="headerlink" title="裁剪消息（Trim messages）"></a><strong>裁剪消息（Trim messages）</strong></h5><p>另一种方法是根据设定一定数量的tokens进行 <a target="_blank" rel="noopener" href="https://python.langchain.com/v0.2/docs/how_to/trim_messages/#getting-the-last-max_tokens-tokens">trim messages</a>。在把对话历史发给大模型之前，按 <strong>token 预算</strong> 把超长消息列表“剪”到合适长度。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">from langchain_core.messages import trim_messages</span><br><span class="line"></span><br><span class="line"># Node</span><br><span class="line">def chat_model_node(state: MessagesState):</span><br><span class="line">    # 使用 trim_messages 函数修剪消息列表</span><br><span class="line">    # max_tokens: 限制消息的最大令牌数</span><br><span class="line">    # strategy: 修剪策略，这里是“last”，表示保留最新的消息</span><br><span class="line">    # token_counter: 用于计算令牌数的模型实例</span><br><span class="line">    # allow_partial: 是否允许部分修剪</span><br><span class="line">    messages = trim_messages(</span><br><span class="line">            state[&quot;messages&quot;],</span><br><span class="line">            max_tokens=100,</span><br><span class="line">            strategy=&quot;last&quot;,</span><br><span class="line">            token_counter= ChatOpenAI(</span><br><span class="line">                model=&quot;qwen-plus-2025-04-28&quot;,</span><br><span class="line">                api_key=&quot;sk-&quot;,</span><br><span class="line">                base_url=&quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;),</span><br><span class="line">            allow_partial=False,</span><br><span class="line">        )</span><br><span class="line">    # 调用语言模型（llm）处理修剪后的消息，并返回结果</span><br><span class="line">    return &#123;&quot;messages&quot;: [llm.invoke(messages)]&#125;</span><br><span class="line"></span><br><span class="line"># Build graph</span><br><span class="line">builder = StateGraph(MessagesState)</span><br><span class="line">builder.add_node(&quot;chat_model&quot;, chat_model_node)</span><br><span class="line">builder.add_edge(START, &quot;chat_model&quot;)</span><br><span class="line">builder.add_edge(&quot;chat_model&quot;, END)</span><br><span class="line">graph = builder.compile()</span><br><span class="line"></span><br><span class="line"># View</span><br><span class="line">display(Image(graph.get_graph().draw_mermaid_png()))</span><br></pre></td></tr></table></figure>
<h4 id="Chatbot-with-message-summarization带有消息总结功能的聊天机器人"><a href="#Chatbot-with-message-summarization带有消息总结功能的聊天机器人" class="headerlink" title="Chatbot with message summarization带有消息总结功能的聊天机器人"></a><strong>Chatbot with message summarization</strong>带有消息总结功能的聊天机器人</h4><p>与其仅仅修剪或过滤消息，我们将展示如何使用大型语言模型（LLMs）来生成对话的实时摘要。</p>
<p>这使我们能够保留整个对话的压缩表示，而不仅仅是通过修剪或过滤将其移除。</p>
<p>我们将为该聊天机器人配备记忆功能，支持长时间对话，同时不会产生高昂的 token 成本或延迟。</p>
<h5 id="定义总结状态"><a href="#定义总结状态" class="headerlink" title="定义总结状态"></a>定义总结状态</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from langgraph.graph import MessagesState</span><br><span class="line">class State(MessagesState):</span><br><span class="line">    summary: str</span><br></pre></td></tr></table></figure>
<p>除了内置的 <code>messages</code> 键之外，我们现在还将包含一个自定义键（<code>summary</code>）。</p>
<h5 id="定义LLM节点"><a href="#定义LLM节点" class="headerlink" title="定义LLM节点"></a>定义LLM节点</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">from langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage </span><br><span class="line"> </span><br><span class="line"> # 定义调用模型的逻辑</span><br><span class="line">def call_model(state: State): </span><br><span class="line">     </span><br><span class="line">     # 获取摘要（如果存在） </span><br><span class="line">     summary = state.get(&quot;summary&quot;, &quot;&quot;) </span><br><span class="line"> </span><br><span class="line">     # 如果有摘要，则添加它</span><br><span class="line">     if summary: </span><br><span class="line">         </span><br><span class="line">         # 将摘要添加到系统消息中</span><br><span class="line">         system_message = f&quot;先前对话的摘要：&#123;summary&#125;&quot; </span><br><span class="line"> </span><br><span class="line">         # 将摘要附加到任何较新的消息中</span><br><span class="line">         messages = [SystemMessage(content=system_message)] + state[&quot;messages&quot;] </span><br><span class="line">     </span><br><span class="line">     else: </span><br><span class="line">         messages = state[&quot;messages&quot;] </span><br><span class="line">     </span><br><span class="line">     response = model.invoke(messages) </span><br><span class="line">     return &#123;&quot;messages&quot;: response&#125; </span><br></pre></td></tr></table></figure>
<p>我们将定义一个节点来<strong>调用我们的LLM</strong>，如果存在摘要，则将其纳入提示中。</p>
<blockquote>
<p>当 call_model 函数返回 {“messages”: response} 时，它是在告诉 langgraph ：“请用 response （即模型的新输出）来更新 State 对象中 messages 键对应的值。” langgraph 会将这个新消息追加到 messages 列表中，从而保持了对话历史的连续性</p>
</blockquote>
<h5 id="定义摘要节点"><a href="#定义摘要节点" class="headerlink" title="定义摘要节点"></a>定义摘要节点</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">def summarize_conversation(state: State): </span><br><span class="line">     </span><br><span class="line">     # 首先，我们获取任何现有的摘要</span><br><span class="line">     summary = state.get(&quot;summary&quot;, &quot;&quot;) </span><br><span class="line"> </span><br><span class="line">     # 创建我们的摘要提示</span><br><span class="line">     if summary: </span><br><span class="line">         </span><br><span class="line">         # 摘要已存在</span><br><span class="line">         summary_message = ( </span><br><span class="line">             f&quot;这是迄今为止对话的摘要：&#123;summary&#125;\n\n&quot; </span><br><span class="line">             &quot;请结合以上新消息扩展摘要：&quot; </span><br><span class="line">         ) </span><br><span class="line">         </span><br><span class="line">     else: </span><br><span class="line">         summary_message = &quot;创建以上对话的摘要：&quot; </span><br><span class="line"> </span><br><span class="line">     # 将提示添加到我们的历史记录中</span><br><span class="line">     messages = state[&quot;messages&quot;] + [HumanMessage(content=summary_message)] </span><br><span class="line">     response = model.invoke(messages) </span><br><span class="line">     </span><br><span class="line">     # 删除除最近2条消息外的所有消息</span><br><span class="line">     delete_messages = [RemoveMessage(id=m.id) for m in state[&quot;messages&quot;][:-2]] </span><br><span class="line">     return &#123;&quot;summary&quot;: response.content, &quot;messages&quot;: delete_messages&#125; </span><br></pre></td></tr></table></figure>
<p>我们将定义一个节点来<strong>生成摘要</strong>。请注意，这里我们将使用 <code>RemoveMessage</code> 在生成摘要后过滤我们的状态。</p>
<h5 id="定义路由函数"><a href="#定义路由函数" class="headerlink" title="定义路由函数"></a>定义路由函数</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">from langgraph.graph import END </span><br><span class="line"> # 决定是结束对话还是总结对话</span><br><span class="line">def should_continue(state: State): </span><br><span class="line">     </span><br><span class="line">     &quot;&quot;&quot;返回要执行的下一个节点。&quot;&quot;&quot; </span><br><span class="line">     </span><br><span class="line">     messages = state[&quot;messages&quot;] </span><br><span class="line">     </span><br><span class="line">     # 如果消息超过六条，那么我们总结对话</span><br><span class="line">     if len(messages) &gt; 6: </span><br><span class="line">         return &quot;summarize_conversation&quot; </span><br><span class="line">     </span><br><span class="line">     # 否则我们就可以结束了</span><br><span class="line">     return END </span><br></pre></td></tr></table></figure>
<p>我们将添加一个条件边，以根据对话长度确定是否生成摘要。</p>
<blockquote>
<p>在 langgraph 中， Command 是一个特殊的类型，用于指导图形（graph）决定接下来应该执行哪个节点。 您可以把它看作是给图形下达的一个“命令”。</p>
</blockquote>
<h5 id="添加内存并编译图"><a href="#添加内存并编译图" class="headerlink" title="添加内存并编译图"></a>添加内存并编译图</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">from IPython.display import Image, display</span><br><span class="line">from langgraph.checkpoint.memory import MemorySaver</span><br><span class="line">from langgraph.graph import StateGraph, START</span><br><span class="line"></span><br><span class="line"># Define a new graph</span><br><span class="line">workflow = StateGraph(State)</span><br><span class="line">workflow.add_node(&quot;conversation&quot;, call_model)</span><br><span class="line">workflow.add_node(&quot;summarize_conversation&quot;,summarize_conversation)</span><br><span class="line"></span><br><span class="line"># Set the entrypoint as conversation</span><br><span class="line">workflow.add_edge(START, &quot;conversation&quot;)</span><br><span class="line">workflow.add_conditional_edges(&quot;conversation&quot;, should_continue)</span><br><span class="line">workflow.add_edge(&quot;summarize_conversation&quot;, END)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Compile</span><br><span class="line">memory = MemorySaver()</span><br><span class="line">graph = workflow.compile(checkpointer=memory)</span><br><span class="line">display(Image(graph.get_graph().draw_mermaid_png()))</span><br></pre></td></tr></table></figure>
<h5 id="使用线程调用"><a href="#使用线程调用" class="headerlink" title="使用线程调用"></a>使用线程调用</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">config = &#123;&quot;configurable&quot;: &#123;&quot;thread_id&quot;: &quot;2&quot;&#125;&#125;</span><br><span class="line"></span><br><span class="line">input_message = HumanMessage(content=&quot;我喜欢玩lol，你知道这个游戏吗&quot;)</span><br><span class="line">output = graph.invoke(&#123;&quot;messages&quot;: [input_message]&#125;, config) </span><br><span class="line">for m in output[&#x27;messages&#x27;][-1:]:</span><br><span class="line">    m.pretty_print()</span><br></pre></td></tr></table></figure>
<p>当对话大于6，可生成概要</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">graph.get_state(config).values.get(&quot;summary&quot;,&quot;&quot;)</span><br></pre></td></tr></table></figure>
<h4 id="Chatbot-with-message-summarization-amp-external-DB-memory具有消息总结和外部数据库记忆的聊天机器人"><a href="#Chatbot-with-message-summarization-amp-external-DB-memory具有消息总结和外部数据库记忆的聊天机器人" class="headerlink" title="Chatbot with message summarization &amp; external DB memory具有消息总结和外部数据库记忆的聊天机器人"></a><strong>Chatbot with message summarization &amp; external DB memory</strong>具有消息总结和外部数据库记忆的聊天机器人</h4><h5 id="使用数据库"><a href="#使用数据库" class="headerlink" title="使用数据库"></a>使用数据库</h5><p><code>SqliteSaver</code> 是 LangGraph 提供的一个 <strong>轻量级状态持久化工具</strong>，它将图的运行状态（即 checkpoint）保存到本地的 SQLite 数据库中，使得你可以在程序中断或重启后<strong>恢复执行上下文</strong>，特别适合本地开发、实验性项目或中小规模应用。</p>
<p>如果我们提供 “:memory:” ，它将创建一个内存中的 SQLite 数据库。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import sqlite3</span><br><span class="line"># In memory</span><br><span class="line">conn = sqlite3.connect(&quot;:memory:&quot;, check_same_thread = False)</span><br></pre></td></tr></table></figure>
<p>如果我们提供一个 db 路径，那么它将为我们创建一个数据库！</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#在本地创建一个目录 state_db，并尝试从 GitHub 下载一个名为 example.db 的 SQLite 数据库文件</span><br><span class="line">!mkdir -p state_db &amp;&amp; [ ! -f state_db/example.db ] &amp;&amp; wget -P state_db https://github.com/langchain-ai/langchain-academy/raw/main/module-2/state_db/example.db</span><br><span class="line"></span><br><span class="line">db_path = &quot;state_db/example.db&quot;</span><br><span class="line">conn = sqlite3.connect(db_path, check_same_thread=False)</span><br></pre></td></tr></table></figure>
<p>定义checkpoint</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from langgraph.checkpoint.sqlite import SqliteSaver</span><br><span class="line">memory = SqliteSaver(conn)</span><br></pre></td></tr></table></figure>
<p>像上一个形式编译图</p>
<p>让我们确认一下我们的状态是否已保存到本地。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">config = &#123;&quot;configurable&quot;: &#123;&quot;thread_id&quot;: &quot;1&quot;&#125;&#125;</span><br><span class="line">graph_state = graph.get_state(config)</span><br><span class="line">graph_state</span><br></pre></td></tr></table></figure>
<p>使用像 Sqlite 这样的数据库意味着状态会被持久化！</p>
<h3 id="module-3"><a href="#module-3" class="headerlink" title="module-3"></a>module-3</h3><h4 id="Streaming-流式传输"><a href="#Streaming-流式传输" class="headerlink" title="Streaming 流式传输"></a><strong>Streaming</strong> 流式传输</h4><p>现在，让我们来谈谈 <a target="_blank" rel="noopener" href="https://langchain-ai.github.io/langgraph/concepts/low_level/#streaming">流式传输我们的图状态</a> 的方法。<code>.stream</code> 和 <code>.astream</code> 是用于流式返回结果的同步和异步方法。</p>
<p><code>values</code>：这将在每个节点被调用后流式传输图的完整状态。 <code>updates</code>：这将在每个节点被调用后流式传输图的状态更新。</p>
<p><img src="/2025/07/18/%E5%AD%A6%E4%B9%A0/ai%E6%A1%86%E6%9E%B6/langgraph-agent/66dbaf892d24625a201744e5_streaming1.png" alt="values_vs_updates.png"></p>
<h5 id="stream-mode-”updates”"><a href="#stream-mode-”updates”" class="headerlink" title="stream_mode=”updates”"></a>stream_mode=”updates”</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># Create a thread</span><br><span class="line">config = &#123;&quot;configurable&quot;: &#123;&quot;thread_id&quot;: &quot;1&quot;&#125;&#125;</span><br><span class="line"></span><br><span class="line"># Start conversation</span><br><span class="line">for chunk in graph.stream(&#123;&quot;messages&quot;: [HumanMessage(content=&quot;你好我是zxj&quot;)]&#125;, config, stream_mode=&quot;updates&quot;):</span><br><span class="line">    print(chunk)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;conversation&#x27;: &#123;&#x27;messages&#x27;: AIMessage(content=&#x27;你好，zxj！很高兴认识你～有什么我可以帮你的吗？😊&#x27;, additional_kwargs=&#123;&#x27;refusal&#x27;: None&#125;, response_metadata=&#123;&#x27;token_usage&#x27;: &#123;&#x27;completion_tokens&#x27;: 16, &#x27;prompt_tokens&#x27;: 576, &#x27;total_tokens&#x27;: 592, &#x27;completion_tokens_details&#x27;: None, &#x27;prompt_tokens_details&#x27;: None&#125;, &#x27;model_name&#x27;: &#x27;qwen-plus-2025-04-28&#x27;, &#x27;system_fingerprint&#x27;: None, &#x27;id&#x27;: &#x27;chatcmpl-891471ae-2fe8-9b3d-b5f7-f4fcd55a4e16&#x27;, &#x27;service_tier&#x27;: None, &#x27;finish_reason&#x27;: &#x27;stop&#x27;, &#x27;logprobs&#x27;: None&#125;, id=&#x27;run--f36409f3-af43-4e9b-8a46-39646ad7c106-0&#x27;, usage_metadata=&#123;&#x27;input_tokens&#x27;: 576, &#x27;output_tokens&#x27;: 16, &#x27;total_tokens&#x27;: 592, &#x27;input_token_details&#x27;: &#123;&#125;, &#x27;output_token_details&#x27;: &#123;&#125;&#125;)&#125;&#125;</span><br></pre></td></tr></table></figure>
<p>让我们来看一下 <code>stream_mode=&quot;updates&quot;</code>。</p>
<p>因为我们使用 <code>updates</code> 进行流式传输，所以只有在图中的节点运行后，我们才能看到状态的更新。每个 <code>chunk</code> 是一个字典，以 <code>node_name</code> 为键，更新后的状态为值。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># Start conversation</span><br><span class="line">for chunk in graph.stream(&#123;&quot;messages&quot;: [HumanMessage(content=&quot;你好我是zxj&quot;)]&#125;, config, stream_mode=&quot;updates&quot;):</span><br><span class="line">    chunk[&#x27;conversation&#x27;][&quot;messages&quot;].pretty_print()</span><br></pre></td></tr></table></figure>
<p>现在我们直接打印状态更新。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">==================================[1m Ai Message [0m==================================</span><br><span class="line"></span><br><span class="line">你好呀，zxj！再次见到你真高兴～😊 有什么我可以帮忙的吗？</span><br></pre></td></tr></table></figure>
<h5 id="stream-mode-”values”"><a href="#stream-mode-”values”" class="headerlink" title="stream_mode=”values”"></a>stream_mode=”values”</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># Start conversation, again</span><br><span class="line">config = &#123;&quot;configurable&quot;: &#123;&quot;thread_id&quot;: &quot;2&quot;&#125;&#125;</span><br><span class="line"></span><br><span class="line"># Start conversation</span><br><span class="line">input_message = HumanMessage(content=&quot;你好我是zxj&quot;)</span><br><span class="line">for event in graph.stream(&#123;&quot;messages&quot;: [input_message]&#125;, config, stream_mode=&quot;values&quot;):</span><br><span class="line">    for m in event[&#x27;messages&#x27;]:</span><br><span class="line">        m.pretty_print()</span><br><span class="line">    print(&quot;---&quot;*25)</span><br></pre></td></tr></table></figure>
<p>现在，我们可以看到 <code>stream_mode=&quot;values&quot;</code>.这是在 <code>conversation</code> 节点被调用后，图的整个状态。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">================================[1m Human Message [0m=================================</span><br><span class="line"></span><br><span class="line">你好我是zxj</span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line">================================[1m Human Message [0m=================================</span><br><span class="line"></span><br><span class="line">你好我是zxj</span><br><span class="line">==================================[1m Ai Message [0m==================================</span><br><span class="line"></span><br><span class="line">你好，zxj！有什么我可以帮你的吗？😊</span><br><span class="line">---------------------------------------------------------------------------</span><br></pre></td></tr></table></figure>
<h5 id="Streaming-tokens-流式传输令牌"><a href="#Streaming-tokens-流式传输令牌" class="headerlink" title="Streaming tokens 流式传输令牌"></a><strong>Streaming tokens</strong> <strong>流式传输令牌</strong></h5><p>在 LangGraph 中，“流式传输令牌（Streaming tokens）”指的是<strong>在节点内部的大模型（LLM）生成过程中，逐 token 地将中间结果实时推送到客户端</strong>的能力。实现这一能力的核心方法是 <code>astream_events</code>，它会以事件流的形式暴露整个执行过程中的所有细节，包括每一次 LLM 调用产生的 token。</p>
<p>每个事件是一个包含几个键的字典：</p>
<p><code>event</code>：这是正在发出的事件的类型。</p>
<p> <code>name</code>：这是事件的名称。</p>
<p><code>data</code>：这是与事件相关联的数据。</p>
<p> <code>metadata</code>：包含 <code>langgraph_node</code>，即发出事件的节点。</p>
<p>要点是，图表中聊天模型的令牌具有 <code>on_chat_model_stream</code> 类型。我们可以使用 <code>event[&#39;metadata&#39;][&#39;langgraph_node&#39;]</code> 来选择要流式的节点。并且我们可以使用 <code>event[&#39;data&#39;]</code> 来获取每个事件的实际数据，而在这种情况下，数据是一个 <code>AIMessageChunk</code>.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">node_to_stream = &#x27;conversation&#x27;#定义流式传输的节点</span><br><span class="line">config = &#123;&quot;configurable&quot;: &#123;&quot;thread_id&quot;: &quot;5&quot;&#125;&#125;</span><br><span class="line">input_message = HumanMessage(content=&quot;为我介绍lol&quot;)</span><br><span class="line">async for event in graph.astream_events(&#123;&quot;messages&quot;: [input_message]&#125;, config, version=&quot;v2&quot;):</span><br><span class="line">    # 从特定节点获取聊天模型生成的 Token</span><br><span class="line">    #事件类型必须是 逐 token 流式输出（on_chat_model_stream）。</span><br><span class="line">    if event[&quot;event&quot;] == &quot;on_chat_model_stream&quot; and event[&#x27;metadata&#x27;].get(&#x27;langgraph_node&#x27;,&#x27;&#x27;) == node_to_stream:</span><br><span class="line">        data = event[&quot;data&quot;]</span><br><span class="line">        print(data[&quot;chunk&quot;].content, end=&quot;|&quot;)</span><br></pre></td></tr></table></figure>
<p>event的常见类型</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>事件类型 (<code>event</code>)</th>
<th>触发时机与说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>on_chain_start</code></td>
<td>任意 Runnable（节点、子图或整个图）开始执行</td>
</tr>
<tr>
<td><code>on_chain_stream</code></td>
<td>节点/图在运行过程中 <strong>增量输出</strong> chunk</td>
</tr>
<tr>
<td><code>on_chain_end</code></td>
<td>任意 Runnable 执行完成</td>
</tr>
<tr>
<td><code>on_chat_model_start</code></td>
<td><strong>ChatModel</strong> 开始调用</td>
</tr>
<tr>
<td><code>on_chat_model_stream</code></td>
<td><strong>ChatModel</strong> 逐 token 返回内容（打字机效果）</td>
</tr>
<tr>
<td><code>on_chat_model_end</code></td>
<td><strong>ChatModel</strong> 调用结束</td>
</tr>
<tr>
<td><code>on_tool_start</code></td>
<td><strong>Tool</strong> 开始调用</td>
</tr>
<tr>
<td><code>on_tool_end</code></td>
<td><strong>Tool</strong> 调用结束</td>
</tr>
<tr>
<td><code>on_retriever_start</code></td>
<td><strong>Retriever</strong> 开始检索</td>
</tr>
<tr>
<td><code>on_retriever_end</code></td>
<td><strong>Retriever</strong> 检索结束</td>
</tr>
</tbody>
</table>
</div>
<h4 id="Breakpoints-断点"><a href="#Breakpoints-断点" class="headerlink" title="Breakpoints 断点"></a><strong>Breakpoints 断点</strong></h4><p><code>human-in-the-loop</code>（人工介入/人在回路）的三大动机：</p>
<p>1️⃣ Approval（审批）我们可以中断智能体，将当前状态呈现给用户，并让用户决定是否执行该操作。</p>
<p>2️⃣ Debugging（调试/回放）我们可以回退图形以重现或避免问题</p>
<p>3️⃣ Editing（编辑）AI 产出的中间结果不符合预期，但不想重跑整图，可以直接修改状态</p>
<p>我们将介绍 <a target="_blank" rel="noopener" href="https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/breakpoints/#simple-usage">breakpoints</a>，它提供了一种在特定步骤停止图的简单方法。</p>
<h5 id="Breakpoints-for-human-approval用于人类审批的断点"><a href="#Breakpoints-for-human-approval用于人类审批的断点" class="headerlink" title="Breakpoints for human approval用于人类审批的断点"></a><strong>Breakpoints for human approval</strong>用于人类审批的断点</h5><p>假设我们关注工具的使用：我们希望批准代理使用其任何工具。</p>
<p>我们所需要做的就是简单地用 <code>interrupt_before=[&quot;tools&quot;]</code> 编译图形，其中 <code>tools</code> 是我们的工具节点。</p>
<p>这意味着在执行工具调用的节点 <code>tools</code> 之前，执行将被中断。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">graph = builder.compile(interrupt_before=[&quot;tools&quot;], checkpointer=memory)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/07/18/%E5%AD%A6%E4%B9%A0/ai%E6%A1%86%E6%9E%B6/langgraph-agent/image-20250720225640772.png" alt="image-20250720225640772"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># Input</span><br><span class="line">initial_input = &#123;&quot;messages&quot;: HumanMessage(content=&quot;2乘3&quot;)&#125;</span><br><span class="line"></span><br><span class="line"># Thread</span><br><span class="line">thread = &#123;&quot;configurable&quot;: &#123;&quot;thread_id&quot;: &quot;1&quot;&#125;&#125;</span><br><span class="line"></span><br><span class="line"># Run the graph until the first interruption</span><br><span class="line">for event in graph.stream(initial_input, thread, stream_mode=&quot;values&quot;):</span><br><span class="line">    event[&#x27;messages&#x27;][-1].pretty_print()</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">================================[1m Human Message [0m=================================</span><br><span class="line"></span><br><span class="line">2乘3</span><br><span class="line">==================================[1m Ai Message [0m==================================</span><br><span class="line">Tool Calls:</span><br><span class="line">  multiply (call_92a4bcf88d25476d925775)</span><br><span class="line"> Call ID: call_92a4bcf88d25476d925775</span><br><span class="line">  Args:</span><br><span class="line">    a: 2</span><br><span class="line">    b: 3</span><br></pre></td></tr></table></figure>
<p>我们可以获取状态并查看要调用的下一个节点。这是一种很好的方法，可以发现图已被中断。</p>
<p>现在，我们将介绍一个很好的技巧。当我们使用 <code>None</code> 调用图时，它将直接从最后一个状态检查点继续！</p>
<p><img src="https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbae7985b747dfed67775d_breakpoints1.png" alt="breakpoints.jpg"></p>
<p>状态快照（StateSnapshot）</p>
<ul>
<li>类型：专门用来存 <strong>一个时刻</strong> 的完整状态</li>
<li>获取方式：<ul>
<li><code>Graph.get_state()</code> → <strong>最新的</strong> 快照</li>
<li><code>Graph.get_state_history()</code> → <strong>所有</strong> 快照列表</li>
</ul>
</li>
</ul>
<p>继续/重跑图</p>
<ul>
<li><code>Graph.stream(None, &#123;&quot;thread_id&quot;: &quot;xxx&quot;&#125;)</code><ul>
<li>不传新输入 <code>None</code> 表示 <strong>从当前最新状态继续跑</strong></li>
<li>也可回退到历史快照，再重跑（调试/回放）</li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">for event in graph.stream(None, thread, stream_mode=&quot;values&quot;):</span><br><span class="line">    event[&#x27;messages&#x27;][-1].pretty_print()</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">==================================[1m Ai Message [0m==================================</span><br><span class="line">Tool Calls:</span><br><span class="line">  multiply (call_92a4bcf88d25476d925775)</span><br><span class="line"> Call ID: call_92a4bcf88d25476d925775</span><br><span class="line">  Args:</span><br><span class="line">    a: 2</span><br><span class="line">    b: 3</span><br><span class="line">=================================[1m Tool Message [0m=================================</span><br><span class="line">Name: multiply</span><br><span class="line"></span><br><span class="line">6</span><br><span class="line">==================================[1m Ai Message [0m==================================</span><br><span class="line"></span><br><span class="line">2乘3的结果是6。</span><br></pre></td></tr></table></figure>
<h4 id="Editing-graph-state-编辑图状态"><a href="#Editing-graph-state-编辑图状态" class="headerlink" title="Editing graph state 编辑图状态"></a><strong>Editing graph state </strong>编辑图状态</h4><p>断点也是<a target="_blank" rel="noopener" href="https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/edit-graph-state/">修改图状态的机会</a>让我们在 <code>assistant</code> 节点之前为代理设置断点。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">graph = builder.compile(interrupt_before=[&quot;assistant&quot;], checkpointer=memory)</span><br></pre></td></tr></table></figure>
<p><img src="/2025/07/18/%E5%AD%A6%E4%B9%A0/ai%E6%A1%86%E6%9E%B6/langgraph-agent/image-20250720230924672.png" alt="image-20250720230924672"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># Input</span><br><span class="line">initial_input = &#123;&quot;messages&quot;: &quot;2乘3&quot;&#125;</span><br><span class="line"></span><br><span class="line"># Thread</span><br><span class="line">thread = &#123;&quot;configurable&quot;: &#123;&quot;thread_id&quot;: &quot;1&quot;&#125;&#125;</span><br><span class="line"></span><br><span class="line"># Run the graph until the first interruption</span><br><span class="line">for event in graph.stream(initial_input, thread, stream_mode=&quot;values&quot;):</span><br><span class="line">    event[&#x27;messages&#x27;][-1].pretty_print()</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">================================[1m Human Message [0m=================================</span><br><span class="line"></span><br><span class="line">2乘3</span><br></pre></td></tr></table></figure>
<p>当状态中断时，我们可以直接应用状态更新</p>
<p>记住，对 <code>messages</code> 键的更新将使用 <code>add_messages</code> reducer：</p>
<p><strong>如果我们想覆盖现有的消息，可以提供带有<em> </em><code>id</code><em> </em>的消息。</strong> 如果我们只想将消息添加到消息列表中，则可以传递未指定 <code>id</code> 的消息，如下所示。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">graph.update_state(</span><br><span class="line">    thread,</span><br><span class="line">    &#123;&quot;messages&quot;: [HumanMessage(content=&quot;不要，实际上要3乘3!&quot;)]&#125;,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">new_state = graph.get_state(thread).values</span><br><span class="line">for m in new_state[&#x27;messages&#x27;]:</span><br><span class="line">    m.pretty_print()</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">================================[1m Human Message [0m=================================</span><br><span class="line"></span><br><span class="line">2乘3</span><br><span class="line">================================[1m Human Message [0m=================================</span><br><span class="line"></span><br><span class="line">不要，实际上要3乘3!</span><br></pre></td></tr></table></figure>
<p>现在，让我们继续进行我们的代理操作，只需传递 <code>None</code> 并允许其从当前状态继续执行。我们输出当前内容，然后继续执行剩余的节点。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">for event in graph.stream(None, thread, stream_mode=&quot;values&quot;):</span><br><span class="line">    event[&#x27;messages&#x27;][-1].pretty_print()</span><br></pre></td></tr></table></figure>
<h4 id="Dynamic-breakpoints-动态断点"><a href="#Dynamic-breakpoints-动态断点" class="headerlink" title="Dynamic breakpoints 动态断点"></a><strong>Dynamic breakpoints </strong>动态断点</h4><p>你可以根据条件来实现它（从节点内部基于开发人员定义的逻辑）。您可以向用户说明其中断原因（通过将您想传递的内容发送到 <code>NodeInterrupt</code>）。</p>
<p>让我们创建一个图表，其中根据输入的长度会抛出一个 <code>NodeInterrupt</code>。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">from IPython.display import Image, display</span><br><span class="line"></span><br><span class="line">from typing_extensions import TypedDict</span><br><span class="line">from langgraph.checkpoint.memory import MemorySaver</span><br><span class="line">from langgraph.errors import NodeInterrupt</span><br><span class="line">from langgraph.graph import START, END, StateGraph</span><br><span class="line"></span><br><span class="line">class State(TypedDict):</span><br><span class="line">    input: str</span><br><span class="line"></span><br><span class="line">def step_1(state: State) -&gt; State:</span><br><span class="line">    print(&quot;---Step 1---&quot;)</span><br><span class="line">    return state</span><br><span class="line"></span><br><span class="line">def step_2(state: State) -&gt; State:</span><br><span class="line">    # 如果输入字符串长度超过5个字符，我们可以选择抛出NodeInterrupt异常</span><br><span class="line">    if len(state[&#x27;input&#x27;]) &gt; 5:</span><br><span class="line">        raise NodeInterrupt(f&quot;收到长度超过5个字符的输入: &#123;state[&#x27;input&#x27;]&#125;&quot;)</span><br><span class="line">    </span><br><span class="line">    print(&quot;---Step 2---&quot;)</span><br><span class="line">    return state</span><br><span class="line"></span><br><span class="line">def step_3(state: State) -&gt; State:</span><br><span class="line">    print(&quot;---Step 3---&quot;)</span><br><span class="line">    return state</span><br><span class="line"></span><br><span class="line">builder = StateGraph(State)</span><br><span class="line">builder.add_node(&quot;step_1&quot;, step_1)</span><br><span class="line">builder.add_node(&quot;step_2&quot;, step_2)</span><br><span class="line">builder.add_node(&quot;step_3&quot;, step_3)</span><br><span class="line">builder.add_edge(START, &quot;step_1&quot;)</span><br><span class="line">builder.add_edge(&quot;step_1&quot;, &quot;step_2&quot;)</span><br><span class="line">builder.add_edge(&quot;step_2&quot;, &quot;step_3&quot;)</span><br><span class="line">builder.add_edge(&quot;step_3&quot;, END)</span><br><span class="line"></span><br><span class="line"># Set up memory</span><br><span class="line">memory = MemorySaver()</span><br><span class="line"></span><br><span class="line"># Compile the graph with memory</span><br><span class="line">graph = builder.compile(checkpointer=memory)</span><br><span class="line"></span><br><span class="line"># View</span><br><span class="line">display(Image(graph.get_graph().draw_mermaid_png()))</span><br></pre></td></tr></table></figure>
<p><img src="/2025/07/18/%E5%AD%A6%E4%B9%A0/ai%E6%A1%86%E6%9E%B6/langgraph-agent/image-20250721222709712.png" alt="image-20250721222709712"></p>
<p>让我们运行一个输入超过5个字符的图。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">initial_input = &#123;&quot;input&quot;: &quot;hello world&quot;&#125;</span><br><span class="line">thread_config = &#123;&quot;configurable&quot;: &#123;&quot;thread_id&quot;: &quot;1&quot;&#125;&#125;</span><br><span class="line"></span><br><span class="line"># Run the graph until the first interruption</span><br><span class="line">for event in graph.stream(initial_input, thread_config, stream_mode=&quot;values&quot;):</span><br><span class="line">    print(event)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;input&#x27;: &#x27;hello world&#x27;&#125;</span><br><span class="line">---Step 1---</span><br><span class="line">&#123;&#x27;input&#x27;: &#x27;hello world&#x27;&#125;</span><br></pre></td></tr></table></figure>
<p>我们可以尝试从断点恢复图。但是，这只会重新运行相同的节点！除非状态发生变化，否则我们将一直卡在这里。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">graph.update_state(</span><br><span class="line">    thread_config,</span><br><span class="line">    &#123;&quot;input&quot;: &quot;hi&quot;&#125;,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>使用update_state更新状态</p>
<h4 id="Time-travel-时间旅行"><a href="#Time-travel-时间旅行" class="headerlink" title="Time travel 时间旅行"></a><strong>Time travel</strong> 时间旅行</h4><p>现在，让我们通过查看、重播，甚至从过去的状态叉出，来展示 LangGraph <a target="_blank" rel="noopener" href="https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/time-travel/">支持debug</a> 的功能。</p>
<h5 id="Browsing-History-浏览历史"><a href="#Browsing-History-浏览历史" class="headerlink" title="Browsing History 浏览历史"></a><strong>Browsing History</strong> <strong>浏览历史</strong></h5><p>我们可以使用 <code>get_state</code> 来查看给定 <code>thread_id</code> 的图的 当前 状态！</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">graph.get_state(&#123;&#x27;configurable&#x27;: &#123;&#x27;thread_id&#x27;: &#x27;1&#x27;&#125;&#125;)</span><br></pre></td></tr></table></figure>
<p>我们还可以浏览代理的状态历史。<code>get_state_history</code> 让我们能够获取所有先前步骤的状态。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">all_states = [s for s in graph.get_state_history(thread)]</span><br><span class="line">len(all_states)</span><br><span class="line">print(all_states)</span><br></pre></td></tr></table></figure>
<h5 id="Replaying-回放"><a href="#Replaying-回放" class="headerlink" title="Replaying 回放"></a><strong>Replaying</strong> <strong>回放</strong></h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">to_replay = all_states[-2]</span><br><span class="line">to_replay.values</span><br><span class="line">&#123;&#x27;messages&#x27;: [HumanMessage(content=&#x27;2乘3&#x27;, additional_kwargs=&#123;&#125;, response_metadata=&#123;&#125;, id=&#x27;0676d9b5-cd59-4630-924d-b5c8d950e8d8&#x27;)]&#125;</span><br><span class="line">to_replay.next</span><br><span class="line">(&#x27;assistant&#x27;,)</span><br></pre></td></tr></table></figure>
<p>我们还获取了配置，它告诉了我们 <code>checkpoint_id</code> 以及 <code>thread_id</code>。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">to_replay.config</span><br><span class="line">&#123;&#x27;configurable&#x27;: &#123;&#x27;thread_id&#x27;: &#x27;1&#x27;,</span><br><span class="line">  &#x27;checkpoint_ns&#x27;: &#x27;&#x27;,</span><br><span class="line">  &#x27;checkpoint_id&#x27;: &#x27;1f066c0e-2ee2-66d5-8000-5dde78194aae&#x27;&#125;&#125;</span><br></pre></td></tr></table></figure>
<p>要从这里重播，我们只需将配置传回给代理！图知道这个检查点已经执行过了。它只是从这个检查点重新播放！</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">for event in graph.stream(None, to_replay.config, stream_mode=&quot;values&quot;):</span><br><span class="line">    event[&#x27;messages&#x27;][-1].pretty_print()</span><br></pre></td></tr></table></figure>
<h5 id="Forking-分叉"><a href="#Forking-分叉" class="headerlink" title="Forking 分叉"></a><strong>Forking</strong> 分叉</h5><p>如果我们想从相同的步骤运行，但使用不同的输入，该怎么办呢？这是分叉。</p>
<p><img src="/2025/07/18/%E5%AD%A6%E4%B9%A0/ai%E6%A1%86%E6%9E%B6/langgraph-agent/66dbb038f89f2d847ee5c336_time-travel3.png" alt="fig3.jpg"></p>
<p>让我们修改此检查点的状态。我们可以直接使用提供的 <code>checkpoint_id</code> 来运行 <code>update_state</code>。</p>
<p>请记住我们对 <code>messages</code> 的 reducer 是如何工作的：</p>
<ul>
<li>它会追加消息，除非我们提供了一个消息 ID。</li>
<li>我们提供消息 ID 是为了覆盖消息，而不是将消息追加到状态中！</li>
</ul>
<p>因此，要覆盖消息，我们只需提供消息 ID，而我们已有 <code>to_fork.values[&quot;messages&quot;].id</code>。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">fork_config = graph.update_state(</span><br><span class="line">    to_fork.config,</span><br><span class="line">    &#123;&quot;messages&quot;: [HumanMessage(content=&#x27;5乘3&#x27;, </span><br><span class="line">                               id=to_fork.values[&quot;messages&quot;][0].id)]&#125;,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h3><h4 id="message"><a href="#message" class="headerlink" title="message"></a>message</h4><p>LangChain 中的 HumanMessage 、 AIMessage 、 SystemMessage 和 ToolMessage 。这些消息类型是构建与语言模型（LLM）交互的核心组件，它们共同构成了一个完整的对话历史，帮助模型理解上下文并做出恰当的回应。</p>
<ol>
<li>SystemMessage</li>
</ol>
<p>SystemMessage 的结构最简单，它只包含内容和类型。</p>
<p>数据结构 :</p>
<ul>
<li>content (str): 消息的具体内容，即给 AI 的指令。</li>
<li>type (str): 固定为字符串 ‘system’ 。</li>
</ul>
<ol>
<li>HumanMessage</li>
</ol>
<p>HumanMessage 的结构也同样简单，代表用户的输入。</p>
<p>数据结构 :</p>
<ul>
<li>content (str): 用户输入的文本。</li>
<li>type (str): 固定为字符串 ‘human’ 。</li>
</ul>
<ol>
<li>AIMessage</li>
</ol>
<p>AIMessage 的结构相对复杂，因为它不仅可以包含文本响应，还可以包含对工具的调用请求。</p>
<p>数据结构 :</p>
<ul>
<li>content (str): AI 生成的文本响应。如果 AI 的回复是发起工具调用，此字段可以为空字符串。</li>
<li>tool_calls (list[dict], 可选): 一个字典列表，每个字典代表一个工具调用请求。这是支持“Function Calling”或“Tool Calling”功能的核心。其结构通常包含：<ul>
<li>name (str): 要调用的工具名称。</li>
<li>args (dict): 调用工具时需要传入的参数。</li>
<li>id (str): 此次工具调用的唯一标识符，用于后续 ToolMessage 的关联。</li>
</ul>
</li>
<li>type (str): 固定为字符串 ‘ai’ 。</li>
</ul>
<ol>
<li>ToolMessage</li>
</ol>
<p>ToolMessage 用于承载工具执行后的返回结果。</p>
<p>数据结构 :</p>
<ul>
<li>content (str): 工具执行返回的结果。通常是一个字符串，比如 JSON 格式的字符串。</li>
<li>tool_call_id (str): 此次工具调用的唯一标识符， 必须 与之前 AIMessage 中 tool_calls 里的 id 相对应。这使得模型能够准确地将结果与请求匹配起来。</li>
<li>type (str): 固定为字符串 ‘tool’ 。</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/07/15/%E5%AD%A6%E4%B9%A0/ai%E6%A1%86%E6%9E%B6/langchain%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/07/15/%E5%AD%A6%E4%B9%A0/ai%E6%A1%86%E6%9E%B6/langchain%E5%AD%A6%E4%B9%A0/" class="post-title-link" itemprop="url">LangChain学习</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-07-15 00:00:00" itemprop="dateCreated datePublished" datetime="2025-07-15T00:00:00+08:00">2025-07-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-08-02 15:52:10" itemprop="dateModified" datetime="2025-08-02T15:52:10+08:00">2025-08-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ai%E6%A1%86%E6%9E%B6/" itemprop="url" rel="index"><span itemprop="name">ai框架</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ai%E6%A1%86%E6%9E%B6/langchain/" itemprop="url" rel="index"><span itemprop="name">langchain</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="实战demo"><a href="#实战demo" class="headerlink" title="实战demo"></a>实战demo</h3><h4 id="agent实战"><a href="#agent实战" class="headerlink" title="agent实战"></a>agent实战</h4><p>langchain的agent与langgraph的agent主要差异点在create_openai_functions_agent, AgentExecutor这两个函数</p>
<p>前者的作用类似<strong>构建 Runnable 链</strong>，返回一个<code>RunnablePassthrough.assign(...)|prompt|llm_with_tools|ToolsAgentOutputParser()</code>，但其invoke仅能完成单步的调用，而<code>AgentExecutor</code> 会自动完成3 步循环（调用工具→拼回结果→再次调用 LLM），直到任务结束。</p>
<blockquote>
<p>以下为ai的解释</p>
<p><strong>直接使用 <code>agent</code> (Runnable) 的局限性:</strong></p>
<ol>
<li><strong>单步执行</strong>: 你直接调用 <code>agent.invoke()</code> 或 <code>agent.ainvoke()</code> 时，它通常只执行<strong>一步</strong>。对于像 <code>create_tool_calling_agent</code> 生成的 <code>agent</code> 来说，这一步就是：<ul>
<li>接收输入（包括历史消息和 <code>agent_scratchpad</code>）。</li>
<li>让 LLM 决定是给出最终答案 (<code>AgentFinish</code>) 还是调用工具 (<code>AgentAction</code>)。</li>
<li>返回这个决定。</li>
</ul>
</li>
<li><strong>工具调用需要手动处理</strong>: 如果 LLM 决定调用工具（返回 <code>AgentAction</code>），<strong>你</strong>需要负责：<ul>
<li>从返回的 <code>AgentAction</code> 中找出工具名称和输入参数。</li>
<li>在你的工具列表中找到对应的工具。</li>
<li>执行这个工具。</li>
<li>获取工具的输出（Observation）。</li>
<li><strong>再次手动调用 <code>agent.invoke(...)</code></strong>，把工具的输出（通常需要格式化成 <code>ToolMessage</code>）放回 <code>agent_scratchpad</code> 或 <code>intermediate_steps</code> 中。</li>
<li>重复这个过程，直到 <code>agent</code> 最终返回 <code>AgentFinish</code>。</li>
</ul>
</li>
</ol>
<p><strong>使用 <code>AgentExecutor</code> 的优势:</strong></p>
<p><code>AgentExecutor</code> 就是为了解决上述问题而设计的。它本质上是一个<strong>自动化的执行引擎</strong>，为你管理整个 Agent 的思考-行动-观察循环。</p>
<ol>
<li><strong>自动化循环</strong>: <code>AgentExecutor</code> 内部会自动运行那个循环：<ul>
<li>调用 <code>agent</code> (Runnable)。</li>
<li>检查返回的是 <code>AgentAction</code> 还是 <code>AgentFinish</code>。</li>
<li>如果是 <code>AgentAction</code>，它会自动根据你提供的 <code>tools</code> 列表找到并执行对应的工具。</li>
<li>它会自动将工具的输出（Observation）记录下来，并作为下一步的输入（放入 <code>agent_scratchpad</code>）再次调用 <code>agent</code>。</li>
<li>这个过程会一直重复。</li>
</ul>
</li>
</ol>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">from langchain.agents import create_openai_functions_agent, AgentExecutor</span><br><span class="line">from langchain.tools import tool</span><br><span class="line">from langchain_openai import ChatOpenAI</span><br><span class="line">from pydantic import BaseModel, Field</span><br><span class="line"></span><br><span class="line"># 1. 定义工具</span><br><span class="line">class WeatherInput(BaseModel):</span><br><span class="line">    location: str = Field(description=&quot;城市名称&quot;)</span><br><span class="line"></span><br><span class="line">@tool(&quot;get_weather&quot;, args_schema=WeatherInput)</span><br><span class="line">def get_weather(location: str) -&gt; str:</span><br><span class="line">    &quot;&quot;&quot;查询城市天气&quot;&quot;&quot;</span><br><span class="line">    return f&quot;&#123;location&#125; 今天是晴天，25°C&quot;</span><br><span class="line"></span><br><span class="line"># 2. 创建Agent</span><br><span class="line">llm = ChatOpenAI(model=&quot;gpt-4&quot;)</span><br><span class="line">tools = [get_weather]</span><br><span class="line">prompt = ChatPromptTemplate.from_messages([</span><br><span class="line">    (&quot;system&quot;, &quot;你是一个助手，可以调用工具&quot;),</span><br><span class="line">    (&quot;human&quot;, &quot;&#123;input&#125;&quot;)</span><br><span class="line">])</span><br><span class="line">agent = create_openai_functions_agent(llm, tools, prompt)</span><br><span class="line">agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)</span><br><span class="line"></span><br><span class="line"># 3. 执行</span><br><span class="line">result = agent_executor.invoke(&#123;&quot;input&quot;: &quot;北京天气如何？&quot;&#125;)</span><br><span class="line">print(result[&quot;output&quot;])</span><br></pre></td></tr></table></figure>
<h4 id="工具调用"><a href="#工具调用" class="headerlink" title="工具调用"></a>工具调用</h4><p>利用bind_tools绑定工具，当大模型需要调用工具的时候，会返回工具信息，tool_calls，如下</p>
<p>[{‘name’: ‘add_numbers’, ‘args’: {‘a’: 15, ‘b’: 27}, ‘id’: ‘4e7b261cce6d4e3da09134086c704c3c’, ‘type’: ‘tool_call’}]</p>
<blockquote>
<p><code>llm_with_tools.invoke(...)</code> 只是一个<strong>单步调用</strong>，LLM 返回的是<strong>“我想调用哪个工具、传什么参数”</strong>（即 <code>tool_calls</code>）。<br><strong>但 LLM 并不会自动执行工具</strong>，所以你必须：</p>
<ol>
<li><strong>手动执行工具</strong>（或让 AgentExecutor 帮你执行）。</li>
<li><strong>把执行结果拼回对话</strong>（作为 <code>ToolMessage</code>）。</li>
<li><strong>再次调用 LLM</strong>，让它基于工具返回的结果生成最终答案。</li>
</ol>
</blockquote>
<p>这里展示的是手动拼接，并传给大模型，如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 将工具的输出发送回LLM，让LLM基于结果生成最终回答</span><br><span class="line">            # 这是一个关键步骤，通常在Agent中自动处理。这里手动演示。</span><br><span class="line">            final_response = llm_with_tools.invoke([</span><br><span class="line">                HumanMessage(content=&quot;What is 15 + 27?&quot;),</span><br><span class="line">                AIMessage(content=&quot;&quot;, tool_calls=[tool_call]), # 告知LLM它之前建议的工具调用</span><br><span class="line">                ToolMessage(content=str(result), tool_call_id=tool_call[&#x27;id&#x27;]) # 告知LLM工具的执行结果</span><br><span class="line">            ])</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line">from langchain_core.tools import tool</span><br><span class="line">from typing import Literal</span><br><span class="line">from langchain_openai import ChatOpenAI</span><br><span class="line">from langchain_core.messages import HumanMessage, AIMessage, ToolMessage</span><br><span class="line"></span><br><span class="line"># 定义一个加法工具</span><br><span class="line">@tool</span><br><span class="line">def add_numbers(a: float, b: float) -&gt; float:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Adds two numbers together.</span><br><span class="line"></span><br><span class="line">    Args:</span><br><span class="line">        a: The first number.</span><br><span class="line">        b: The second number.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    return a + b</span><br><span class="line"></span><br><span class="line"># 我们可以定义更多的工具，例如一个乘法工具</span><br><span class="line">@tool</span><br><span class="line">def multiply_numbers(a: float, b: float) -&gt; float:</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Multiplies two numbers together.</span><br><span class="line"></span><br><span class="line">    Args:</span><br><span class="line">        a: The first number.</span><br><span class="line">        b: The second number.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    return a * b</span><br><span class="line"></span><br><span class="line"># 将我们定义的工具放在一个列表中</span><br><span class="line">tools = [add_numbers, multiply_numbers]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 初始化LLM</span><br><span class="line">llm = ChatOpenAI(</span><br><span class="line">    temperature=0.5,</span><br><span class="line">    model_name=&quot;deepseek-v3-0324&quot;, # 聊天模型通常使用&quot;gpt-3.5-turbo&quot;或&quot;gpt-4&quot;</span><br><span class="line">    openai_api_base=&quot;https://api.qnaigc.com/v1&quot;, # 例如，您可以指定base_url</span><br><span class="line">    openai_api_key=&quot;sk-&quot; # 直接在此处设置API密钥，或者通过环境变量设置</span><br><span class="line">)</span><br><span class="line"># 将工具绑定到LLM</span><br><span class="line"># LLM现在知道了add_numbers和multiply_numbers这两个工具及其功能</span><br><span class="line">llm_with_tools = llm.bind_tools(tools)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 场景一：LLM直接回答，不需要工具</span><br><span class="line">print(&quot;--- 场景一：LLM直接回答 ---&quot;)</span><br><span class="line">response1 = llm_with_tools.invoke([HumanMessage(content=&quot;Hello, what&#x27;s your name?&quot;)])</span><br><span class="line">print(response1.content) # LLM直接生成文本回复</span><br><span class="line"></span><br><span class="line">print(&quot;\n--- 场景二：LLM决定调用工具 ---&quot;)</span><br><span class="line"># 场景二：LLM决定调用工具</span><br><span class="line"># 当LLM的响应中包含tool_calls时，意味着它想要调用一个或多个工具</span><br><span class="line">response2 = llm_with_tools.invoke([HumanMessage(content=&quot;What is 15 + 27?&quot;)])</span><br><span class="line">print(response2.tool_calls) # 打印LLM决定调用的工具信息</span><br><span class="line"></span><br><span class="line"># 检查并执行LLM建议的工具调用</span><br><span class="line">if response2.tool_calls:</span><br><span class="line">    for tool_call in response2.tool_calls:</span><br><span class="line">        if tool_call[&#x27;name&#x27;] == &quot;add_numbers&quot;:</span><br><span class="line">            # 提取LLM为工具调用生成的参数</span><br><span class="line">            args = tool_call[&#x27;args&#x27;]</span><br><span class="line">            result = add_numbers.invoke(args) # 执行工具</span><br><span class="line">            print(f&quot;Tool call: add_numbers(&#123;args[&#x27;a&#x27;]&#125;, &#123;args[&#x27;b&#x27;]&#125;) = &#123;result&#125;&quot;)</span><br><span class="line"></span><br><span class="line">            # 将工具的输出发送回LLM，让LLM基于结果生成最终回答</span><br><span class="line">            # 这是一个关键步骤，通常在Agent中自动处理。这里手动演示。</span><br><span class="line">            final_response = llm_with_tools.invoke([</span><br><span class="line">                HumanMessage(content=&quot;What is 15 + 27?&quot;),</span><br><span class="line">                AIMessage(content=&quot;&quot;, tool_calls=[tool_call]), # 告知LLM它之前建议的工具调用</span><br><span class="line">                ToolMessage(content=str(result), tool_call_id=tool_call[&#x27;id&#x27;]) # 告知LLM工具的执行结果</span><br><span class="line">            ])</span><br><span class="line">            print(&quot;Final LLM response based on tool output:&quot;)</span><br><span class="line">            print(final_response.content)</span><br></pre></td></tr></table></figure>
<h3 id="概念扫盲"><a href="#概念扫盲" class="headerlink" title="概念扫盲"></a>概念扫盲</h3><h4 id="Document-对象"><a href="#Document-对象" class="headerlink" title="Document 对象"></a>Document 对象</h4><p>Document 对象是 LangChain 用来封装和处理文本数据的基本单位。无论您是从 PDF、Markdown 文件、网站还是数据库加载数据，LangChain 都会将这些数据转换成一个或多个 Document 对象，以便在后续的流程中使用。</p>
<p>一个 Document 对象主要包含两个部分：</p>
<ol>
<li><p>page_content (字符串)</p>
<ul>
<li>这是文档对象的核心，存储了原始的文本内容。例如，如果加载一个 Markdown 文件， page_content 就会包含该文件的所有文本。</li>
</ul>
</li>
<li><p>metadata (字典)</p>
<ul>
<li>这是一个字典，用于存储关于文档的“元数据”或附加信息。这些信息对于过滤、追踪或增强文档处理流程非常有用。常见的元数据包括：<ul>
<li>source ：文档的来源，比如文件名、URL等。</li>
<li>page ：如果文档来自多页文件（如PDF），这里可以存储页码。</li>
<li>其他自定义信息：您可以添加任何有助于您应用的信息，如作者、创建日期等。</li>
</ul>
</li>
</ul>
</li>
</ol>
<p>除了通过文档加载器（Loaders）自动创建，您也可以手动创建一个 Document 对象。这在测试或处理简单文本时非常方便。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 创建一个简单的 Document 对象</span><br><span class="line">doc = Document(</span><br><span class="line">    page_content=&quot;这是文档的主要内容。LangChain 真酷！&quot;,</span><br><span class="line">    metadata=&#123;</span><br><span class="line">        &#x27;source&#x27;: &#x27;my_notebook.ipynb&#x27;,</span><br><span class="line">        &#x27;author&#x27;: &#x27;AI Assistant&#x27;,</span><br><span class="line">        &#x27;chapter&#x27;: 2</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h4 id="Runnable协议"><a href="#Runnable协议" class="headerlink" title="Runnable协议"></a>Runnable协议</h4><p><a target="_blank" rel="noopener" href="https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable">“Runnable”</a>协议</p>
<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV12TLAzuEni/?spm_id_from=333.337.search-card.all.click&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">2025最新版！langchain入门到精通实战教程！结合实战案例，干货拉满！99%的人不知道的暴利玩法，学完敢谷歌工程师叫板！_哔哩哔哩_bilibili</a></p>
<p><a target="_blank" rel="noopener" href="https://www.langchain.com.cn/docs/introduction/">introduction | LangChain中文网</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1XudVYzEcW?spm_id_from=333.788.videopod.episodes&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">跟着官网学langchain2025(version 0.3)_哔哩哔哩_bilibili</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.langchain.com/langgraph-platform">LangGraph Platform - Docs by LangChain</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/07/15/%E5%AD%A6%E4%B9%A0/ai%E6%A1%86%E6%9E%B6/langgraph%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/07/15/%E5%AD%A6%E4%B9%A0/ai%E6%A1%86%E6%9E%B6/langgraph%E5%AD%A6%E4%B9%A0/" class="post-title-link" itemprop="url">LangGraph学习——快速入门</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-07-15 00:00:00" itemprop="dateCreated datePublished" datetime="2025-07-15T00:00:00+08:00">2025-07-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-08-28 09:43:40" itemprop="dateModified" datetime="2025-08-28T09:43:40+08:00">2025-08-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ai%E6%A1%86%E6%9E%B6/" itemprop="url" rel="index"><span itemprop="name">ai框架</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ai%E6%A1%86%E6%9E%B6/langgraph/" itemprop="url" rel="index"><span itemprop="name">langgraph</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="构建langgraph聊天机器人的基本流程"><a href="#构建langgraph聊天机器人的基本流程" class="headerlink" title="构建langgraph聊天机器人的基本流程"></a>构建langgraph聊天机器人的基本流程</h3><h4 id="创建一个-StateGraph"><a href="#创建一个-StateGraph" class="headerlink" title="创建一个 StateGraph"></a>创建一个 <code>StateGraph</code></h4><p>首先创建一个 <code>StateGraph</code>。一个 <code>StateGraph</code> 对象将我们的聊天机器人结构定义为“状态机”。我们将添加 <code>节点</code> 来表示 LLM 和聊天机器人可以调用的函数，并添加 <code>边</code> 来指定机器人应如何在这些函数之间进行转换。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> Annotated</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> typing_extensions <span class="keyword">import</span> TypedDict</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> langgraph.graph <span class="keyword">import</span> StateGraph, START</span><br><span class="line"><span class="keyword">from</span> langgraph.graph.message <span class="keyword">import</span> add_messages</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">State</span>(<span class="title class_ inherited__">TypedDict</span>):</span><br><span class="line">    messages: Annotated[<span class="built_in">list</span>, add_messages]</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义状态</span></span><br><span class="line">graph_builder = StateGraph(State)</span><br></pre></td></tr></table></figure>
<p>在 langgraph 中，状态会在图的各个节点之间传递。当一个节点产生新的消息时，它会更新 State 中的 messages 字段。</p>
<p>我们的图现在可以处理两个关键任务</p>
<ol>
<li>每个 <code>节点</code> 都可以接收当前 <code>状态</code> 作为输入，并输出状态的更新。</li>
<li>对 <code>消息</code> 的更新将追加到现有列表而不是覆盖它，这得益于与 <code>Annotated</code> 语法一起使用的预构建 <a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langgraph/reference/graphs/?h=add+messages#add_messages"><code>add_messages</code></a> 函数。</li>
</ol>
<blockquote>
<p>langgraph中每个消息对象通常包含以下关键属性：</p>
<ul>
<li>role : 一个字符串，标识消息的发送者（例如 ‘human’ , ‘ai’ , ‘system’ ）。</li>
<li>content : 消息的具体内容，通常是字符串，但也可以是更复杂的结构（例如，用于多模态输入）。</li>
<li>id : 一个可选的唯一标识符。</li>
</ul>
<p>Annotated 的作用 : 通过使用 Annotated[list, add_messages] ，你改变了这个默认行为。 add_messages 函数（由 langgraph 提供或由你自定义）的逻辑是 追加 而不是覆盖。所以，当一个新节点返回消息时， langgraph 会调用 add_messages 函数，将新消息 追加 到现有 messages 列表的末尾。</p>
</blockquote>
<h4 id="定义一个聊天模型"><a href="#定义一个聊天模型" class="headerlink" title="定义一个聊天模型"></a>定义一个聊天模型</h4><p>两种方法：</p>
<p>1.使用 <code>init_chat_model</code>(通用高层封装)<br>这是一个通用的辅助函数，旨在提供一个统一的接口来初始化来自 不同提供商 的聊天模型。</p>
<p><a target="_blank" rel="noopener" href="https://python.langchain.com/api_reference/langchain/chat_models/langchain.chat_models.base.init_chat_model.html">init_chat_model — 🦜🔗 LangChain 文档 —- init_chat_model — 🦜🔗 LangChain documentation</a></p>
<p>2.使用如ChatOpenAI (特定于提供商的类)<br>这是一个专门为 OpenAI API 设计的类，提供了对 OpenAI 模型所有功能的完全访问。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from langchain.chat_models import init_chat_model</span><br><span class="line"></span><br><span class="line">os.environ[&quot;OPENAI_API_KEY&quot;] = &quot;sk-&quot;</span><br><span class="line">#使用‘&#123;model_provider&#125;:&#123;model&#125;’格式在单个参数中指定模型和模型提供者，例如“openai:o1”</span><br><span class="line">llm = init_chat_model(&quot;openai:qwen-plus-2025-04-28&quot;,</span><br><span class="line">                      base_url=&quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;</span><br><span class="line">                      )</span><br></pre></td></tr></table></figure>
<h4 id="添加一个节点"><a href="#添加一个节点" class="headerlink" title="添加一个节点"></a>添加一个节点</h4><p>现在我们可以将聊天模型集成到一个简单的节点中</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#定义节点chatbot</span><br><span class="line">def chatbot(state: State):</span><br><span class="line">    return &#123;&quot;messages&quot;: [llm.invoke(state[&quot;messages&quot;])]&#125;</span><br><span class="line"></span><br><span class="line">graph_builder.add_node(&quot;chatbot&quot;, chatbot)</span><br></pre></td></tr></table></figure>
<p> <code>chatbot</code> 节点函数如何将当前 <code>状态</code> 作为输入，并返回一个包含更新的 <code>消息</code> 列表的字典，键为“messages”。这是所有 LangGraph 节点函数的基本模式。</p>
<p>我们 <code>状态</code> 中的 <code>add_messages</code> 函数会将 LLM 的响应消息追加到状态中已有的消息之后。</p>
<h4 id="添加一个-入口-点"><a href="#添加一个-入口-点" class="headerlink" title="添加一个 入口 点"></a>添加一个 <code>入口</code> 点</h4><p>添加一个 <code>入口</code> 点，以告诉图每次运行时<strong>从何处开始工作</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">graph_builder.add_edge(START, &quot;chatbot&quot;)</span><br></pre></td></tr></table></figure>
<h4 id="编译图"><a href="#编译图" class="headerlink" title="编译图"></a>编译图</h4><p>在运行图之前，我们需要对其进行编译。我们可以通过在图构建器上调用 <code>compile()</code> 来完成。这将创建一个 <code>CompiledGraph</code>，我们可以在我们的状态上调用它。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">graph = graph_builder.compile()</span><br></pre></td></tr></table></figure>
<h4 id="可视化图"><a href="#可视化图" class="headerlink" title="可视化图"></a>可视化图</h4><p>您可以使用 <code>get_graph</code> 方法和其中一个“绘图”方法（例如 <code>draw_ascii</code> 或 <code>draw_png</code>）来可视化图。这些 <code>draw</code> 方法都需要额外的依赖项。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from IPython.display import Image, display</span><br><span class="line"></span><br><span class="line">try:</span><br><span class="line">    display(Image(graph.get_graph().draw_mermaid_png()))</span><br><span class="line">except Exception:</span><br><span class="line">    # This requires some extra dependencies and is optional</span><br><span class="line">    pass</span><br></pre></td></tr></table></figure>
<h4 id="运行聊天机器人"><a href="#运行聊天机器人" class="headerlink" title="运行聊天机器人"></a>运行聊天机器人</h4><p>运行聊天机器人</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">stream_graph_updates</span>(<span class="params">user_input: <span class="built_in">str</span></span>):</span><br><span class="line">    <span class="keyword">for</span> event <span class="keyword">in</span> graph.stream(&#123;<span class="string">&quot;messages&quot;</span>: [&#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: user_input&#125;]&#125;):</span><br><span class="line">        <span class="built_in">print</span>(event)</span><br><span class="line">        <span class="comment">#stream 返回的每个 event 通常是一个字典，键是图中节点的名称，值是该节点完成后的状态更新。</span></span><br><span class="line">        <span class="keyword">for</span> value <span class="keyword">in</span> event.values():</span><br><span class="line">            <span class="comment">#消息列表中的最后一条消息的文本内容</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Assistant:&quot;</span>, value[<span class="string">&quot;messages&quot;</span>][-<span class="number">1</span>].content)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        user_input = <span class="built_in">input</span>(<span class="string">&quot;User: &quot;</span>)</span><br><span class="line">        <span class="comment">#退出</span></span><br><span class="line">        <span class="keyword">if</span> user_input.lower() <span class="keyword">in</span> [<span class="string">&quot;quit&quot;</span>, <span class="string">&quot;exit&quot;</span>, <span class="string">&quot;q&quot;</span>]:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Goodbye!&quot;</span>)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="comment">#调用</span></span><br><span class="line">        stream_graph_updates(user_input)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="comment"># fallback if input() is not available</span></span><br><span class="line">        user_input = <span class="string">&quot;What do you know about LangGraph?&quot;</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;User: &quot;</span> + user_input)</span><br><span class="line">        stream_graph_updates(user_input)</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>graph.stream() 是 LangGraph 的核心功能之一。它会执行整个图（Graph），但不是一次性返回最终结果，而是像视频流一样，一步一步地返回中间过程的更新。这使得您可以实时看到模型生成内容的每一个部分。</p>
</blockquote>
<h3 id="添加网页搜索工具"><a href="#添加网页搜索工具" class="headerlink" title="添加网页搜索工具"></a>添加网页搜索工具</h3><h4 id="获取Tavily-api"><a href="#获取Tavily-api" class="headerlink" title="获取Tavily api"></a>获取Tavily api</h4><p><a target="_blank" rel="noopener" href="https://tavily.com/">Tavily 的搜索 API</a> 是一款专为 AI 代理 (LLM) 构建的搜索引擎，能够快速提供实时、准确和基于事实的结果。</p>
<p>每月 1,000 次免费搜索</p>
<p><a target="_blank" rel="noopener" href="https://python.langchain.ac.cn/docs/integrations/tools/tavily_search/">Tavily Search | 🦜️🔗 LangChain 框架</a></p>
<p>获取api<a target="_blank" rel="noopener" href="https://app.tavily.com/home">Tavily AI —- Tavily AI</a></p>
<h4 id="添加工具"><a href="#添加工具" class="headerlink" title="添加工具"></a>添加工具</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from langchain_tavily import TavilySearch</span><br><span class="line"></span><br><span class="line">tool = TavilySearch(</span><br><span class="line">    tavily_api_key=&quot;tvly-dev-&quot;,</span><br><span class="line">    max_results=2)</span><br><span class="line">tools = [tool]</span><br><span class="line">tool.invoke(&quot;李超是谁?&quot;)</span><br></pre></td></tr></table></figure>
<h4 id="定义图"><a href="#定义图" class="headerlink" title="定义图"></a>定义图</h4><p>在LLM上添加<code>bind_tools</code>。这让LLM知道如果它想使用搜索引擎，应使用正确的JSON格式。</p>
<p>定义聊天模型llm（代码同上）</p>
<p>将tools整合到<code>StateGraph</code>中</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> Annotated</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> typing_extensions <span class="keyword">import</span> TypedDict</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> langgraph.graph <span class="keyword">import</span> StateGraph, START, END</span><br><span class="line"><span class="keyword">from</span> langgraph.graph.message <span class="keyword">import</span> add_messages</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">State</span>(<span class="title class_ inherited__">TypedDict</span>):</span><br><span class="line">    messages: Annotated[<span class="built_in">list</span>, add_messages]</span><br><span class="line"></span><br><span class="line">graph_builder = StateGraph(State)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将一个或多个**工具（tools） 绑定到一个 大型语言模型（LLM）**上，从而创建一个新的、具备工具调用能力的 LLM 实例</span></span><br><span class="line">llm_with_tools = llm.bind_tools(tools)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">chatbot</span>(<span class="params">state: State</span>):</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&quot;messages&quot;</span>: [llm_with_tools.invoke(state[<span class="string">&quot;messages&quot;</span>])]&#125;</span><br><span class="line"></span><br><span class="line">graph_builder.add_node(<span class="string">&quot;chatbot&quot;</span>, chatbot)</span><br></pre></td></tr></table></figure>
<h4 id="创建一个运行工具的函数"><a href="#创建一个运行工具的函数" class="headerlink" title="创建一个运行工具的函数"></a>创建一个运行工具的函数</h4><p>现在，创建一个函数来运行被调用的工具。通过将工具添加到一个名为<code>BasicToolNode</code>的新节点来完成，该节点检查状态中的最新消息，如果消息包含<code>tool_calls</code>，则调用工具。它依赖于LLM的<code>tool_calling</code>支持，该支持在Anthropic、OpenAI、Google Gemini以及许多其他LLM提供商中可用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义 __call__ 方法，让这个类的实例可以像函数一样被调用</span></span><br><span class="line"><span class="comment"># inputs 是 langgraph 传进来的当前状态，是一个字典</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, inputs: <span class="built_in">dict</span></span>):</span><br><span class="line">    <span class="comment"># 1. 从状态中获取最新的消息</span></span><br><span class="line">    <span class="comment"># 使用了“海象操作符” :=，先从 inputs 中获取 &#x27;messages&#x27; 列表，如果不存在则返回空列表 []</span></span><br><span class="line">    <span class="comment"># 然后检查列表是否为空。如果不为空，则取出最后一条消息。</span></span><br><span class="line">    <span class="keyword">if</span> messages := inputs.get(<span class="string">&quot;messages&quot;</span>, []):</span><br><span class="line">        message = messages[-<span class="number">1</span>]  <span class="comment"># 通常，最后一条消息是 AI 发出的，其中包含工具调用请求</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 如果没有消息，就报错，因为这个节点不知道该做什么</span></span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;No message found in input&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. 准备一个列表，用来存放所有工具的执行结果</span></span><br><span class="line">    outputs = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. 遍历 AI 消息中请求的所有工具调用</span></span><br><span class="line">    <span class="keyword">for</span> tool_call <span class="keyword">in</span> message.tool_calls:</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 4. 执行工具</span></span><br><span class="line">        <span class="comment"># a. tool_call[&quot;name&quot;] 获取工具名称 (例如 &#x27;tavily_search_results_json&#x27;)</span></span><br><span class="line">        <span class="comment"># b. self.tools_by_name[...] 从预存的工具字典中找到对应的工具对象</span></span><br><span class="line">        <span class="comment"># c. .invoke(tool_call[&quot;args&quot;]) 使用 LLM 提供的参数来调用该工具</span></span><br><span class="line">        tool_result = <span class="variable language_">self</span>.tools_by_name[tool_call[<span class="string">&quot;name&quot;</span>]].invoke(</span><br><span class="line">            tool_call[<span class="string">&quot;args&quot;</span>]</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 5. 将工具执行结果打包成 ToolMessage</span></span><br><span class="line">        <span class="comment"># 这是 langgraph/langchain 的标准格式，用于告诉 LLM 工具执行的结果是什么</span></span><br><span class="line">        outputs.append(</span><br><span class="line">            ToolMessage(</span><br><span class="line">                content=json.dumps(tool_result),  <span class="comment"># 工具结果必须是字符串，所以用 json.dumps 序列化</span></span><br><span class="line">                name=tool_call[<span class="string">&quot;name&quot;</span>],  <span class="comment"># 告诉 LLM 这是哪个工具的结果</span></span><br><span class="line">                tool_call_id=tool_call[<span class="string">&quot;id&quot;</span>],  <span class="comment"># 必须提供原始请求的 ID，以便 LLM 知道这个结果对应哪个请求</span></span><br><span class="line">            )</span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 6. 返回结果，更新图的状态</span></span><br><span class="line">    <span class="comment"># 返回一个字典，其中 &#x27;messages&#x27; 键对应着包含所有 ToolMessage 的列表</span></span><br><span class="line">    <span class="comment"># langgraph 会将这个列表中的消息追加到主状态的 &#x27;messages&#x27; 列表中</span></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&quot;messages&quot;</span>: outputs&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>call</strong> 是 Python 中一个非常特殊的“魔术方法”（magic method）。它的作用是 让一个类的实例（对象）能够像函数一样被调用 。</p>
<p>这在 langgraph 中是一种常见且核心的设计模式。它的含义是：</p>
<ol>
<li>节点即函数 ： BasicToolNode 的实例（比如 tool_node ）本身就代表了图中的一个可执行节点。</li>
<li>执行逻辑 ：当 langgraph 的状态机运行到这个 tool_node 节点时，它会直接“调用”这个节点对象，并把当前的状态（ inputs 字典）传递给它</li>
</ol>
</blockquote>
<p>可以使用LangGraph预构建的<a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langgraph/reference/agents/#langgraph.prebuilt.tool_node.ToolNode">ToolNode</a>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langgraph.prebuilt <span class="keyword">import</span> ToolNode</span><br><span class="line"></span><br><span class="line">tool_node = ToolNode(tools=[tool])</span><br><span class="line">graph_builder.add_node(<span class="string">&quot;tools&quot;</span>, tool_node)</span><br></pre></td></tr></table></figure>
<h4 id="定义conditional-edges"><a href="#定义conditional-edges" class="headerlink" title="定义conditional_edges"></a>定义<code>conditional_edges</code></h4><p>添加了工具节点后，现在您可以定义<code>conditional_edges</code>。</p>
<p><strong>边（Edges）</strong>将控制流从一个节点路由到下一个节点。<strong>条件边（Conditional edges）</strong>从单个节点开始，通常包含“if”语句，根据当前图状态路由到不同的节点。这些函数接收当前的图<code>state</code>并返回一个字符串或字符串列表，指示接下来要调用哪个（或哪些）节点。</p>
<p>接下来，定义一个名为<code>route_tools</code>的路由函数，它检查聊天机器人输出中的<code>tool_calls</code>。通过调用<code>add_conditional_edges</code>将此函数提供给图，这会告诉图，无论何时<code>chatbot</code>节点完成，都要检查此函数以确定下一步去哪里。</p>
<p>如果存在工具调用，条件将路由到<code>tools</code>；如果不存在，则路由到<code>END</code>。由于条件可以返回<code>END</code>，因此这次您不需要明确设置<code>finish_point</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">route_tools</span>(<span class="params"></span></span><br><span class="line"><span class="params">     state: State,</span></span><br><span class="line"><span class="params"> </span>):</span><br><span class="line">     <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">     用于 conditional_edge 的路由函数：</span></span><br><span class="line"><span class="string">     - 如果最后一条消息包含工具调用，则路由到 ToolNode</span></span><br><span class="line"><span class="string">     - 否则路由到结束节点</span></span><br><span class="line"><span class="string">     &quot;&quot;&quot;</span></span><br><span class="line">     <span class="comment"># 处理 state 为列表的情况（可能是消息列表）</span></span><br><span class="line">     <span class="keyword">if</span> <span class="built_in">isinstance</span>(state, <span class="built_in">list</span>):</span><br><span class="line">         ai_message = state[-<span class="number">1</span>]  <span class="comment"># 获取最后一条消息</span></span><br><span class="line">     <span class="comment"># 处理 state 为字典的情况（包含 messages 字段）</span></span><br><span class="line">     <span class="keyword">elif</span> messages := state.get(<span class="string">&quot;messages&quot;</span>, []):</span><br><span class="line">         ai_message = messages[-<span class="number">1</span>]  <span class="comment"># 获取最后一条消息</span></span><br><span class="line">     <span class="keyword">else</span>:</span><br><span class="line">         <span class="keyword">raise</span> ValueError(<span class="string">f&quot;输入状态中没有找到消息: <span class="subst">&#123;state&#125;</span>&quot;</span>)</span><br><span class="line">     </span><br><span class="line">     <span class="comment"># 检查消息是否有工具调用</span></span><br><span class="line">     <span class="keyword">if</span> <span class="built_in">hasattr</span>(ai_message, <span class="string">&quot;tool_calls&quot;</span>) <span class="keyword">and</span> <span class="built_in">len</span>(ai_message.tool_calls) &gt; <span class="number">0</span>:</span><br><span class="line">         <span class="keyword">return</span> <span class="string">&quot;tools&quot;</span>  <span class="comment"># 有工具调用，返回 &quot;tools&quot; 路由到工具节点</span></span><br><span class="line">     <span class="keyword">return</span> END  <span class="comment"># 没有工具调用，返回 END 结束流程</span></span><br></pre></td></tr></table></figure>
<p>可以使用预构建的<a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langgraph/reference/prebuilt/#tools_condition">tools_condition</a>代替route_tools以使其更简洁。</p>
<blockquote>
<p><code>tools_condition</code> 函数在聊天机器人需要使用工具时返回 “tools”，如果可以不使用响应则返回 “END”。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langgraph.prebuilt <span class="keyword">import</span> tools_condition</span><br><span class="line">graph_builder.add_conditional_edges(</span><br><span class="line">    <span class="string">&quot;chatbot&quot;</span>,</span><br><span class="line">    tools_condition,</span><br><span class="line">    &#123;<span class="string">&quot;tools&quot;</span>: <span class="string">&quot;tools&quot;</span>, END: END&#125;,</span><br><span class="line">)</span><br><span class="line">graph_builder.add_edge(<span class="string">&quot;tools&quot;</span>, <span class="string">&quot;chatbot&quot;</span>)</span><br><span class="line">graph_builder.add_edge(START, <span class="string">&quot;chatbot&quot;</span>)</span><br><span class="line">graph = graph_builder.<span class="built_in">compile</span>()</span><br></pre></td></tr></table></figure>
<h4 id="可视化图-1"><a href="#可视化图-1" class="headerlink" title="可视化图"></a>可视化图</h4><p>如上</p>
<h4 id="向机器人提问"><a href="#向机器人提问" class="headerlink" title="向机器人提问"></a>向机器人提问</h4><p>现在您可以向聊天机器人提出超出其训练数据范围的问题。</p>
<p>如上</p>
<h3 id="添加记忆功能"><a href="#添加记忆功能" class="headerlink" title="添加记忆功能"></a>添加记忆功能</h3><p>LangGraph 通过<strong>持久性检查点</strong>解决了这个问题。如果您在编译图时提供一个<code>checkpointer</code>，并在调用图时提供一个<code>thread_id</code>，LangGraph 会在每一步之后自动保存状态。当您使用相同的<code>thread_id</code>再次调用图时，图会加载其保存的状态，允许聊天机器人从上次中断的地方继续。</p>
<p>我们稍后会看到，<strong>检查点</strong>比简单的聊天记忆功能<em>强大得多</em>——它允许您随时保存和恢复复杂状态，用于错误恢复、人工干预工作流、时间旅行交互等。但首先，让我们添加检查点以实现多轮对话。</p>
<h4 id="创建-MemorySaver-检查点"><a href="#创建-MemorySaver-检查点" class="headerlink" title="创建 MemorySaver 检查点"></a>创建 <code>MemorySaver</code> 检查点</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langgraph.checkpoint.memory <span class="keyword">import</span> MemorySaver</span><br><span class="line"></span><br><span class="line">memory = MemorySaver()</span><br></pre></td></tr></table></figure>
<p>这是一个内存中的检查点，方便本教程使用。然而，在生产应用程序中，您可能会将其更改为使用 <code>SqliteSaver</code> 或 <code>PostgresSaver</code> 并连接数据库。</p>
<h4 id="编译图-1"><a href="#编译图-1" class="headerlink" title="编译图"></a>编译图</h4><p>使用提供的检查点编译图，图在遍历每个节点时将对 <code>State</code> 进行检查点。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">graph = graph_builder.compile(checkpointer=memory)</span><br></pre></td></tr></table></figure>
<h4 id="与您的聊天机器人互动"><a href="#与您的聊天机器人互动" class="headerlink" title="与您的聊天机器人互动"></a>与您的聊天机器人互动</h4><ol>
<li><p>选择一个线程作为此对话的键。</p>
<p>thread_id决定对话窗口</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">config = &#123;&quot;configurable&quot;: &#123;&quot;thread_id&quot;: &quot;1&quot;&#125;&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>调用您的聊天机器人</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">user_input = &quot;我是谁&quot;</span><br><span class="line"></span><br><span class="line"># The config is the **second positional argument** to stream() or invoke()!</span><br><span class="line">events = graph.stream(</span><br><span class="line">    &#123;&quot;messages&quot;: [&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_input&#125;]&#125;,</span><br><span class="line">    config,</span><br><span class="line">    stream_mode=&quot;values&quot;,</span><br><span class="line">)</span><br><span class="line">for event in events:</span><br><span class="line">    event[&quot;messages&quot;][-1].pretty_print()</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="添加人工干预"><a href="#添加人工干预" class="headerlink" title="添加人工干预"></a>添加人工干预</h3><p>代理可能不可靠，并且可能需要人工输入才能成功完成任务。同样，对于某些操作，您可能需要在运行前要求人工批准，以确保一切按预期运行。</p>
<p>LangGraph 的<a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langgraph/concepts/persistence/">持久化</a>层支持<strong>人工干预</strong>工作流，允许根据用户反馈暂停和恢复执行。此功能的主要接口是<a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langgraph/how-tos/human_in_the_loop/add-human-in-the-loop/"><code>interrupt</code></a>函数。在节点内调用<code>interrupt</code>将暂停执行。通过传入<a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langgraph/concepts/low_level/#command">Command</a>，可以恢复执行并接收来自人工的新输入。<code>interrupt</code>在功能上类似于 Python 的内置<code>input()</code>，<a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langgraph/how-tos/human_in_the_loop/add-human-in-the-loop/">但有一些注意事项</a>。</p>
<h4 id="添加human-assistance工具"><a href="#添加human-assistance工具" class="headerlink" title="添加human_assistance工具"></a>添加<code>human_assistance</code>工具</h4><p>将<code>human_assistance</code>工具添加到聊天机器人。此工具使用<code>interrupt</code>从人工接收信息。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># 添加人工干预功能</span><br><span class="line"># 导入LangGraph的中断机制和命令类型</span><br><span class="line">from langgraph.types import Command, interrupt</span><br><span class="line"># 导入LangChain的工具装饰器</span><br><span class="line">from langchain_core.tools import tool</span><br><span class="line"></span><br><span class="line"># 使用@tool装饰器将函数标记为可被LLM调用的工具</span><br><span class="line">@tool</span><br><span class="line">def human_assistance(query: str) -&gt; str:</span><br><span class="line">    &quot;&quot;&quot;请求人工协助的工具函数。</span><br><span class="line">    当LLM遇到需要人工判断或帮助的情况时，会调用此工具。</span><br><span class="line">    该函数会暂停图的执行，等待人工操作员提供响应。</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    # interrupt()函数会暂停图的执行，等待人工输入</span><br><span class="line">    # 传入的字典包含查询信息，人工操作员会看到这个查询</span><br><span class="line">    human_response = interrupt(&#123;&quot;query&quot;: query&#125;)</span><br><span class="line">    </span><br><span class="line">    # 从人工响应中提取数据并返回给LLM</span><br><span class="line">    # human_response是一个字典，&quot;data&quot;字段包含人工提供的实际响应</span><br><span class="line">    return human_response[&quot;data&quot;]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>简单来说， <strong>调用哪个工具，以及何时调用，完全是由大语言模型（LLM）根据你给它的指令（Prompt）来决定的。</strong></p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tool = TavilySearch(max_results=2)</span><br><span class="line">tools = [tool, human_assistance]</span><br><span class="line">llm_with_tools = llm.bind_tools(tools)</span><br></pre></td></tr></table></figure>
<h4 id="定义chatbot"><a href="#定义chatbot" class="headerlink" title="定义chatbot"></a>定义chatbot</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">def chatbot(state: State):</span><br><span class="line">    # 调用绑定了工具的LLM（llm_with_tools），传入当前的消息历史</span><br><span class="line">    # LLM会根据最新的消息决定是生成文本回复，还是调用一个或多个工具</span><br><span class="line">    message = llm_with_tools.invoke(state[&quot;messages&quot;])</span><br><span class="line">    </span><br><span class="line">    # --- 关键断言逻辑 ---</span><br><span class="line">    assert len(message.tool_calls) &lt;= 1</span><br><span class="line">    </span><br><span class="line">    # 将LLM生成的新消息（可能是文本回复，也可能是工具调用请求）返回</span><br><span class="line">    # 这个返回值会以字典的形式更新到状态（State）对象中</span><br><span class="line">    return &#123;&quot;messages&quot;: [message]&#125;</span><br><span class="line"></span><br><span class="line"># 将 chatbot 函数作为名为 &quot;chatbot&quot; 的节点添加到图构建器中</span><br><span class="line">graph_builder.add_node(&quot;chatbot&quot;, chatbot)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>中断安全性的断言 ( assert ) : assert len(message.tool_calls) &lt;= 1 是在实现人工干预时一个非常重要的 安全措施 。</p>
<ul>
<li>问题 : 现代 LLM 支持并行工具调用（一次请求执行多个工具）。但如果其中一个工具是 human_assistance 并触发了中断，整个图会暂停。当人工操作完成后，图会从中断点恢复。此时，如果不对工具调用数量做限制，LangGraph 可能会重新尝试执行所有在中断前请求的工具，导致已经执行过的工具被再次调用。</li>
<li>解决方案 : 这个断言强制要求 LLM 在每一步最多只能请求调用一个工具。这样就保证了当中断发生并恢复后，不会有重复执行工具的风险，确保了流程的稳定性和可预测性。</li>
</ul>
</blockquote>
<h4 id="编译图-2"><a href="#编译图-2" class="headerlink" title="编译图"></a>编译图</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">memory = MemorySaver()</span><br><span class="line"></span><br><span class="line">graph = graph_builder.compile(checkpointer=memory)</span><br></pre></td></tr></table></figure>
<h4 id="调用聊天机器人并中断"><a href="#调用聊天机器人并中断" class="headerlink" title="调用聊天机器人并中断"></a>调用聊天机器人并中断</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">user_input = &quot;我需要一些关于构建 AI 代理的专家指导。你能帮我请求协助吗？&quot;</span><br><span class="line">config = &#123;&quot;configurable&quot;: &#123;&quot;thread_id&quot;: &quot;1&quot;&#125;&#125;</span><br><span class="line"></span><br><span class="line">events = graph.stream(</span><br><span class="line">    &#123;&quot;messages&quot;: [&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_input&#125;]&#125;,</span><br><span class="line">    config,</span><br><span class="line">    #它的作用是指定在进行流式处理时，你希望接收到的数据是以 完整的、累积的值 的形式返回，而不是以增量的、片段的形式返回。</span><br><span class="line">    stream_mode=&quot;values&quot;,</span><br><span class="line">)</span><br><span class="line">for event in events:</span><br><span class="line">    if &quot;messages&quot; in event:</span><br><span class="line">        event[&quot;messages&quot;][-1].pretty_print()</span><br></pre></td></tr></table></figure>
<p>聊天机器人生成了一个工具调用，但随后执行被中断。如果您检查图状态，您会看到它停止在工具节点</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">snapshot = graph.get_state(config)</span><br><span class="line">snapshot.next</span><br></pre></td></tr></table></figure>
<h4 id="恢复执行"><a href="#恢复执行" class="headerlink" title="恢复执行"></a>恢复执行</h4><p>要恢复执行，请传入一个包含工具所需数据的<a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langgraph/concepts/low_level/#command"><code>Command</code></a>对象。此数据的格式可以根据需要进行自定义。对于本示例，请使用一个带有键<code>&quot;data&quot;</code>的字典（由human_assistance决定）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">human_response = (</span><br><span class="line">&quot;我们专家在此为您提供帮助！我们建议您查看 LangGraph 来构建您的代理。它比简单的自主代理更可靠、更具可扩展性。&quot;</span><br><span class="line">)   </span><br><span class="line">#从暂停状态恢复执行</span><br><span class="line">human_command = Command(resume=&#123;&quot;data&quot;: human_response&#125;)</span><br><span class="line"></span><br><span class="line">events = graph.stream(human_command, config, stream_mode=&quot;values&quot;)</span><br><span class="line">for event in events:</span><br><span class="line">    if &quot;messages&quot; in event:</span><br><span class="line">        event[&quot;messages&quot;][-1].pretty_print()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>1.工具的定义 ( human_assistance function):</p>
<ul>
<li>当 LLM 调用 human_assistance 工具时，这个函数被执行。</li>
<li>函数内部， interrupt() 被调用，导致图暂停，并等待人工输入。</li>
<li>在图恢复后， interrupt() 函数会返回一个值，这个值就是您通过 Command(resume=…) 注入的内容，也就是 {“data”: human_response} 。</li>
<li>因此， human_assistance 函数中的 human_response 变量实际上就等于 {“data”: human_response} 。</li>
<li>最后， return human_response[“data”] 从这个字典中提取出 “data” 键对应的值 ，并将其作为 human_assistance 工具的最终返回结果。</li>
</ul>
<p>2.恢复指令 ( Command(resume=…) ):</p>
<ul>
<li>当您构建 Command(resume={“data”: human_response}) 时，您正在创建一个符合 human_assistance 函数期望的结构。</li>
<li>您将人工回复包装在一个字典里，并使用 “data” 作为键。</li>
<li>这个结构被传递回 interrupt() ，然后被 human_assistance 函数接收和解析。</li>
</ul>
<p>因为 human_assistance 函数的 return 语句期望从返回的字典中访问 “data” 键，所以我们在恢复执行时必须提供一个具有相同结构的字典。这是为了确保数据能够正确地在中断和恢复的过程中传递。</p>
</blockquote>
<p>在 LangGraph 中，<code>Command</code> 是一个用于<strong>控制图执行流程、更新状态、实现人机交互</strong>的核心类。它支持以下四个参数：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">参数名</th>
<th style="text-align:left">类型</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>update</code></td>
<td style="text-align:left"><code>dict</code></td>
<td style="text-align:left">用于更新图的状态（state）。例如：<code>Command(update=&#123;&quot;foo&quot;: &quot;bar&quot;&#125;)</code>。</td>
</tr>
<tr>
<td style="text-align:left"><code>resume</code></td>
<td style="text-align:left"><code>Any</code></td>
<td style="text-align:left">与 <code>interrupt()</code> 配合使用，用于恢复被中断的图执行，并传递用户输入。</td>
</tr>
<tr>
<td style="text-align:left"><code>goto</code></td>
<td style="text-align:left"><code>str</code> 或 <code>Send</code> 或 `List[str</td>
<td style="text-align:left">Send]`</td>
<td>控制下一步要执行的节点，支持跳转到指定节点、多个节点序列，或使用 <code>Send</code> 对象。</td>
</tr>
<tr>
<td style="text-align:left"><code>graph</code></td>
<td style="text-align:left"><code>str</code></td>
<td style="text-align:left">可选，指定命令作用的图。默认是当前图，也可以设为 <code>Command.PARENT</code> 表示父图。</td>
</tr>
</tbody>
</table>
</div>
<h3 id="自定义状态"><a href="#自定义状态" class="headerlink" title="自定义状态"></a>自定义状态</h3><p>在本教程中，您将向状态添加额外字段，以定义复杂行为，而无需依赖消息列表。聊天机器人将使用其搜索工具查找特定信息，并将其转发给人工进行审查。</p>
<h4 id="向状态添加键"><a href="#向状态添加键" class="headerlink" title="向状态添加键"></a>向状态添加键</h4><p>通过向状态添加 <code>name</code> 和 <code>birthday</code> 键，更新聊天机器人以研究实体的生日</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">class State(TypedDict):</span><br><span class="line">    messages: Annotated[list, add_messages]</span><br><span class="line">    name: str</span><br><span class="line">    birthday: str</span><br></pre></td></tr></table></figure>
<p>将此信息添加到状态中，可以使其轻松被其他图节点（例如存储或处理信息的下游节点）以及图的持久层访问。</p>
<h4 id="在工具内部更新状态"><a href="#在工具内部更新状态" class="headerlink" title="在工具内部更新状态"></a>在工具内部更新状态</h4><p>现在，在 <code>human_assistance</code> 工具内部填充状态键。这允许人工在信息存储到状态之前对其进行审查。使用 <a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langgraph/concepts/low_level/#using-inside-tools"><code>Command</code></a> 从<strong>工具内部</strong>发出状态更新。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"># 从 langchain_core.messages 导入 ToolMessage，用于创建工具调用的响应消息</span><br><span class="line">from langchain_core.messages import ToolMessage </span><br><span class="line"># 从 langchain_core.tools 导入 InjectedToolCallId（用于自动注入工具调用ID）和 tool（工具装饰器）</span><br><span class="line">from langchain_core.tools import InjectedToolCallId, tool </span><br><span class="line"></span><br><span class="line"># 从 langgraph.types 导入 Command（用于向图发送指令）和 interrupt（用于中断图的执行）</span><br><span class="line">from langgraph.types import Command, interrupt </span><br><span class="line">from typing import Annotated</span><br><span class="line"># @tool 装饰器将这个函数声明为一个可供 LLM 调用的工具</span><br><span class="line">@tool </span><br><span class="line">def human_assistance(</span><br><span class="line">    name: str, </span><br><span class="line">    birthday: str, </span><br><span class="line">    # tool_call_id 这个参数非常特殊。Annotated[...] 和 InjectedToolCallId 告诉 LangGraph：</span><br><span class="line">    # 1. 这个参数不应暴露给 LLM，LLM 在调用此工具时不需要提供它。</span><br><span class="line">    # 2. LangGraph 在执行时，会自动将触发此工具的那个工具调用的 ID 注入到这个参数中。</span><br><span class="line">    # 这个 ID 对于创建与原始请求相关联的 ToolMessage 至关重要。</span><br><span class="line">    tool_call_id: Annotated[str, InjectedToolCallId]</span><br><span class="line">) -&gt; str: </span><br><span class="line">    &quot;&quot;&quot;当需要人工确认或更正信息时，请求人类协助。&quot;&quot;&quot; </span><br><span class="line">    # 调用 interrupt() 来暂停图的执行，并向人类审核者呈现一个包含问题和待确认数据的字典。</span><br><span class="line">    # 图会在此处暂停，直到人类通过 resume 指令提供了响应。</span><br><span class="line">    human_response = interrupt( </span><br><span class="line">        &#123; </span><br><span class="line">            &quot;question&quot;: &quot;Is this correct?&quot;, </span><br><span class="line">            &quot;name&quot;: name, </span><br><span class="line">            &quot;birthday&quot;: birthday, </span><br><span class="line">        &#125;,</span><br><span class="line">    ) </span><br><span class="line">    # 检查人类的响应。如果响应中 &#x27;correct&#x27; 键的值是 &#x27;yes&#x27; 或 &#x27;y&#x27; 开头，</span><br><span class="line">    # 则认为信息是正确的。</span><br><span class="line">    if human_response.get(&quot;correct&quot;, &quot;&quot;).lower().startswith(&quot;y&quot;): </span><br><span class="line">        # 如果信息正确，直接使用从 LLM 获取的原始信息。</span><br><span class="line">        verified_name = name </span><br><span class="line">        verified_birthday = birthday </span><br><span class="line">        response = &quot;Correct&quot; </span><br><span class="line">    # 否则，认为人类审核者提供了更正后的信息。</span><br><span class="line">    else: </span><br><span class="line">        # 从人类的响应中获取更正后的姓名和生日。</span><br><span class="line">        # 如果人类没有提供新的值，则使用 .get() 的默认值，即原始值。</span><br><span class="line">        verified_name = human_response.get(&quot;name&quot;, name) </span><br><span class="line">        verified_birthday = human_response.get(&quot;birthday&quot;, birthday) </span><br><span class="line">        response = f&quot;Made a correction: &#123;human_response&#125;&quot; </span><br><span class="line"></span><br><span class="line">    # 在工具内部直接构造一个用于更新图状态的字典。</span><br><span class="line">    state_update = &#123; </span><br><span class="line">        &quot;name&quot;: verified_name, # 更新状态中的 &#x27;name&#x27; 字段</span><br><span class="line">        &quot;birthday&quot;: verified_birthday, # 更新状态中的 &#x27;birthday&#x27; 字段</span><br><span class="line">        # 创建一个 ToolMessage，将其添加到状态的 &#x27;messages&#x27; 列表中。</span><br><span class="line">        # 这个消息将作为此工具调用的正式“答复”出现在对话历史中。</span><br><span class="line">        # tool_call_id 是必需的，用于将此答复与 LLM 的原始工具调用请求关联起来。</span><br><span class="line">        &quot;messages&quot;: [ToolMessage(response, tool_call_id=tool_call_id)], </span><br><span class="line">    &#125; </span><br><span class="line">    # 这个工具不返回一个简单的字符串或数字，而是返回一个 Command 对象。</span><br><span class="line">    # Command(update=...) 是一个明确的指令，告诉 LangGraph 执行器：</span><br><span class="line">    # “请不要将我的返回值当作普通工具输出，而是用 state_update 字典里的内容来直接更新当前的图状态。”</span><br><span class="line">    return Command(update=state_update) </span><br></pre></td></tr></table></figure>
<p>图的其余部分保持不变。</p>
<h4 id="提示聊天机器人调用人工审查"><a href="#提示聊天机器人调用人工审查" class="headerlink" title="提示聊天机器人调用人工审查"></a>提示聊天机器人调用人工审查</h4><p>提示聊天机器人查找 LangGraph 库的“生日”，并在其获取所需信息后，指示聊天机器人使用 <code>human_assistance</code> 工具。通过在工具参数中设置 <code>name</code> 和 <code>birthday</code>，您将强制聊天机器人为这些字段生成提议。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">user_input = (</span><br><span class="line">    &quot;你能查一下 LangGraph 是什么时候发布的吗？ &quot;</span><br><span class="line">    &quot;当你有了答案后，使用 human_assistance 工具进行审查。&quot;</span><br><span class="line">)</span><br><span class="line">config = &#123;&quot;configurable&quot;: &#123;&quot;thread_id&quot;: &quot;1&quot;&#125;&#125;</span><br><span class="line"></span><br><span class="line">events = graph.stream(</span><br><span class="line">    &#123;&quot;messages&quot;: [&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_input&#125;]&#125;,</span><br><span class="line">    config,</span><br><span class="line">    stream_mode=&quot;values&quot;,</span><br><span class="line">)</span><br><span class="line">for event in events:</span><br><span class="line">    if &quot;messages&quot; in event:</span><br><span class="line">        event[&quot;messages&quot;][-1].pretty_print()</span><br></pre></td></tr></table></figure>
<p>我们再次在 <code>human_assistance</code> 工具中触发了 <code>interrupt</code>。</p>
<h4 id="添加人工协助"><a href="#添加人工协助" class="headerlink" title="添加人工协助"></a>添加人工协助</h4><p>聊天机器人未能识别正确的日期，因此为其提供信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">human_command = Command(</span><br><span class="line">    resume=&#123;</span><br><span class="line">        &quot;name&quot;: &quot;LangGraph&quot;,</span><br><span class="line">        &quot;birthday&quot;: &quot;Jan 17, 2024&quot;,</span><br><span class="line">    &#125;,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">events = graph.stream(human_command, config, stream_mode=&quot;values&quot;)</span><br><span class="line">for event in events:</span><br><span class="line">    if &quot;messages&quot; in event:</span><br><span class="line">        event[&quot;messages&quot;][-1].pretty_print()</span><br></pre></td></tr></table></figure>
<p>请注意，这些字段现在已反映在状态中</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">snapshot = graph.get_state(config)</span><br><span class="line"></span><br><span class="line">&#123;k: v for k, v in snapshot.values.items() if k in (&quot;name&quot;, &quot;birthday&quot;)&#125;</span><br></pre></td></tr></table></figure>
<p>这使得下游节点（例如，进一步处理或存储信息的节点）可以轻松访问它们。</p>
<h3 id="时间功能（从之前的某个状态开始）"><a href="#时间功能（从之前的某个状态开始）" class="headerlink" title="时间功能（从之前的某个状态开始）"></a>时间功能（从之前的某个状态开始）</h3><p>在典型的聊天机器人工作流程中，用户与机器人进行一次或多次交互以完成任务。<a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langgraph/tutorials/get-started/3-add-memory/">记忆</a>和<a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langgraph/tutorials/get-started/4-human-in-the-loop/">人工干预</a>功能可以为图状态启用检查点并控制未来的响应。</p>
<p>如果您希望用户能够从之前的响应开始并探索不同的结果，该怎么办？或者，如果您希望用户能够回溯聊天机器人的工作以纠正错误或尝试不同的策略，这在自主软件工程师等应用程序中很常见，那又该怎么办？</p>
<p>您可以使用 LangGraph 内置的<strong>时光旅行</strong>功能创建这些类型的体验。</p>
<h4 id="回溯您的图"><a href="#回溯您的图" class="headerlink" title="回溯您的图"></a>回溯您的图</h4><p>通过使用图的<code>get_state_history</code>方法获取检查点来回溯您的图。然后，您可以从之前的这个时间点恢复执行。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"># 初始化一个变量 to_replay 为 None，它将用于存储我们想要“时间旅行”回去的特定状态。</span><br><span class="line"># to_replay 将在循环中被赋值为我们感兴趣的那个历史状态快照。</span><br><span class="line">to_replay = None</span><br><span class="line"></span><br><span class="line"># 遍历 `graph` 在给定 `config` 下的所有历史状态。</span><br><span class="line"># `graph.get_state_history(config)` 会返回一个迭代器，其中包含了从开始到当前的所有状态快照。</span><br><span class="line">for state in graph.get_state_history(config):</span><br><span class="line">    # 打印当前状态快照中的一些信息，以便我们观察和选择。</span><br><span class="line">    # `len(state.values[&quot;messages&quot;])` 显示了到该状态为止，对话历史中的消息总数。</span><br><span class="line">    # `state.next` 显示了在该状态之后，图将要执行的下一个节点或步骤的名称。</span><br><span class="line">    print(&quot;Num Messages: &quot;, len(state.values[&quot;messages&quot;]), &quot;Next: &quot;, state.next)</span><br><span class="line">    </span><br><span class="line">    # 打印一条分隔线，使输出更易读。</span><br><span class="line">    print(&quot;-&quot; * 80)</span><br><span class="line">    </span><br><span class="line">    # 这里是选择“时间旅行”目标点的关键逻辑。</span><br><span class="line">    # 我们设定一个条件：当对话历史中的消息数量正好等于4时，我们就找到了想要回到的那个点。</span><br><span class="line">    # 这是一个为了演示而设定的任意条件，在实际应用中，您可以根据需要设置更复杂的选择逻辑。</span><br><span class="line">    if len(state.values[&quot;messages&quot;]) == 4:</span><br><span class="line">        # We are somewhat arbitrarily selecting a specific state based on the number of chat messages in the state.</span><br><span class="line">        # 将当前这个符合条件的状态（state）保存到 to_replay 变量中。</span><br><span class="line">        # 循环结束后，to_replay 变量将持有我们选中的那个历史时刻的完整状态，</span><br><span class="line">        # 之后我们就可以用它来恢复或修改执行流程。</span><br><span class="line">        to_replay = state</span><br></pre></td></tr></table></figure>
<p>图的每一步都会保存检查点。这<strong>跨越了调用</strong>，因此您可以回溯整个线程的历史。</p>
<h4 id="从特定时间点加载状态"><a href="#从特定时间点加载状态" class="headerlink" title="从特定时间点加载状态"></a>从特定时间点加载状态</h4><p>从<code>to_replay</code>状态恢复。从这一点恢复将接下来调用<strong>action</strong>节点。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(to_replay.next)</span><br><span class="line">print(to_replay.config)</span><br></pre></td></tr></table></figure>
<p>检查点的<code>to_replay.config</code>包含一个<code>checkpoint_id</code>时间戳。提供此<code>checkpoint_id</code>值会告诉 LangGraph 的检查点器从该时间点<strong>加载</strong>状态。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">for event in graph.stream(None, to_replay.config, stream_mode=&quot;values&quot;):</span><br><span class="line">    if &quot;messages&quot; in event:</span><br><span class="line">        event[&quot;messages&quot;][-1].pretty_print()</span><br></pre></td></tr></table></figure>
<h3 id="运行本地服务器"><a href="#运行本地服务器" class="headerlink" title="运行本地服务器"></a>运行本地服务器</h3><h4 id="安装-LangGraph-CLI"><a href="#安装-LangGraph-CLI" class="headerlink" title="安装 LangGraph CLI"></a>安装 LangGraph CLI</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># Python &gt;= 3.11 is required.</span><br><span class="line"></span><br><span class="line">pip install --upgrade &quot;langgraph-cli[inmem]&quot;</span><br></pre></td></tr></table></figure>
<h4 id="创建-LangGraph-应用-🌱"><a href="#创建-LangGraph-应用-🌱" class="headerlink" title="创建 LangGraph 应用 🌱"></a>创建 LangGraph 应用 🌱</h4><p>从 <a target="_blank" rel="noopener" href="https://github.com/langchain-ai/new-langgraph-project"><code>new-langgraph-project-python</code> 模板</a> 或 <a target="_blank" rel="noopener" href="https://github.com/langchain-ai/new-langgraphjs-project"><code>new-langgraph-project-js</code> 模板</a> 创建一个新应用。此模板展示了一个单节点应用程序，您可以根据自己的逻辑进行扩展。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">langgraph new . --template new-langgraph-project-python</span><br></pre></td></tr></table></figure>
<blockquote>
<p>使用 <code>langgraph new</code> 而不指定模板，系统将显示一个交互式菜单，您可以从中选择可用的模板列表。</p>
</blockquote>
<h4 id="使用uv安装依赖项"><a href="#使用uv安装依赖项" class="headerlink" title="使用uv安装依赖项"></a>使用uv安装依赖项</h4><p>uv 是一个用 Rust 编写的极速 Python 包和项目管理器 。它旨在解决传统 Python 包管理工具（如 pip 、 poetry 等）在速度和效率方面的痛点，提供更快的安装、依赖解析和环境管理。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install uv#安装uv</span><br></pre></td></tr></table></figure>
<p>运行<code>uv sync</code>会根据 <code>pyproject.toml</code>的依赖创建虚拟环境并安装依赖</p>
<blockquote>
<p><code>pyproject.toml</code> 文件是 Python 项目中用于统一配置项目元数据、构建系统、依赖管理和各种工具设置的标准化文件。它通常用于替代旧的 requirements.txt 文件，提供更现代和集中的项目配置方式。</p>
</blockquote>
<h4 id="创建一个-env-文件"><a href="#创建一个-env-文件" class="headerlink" title="创建一个 .env 文件"></a>创建一个 <code>.env</code> 文件</h4><p>您将在新 LangGraph 应用的根目录下找到一个 <code>.env.example</code> 文件。在新 LangGraph 应用的根目录下创建一个 <code>.env</code> 文件，并将 <code>.env.example</code> 文件的内容复制到其中，填入所需的 API 密钥。</p>
<p>添加环境变量如LANGSMITH_API_KEY，OPENAI_API_KEY等</p>
<h4 id="启动-LangGraph-服务器"><a href="#启动-LangGraph-服务器" class="headerlink" title="启动 LangGraph 服务器"></a>启动 LangGraph 服务器</h4><p>在本地启动 LangGraph API 服务器</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">langgraph dev</span><br></pre></td></tr></table></figure>
<p>示例输出</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;    Ready!</span><br><span class="line">&gt;</span><br><span class="line">&gt;    - API: [https://:2024](https://:2024/)</span><br><span class="line">&gt;</span><br><span class="line">&gt;    - Docs: https://:2024/docs</span><br><span class="line">&gt;</span><br><span class="line">&gt;    - LangGraph Studio Web UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024</span><br></pre></td></tr></table></figure>
<p>LangGraph 服务器（如您通过 langgraph dev 命令启动的服务器）的主要作用是提供一个运行环境和接口，用于开发、测试、部署和管理基于 LangGraph 构建的 AI 代理和应用程序。具体来说，它有以下几个主要用途：</p>
<ol>
<li>API 接口暴露 ：它将您用 LangGraph 定义的复杂代理逻辑（即图结构）通过标准的 RESTful API 接口暴露出来。这意味着其他应用程序、前端界面或者其他服务可以通过 HTTP 请求与您的 LangGraph 代理进行交互，而无需直接集成 LangGraph 的 Python 代码。</li>
<li>简化部署 ：通过将 LangGraph 应用程序打包成一个可运行的服务，您可以更容易地将其部署到云服务器、容器（如 Docker）或其他生产环境中。这使得 LangGraph 代理可以作为一个独立的微服务运行，方便扩展和管理。</li>
<li><p>开发和调试便利 ：</p>
<ul>
<li>实时预览和调试 ：服务器通常会提供一个 Studio UI（如您在 <a target="_blank" rel="noopener" href="http://127.0.0.1:2024/studio">http://127.0.0.1:2024/studio</a> 看到的），让开发者能够可视化地查看代理的图结构、执行流程、状态变化和中间步骤，这对于理解和调试复杂的代理行为至关重要。</li>
<li>API 文档 ：自动生成的 API 文档（如 <a target="_blank" rel="noopener" href="http://127.0.0.1:2024/docs">http://127.0.0.1:2024/docs</a> ）提供了所有可用接口的详细说明和交互式测试功能，极大地加速了开发和集成过程。</li>
</ul>
</li>
<li>状态管理和持久化 ：LangGraph 代理通常涉及复杂的状态管理。服务器可以负责处理这些状态的持久化，确保代理在多次交互之间能够记住上下文和历史信息。</li>
</ol>
<h4 id="在-LangGraph-Studio-中测试您的应用程序"><a href="#在-LangGraph-Studio-中测试您的应用程序" class="headerlink" title="在 LangGraph Studio 中测试您的应用程序"></a>在 LangGraph Studio 中测试您的应用程序</h4><p><a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langgraph/concepts/langgraph_studio/">LangGraph Studio</a> 是一个专门的 UI，您可以连接到 LangGraph API 服务器，以便在本地可视化、交互和调试您的应用程序。通过访问 <code>langgraph dev</code> 命令输出中提供的 URL，在 LangGraph Studio 中测试您的图。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;    - LangGraph Studio Web UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024</span><br></pre></td></tr></table></figure>
<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langgraph/tutorials/get-started/1-build-basic-chatbot/#1-install-packages">构建一个基本聊天机器人 - LangChain 框架</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.langchain.com/oss/python/overview">Overview - Docs by LangChain</a></p>
<p>官方教程，但是英文<a target="_blank" rel="noopener" href="https://academy.langchain.com/collections">https://academy.langchain.com/collections</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1AY4aeRETY/?spm_id_from=333.1007.top_right_bar_window_history.content.click&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">3.5 小时出证！LangGraph 官方课程 🆓 重磅上线🔥🔥🔥_哔哩哔哩_bilibili</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/07/15/%E5%AD%A6%E4%B9%A0/ai%E6%A1%86%E6%9E%B6/langgraph%E6%9F%A5%E6%BC%8F%E8%A1%A5%E7%BC%BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/07/15/%E5%AD%A6%E4%B9%A0/ai%E6%A1%86%E6%9E%B6/langgraph%E6%9F%A5%E6%BC%8F%E8%A1%A5%E7%BC%BA/" class="post-title-link" itemprop="url">langgraph查漏补缺</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-07-15 00:00:00" itemprop="dateCreated datePublished" datetime="2025-07-15T00:00:00+08:00">2025-07-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-08-04 17:14:00" itemprop="dateModified" datetime="2025-08-04T17:14:00+08:00">2025-08-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ai%E6%A1%86%E6%9E%B6/" itemprop="url" rel="index"><span itemprop="name">ai框架</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ai%E6%A1%86%E6%9E%B6/langgraph/" itemprop="url" rel="index"><span itemprop="name">langgraph</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="注入上下文"><a href="#注入上下文" class="headerlink" title="注入上下文"></a>注入上下文</h3><p>“注入上下文”就是<strong>在运行过程中节点/大模型</strong> 可能需要、但<strong>不会</strong>（也不应该）去改变的<strong>只读信息</strong>的集合。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>场景</th>
<th>注入上下文里可能放什么</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>权限控制</strong></td>
<td><code>user_id</code>, <code>tenant_id</code>（决定能访问哪些数据）</td>
</tr>
<tr>
<td><strong>外部依赖</strong></td>
<td><code>db_connection</code>, <code>api_key</code>, <code>s3_bucket</code>（节点里要用）</td>
</tr>
<tr>
<td><strong>个性化参数</strong></td>
<td><code>language</code>, <code>timezone</code>, <code>model_temperature</code></td>
</tr>
<tr>
<td><strong>会话元信息</strong></td>
<td><code>session_id</code>, <code>channel</code>（Slack / 微信 / Web）</td>
</tr>
</tbody>
</table>
</div>
<h4 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">from dataclasses import dataclass</span><br><span class="line"></span><br><span class="line">from langgraph.graph import StateGraph</span><br><span class="line">from langgraph.runtime import Runtime</span><br><span class="line"></span><br><span class="line">@dataclass</span><br><span class="line">class Context:</span><br><span class="line">    &quot;&quot;&quot;Context schema defined by the developer.&quot;&quot;&quot;    </span><br><span class="line">    user_id: str    </span><br><span class="line">    db_connection: str</span><br><span class="line">    </span><br><span class="line">def node(state: State, runtime: Runtime[Context]):</span><br><span class="line">    # type safe access to context attributes    </span><br><span class="line">    user_id = runtime.context.user_id</span><br><span class="line">    db_conn = runtime.context.db_connection</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">builder = StateGraph(state_schema=State, context_schema=Context)</span><br><span class="line"></span><br><span class="line"># add nodes, edges, compile the graph...</span><br><span class="line"></span><br><span class="line"># top level context arg is typed as Context for autocomplete and type checking</span><br><span class="line">result = graph.invoke(</span><br><span class="line">    &#123;&#x27;input&#x27;: &#x27;abc&#x27;&#125;,</span><br><span class="line">    context=Context(user_id=&#x27;123&#x27;, db_conn=&#x27;conn_mock&#x27;)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="https://langchain-ai.github.io/langgraph/reference/runtime/#runtime"><code>Runtime</code></a> 类提供了一个单一接口，用于访问信息，例如：</p>
<ul>
<li>上下文：在运行开始时传递的静态数据</li>
<li>存储：长期记忆的存储机制</li>
<li>流写入器：用于向图输出流写入的自定义函数</li>
<li>对于功能 API 用户，<code>previous</code> 也可用：给定线程的前一个返回值</li>
</ul>
<p>现在，开发者不再需要将上述所有内容作为单独的参数注入到节点函数中，<br>而是可以通过一个 <code>runtime</code> 参数来访问它们。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/07/15/%E5%AD%A6%E4%B9%A0/ai%E6%A1%86%E6%9E%B6/langsmith/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/07/15/%E5%AD%A6%E4%B9%A0/ai%E6%A1%86%E6%9E%B6/langsmith/" class="post-title-link" itemprop="url">Langsmith</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2025-07-15 00:00:00 / 修改时间：17:55:02" itemprop="dateCreated datePublished" datetime="2025-07-15T00:00:00+08:00">2025-07-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ai%E6%A1%86%E6%9E%B6/" itemprop="url" rel="index"><span itemprop="name">ai框架</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ai%E6%A1%86%E6%9E%B6/langsmith/" itemprop="url" rel="index"><span itemprop="name">langsmith</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="配置langsmith"><a href="#配置langsmith" class="headerlink" title="配置langsmith"></a>配置langsmith</h3><h4 id="安装LangSmith-SDK"><a href="#安装LangSmith-SDK" class="headerlink" title="安装LangSmith SDK"></a>安装LangSmith SDK</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install langsmith</span><br></pre></td></tr></table></figure>
<h4 id="环境变量"><a href="#环境变量" class="headerlink" title="环境变量"></a>环境变量</h4><p>获取api<a target="_blank" rel="noopener" href="https://smith.langchain.com/o/56031c2a-c402-41d4-83ed-ed15d0693548/settings/apikeys">LangSmith</a></p>
<p>设置相应的环境变量。这将把跟踪记录到<code>default</code>项目（尽管您可以轻松更改）。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export LANGSMITH_TRACING=true</span><br><span class="line">export LANGSMITH_API_KEY=</span><br><span class="line">export LANGSMITH_PROJECT=default</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">LANGSMITH_TRACING=true</span><br><span class="line">LANGSMITH_ENDPOINT=&quot;https://api.smith.langchain.com&quot;</span><br><span class="line">LANGSMITH_API_KEY=&quot;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&quot;</span><br><span class="line">LANGSMITH_PROJECT=&quot;langgraph&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="资源"><a href="#资源" class="headerlink" title="资源"></a>资源</h3><p>官网<a target="_blank" rel="noopener" href="https://smith.langchain.com/o/56031c2a-c402-41d4-83ed-ed15d0693548/">《LangSmith》 —- LangSmith</a></p>
<p>参考文档<a target="_blank" rel="noopener" href="https://langsmith.langchain.ac.cn/">LangSmith 入门 | 🦜️🛠️ LangSmith 文档</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.smith.langchain.com/">Get started with LangSmith | 🦜️🛠️ LangSmith</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/07/14/%E5%AD%A6%E4%B9%A0/ai%E6%A1%86%E6%9E%B6/LangGraphChatBot/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/07/14/%E5%AD%A6%E4%B9%A0/ai%E6%A1%86%E6%9E%B6/LangGraphChatBot/" class="post-title-link" itemprop="url">LangGraphChatBot</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-07-14 00:00:00" itemprop="dateCreated datePublished" datetime="2025-07-14T00:00:00+08:00">2025-07-14</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-07-15 17:54:50" itemprop="dateModified" datetime="2025-07-15T17:54:50+08:00">2025-07-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ai%E6%A1%86%E6%9E%B6/" itemprop="url" rel="index"><span itemprop="name">ai框架</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/ai%E6%A1%86%E6%9E%B6/langgraph/" itemprop="url" rel="index"><span itemprop="name">langgraph</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h3><p>python虚拟环境构建</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m venv .venv</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pip install langgraph==0.2.74                  </span><br><span class="line">pip install langchain-openai==0.3.6            </span><br><span class="line">pip install fastapi==0.115.8                         </span><br><span class="line">pip install uvicorn==0.34.0                          </span><br><span class="line">pip install gradio==5.18.0</span><br></pre></td></tr></table></figure>
<p>查看包<code>pip list</code></p>
<h3 id="构建一个基本的fastapi-langgraph应用"><a href="#构建一个基本的fastapi-langgraph应用" class="headerlink" title="构建一个基本的fastapi+langgraph应用"></a>构建一个基本的fastapi+langgraph应用</h3><h4 id="llm示例的构建（利用ChatOpenAI）"><a href="#llm示例的构建（利用ChatOpenAI）" class="headerlink" title="llm示例的构建（利用ChatOpenAI）"></a>llm示例的构建（利用ChatOpenAI）</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 创建LLM实例</span><br><span class="line">llm = ChatOpenAI(</span><br><span class="line">    base_url=config[&quot;base_url&quot;],</span><br><span class="line">    api_key=config[&quot;api_key&quot;],</span><br><span class="line">    model=config[&quot;model&quot;],</span><br><span class="line">    temperature=DEFAULT_TEMPERATURE,</span><br><span class="line">    timeout=30,  # 添加超时配置（秒）</span><br><span class="line">    max_retries=2  # 添加重试次数</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h4 id="数据类型的构建"><a href="#数据类型的构建" class="headerlink" title="数据类型的构建"></a>数据类型的构建</h4><p>继承于pydantic</p>
<p>规范化 API 请求和响应的数据结构</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义消息类，用于封装API接口返回数据</span></span><br><span class="line"><span class="comment">#基于 Pydantic 的数据模型</span></span><br><span class="line"><span class="comment"># 定义Message类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Message</span>(<span class="title class_ inherited__">BaseModel</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    role (角色): 这是一个字符串，表示消息的发送者。常见的角色包括：</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">- user (用户): 表示用户输入的消息。</span></span><br><span class="line"><span class="string">- assistant (助手): 表示聊天机器人或模型生成的消息。</span></span><br><span class="line"><span class="string">- system (系统): 表示为模型提供上下文或指令的系统消息。</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    role: <span class="built_in">str</span></span><br><span class="line">    content: <span class="built_in">str</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义ChatCompletionRequest类</span></span><br><span class="line"><span class="comment">#聊天 API 请求</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ChatCompletionRequest</span>(<span class="title class_ inherited__">BaseModel</span>):</span><br><span class="line">    messages: <span class="type">List</span>[Message]</span><br><span class="line">    stream: <span class="type">Optional</span>[<span class="built_in">bool</span>] = <span class="literal">False</span><span class="comment">#是否流式方式响应</span></span><br><span class="line">    userId: <span class="type">Optional</span>[<span class="built_in">str</span>] = <span class="literal">None</span><span class="comment">#用于标识发起请求的用户</span></span><br><span class="line">    conversationId: <span class="type">Optional</span>[<span class="built_in">str</span>] = <span class="literal">None</span><span class="comment">#用于标识特定的对话会话，这对于管理对话上下文或历史记录非常有用</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义ChatCompletionResponseChoice类</span></span><br><span class="line"><span class="comment">#聊天完成响应中的一个“选择”或一个生成的回复</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ChatCompletionResponseChoice</span>(<span class="title class_ inherited__">BaseModel</span>):</span><br><span class="line">    index: <span class="built_in">int</span></span><br><span class="line">    message: Message</span><br><span class="line">    finish_reason: <span class="type">Optional</span>[<span class="built_in">str</span>] = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义ChatCompletionResponse类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ChatCompletionResponse</span>(<span class="title class_ inherited__">BaseModel</span>):</span><br><span class="line">    <span class="built_in">id</span>: <span class="built_in">str</span> = Field(default_factory=<span class="keyword">lambda</span>: <span class="string">f&quot;chatcmpl-<span class="subst">&#123;uuid.uuid4().<span class="built_in">hex</span>&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">object</span>: <span class="built_in">str</span> = <span class="string">&quot;chat.completion&quot;</span></span><br><span class="line">    created: <span class="built_in">int</span> = Field(default_factory=<span class="keyword">lambda</span>: <span class="built_in">int</span>(time.time()))</span><br><span class="line">    choices: <span class="type">List</span>[ChatCompletionResponseChoice]<span class="comment">#模型生成的所有可能的回复选项</span></span><br><span class="line">    system_fingerprint: <span class="type">Optional</span>[<span class="built_in">str</span>] = <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<h4 id="定义fastapi应用并管理应用的生命周期"><a href="#定义fastapi应用并管理应用的生命周期" class="headerlink" title="定义fastapi应用并管理应用的生命周期"></a>定义fastapi应用并管理应用的生命周期</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义了一个异步函数lifespan，它接收一个FastAPI应用实例app作为参数。这个函数将管理应用的生命周期，包括启动和关闭时的操作</span></span><br><span class="line"><span class="comment"># 函数在应用启动时执行一些初始化操作，如加载上下文数据、以及初始化问题生成器</span></span><br><span class="line"><span class="comment"># 函数在应用关闭时执行一些清理操作</span></span><br><span class="line"><span class="comment"># @asynccontextmanager 装饰器用于创建一个异步上下文管理器，它允许你在 yield 之前和之后执行特定的代码块，分别表示启动和关闭时的操作</span></span><br><span class="line"><span class="meta">@asynccontextmanager</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">lifespan</span>(<span class="params">app: FastAPI</span>):</span><br><span class="line">    <span class="comment"># 启动时执行</span></span><br><span class="line">    <span class="comment"># 申明引用全局变量，在函数中被初始化，并在整个应用中使用</span></span><br><span class="line">    <span class="keyword">global</span> graph</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        logger.info(<span class="string">&quot;正在初始化模型、定义Graph...&quot;</span>)</span><br><span class="line">        <span class="comment">#（1）初始化LLM</span></span><br><span class="line">        llm = get_llm(llm_type)</span><br><span class="line">        <span class="comment">#（2）定义Graph</span></span><br><span class="line">        graph = create_graph(llm)</span><br><span class="line">        <span class="comment">#（3）将Graph可视化图保存</span></span><br><span class="line">        save_graph_visualization(graph)</span><br><span class="line">        logger.info(<span class="string">&quot;初始化完成&quot;</span>)</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        logger.error(<span class="string">f&quot;初始化过程中出错: <span class="subst">&#123;<span class="built_in">str</span>(e)&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="comment"># raise 关键字重新抛出异常，以确保程序不会在错误状态下继续运行</span></span><br><span class="line">        <span class="keyword">raise</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># yield 关键字将控制权交还给FastAPI框架，使应用开始运行</span></span><br><span class="line">    <span class="comment"># 分隔了启动和关闭的逻辑。在yield 之前的代码在应用启动时运行，yield 之后的代码在应用关闭时运行</span></span><br><span class="line">    <span class="keyword">yield</span></span><br><span class="line">    <span class="comment"># 关闭时执行</span></span><br><span class="line">    logger.info(<span class="string">&quot;正在关闭...&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># lifespan参数用于在应用程序生命周期的开始和结束时执行一些初始化或清理工作</span></span><br><span class="line">app = FastAPI(lifespan=lifespan)</span><br></pre></td></tr></table></figure>
<h4 id="langgraph核心逻辑"><a href="#langgraph核心逻辑" class="headerlink" title="langgraph核心逻辑"></a>langgraph核心逻辑</h4><p>创建langgraph</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义chatbot的状态</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">State</span>(<span class="title class_ inherited__">TypedDict</span>):</span><br><span class="line">    messages: Annotated[<span class="built_in">list</span>, add_messages]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建和配置chatbot的状态图</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_graph</span>(<span class="params">llm</span>) -&gt; StateGraph:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="comment"># 构建graph</span></span><br><span class="line">        <span class="comment">#创建一个 StateGraph 的实例，并将其配置为使用 State 类作为其状态管理的数据模型</span></span><br><span class="line">        graph_builder = StateGraph(State)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义chatbot的node</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">chatbot</span>(<span class="params">state: State</span>) -&gt; <span class="built_in">dict</span>:</span><br><span class="line">            <span class="comment"># 处理当前状态并返回 LLM 响应</span></span><br><span class="line">            <span class="keyword">return</span> &#123;<span class="string">&quot;messages&quot;</span>: [llm.invoke(state[<span class="string">&quot;messages&quot;</span>])]&#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 配置graph</span></span><br><span class="line">        <span class="comment">#第二个参数 chatbot ：这是一个可调用对象（通常是一个函数或方法），它定义了当执行流程到达这个名为 &quot;chatbot&quot; 的节点时，应该执行什么操作。</span></span><br><span class="line">        graph_builder.add_node(<span class="string">&quot;chatbot&quot;</span>, chatbot)</span><br><span class="line">        graph_builder.add_edge(START, <span class="string">&quot;chatbot&quot;</span>)</span><br><span class="line">        graph_builder.add_edge(<span class="string">&quot;chatbot&quot;</span>, END)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 这里使用内存存储 也可以持久化到数据库</span></span><br><span class="line">        memory = MemorySaver()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 编译生成graph并返回</span></span><br><span class="line">        <span class="comment">#checkpointer 参数将 memory 实例传递给编译过程，使得图能够管理其状态的保存和加载。编译后的图对象被返回，这个对象可以被调用来运行聊天机器人。</span></span><br><span class="line">        <span class="keyword">return</span> graph_builder.<span class="built_in">compile</span>(checkpointer=memory)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        <span class="keyword">raise</span> RuntimeError(<span class="string">f&quot;Failed to create graph: <span class="subst">&#123;<span class="built_in">str</span>(e)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>可视化langgraph节点</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 将构建的graph可视化保存为 PNG 文件</span><br><span class="line">def save_graph_visualization(graph: StateGraph, filename: str = &quot;graph.png&quot;) -&gt; None:</span><br><span class="line">    try:</span><br><span class="line">        with open(filename, &quot;wb&quot;) as f:</span><br><span class="line">            f.write(graph.get_graph().draw_mermaid_png())</span><br><span class="line">        logger.info(f&quot;Graph visualization saved as &#123;filename&#125;&quot;)</span><br><span class="line">    except IOError as e:</span><br><span class="line">        logger.info(f&quot;Warning: Failed to save graph visualization: &#123;str(e)&#125;&quot;)</span><br></pre></td></tr></table></figure>
<h4 id="封装接口"><a href="#封装接口" class="headerlink" title="封装接口"></a>封装接口</h4><p>包含流式输出与非流式输出的处理</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 封装POST请求接口，与大模型进行问答</span></span><br><span class="line"><span class="meta">@app.post(<span class="params"><span class="string">&quot;/v1/chat/completions&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">chat_completions</span>(<span class="params">request: ChatCompletionRequest</span>):</span><br><span class="line">    <span class="comment"># 判断初始化是否完成</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> graph:</span><br><span class="line">        logger.error(<span class="string">&quot;服务未初始化&quot;</span>)</span><br><span class="line">        <span class="keyword">raise</span> HTTPException(status_code=<span class="number">500</span>, detail=<span class="string">&quot;服务未初始化&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        logger.info(<span class="string">f&quot;收到聊天完成请求: <span class="subst">&#123;request&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        query_prompt = request.messages[-<span class="number">1</span>].content</span><br><span class="line">        logger.info(<span class="string">f&quot;用户问题是: <span class="subst">&#123;query_prompt&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        config = &#123;<span class="string">&quot;configurable&quot;</span>: &#123;<span class="string">&quot;thread_id&quot;</span>: request.userId+<span class="string">&quot;@@&quot;</span>+request.conversationId&#125;&#125;</span><br><span class="line">        logger.info(<span class="string">f&quot;用户当前会话信息: <span class="subst">&#123;config&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        prompt_template_system = PromptTemplate.from_file(PROMPT_TEMPLATE_TXT_SYS)</span><br><span class="line">        prompt_template_user = PromptTemplate.from_file(PROMPT_TEMPLATE_TXT_USER)</span><br><span class="line">        prompt = [</span><br><span class="line">            &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: prompt_template_system.template&#125;,</span><br><span class="line">            &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: prompt_template_user.template.<span class="built_in">format</span>(query=query_prompt)&#125;</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 处理流式响应</span></span><br><span class="line">        <span class="keyword">if</span> request.stream:</span><br><span class="line">            <span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">generate_stream</span>():</span><br><span class="line">                chunk_id = <span class="string">f&quot;chatcmpl-<span class="subst">&#123;uuid.uuid4().<span class="built_in">hex</span>&#125;</span>&quot;</span></span><br><span class="line">                <span class="keyword">async</span> <span class="keyword">for</span> message_chunk, metadata <span class="keyword">in</span> graph.astream(&#123;<span class="string">&quot;messages&quot;</span>: prompt&#125;, config, stream_mode=<span class="string">&quot;messages&quot;</span>):</span><br><span class="line">                    chunk = message_chunk.content</span><br><span class="line">                    logger.info(<span class="string">f&quot;chunk: <span class="subst">&#123;chunk&#125;</span>&quot;</span>)</span><br><span class="line">                    <span class="comment"># 在处理过程中产生每个块</span></span><br><span class="line">                    <span class="keyword">yield</span> <span class="string">f&quot;data: <span class="subst">&#123;json.dumps(&#123;<span class="string">&#x27;id&#x27;</span>: chunk_id,<span class="string">&#x27;object&#x27;</span>: <span class="string">&#x27;chat.completion.chunk&#x27;</span>,<span class="string">&#x27;created&#x27;</span>: <span class="built_in">int</span>(time.time()),<span class="string">&#x27;choices&#x27;</span>: [&#123;<span class="string">&#x27;index&#x27;</span>: <span class="number">0</span>,<span class="string">&#x27;delta&#x27;</span>: &#123;<span class="string">&#x27;content&#x27;</span>: chunk&#125;</span>,&#x27;finish_reason&#x27;: None&#125;]&#125;)&#125;\n\n&quot;</span></span><br><span class="line">                <span class="comment"># 流结束的最后一块</span></span><br><span class="line">                <span class="keyword">yield</span> <span class="string">f&quot;data: <span class="subst">&#123;json.dumps(&#123;<span class="string">&#x27;id&#x27;</span>: chunk_id,<span class="string">&#x27;object&#x27;</span>: <span class="string">&#x27;chat.completion.chunk&#x27;</span>,<span class="string">&#x27;created&#x27;</span>: <span class="built_in">int</span>(time.time()),<span class="string">&#x27;choices&#x27;</span>: [&#123;<span class="string">&#x27;index&#x27;</span>: <span class="number">0</span>,<span class="string">&#x27;delta&#x27;</span>: &#123;&#125;</span>,&#x27;finish_reason&#x27;: &#x27;stop&#x27;&#125;]&#125;)&#125;\n\n&quot;</span></span><br><span class="line">            <span class="comment"># 返回fastapi.responses中StreamingResponse对象</span></span><br><span class="line">            <span class="keyword">return</span> StreamingResponse(generate_stream(), media_type=<span class="string">&quot;text/event-stream&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 处理非流式响应处理</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                events = graph.stream(&#123;<span class="string">&quot;messages&quot;</span>: prompt&#125;, config)</span><br><span class="line">                <span class="keyword">for</span> event <span class="keyword">in</span> events:</span><br><span class="line">                    <span class="keyword">for</span> value <span class="keyword">in</span> event.values():</span><br><span class="line">                        result = value[<span class="string">&quot;messages&quot;</span>][-<span class="number">1</span>].content</span><br><span class="line">            <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">                logger.info(<span class="string">f&quot;Error processing response: <span class="subst">&#123;<span class="built_in">str</span>(e)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">            formatted_response = <span class="built_in">str</span>(format_response(result))</span><br><span class="line">            logger.info(<span class="string">f&quot;格式化的搜索结果: <span class="subst">&#123;formatted_response&#125;</span>&quot;</span>)</span><br><span class="line">			<span class="comment">#封装响应</span></span><br><span class="line">            response = ChatCompletionResponse(</span><br><span class="line">                choices=[</span><br><span class="line">                    ChatCompletionResponseChoice(</span><br><span class="line">                        index=<span class="number">0</span>,</span><br><span class="line">                        message=Message(role=<span class="string">&quot;assistant&quot;</span>, content=formatted_response),</span><br><span class="line">                        finish_reason=<span class="string">&quot;stop&quot;</span></span><br><span class="line">                    )</span><br><span class="line">                ]</span><br><span class="line">            )</span><br><span class="line">            logger.info(<span class="string">f&quot;发送响应内容: \n<span class="subst">&#123;response&#125;</span>&quot;</span>)</span><br><span class="line">            <span class="comment"># 返回fastapi.responses中JSONResponse对象</span></span><br><span class="line">            <span class="comment"># model_dump()方法通常用于将Pydantic模型实例的内容转换为一个标准的Python字典，以便进行序列化</span></span><br><span class="line">            <span class="keyword">return</span> JSONResponse(content=response.model_dump())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        logger.error(<span class="string">f&quot;处理聊天完成时出错:\n\n <span class="subst">&#123;<span class="built_in">str</span>(e)&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">raise</span> HTTPException(status_code=<span class="number">500</span>, detail=<span class="built_in">str</span>(e))</span><br></pre></td></tr></table></figure>
<h3 id="langgraph的短期记忆与长期记忆"><a href="#langgraph的短期记忆与长期记忆" class="headerlink" title="langgraph的短期记忆与长期记忆"></a>langgraph的短期记忆与长期记忆</h3><p>LangGraph支持两种对于构建对话代理至关重要的内存类型：</p>
<ul>
<li><strong><a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langgraph/agents/memory/#short-term-memory">短期内存</a></strong>：通过在会话中维护消息历史来跟踪正在进行的对话。</li>
<li><strong><a target="_blank" rel="noopener" href="https://github.langchain.ac.cn/langgraph/agents/memory/#long-term-memory">长期内存</a></strong>：在不同会话之间存储用户特定或应用程序级别的数据。</li>
</ul>
<p><img src="/2025/07/14/%E5%AD%A6%E4%B9%A0/ai%E6%A1%86%E6%9E%B6/LangGraphChatBot/image-20250715094855646.png" alt="image-20250715094855646"></p>
<p>在LangGraph中</p>
<ul>
<li><em>短期内存</em>也称为<strong>线程级内存</strong>。</li>
<li><em>长期内存</em>也称为<strong>跨线程内存</strong>。</li>
</ul>
<h3 id="教程地址"><a href="#教程地址" class="headerlink" title="教程地址"></a>教程地址</h3><p><a target="_blank" rel="noopener" href="https://github.com/NanGePlus/LangGraphChatBot">NanGePlus/LangGraphChatBot: 使用LangGraph+DeepSeek-R1+FastAPI+Gradio实现一个带有记忆功能的流量包推荐智能客服web端用例,同时也支持gpt大模型、国产大模型(OneApi方式)、Ollama本地开源大模型、阿里通义千问大模型</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1m89NYKE2J/?vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">LangGraph+deepseek-r1+FastAPI+Gradio实现拥有记忆的流量包推荐智能客服web端用例,同时也支持gpt、国产大模型、Ollama_哔哩哔哩_bilibili</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/07/13/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/prompt%20Engineering%E4%B8%8Econtext%20Engineering/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/07/13/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/prompt%20Engineering%E4%B8%8Econtext%20Engineering/" class="post-title-link" itemprop="url">prompt Engineering与context Engineering</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2025-07-13 00:00:00 / 修改时间：13:36:34" itemprop="dateCreated datePublished" datetime="2025-07-13T00:00:00+08:00">2025-07-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AE%9E%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">实习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/" itemprop="url" rel="index"><span itemprop="name">晨晟智控</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="prompt-Engineering"><a href="#prompt-Engineering" class="headerlink" title="prompt Engineering"></a>prompt Engineering</h3><p>Prompt Engineering是与大型语言模型（LLM）交互的基础，其核心在于精心设计输入内容，以引导模型生成期望的输出。</p>
<p>尽管 Prompt Engineering 至关重要，但对于构建稳健、可用于生产环境的系统而言，它存在固有的局限性：</p>
<ul>
<li><p><strong>脆弱性&amp;不可复现性：</strong> 提示中微小的措辞变化可能导致输出结果的巨大差异，使得这一过程更像是一种依赖反复试错的“艺术”，而非可复现的“科学” 。</p>
</li>
<li><p><strong>扩展性差：</strong> 手动、迭代地优化提示的过程，在面对大量用户、多样化用例和不断出现的边缘情况时，难以有效扩展 。</p>
</li>
<li><p><strong>用户负担：</strong> 这种方法将精心构建一套详尽指令的负担完全压在了用户身上，对于需要自主运行、或处理高并发请求的系统而言是不切实际的 。</p>
</li>
<li><p><strong>无状态性：</strong> Prompt Engineering 本质上是为单轮、“一次性”的交互而设计的，难以处理需要记忆和状态管理的长对话或多步骤任务 。</p>
</li>
</ul>
<h3 id="Context-Engineering"><a href="#Context-Engineering" class="headerlink" title="Context Engineering"></a>Context Engineering</h3><p><strong>Context Engineering是一门设计、构建并优化动态自动化系统的学科，旨在为大型语言模型在正确的时间、以正确的格式，提供正确的信息和工具，从而可靠、可扩展地完成复杂任务</strong> 。</p>
<p><strong>prompt 告诉模型如何思考，而 Context 则赋予模型完成工作所需的知识和工具。</strong></p>
<ul>
<li><p>Context Engineering 决定<strong>用什么内容填充 Context Window</strong> ，</p>
</li>
<li><p>Prompt Engineering 则负责优化<strong>窗口内的具体指令</strong> 。</p>
</li>
</ul>
<h3 id="Context-Engineering-的基石：RAG（Retrieval-Augmented-Generation）"><a href="#Context-Engineering-的基石：RAG（Retrieval-Augmented-Generation）" class="headerlink" title="Context Engineering 的基石：RAG（Retrieval-Augmented Generation）"></a>Context Engineering 的基石：RAG（Retrieval-Augmented Generation）</h3><p>本部分将阐述检索增强生成（RAG）作为实现 Context Engineering 的主要架构模式。</p>
<h4 id="解决LLM的核心弱点"><a href="#解决LLM的核心弱点" class="headerlink" title="解决LLM的核心弱点"></a>解决LLM的核心弱点</h4><p>RAG直接解决了标准LLM在企业应用中存在的固有局限性：</p>
<ul>
<li><p><strong>知识冻结：</strong> LLM的知识被冻结在<strong>其训练数据的时间点</strong>。RAG通过在推理时注入实时的、最新的信息来解决这个问题 。</p>
</li>
<li><p><strong>缺乏领域专有知识：</strong> 标准LLM无法访问组织的内部私有数据。RAG则能够将LLM连接到这些内部知识库，如技术手册、政策文件等 。</p>
</li>
<li><p><strong>幻觉（Hallucination）：</strong> LLM 会不同程度上地编造事实。RAG通过将模型的回答“锚定”在可验证的、检索到的证据上，提高事实的准确性和可信度 。</p>
</li>
</ul>
<h4 id="RAG工作流"><a href="#RAG工作流" class="headerlink" title="RAG工作流"></a>RAG工作流</h4><ol>
<li><p><strong>索引（离线阶段）：</strong> 在这个阶段，系统会处理外部知识源。文档被加载、分割成更小的 chunks，然后通过Embedding Model 转换为向量表示，并最终存储在专门的向量数据库中以备检索 。</p>
</li>
<li><p><strong>推理（在线阶段）：</strong> 当用户提出请求时，系统执行以下步骤：</p>
<ol>
<li><strong>检索（Retrieve）：</strong> 将用户的查询同样转换为向量，然后在向量数据库中进行相似性搜索，找出与查询最相关的文档块。</li>
<li><strong>增强（Augment）：</strong> 将检索到的这些文档块与原始的用户查询、系统指令等结合起来，构建一个内容丰富的、增强的最终提示。</li>
<li><strong>生成（Generate）：</strong> 将这个增强后的提示输入给LLM，LLM会基于提供的上下文生成一个有理有据的回答 。</li>
</ol>
</li>
</ol>
<h3 id="Context-工程化：如何判断和提取哪些内容应该进入上下文？"><a href="#Context-工程化：如何判断和提取哪些内容应该进入上下文？" class="headerlink" title="Context 工程化：如何判断和提取哪些内容应该进入上下文？"></a>Context 工程化：如何判断和提取哪些内容应该进入上下文？</h3><h4 id="1-chunking"><a href="#1-chunking" class="headerlink" title="1.chunking"></a>1.chunking</h4><p>文本分块（Chunking）是RAG流程中最关键也最容易被忽视的一步。其目标是创建在语义上自成一体的文本块。</p>
<h4 id="2-Reranking"><a href="#2-Reranking" class="headerlink" title="2.Reranking"></a>2.Reranking</h4><p>为了平衡检索的速度和准确性，业界普遍采用两阶段检索流程。</p>
<ul>
<li><p><strong>两阶段流程：</strong></p>
<ul>
<li><strong>第一阶段（召回）：</strong> 使用一个快速、高效的检索器（如基于 bi-encoder 的向量搜索或BM25等词法搜索）进行广泛撒网，召回一个较大的候选文档集（例如，前100个） 。</li>
<li><strong>第二阶段（精排/重排序）：</strong> 使用一个更强大但计算成本更高的模型，对这个较小的候选集进行重新评估，以识别出最相关的少数几个文档（例如，前5个） 。</li>
</ul>
</li>
<li><p><strong>Cross-Encoder：</strong> 交叉编码器之所以在重排序阶段表现优越，是因为它与双编码器的工作方式不同。双编码器独立地为查询和文档生成嵌入向量，然后计算它们的相似度。而交叉编码器则是将查询和文档<strong>同时</strong>作为输入，让模型在内部通过 Attention Mechanism 对二者进行深度交互。这使得模型能够捕捉到更细微的语义关系，从而给出更准确的相关性评分 。</p>
</li>
<li><p><strong>实际影响：</strong> 重排序显著提高了最终送入LLM的上下文质量，从而产出更准确、幻觉更少的答案。在金融、法律等高风险领域，重排序被认为是必不可少而非可选的步骤 。</p>
</li>
</ul>
<h4 id="3-优化上下文窗口：压缩与摘要"><a href="#3-优化上下文窗口：压缩与摘要" class="headerlink" title="3.优化上下文窗口：压缩与摘要"></a>3.优化上下文窗口：压缩与摘要</h4><p>本节详细介绍用于主动管理上下文的技术，确保最有价值的信息被优先呈现。</p>
<ul>
<li><p><strong>上下文压缩的目标：</strong> 缩短检索到的文档列表和/或精简单个文档的内容，只将<strong>最相关的信息传递给LLM</strong>。这能有效降低API调用成本、减少延迟，并缓解 Lost in the Middle 的问题 。</p>
</li>
<li><p><strong>压缩方法：</strong></p>
<ul>
<li><strong>过滤式压缩：</strong> 这类方法决定是保留还是丢弃整个检索到的文档。<ul>
<li><strong>LLMChainFilter：</strong> 利用一个LLM对每个文档的相关性做出简单的“是/否”判断 。</li>
<li><strong>EmbeddingsFilter：</strong> 更经济快速的方法，根据文档嵌入与查询嵌入的余弦相似度来过滤文档 。</li>
</ul>
</li>
<li><strong>内容提取式压缩：</strong> 这类方法会直接修改文档内容。<ul>
<li><strong>LLMChainExtractor：</strong> 遍历每个文档，并使用LLM从中提取仅与查询相关的句子或陈述 。</li>
</ul>
</li>
<li><strong>用 top N 代替压缩：</strong> 像LLMListwiseRerank这样的技术，使用LLM对检索到的文档进行重排序，并只返回排名最高的N个，从而起到高质量过滤器的作用 。</li>
</ul>
</li>
<li><p><strong>作为压缩策略的摘要：</strong> 对于非常长的文档或冗长的对话历史，可以利用LLM生成摘要。这些摘要随后被注入上下文，既保留了关键信息，又大幅减少了 Token 数量。这是在长时程运行的智能体中管理上下文的关键技术 。</p>
</li>
</ul>
<h3 id="智能体架构中的数据流与工作流编排"><a href="#智能体架构中的数据流与工作流编排" class="headerlink" title="智能体架构中的数据流与工作流编排"></a>智能体架构中的数据流与工作流编排</h3><h4 id="工作流（Workflow）-vs-智能体（Agent）"><a href="#工作流（Workflow）-vs-智能体（Agent）" class="headerlink" title="工作流（Workflow） vs. 智能体（Agent）"></a>工作流（Workflow） vs. 智能体（Agent）</h4><ul>
<li><p><strong>工作流（Workflows）</strong></p>
<ul>
<li>指的是LLM和工具通过<strong>预定义的代码路径</strong>进行编排的系统。在这种模式下，数据流动的路径是固定的、由开发者明确设计的，类似于上世纪流行的“专家系统”。例如，“第一步：分析用户邮件；第二步：根据分析结果在日历中查找空闲时段；第三步：起草会议邀请邮件”。这种模式确定性高，易于调试和控制，非常适合有明确业务流程的场景（如风控需求高、数据敏感、安全等级要求）。</li>
</ul>
</li>
<li><p><strong>智能体（Agents）</strong></p>
<ul>
<li>指的是LLM<strong>动态地指导</strong>自己的流程和工具使用，自主控制如何完成任务的系统。在这种模式下，数据流动的路径不是预先固定的，而是由LLM在每一步根据当前情况和目标动态决定的。这种模式灵活性高，能处理开放式问题，但可控性和可预测性较低 。</li>
</ul>
</li>
</ul>
<p>复杂的智能体通常是这两种模式的混合体，在宏观层面遵循一个预定义的工作流，但在某些节点内部，又赋予LLM一定的自主决策权。管理这一切的核心，我们称之为<strong>编排层（Orchestration Layer）</strong> 。</p>
<h4 id="核心架构：预定义数据流的实现"><a href="#核心架构：预定义数据流的实现" class="headerlink" title="核心架构：预定义数据流的实现"></a><strong>核心架构：预定义数据流的实现</strong></h4><ol>
<li><p><strong>链式工作流（Prompt Chaining）</strong></p>
</li>
<li><p><strong>路由工作流（Routing)</strong></p>
</li>
<li><p><strong>编排器-工作者模式（Orchestrator-Workers）</strong></p>
</li>
</ol>
<h4 id="框架与工具"><a href="#框架与工具" class="headerlink" title="框架与工具"></a>框架与工具</h4><p>上述的架构和机制并非凭空存在，而是通过具体的开发框架实现的。其中，LangGraph作为LangChain的扩展，为构建具有显式数据流的智能体系统提供了强大的工具集。</p>
<p><strong>LangGraph：用图（Graph）定义工作流（Workflow）</strong></p>
<p>LangGraph的核心思想是将智能体应用构建成一个<strong>状态图（State Graph）</strong> 。这个图由节点和边组成，清晰地定义了数据如何在不同模块间流动</p>
<ul>
<li><p><strong>状态（State）：</strong> 这是整个图的核心，一个所有节点共享的中央数据对象。</p>
<ul>
<li>你可以把它想象成一个“数据总线”或共享内存。开发者需要预先定义State的结构，每个节点在执行时都可以读取和更新这个State对象 。</li>
</ul>
</li>
<li><p><strong>节点（Nodes）：</strong> 代表工作流中的一个计算单元或一个步骤。</p>
<ul>
<li>每个节点通常是一个Python函数，它接收当前的State作为输入，执行特定任务（如调用LLM、执行工具、处理数据），然后返回对State的更新 。</li>
</ul>
</li>
<li><p><strong>边（Edges）</strong>： 连接节点，定义了工作流的路径，即数据在State更新后应该流向哪个节点。</p>
<ul>
<li><strong>简单边（Simple Edges）：</strong> 定义了固定的、无条件的流向，用于实现链式工作流 。</li>
<li><strong>条件边（Conditional Edges）：</strong> 用于实现路由逻辑。它会根据一个函数的输出来决定接下来应该走向哪个节点，从而实现流程的分支 。</li>
</ul>
</li>
<li><p><strong>检查点（Checkpointer）：</strong> LangGraph提供了持久化机制，可以在每一步执行后自动保存State的状态。这对于构建需要长期记忆、可中断和恢复、或需要 Human-in-the-Loop 的复杂业务流程至关重要 。</p>
</li>
</ul>
<p>复杂业务流程的AI智能体，其核心挑战已从单纯优化信息检索（如RAG）或提示词，转向了对内部<strong>工作流和数据流的精心设计与编排</strong>。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/07/11/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/rag%E8%AF%84%E4%BC%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/07/11/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/rag%E8%AF%84%E4%BC%B0/" class="post-title-link" itemprop="url">rag与检索评估</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-07-11 00:00:00" itemprop="dateCreated datePublished" datetime="2025-07-11T00:00:00+08:00">2025-07-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-08-18 09:54:44" itemprop="dateModified" datetime="2025-08-18T09:54:44+08:00">2025-08-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AE%9E%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">实习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/" itemprop="url" rel="index"><span itemprop="name">晨晟智控</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="rag评估的指标"><a href="#rag评估的指标" class="headerlink" title="rag评估的指标"></a>rag评估的指标</h3><h4 id="忠诚度Faithfulness"><a href="#忠诚度Faithfulness" class="headerlink" title="忠诚度Faithfulness"></a>忠诚度Faithfulness</h4><p>Faithfulness：衡量生成答案与给定上下文之间的事实一致性。忠实度得分是基于答案和检索到的上下文<br>计算出来的，答案的评分范围在0到1之间，分数越高越好。</p>
<p><img src="/2025/07/11/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/rag%E8%AF%84%E4%BC%B0/image-20250711154457878.png" alt="image-20250711154457878"></p>
<p>计算方式：将大模型给出的答案进行切片，检索给出的上下文，计算这些切片是否在上下文中</p>
<p><img src="/2025/07/11/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/rag%E8%AF%84%E4%BC%B0/image-20250711155257239.png" alt="image-20250711155257239"></p>
<h4 id="答案相关性Answerrelevance"><a href="#答案相关性Answerrelevance" class="headerlink" title="答案相关性Answerrelevance"></a>答案相关性Answerrelevance</h4><p>Answerrelevance：答案相关性的评估指标旨在评估生成的答案与给定提示的相关程度。如果答案不完<br>整或包含冗余信息，则会被赋予较低的分数。这个指标使用问题和答案来计算，其值介于0到1之间，得<br>分越高表明答案的相关性越好</p>
<p><img src="/2025/07/11/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/rag%E8%AF%84%E4%BC%B0/image-20250711155128553.png" alt="image-20250711155128553"></p>
<p>计算方式：根据答案生成多个问题，然后计算生成的答案与原答案的余弦相似度，再取平均</p>
<p><img src="/2025/07/11/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/rag%E8%AF%84%E4%BC%B0/image-20250711155407518.png" alt="image-20250711155407518"></p>
<h4 id="上下文精确度ContextPrecision"><a href="#上下文精确度ContextPrecision" class="headerlink" title="上下文精确度ContextPrecision"></a>上下文精确度ContextPrecision</h4><p>ContextPrecision：上下文精确度衡量上下文中所有相关的真实信息是否被排在了较高的位置。理想情<br>况下，所有相关的信息块都应该出现在排名的最前面。这个指标是根据问题和上下文来计算的，数值范<br>围在0到1之间，分数越高表示精确度越好。</p>
<script type="math/tex; mode=display">
\text{Context Precision} = \frac{\sum_{k=1}^{K} (\text{rel}(k) \times \frac{\text{Precision@k}}{\text{Ideal Precision@k}})}{\text{Total Relevant Documents}}</script><ul>
<li><code>K</code>：检索返回的文档总数（如 top-5）</li>
<li><code>rel(k)</code>：第 <code>k</code> 个文档是否相关（相关=1，无关=0）</li>
<li><code>Precision@k</code>：前 <code>k</code> 个文档的精确率（相关文档数 / k）</li>
<li><code>Ideal Precision@k</code>：理想情况下前 <code>k</code> 个文档的精确率（假设所有相关文档都排在最前面）</li>
</ul>
<h4 id="上下文召回率ContextRecall"><a href="#上下文召回率ContextRecall" class="headerlink" title="上下文召回率ContextRecall"></a>上下文召回率ContextRecall</h4><p>ContextRecall：用来衡量检索到的上下文与被视为事实真相的标注答案的一致性程度。它根据事实真相<br>和检索到的上下文来计算，数值范围在0到1之间，数值越高表示性能越好。<br>为了从事实真相的答案中估计上下文召回率，需要分析答案中的每个句子是否可以归因于检索到的<br>上下文。在理想情况下，事实真相答案中的所有句子都应该能够对应到检索到的上下文中。</p>
<script type="math/tex; mode=display">
\text{Context Recall} = \frac{|\{\text{返回的相关文档}\} \cap \{\text{标准相关文档}\}|}{|\{\text{标准相关文档}\}|}</script><p>计算方式：上下文是否包括了标准答案的内容</p>
<h3 id="检索性能的评估"><a href="#检索性能的评估" class="headerlink" title="检索性能的评估"></a>检索性能的评估</h3><h4 id="平均倒数排名（Mean-Reciprocal-Rank-MRR）"><a href="#平均倒数排名（Mean-Reciprocal-Rank-MRR）" class="headerlink" title="平均倒数排名（Mean Reciprocal Rank, MRR）"></a>平均倒数排名（Mean Reciprocal Rank, MRR）</h4><p><strong>平均倒数排名（Mean Reciprocal Rank, MRR）</strong> 是一种常用于评估信息检索系统、推荐系统或问答系统性能的评价指标。它特别适用于“每个查询只有一个正确答案”或“我们只关心第一个正确结果”的场景。</p>
<ul>
<li><p><strong>倒数排名（Reciprocal Rank, RR）</strong>：对于一个查询，如果第一个正确答案出现在排序结果的第 $ k $ 位，那么它的倒数排名为：</p>
<script type="math/tex; mode=display">
RR = \frac{1}{k}</script><p>如果没有正确答案，则 $ RR = 0 $。</p>
</li>
<li><p><strong>平均倒数排名（MRR）</strong>：对多个查询的倒数排名取平均值：</p>
<script type="math/tex; mode=display">
MRR = \frac{1}{|Q|} \sum_{i=1}^{|Q|} \frac{1}{\text{rank}_i}</script><p>其中：</p>
<ul>
<li>$ |Q| $ 是查询的总数，</li>
<li>$ \text{rank}_i $ 是第 $ i $ 个查询中第一个正确答案的排名（位置）。</li>
</ul>
</li>
</ul>
<p><img src="/2025/07/11/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/rag%E8%AF%84%E4%BC%B0/image-20250818093714324.png" alt="image-20250818093714324"></p>
<h4 id="平均精确率均值（Mean-Average-Precision-MAP）"><a href="#平均精确率均值（Mean-Average-Precision-MAP）" class="headerlink" title="平均精确率均值（Mean Average Precision, MAP）"></a>平均精确率均值（Mean Average Precision, MAP）</h4><p><strong>MAP（Mean Average Precision）</strong> 是对多个查询或样本的 <strong>平均精确率（Average Precision, AP）</strong> 取平均，用来衡量排序结果的相关性质量。它综合考虑了：</p>
<ul>
<li>排序中相关结果的数量（召回）</li>
<li>相关结果在排序中的位置（越靠前越好）</li>
</ul>
<p><strong>平均精确率（Average Precision, AP）</strong></p>
<p>AP 是对一个查询而言的，衡量该查询下所有相关文档在排序中的整体表现。</p>
<blockquote>
<p><strong>直观理解</strong>：AP 是“在每个相关文档被检索到时”的精确率的平均值。</p>
</blockquote>
<p>公式定义：</p>
<script type="math/tex; mode=display">
AP = \frac{\sum_{k=1}^{n} (P(k) \times \text{rel}(k))}{\text{总相关文档数}}</script><p>其中：</p>
<ul>
<li>$ P(k) $：在第 $ k $ 个位置的精确率（即前 k 个结果中有多少是相关的）</li>
<li>$ \text{rel}(k) $：第 $ k $ 个文档是否相关（1 表示相关，0 表示不相关）</li>
</ul>
<blockquote>
<p>也就是说，只在相关文档出现的位置计算并累加精确率，最后除以总相关文档数。</p>
</blockquote>
<p><strong>平均精确率均值（MAP）</strong></p>
<p>将所有查询的 AP 求平均：</p>
<script type="math/tex; mode=display">
MAP = \frac{1}{|Q|} \sum_{i=1}^{|Q|} AP_i</script><p>其中：</p>
<ul>
<li>$ |Q| $：查询总数</li>
<li>$ AP_i $：第 $ i $ 个查询的平均精确率</li>
</ul>
<p><img src="/2025/07/11/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/rag%E8%AF%84%E4%BC%B0/image-20250818094149682.png" alt="image-20250818094149682"></p>
<h4 id="归一化折损累积增益（Normalized-Discounted-Cumulative-Gain-nDCG）"><a href="#归一化折损累积增益（Normalized-Discounted-Cumulative-Gain-nDCG）" class="headerlink" title="归一化折损累积增益（Normalized Discounted Cumulative Gain, nDCG）"></a>归一化折损累积增益（Normalized Discounted Cumulative Gain, nDCG）</h4><p>nDCG 的核心思想是：</p>
<ol>
<li><strong>高相关性的文档更有价值</strong></li>
<li><strong>排在前面的结果比排在后面的价值更高</strong>（位置越靠前，权重越大）</li>
<li>将系统的得分与“理想排序”对比，进行归一化，便于跨查询比较</li>
</ol>
<p>1.<strong>累积增益（Cumulative Gain, CG）</strong></p>
<p>CG 是前 $ k $ 个结果的相关性评分之和，<strong>不考虑位置</strong>。</p>
<script type="math/tex; mode=display">
CG@k = \sum_{i=1}^{k} rel_i</script><p>其中 $ rel_i $ 是第 $ i $ 个文档的相关性评分。</p>
<blockquote>
<p>❌ 缺点：CG 不关心排序顺序。无论相关文档排第1还是第10，CG 都一样。</p>
</blockquote>
<ol>
<li><strong>折损累积增益（Discounted Cumulative Gain, DCG）</strong></li>
</ol>
<p>DCG 引入“位置折损”：越靠后的结果，其贡献被“打折”。</p>
<p>常用公式（两种形式，第二种更常见）：</p>
<script type="math/tex; mode=display">
DCG@k = \sum_{i=1}^{k} \frac{rel_i}{\log_2(i+1)} \quad \text{或}</script><script type="math/tex; mode=display">
DCG@k = rel_1 + \sum_{i=2}^{k} \frac{rel_i}{\log_2(i)} \quad \text{(更常用)}</script><blockquote>
<p>💡 解释：第1个位置不打折，第2个位置除以 $ \log_2(2) = 1 $，第3个位置除以 $ \log_2(3) \approx 1.58 $，相当于打了约 63% 的折扣。</p>
</blockquote>
<p>这样，<strong>相关文档越早出现，DCG 越高</strong>。</p>
<ol>
<li><strong>理想折损累积增益（Ideal DCG, IDCG）</strong></li>
</ol>
<p>IDCG 是在<strong>理想排序下</strong>（所有相关文档按相关性从高到低排列）的 DCG 值。</p>
<script type="math/tex; mode=display">
IDCG@k = \text{将前 } k \text{ 个最相关文档按最优顺序排列时的 } DCG</script><p>IDCG 是当前查询下 DCG 的理论最大值。</p>
<ol>
<li><strong>归一化折损累积增益（nDCG@k）</strong></li>
</ol>
<script type="math/tex; mode=display">
nDCG@k = \frac{DCG@k}{IDCG@k}</script><blockquote>
<p>✅ nDCG 的取值范围是 $[0, 1]$：</p>
<ul>
<li>1.0：排序完全理想</li>
<li>接近 1：排序质量高</li>
<li>接近 0：排序很差</li>
</ul>
</blockquote>
<p><img src="/2025/07/11/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/rag%E8%AF%84%E4%BC%B0/image-20250818095351579.png" alt="image-20250818095351579"></p>
<h3 id="利用RAGAS评估rag性能"><a href="#利用RAGAS评估rag性能" class="headerlink" title="利用RAGAS评估rag性能"></a>利用RAGAS评估rag性能</h3><p><a target="_blank" rel="noopener" href="https://github.com/zxj-2023/learn-rag-langchain/blob/main/RAGAS-langchian.ipynb">learn-rag-langchain/RAGAS-langchian.ipynb at main · zxj-2023/learn-rag-langchain</a></p>
<p>检索器<br>1.Contextprecision(上下文精确度)：评估检索质量。<br>2.Context Recall(上下文召回率)：衡量检索的完整性。<br>生成器<br>1.Faithfulness(忠实度)：衡量生成答案中的幻觉情况。<br>2.AnswerRelevance(答案相关性):衡量答案对问题的直接性(紧扣问题的核心)。</p>
<p>最终的RAGAS得分是以上各个指标得分的调和平均值。简而言之，这些指标用来综合评估<br>-个系统整体的性能。</p>
<h4 id="RAG的构建"><a href="#RAG的构建" class="headerlink" title="RAG的构建"></a>RAG的构建</h4><p>创建RAG文本分割、Embedding model 、 向量库存储Chroma</p>
<p>我们主要使用 <code>RecursiveCharacterTextSplitter</code> 切割文本，通过<code>OpenAIEmbeddings()</code>进行文本编码，存储到 <code>VectorStore</code>。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">from langchain.vectorstores import Chroma</span><br><span class="line">from langchain.embeddings import OpenAIEmbeddings</span><br><span class="line">from langchain.text_splitter import RecursiveCharacterTextSplitter</span><br><span class="line">from langchain_community.embeddings import DashScopeEmbeddings</span><br><span class="line">embeddings_model = DashScopeEmbeddings(</span><br><span class="line">        model=&quot;text-embedding-v2&quot;,</span><br><span class="line">        dashscope_api_key=openai.api_key,</span><br><span class="line">    )</span><br><span class="line">text_splitter = RecursiveCharacterTextSplitter(chunk_size=500)</span><br><span class="line">#进行文本分割，生成更小、更易于处理的文档块</span><br><span class="line">docs = text_splitter.split_documents(paper_docs)</span><br><span class="line"></span><br><span class="line">vectorstore = Chroma.from_documents(docs, embeddings_model)</span><br></pre></td></tr></table></figure>
<p>Chroma 向量数据库默认情况下是内存存储，这意味着数据在程序运行结束后不会保留。<br>但是，Chroma 也支持持久化存储，您可以指定一个路径将数据保存到磁盘上。这样，即使程序关闭，数据也会被保留，并在下次启动时自动加载。</p>
<h4 id="检索器的构建"><a href="#检索器的构建" class="headerlink" title="检索器的构建"></a>检索器的构建</h4><p>现在我们可以利用 <code>Chroma</code> 向量库的 <code>.as_retriever()</code> 方式进行检索，需要控制的主要参数为 <code>k</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">base_retriever = vectorstore.as_retriever(search_kwargs=&#123;&quot;k&quot; : 3&#125;)</span><br></pre></td></tr></table></figure>
<ul>
<li>ectorstore.as_retriever() : 这个方法的作用是将一个向量数据库实例（ vectorstore ）转换为 LangChain 中的一个检索器（ Retriever ）对象。检索器是 LangChain 中负责根据用户查询从数据源中获取相关文档的核心组件。</li>
<li>“k” : 这个键表示要检索的“最相似”文档的数量。在这里， “k” : 3 意味着当检索器接收到一个查询时，它将从向量存储中返回与该查询最相似的 3 个文档。这在 RAG（检索增强生成）系统中非常常见，用于限制传递给大型语言模型的上下文信息量，以提高效率和相关性。</li>
</ul>
<p>检索器的作用<br>检索器（Retriever）是一个核心组件，其主要作用是从一个数据源（如向量数据库、文档加载器等）中根据给定的查询（query）检索出相关的文档或信息。</p>
<h4 id="prompt的构建"><a href="#prompt的构建" class="headerlink" title="prompt的构建"></a>prompt的构建</h4><p>我们需要利用<code>LLM</code>对<code>Context</code> 生成一系列的问题的<code>answer</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">from langchain import PromptTemplate</span><br><span class="line"></span><br><span class="line">template = &quot;&quot;&quot;You are an assistant for question-answering tasks. </span><br><span class="line">Use the following pieces of retrieved context to answer the question. </span><br><span class="line">If you don&#x27;t know the answer, just say that you don&#x27;t know. </span><br><span class="line"></span><br><span class="line">Question: &#123;question&#125; </span><br><span class="line"></span><br><span class="line">Context: &#123;context&#125; </span><br><span class="line"></span><br><span class="line">Answer:</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">prompt = PromptTemplate(</span><br><span class="line">    template=template, </span><br><span class="line">    input_variables=[&quot;context&quot;,&quot;question&quot;]</span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line">print(prompt)</span><br></pre></td></tr></table></figure>
<h4 id="生成answer-利用LLM"><a href="#生成answer-利用LLM" class="headerlink" title="生成answer,利用LLM"></a>生成<code>answer</code>,利用LLM</h4><p>利用 <code>Runnable</code> 定义一个 <code>chain</code> 实现rag全流程。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">from langchain.schema.runnable import RunnablePassthrough</span><br><span class="line">from langchain.schema.output_parser import StrOutputParser</span><br><span class="line">from langchain_openai import ChatOpenAI</span><br><span class="line">llm = ChatOpenAI(</span><br><span class="line">    model_name=&quot;qwen-plus-2025-04-28&quot;, </span><br><span class="line">    temperature=0,</span><br><span class="line">    api_key=&quot;&quot;,</span><br><span class="line">    base_url=&quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;</span><br><span class="line">    )</span><br><span class="line">#RunnablePassthrough将输入数据原封不动地传递到输出</span><br><span class="line">#StrOutputParser() 它被用作 RAG 链的最后一步，确保最终的答案以字符串形式输出。</span><br><span class="line">rag_chain = (</span><br><span class="line">    &#123;&quot;context&quot;: base_retriever,  &quot;question&quot;: RunnablePassthrough()&#125; </span><br><span class="line">    | prompt </span><br><span class="line">    | llm</span><br><span class="line">    | StrOutputParser() </span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h4 id="创建-RAGAs-所需的数据"><a href="#创建-RAGAs-所需的数据" class="headerlink" title="创建 RAGAs 所需的数据"></a>创建 RAGAs 所需的数据</h4><p>question  Answer   contexts  ground_truths</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"># Ragas 数据集格式要求  [&#x27;question&#x27;, &#x27;answer&#x27;, &#x27;contexts&#x27;, &#x27;ground_truths&#x27;]</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">&#123;</span><br><span class="line">    &quot;question&quot;: [], &lt;-- 问题基于Context的</span><br><span class="line">    &quot;answer&quot;: [], &lt;-- 答案基于LLM生成的</span><br><span class="line">    &quot;contexts&quot;: [], &lt;-- context</span><br><span class="line">    &quot;ground_truths&quot;: [] &lt;-- 标准答案</span><br><span class="line">&#125;</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line"></span><br><span class="line">from datasets import Dataset</span><br><span class="line">#构建问题与标准答案（黄金数据集）</span><br><span class="line">questions = [&quot;What is faithfulness ?&quot;, </span><br><span class="line">             &quot;How many pages are included in the WikiEval dataset, and which years do they cover information from?&quot;,</span><br><span class="line">             &quot;Why is evaluating Retrieval Augmented Generation (RAG) systems challenging?&quot;,</span><br><span class="line">            ]</span><br><span class="line">ground_truths = [&quot;Faithfulness refers to the idea that the answer should be grounded in the given context.&quot;,</span><br><span class="line">                  &quot; To construct the dataset, we first selected 50 Wikipedia pages covering events that have happened since the start of 2022.&quot;,</span><br><span class="line">                &quot;Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself.&quot;]              </span><br><span class="line">answers = []</span><br><span class="line">contexts = []</span><br><span class="line"></span><br><span class="line"># 生成答案</span><br><span class="line">for query in questions:</span><br><span class="line">    answers.append(rag_chain.invoke(query))</span><br><span class="line">    contexts.append([docs.page_content for docs in base_retriever.get_relevant_documents(query)])</span><br><span class="line"></span><br><span class="line"># 构建数据</span><br><span class="line">data = &#123;</span><br><span class="line">    &quot;user_input&quot;: questions,</span><br><span class="line">    &quot;response&quot;: answers,</span><br><span class="line">    &quot;retrieved_contexts&quot;: contexts,</span><br><span class="line">    &quot;reference&quot;: ground_truths</span><br><span class="line">&#125;</span><br><span class="line">dataset = Dataset.from_dict(data)</span><br></pre></td></tr></table></figure>
<h4 id="使用RAGAs-进行评估"><a href="#使用RAGAs-进行评估" class="headerlink" title="使用RAGAs 进行评估"></a>使用RAGAs 进行评估</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#将评估数据转换成 Ragas 框架专用的格式 。</span><br><span class="line">from ragas import EvaluationDataset</span><br><span class="line">evaluation_dataset = EvaluationDataset.from_list(dataset)</span><br></pre></td></tr></table></figure>
<p>我们可以使用一组常用的RAG评估指标，在收集的数据集上评估我们的RAG系统。您可以选择任何模型作为评估用LLM来进行评估。<br>ragas默认使用openai的api</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from ragas.llms import LangchainLLMWrapper</span><br><span class="line">evaluator_llm = LangchainLLMWrapper(llm)</span><br></pre></td></tr></table></figure>
<p>调用</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness</span><br><span class="line">from ragas import evaluate</span><br><span class="line">result = evaluate(dataset=evaluation_dataset,metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness()],llm=evaluator_llm)</span><br><span class="line">result</span><br></pre></td></tr></table></figure>
<p><img src="/2025/07/11/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/rag%E8%AF%84%E4%BC%B0/image-20250713164037571.png" alt="image-20250713164037571"></p>
<h4 id="查看结果"><a href="#查看结果" class="headerlink" title="查看结果"></a>查看结果</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">pd.set_option(&quot;display.max_colwidth&quot;, None)</span><br><span class="line"></span><br><span class="line">df = result.to_pandas()</span><br><span class="line">df</span><br></pre></td></tr></table></figure>
<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/1892529470419736435">RAG系统效果难评？2025年必备的RAG评估框架与工具详解 - 知乎</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Jz421Q7Lw?spm_id_from=333.788.videopod.sections&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">如何利用RAGAs评估RAG系统的好坏_哔哩哔哩_bilibili</a></p>
<p>ragas中文文档<a target="_blank" rel="noopener" href="https://www.aidoczh.com/ragas/getstarted/rag_eval/index.html#want-help-in-improving-your-ai-application-using-evals">Evaluate a simple RAG - Ragas</a></p>
<p><a target="_blank" rel="noopener" href="https://segmentfault.com/a/1190000045262257#item-4-5">人工智能 - RAG系统的7个检索指标：信息检索任务准确性评估指南 - deephub - SegmentFault 思否</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/07/09/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/chunk%E5%88%86%E5%9D%97%E7%AD%96%E7%95%A5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/07/09/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/chunk%E5%88%86%E5%9D%97%E7%AD%96%E7%95%A5/" class="post-title-link" itemprop="url">chunk分块策略</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-07-09 00:00:00" itemprop="dateCreated datePublished" datetime="2025-07-09T00:00:00+08:00">2025-07-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-08-04 15:13:06" itemprop="dateModified" datetime="2025-08-04T15:13:06+08:00">2025-08-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AE%9E%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">实习</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/" itemprop="url" rel="index"><span itemprop="name">晨晟智控</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="分块策略"><a href="#分块策略" class="headerlink" title="分块策略"></a>分块策略</h3><p>以下是 RAG 应用程序的典型工作流程：</p>
<p><img src="/2025/07/09/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/chunk%E5%88%86%E5%9D%97%E7%AD%96%E7%95%A5/6878b8fa-5e74-45a1-9a89-5aab92889126_2366x990.gif" alt="6878b8fa-5e74-45a1-9a89-5aab92889126_2366x990"></p>
<p>主流主要有五种分块策略：</p>
<p><img src="https___substack-post-media.s3.amazonaws.com_public_images_92c70184-ba0f-4877-9a55-e4add0e311ad_870x1116.gif" alt="https___substack-post-media.s3.amazonaws.com_public_images_92c70184-ba0f-4877-9a55-e4add0e311ad_870x1116"></p>
<h4 id="Fixed-size-chunking-固定大小的分块"><a href="#Fixed-size-chunking-固定大小的分块" class="headerlink" title="Fixed-size chunking 固定大小的分块"></a>Fixed-size chunking 固定大小的分块</h4><p><img src="/2025/07/09/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/chunk%E5%88%86%E5%9D%97%E7%AD%96%E7%95%A5/98c422a0-f0e2-457c-a256-4476a56a601f_943x232.png" alt="98c422a0-f0e2-457c-a256-4476a56a601f_943x232"></p>
<p>将文本以固定长度分块，overlap为每个块的重合程度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">text = <span class="string">&quot;大家好，我是果粒奶优有果粒，欢迎关注我，让我们一起探索AI。&quot;</span></span><br><span class="line"><span class="keyword">from</span> langchain_text_splitters <span class="keyword">import</span> CharacterTextSplitter</span><br><span class="line"></span><br><span class="line">text_splitter = CharacterTextSplitter(</span><br><span class="line">    separator=<span class="string">&quot;&quot;</span>,<span class="comment">#按字切分</span></span><br><span class="line">    chunk_size=<span class="number">5</span>,</span><br><span class="line">    chunk_overlap=<span class="number">1</span>,</span><br><span class="line">    length_function=<span class="built_in">len</span>,<span class="comment">#以长度计算</span></span><br><span class="line">    is_separator_regex=<span class="literal">False</span>,<span class="comment">#不视为正则表达式</span></span><br><span class="line">)</span><br><span class="line">text_splitter.split_text(text)</span><br></pre></td></tr></table></figure>
<h4 id="Semantic-chunking-语义分块"><a href="#Semantic-chunking-语义分块" class="headerlink" title="Semantic chunking 语义分块"></a>Semantic chunking 语义分块</h4><p><img src="https___substack-post-media.s3.amazonaws.com_public_images_a6ad83a6-2879-4c77-9e49-393f16577aef_1066x288.gif" alt="https___substack-post-media.s3.amazonaws.com_public_images_a6ad83a6-2879-4c77-9e49-393f16577aef_1066x288"></p>
<p>先将文本分段，然后为每个段进行嵌入，若两个段有较高的余弦相似度，则合并成一个块，一直合并到余弦相似度显著下降，再从新的块开始</p>
<p>需要设定阈值来确定余弦相似度是否显著下降，这因文档而异。</p>
<p><img src="/2025/07/09/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/chunk%E5%88%86%E5%9D%97%E7%AD%96%E7%95%A5/image-20250710150106274.png" alt="image-20250710150106274"></p>
<p>具体实现思路：利用滑动窗口，从第一句往后移动滑动窗口，如图，emed1与emed2相差sen3，计算出来的distance决定sen3是否加入chunk1，以此类推</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#利用langchain调用</span></span><br><span class="line"><span class="keyword">from</span> langchain_experimental.text_splitter <span class="keyword">import</span> SemanticChunker</span><br><span class="line"><span class="keyword">from</span> langchain_community.embeddings <span class="keyword">import</span> DashScopeEmbeddings</span><br><span class="line">embeddings_model = DashScopeEmbeddings(</span><br><span class="line">        model=<span class="string">&quot;text-embedding-v2&quot;</span>,</span><br><span class="line">        dashscope_api_key=<span class="string">&quot;&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">semantic_chunk=SemanticChunker(</span><br><span class="line">    embeddings=embeddings_model,<span class="comment">#嵌入模型</span></span><br><span class="line">    breakpoint_threshold_type=<span class="string">&quot;percentile&quot;</span>,<span class="comment">#定义如何计算语义断点阈值</span></span><br><span class="line">    breakpoint_threshold_amount=<span class="number">95</span>,<span class="comment">#设定阈值</span></span><br><span class="line">    <span class="comment">#min_chunk_size=500#限制生成块最小的字符数，避免生成无意义的块</span></span><br><span class="line">    sentence_split_regex=<span class="string">r&#x27;[。！？.\n]&#x27;</span>,<span class="comment">#语句切分</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://developer.aliyun.com/article/1664186">LangChain 搭配 QWen 踩坑-阿里云开发者社区</a></p>
<p>使用OpenAIEmbeddings配置embedding模型，需要设置一个关键参数</p>
<p><code>check_embedding_ctx_length = False</code> 的作用是：</p>
<blockquote>
<p><strong>关闭 langchain_openai 在调用嵌入模型前对输入文本长度的检查与自动截断/分段逻辑。</strong></p>
</blockquote>
<p>但 DashScope 的 <code>text-embedding-v4</code> 接口：</p>
<ul>
<li>对输入格式要求更严格（必须是字符串或字符串列表，不能是分段后的复杂结构）。</li>
<li>不接受 <code>langchain_openai</code> 默认生成的<strong>分段后的列表嵌套结构</strong>。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from langchain_openai import OpenAIEmbeddings,  OpenAI</span><br><span class="line">embeddings = OpenAIEmbeddings(</span><br><span class="line">api_key=&quot;sk-&quot;, </span><br><span class="line">base_url=&quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;,</span><br><span class="line">model=&quot;text-embedding-v4&quot;,</span><br><span class="line">check_embedding_ctx_length = False,</span><br><span class="line">dimensions=1536</span><br><span class="line">)</span><br><span class="line">result=embeddings.embed_query(&quot;Hello, world!&quot;)</span><br><span class="line">print(len(result))</span><br></pre></td></tr></table></figure>
</blockquote>
<p>源代码理解见最后</p>
<h4 id="Recursive-chunking-递归分块"><a href="#Recursive-chunking-递归分块" class="headerlink" title="Recursive chunking 递归分块"></a>Recursive chunking 递归分块</h4><p><img src="/2025/07/09/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/chunk%E5%88%86%E5%9D%97%E7%AD%96%E7%95%A5/f4009caa-34fc-48d6-8102-3d0f6f2c1386_1066x316.gif" alt="f4009caa-34fc-48d6-8102-3d0f6f2c1386_1066x316"></p>
<p>先依据大的段落进行分块，再对每个块进行处理，若符合chunk-size的限制，则不会再分</p>
<p>结果可能如下</p>
<p><img src="/2025/07/09/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/chunk%E5%88%86%E5%9D%97%E7%AD%96%E7%95%A5/b0e40cc1-996f-48f4-9306-781b112536e4_984x428.png" alt="b0e40cc1-996f-48f4-9306-781b112536e4_984x428"></p>
<p>首先，我们定义两个块（紫色的两个段落。接下来，第1段进一步拆分为更小的块。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_text_splitters <span class="keyword">import</span> RecursiveCharacterTextSplitter</span><br><span class="line"></span><br><span class="line">recursive_splitter_chinese = RecursiveCharacterTextSplitter(</span><br><span class="line">    chunk_size=<span class="number">50</span>,</span><br><span class="line">    chunk_overlap=<span class="number">10</span>,</span><br><span class="line">    length_function=<span class="built_in">len</span>,</span><br><span class="line">    separators=[<span class="string">&quot;\n\n&quot;</span>, <span class="string">&quot;。&quot;</span>, <span class="string">&quot;，&quot;</span>, <span class="string">&quot; &quot;</span>, <span class="string">&quot;&quot;</span>]<span class="comment">#中文的分隔符，可以用逗号句号</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h4 id="Document-structure-based-chunking-基于文档结构的分块"><a href="#Document-structure-based-chunking-基于文档结构的分块" class="headerlink" title="Document structure-based chunking 基于文档结构的分块"></a>Document structure-based chunking 基于文档结构的分块</h4><p><img src="/2025/07/09/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/chunk%E5%88%86%E5%9D%97%E7%AD%96%E7%95%A5/e8febecd-ee68-42ff-ab06-41a0a3a43cd3_1102x306.gif" alt="e8febecd-ee68-42ff-ab06-41a0a3a43cd3_1102x306"></p>
<p>根据文档的固有结构进行分块，如markdown的一级标题二级标题等</p>
<p><code>langchain.text_splitter</code>中有两个用于md文档分块的类，<code>MarkdownTextSplitter</code>与<code>MarkdownHeaderTextSplitter</code></p>
<p>二者区别主要在：前者继承于<code>RecursiveCharacterTextSplitter</code>递归分块，它会尝试沿着 Markdown 格式的标题进行分割，但其核心仍然是基于字符的递归分割；后者专注于 基于 Markdown 标题的结构化分割 ，并能将标题信息作为元数据保留，更适合需要保持 Markdown 文档层级结构的应用场景。</p>
<p>需要注意的是<code>MarkdownHeaderTextSplitter</code> 本身不直接提供限制块内容长度的参数，但可以通过与 <code>RecursiveCharacterTextSplitter</code> 等其他文本分割器结合使用来有效控制块的大小。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.text_splitter <span class="keyword">import</span> MarkdownHeaderTextSplitter</span><br><span class="line">headers_to_split_on = [</span><br><span class="line">    (<span class="string">&quot;#&quot;</span>, <span class="string">&quot;Header 1&quot;</span>),</span><br><span class="line">    (<span class="string">&quot;##&quot;</span>, <span class="string">&quot;Header 2&quot;</span>),</span><br><span class="line">    (<span class="string">&quot;###&quot;</span>, <span class="string">&quot;Header 3&quot;</span>),</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on)</span><br><span class="line">md_header_splits = markdown_splitter.split_text(markdown_document)</span><br></pre></td></tr></table></figure>
<p>存储结构类似如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[Document(metadata=&#123;&#x27;Header 1&#x27;: &#x27;Foo&#x27;, &#x27;Header 2&#x27;: &#x27;Bar&#x27;&#125;, page_content=&#x27;Hi this is Jim  \nHi this is Joe&#x27;),</span><br><span class="line"> Document(metadata=&#123;&#x27;Header 1&#x27;: &#x27;Foo&#x27;, &#x27;Header 2&#x27;: &#x27;Bar&#x27;, &#x27;Header 3&#x27;: &#x27;Boo&#x27;&#125;, page_content=&#x27;Hi this is Lance&#x27;),</span><br><span class="line"> Document(metadata=&#123;&#x27;Header 1&#x27;: &#x27;Foo&#x27;, &#x27;Header 2&#x27;: &#x27;Baz&#x27;&#125;, page_content=&#x27;Hi this is Molly&#x27;)]</span><br></pre></td></tr></table></figure>
<h4 id="LLM-based-chunking-基于-LLM-的分块"><a href="#LLM-based-chunking-基于-LLM-的分块" class="headerlink" title="LLM-based chunking 基于 LLM 的分块"></a>LLM-based chunking 基于 LLM 的分块</h4><p><img src="/2025/07/09/%E5%AE%9E%E4%B9%A0/%E6%99%A8%E6%99%9F%E6%99%BA%E6%8E%A7/chunk%E5%88%86%E5%9D%97%E7%AD%96%E7%95%A5/4d1b6d60-8956-4030-8525-d899ee61a9d5_1140x198.gif" alt="4d1b6d60-8956-4030-8525-d899ee61a9d5_1140x198"></p>
<p>利用大模型进行分块</p>
<p>langchain没有提供官方的类实现LLM-based chunking</p>
<p>但是我在找到了别人实现的agentic_chunker<a target="_blank" rel="noopener" href="https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/agentic_chunker.py">RetrievalTutorials/tutorials/LevelsOfTextSplitting/agentic_chunker.py at main · FullStackRetrieval-com/RetrievalTutorials</a>，可供参考</p>
<p>后记：agentic chunk大概的思路为先进行初步分段，按照长度或递归，然后让大模型生成这一段的概要，将段与段合并生成块，但是测试下来，一个文档的内容同质化很严重，基本上都分到一块里了，而且这个主要还是提示词工程，分块并不系统，看个乐吧</p>
<p><a target="_blank" rel="noopener" href="https://github.com/zxj-2023/chunks-strategy-/blob/main/agentic_chunker.py">chunks-strategy-/agentic_chunker.py at main · zxj-2023/chunks-strategy-</a>代码稍作更新，弃用了部分库</p>
<h3 id="embedding"><a href="#embedding" class="headerlink" title="embedding"></a>embedding</h3><p>之前对chunking和embedding的理解不够清晰，chunking是对文本进行分块，由于大多数文本嵌入模型对输入文本长度有严格限制，如果不分块则无法embedding，从而无法更好的进行向量化或者更好地储存在知识库中，提升retriever性能；embedding则是将文本映射到向量空间，为了更好的相似度计算</p>
<h3 id="语义分块的源代码实战"><a href="#语义分块的源代码实战" class="headerlink" title="语义分块的源代码实战"></a>语义分块的源代码实战</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">将文本划分成单句，可以按照标点符号划分</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">single_sentences_list = re.split(<span class="string">r&#x27;(?&lt;=[。！？])&#x27;</span>, essay)</span><br><span class="line"><span class="comment"># 移除可能存在的空字符串</span></span><br><span class="line">single_sentences_list = [s.strip() <span class="keyword">for</span> s <span class="keyword">in</span> single_sentences_list <span class="keyword">if</span> s.strip()]</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">我们需要为单个句子拼接更多的句子，但是 `list` 添加比较困难。因此将其转换为字典列（`List[dict]`）</span></span><br><span class="line"><span class="string">&#123; &#x27;sentence&#x27; : XXX  , &#x27;index&#x27; : 0&#125;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">sentences = [&#123;<span class="string">&#x27;sentence&#x27;</span>: x, <span class="string">&#x27;index&#x27;</span> : i&#125; <span class="keyword">for</span> i, x <span class="keyword">in</span> <span class="built_in">enumerate</span>(single_sentences_list)]</span><br><span class="line"></span><br><span class="line"><span class="comment">#利用滑动窗口分段</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">combine_sentences</span>(<span class="params">sentences, buffer_size=<span class="number">1</span></span>):</span><br><span class="line">    combined_sentences = [</span><br><span class="line">        <span class="string">&#x27; &#x27;</span>.join(sentences[j][<span class="string">&#x27;sentence&#x27;</span>] <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">max</span>(i - buffer_size, <span class="number">0</span>), <span class="built_in">min</span>(i + buffer_size + <span class="number">1</span>, <span class="built_in">len</span>(sentences))))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(sentences))</span><br><span class="line">    ]   </span><br><span class="line">    <span class="comment"># 更新原始字典列表，添加组合后的句子</span></span><br><span class="line">    <span class="keyword">for</span> i, combined_sentence <span class="keyword">in</span> <span class="built_in">enumerate</span>(combined_sentences):</span><br><span class="line">        sentences[i][<span class="string">&#x27;combined_sentence&#x27;</span>] = combined_sentence</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> sentences</span><br><span class="line"></span><br><span class="line">sentences = combine_sentences(sentences)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">接下来使用**embedding model**对**sentences** 进行编码</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">from</span> langchain_community.embeddings <span class="keyword">import</span> DashScopeEmbeddings</span><br><span class="line">embeddings_model = DashScopeEmbeddings(</span><br><span class="line">        model=<span class="string">&quot;text-embedding-v2&quot;</span>,</span><br><span class="line">        dashscope_api_key=<span class="string">&quot;&quot;</span>,</span><br><span class="line"></span><br><span class="line">    )</span><br><span class="line"><span class="comment"># 提取所有组合后的句子用于 embedding</span></span><br><span class="line">combined_sentences_to_embed = [x[<span class="string">&#x27;combined_sentence&#x27;</span>] <span class="keyword">for</span> x <span class="keyword">in</span> sentences]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对句子进行 embedding</span></span><br><span class="line">embeddings = embeddings_model.embed_documents(combined_sentences_to_embed)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;成功对 <span class="subst">&#123;<span class="built_in">len</span>(embeddings)&#125;</span> 个句子进行了 embedding。&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#将embedding添加到sentence中</span></span><br><span class="line"><span class="keyword">for</span> i, sentence <span class="keyword">in</span> <span class="built_in">enumerate</span>(sentences):</span><br><span class="line">    sentence[<span class="string">&#x27;combined_sentence_embedding&#x27;</span>] = embeddings[i]</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">接下来需要根据余弦相似度进行切分</span></span><br><span class="line"><span class="string">通过计算两个向量的夹角余弦值来衡量相似性</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cosine_similarity</span>(<span class="params">vec1, vec2</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Calculate the cosine similarity between two vectors.&quot;&quot;&quot;</span></span><br><span class="line">    dot_product = np.dot(vec1, vec2)</span><br><span class="line">    norm_vec1 = np.linalg.norm(vec1)</span><br><span class="line">    norm_vec2 = np.linalg.norm(vec2)</span><br><span class="line">    <span class="keyword">return</span> dot_product / (norm_vec1 * norm_vec2)</span><br><span class="line"><span class="comment">#遍历，计算余弦相似度</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calculate_cosine_distances</span>(<span class="params">sentences</span>):</span><br><span class="line">    distances = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(sentences) - <span class="number">1</span>):</span><br><span class="line">        embedding_current = sentences[i][<span class="string">&#x27;combined_sentence_embedding&#x27;</span>]</span><br><span class="line">        embedding_next = sentences[i + <span class="number">1</span>][<span class="string">&#x27;combined_sentence_embedding&#x27;</span>]</span><br><span class="line">        <span class="comment"># Calculate cosine similarity</span></span><br><span class="line">        similarity = cosine_similarity(embedding_current, embedding_next)</span><br><span class="line">        <span class="comment"># Convert to cosine distance</span></span><br><span class="line">        distance = <span class="number">1</span> - similarity</span><br><span class="line">        distances.append(distance)</span><br><span class="line">        <span class="comment"># Store distance in the dictionary</span></span><br><span class="line">        sentences[i][<span class="string">&#x27;distance_to_next&#x27;</span>] = distance</span><br><span class="line">    <span class="keyword">return</span> distances, sentences</span><br><span class="line"></span><br><span class="line">distances, sentences = calculate_cosine_distances(sentences)</span><br><span class="line"></span><br><span class="line"><span class="comment">#根据阈值划分</span></span><br><span class="line">breakpoint_percentile_threshold = <span class="number">95</span></span><br><span class="line">breakpoint_distance_threshold = np.percentile(distances, breakpoint_percentile_threshold)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;距离的第95个百分位阈值是:&quot;</span>, breakpoint_distance_threshold)</span><br><span class="line"><span class="comment"># 找到所有距离大于阈值的点的索引，这些索引就是我们的切分点</span></span><br><span class="line">indices_above_thresh = [i <span class="keyword">for</span> i, x <span class="keyword">in</span> <span class="built_in">enumerate</span>(distances) <span class="keyword">if</span> x &gt; breakpoint_distance_threshold]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化块的起始句子索引。我们将根据之前计算出的语义分割点（`indices_above_thresh`）来切分句子列表。</span></span><br><span class="line">start_index = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个列表，用于存储最终组合成的、具有语义连贯性的文本块。</span></span><br><span class="line">chunks = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历所有识别出的语义分割点（这些是句子列表 `sentences` 中的索引）。</span></span><br><span class="line"><span class="keyword">for</span> index <span class="keyword">in</span> indices_above_thresh:</span><br><span class="line">    <span class="comment"># 确定当前文本块的结束点，即当前的分割点索引。</span></span><br><span class="line">    end_index = index</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 从原始句子列表（`sentences`）中切片，提取从上一个分割点到当前分割点之间的所有句子。</span></span><br><span class="line">    <span class="comment"># `end_index + 1` 是为了在切片时包含结束索引指向的那个句子。</span></span><br><span class="line">    group = sentences[start_index:end_index + <span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将切分出的句子组（`group`）中的所有 &#x27;sentence&#x27; 字段的值合并成一个单独的字符串，句子之间用空格隔开。</span></span><br><span class="line">    combined_text = <span class="string">&#x27; &#x27;</span>.join([d[<span class="string">&#x27;sentence&#x27;</span>] <span class="keyword">for</span> d <span class="keyword">in</span> group])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将合并后的文本块添加到 `chunks` 列表中。</span></span><br><span class="line">    chunks.append(combined_text)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 更新下一个文本块的起始索引，设置为当前分割点的下一个位置，为处理下一个块做准备。</span></span><br><span class="line">    start_index = index + <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理最后一个文本块。</span></span><br><span class="line"><span class="comment"># 循环结束后，如果 `start_index` 仍然小于句子总数，说明从最后一个分割点到文本末尾还有剩余的句子。</span></span><br><span class="line"><span class="keyword">if</span> start_index &lt; <span class="built_in">len</span>(sentences):</span><br><span class="line">    <span class="comment"># 将这些剩余的句子合并成最后一个文本块。</span></span><br><span class="line">    combined_text = <span class="string">&#x27; &#x27;</span>.join([d[<span class="string">&#x27;sentence&#x27;</span>] <span class="keyword">for</span> d <span class="keyword">in</span> sentences[start_index:]])</span><br><span class="line">    chunks.append(combined_text)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 此时，`chunks` 列表包含了所有根据语义距离切分和重组后的文本块。</span></span><br><span class="line"><span class="keyword">for</span> i, chunk <span class="keyword">in</span> <span class="built_in">enumerate</span>(chunks):</span><br><span class="line">    buffer = <span class="number">200</span></span><br><span class="line">    <span class="built_in">print</span> (<span class="string">f&quot;Chunk #<span class="subst">&#123;i&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span> (chunk[:buffer].strip())</span><br><span class="line">    <span class="built_in">print</span> (<span class="string">&quot;...&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span> (chunk[-buffer:].strip())</span><br><span class="line">    <span class="built_in">print</span> (<span class="string">&quot;\n&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a target="_blank" rel="noopener" href="https://www.dailydoseofds.com/p/5-chunking-strategies-for-rag/">RAG 的 5 种分块策略 —- 5 Chunking Strategies For RAG</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/wjinjie/article/details/148660229">一文读懂 Qwen3 最新开源的 Embedding 和 Rerank 模型优势！_qwen-rerank-CSDN博客</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1dr421x7Su/?spm_id_from=333.337.search-card.all.click&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">一站帮你选择RAG中的文本切分策略_哔哩哔哩_bilibili</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/The_Thieves/article/details/148747334">LangChain 语义文本拆分指南：基于语义相似度的智能分块技术实战_langchain 语义分割-CSDN博客</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="上一页" aria-label="上一页" href="/page/5/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><span class="page-number current">6</span><a class="page-number" href="/page/7/">7</a><span class="space">&hellip;</span><a class="page-number" href="/page/16/">16</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/7/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">张熙浚</span>
  </div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="本站访问数 fa fa-user 次"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="本站总访问量 fa fa-eye 次"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script size="400" alpha="0.6" zIndex="-1" src="https://cdnjs.cloudflare.com/ajax/libs/ribbon.js/1.0.2/ribbon.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.1.0/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script src="/js/pjax.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  <script src="/js/third-party/pace.js"></script>


  
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
