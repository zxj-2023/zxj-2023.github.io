<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha256-XOqroi11tY4EFQMR9ZYwZWKj5ZXiftSx36RRuC3anlA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-bounce.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.20.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="zxj Blogs">
<meta property="og:type" content="website">
<meta property="og:title" content="Zhang XiJun">
<meta property="og:url" content="http://example.com/page/6/index.html">
<meta property="og:site_name" content="Zhang XiJun">
<meta property="og:description" content="zxj Blogs">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="张熙浚">
<meta property="article:tag" content="zxj">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/page/6/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"page/6/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Zhang XiJun</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Zhang XiJun</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">BLOGS</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="张熙浚"
      src="/images/zxjavatar.gif">
  <p class="site-author-name" itemprop="name">张熙浚</p>
  <div class="site-description" itemprop="description">zxj Blogs</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">132</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">50</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">56</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/zxj-2023" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;zxj-2023" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="http://wpa.qq.com/msgrd?v=3&uin=2902065320&site=qq&menu=yes" title="QQ → http:&#x2F;&#x2F;wpa.qq.com&#x2F;msgrd?v&#x3D;3&amp;uin&#x3D;2902065320&amp;site&#x3D;qq&amp;menu&#x3D;yes" rel="noopener me" target="_blank"><i class="fab fa-qq fa-fw"></i>QQ</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
    <div class="sidebar-inner sidebar-blogroll">
      <div class="links-of-blogroll animated">
        <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
          链接
        </div>
        <ul class="links-of-blogroll-list">
            <li class="links-of-blogroll-item">
              <a href="https://zxj-2023.github.io/" title="https:&#x2F;&#x2F;zxj-2023.github.io" rel="noopener" target="_blank">Zhang XiJun</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://theme-next.js.org/" title="https:&#x2F;&#x2F;theme-next.js.org" rel="noopener" target="_blank">NexT</a>
            </li>
        </ul>
      </div>
    </div>
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/" class="post-title-link" itemprop="url">机器学习——期末复习（下）</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-06-06 00:00:00" itemprop="dateCreated datePublished" datetime="2025-06-06T00:00:00+08:00">2025-06-06</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-06-22 19:53:19" itemprop="dateModified" datetime="2025-06-22T19:53:19+08:00">2025-06-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%A4%A7%E4%BA%8C%E4%B8%8B/" itemprop="url" rel="index"><span itemprop="name">大二下</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="期末复习">期末复习</h2>
<h3 id="方差与偏差">方差与偏差</h3>
<p>方差（Variance）和偏差（Bias）是机器学习中衡量模型性能的两个核心概念，它们共同构成了<strong>偏差-方差权衡</strong>（Bias-Variance
Tradeoff）的基础框架。以下是两者的定义与区别：</p>
<p><strong>1. 偏差（Bias）</strong></p>
<ul>
<li><strong>定义</strong>：偏差是指模型预测的期望值与真实值之间的差异。它反映了模型本身的拟合能力，即是否能够准确捕捉数据中的规律。</li>
</ul>
<p><strong>2. 方差（Variance）</strong></p>
<ul>
<li><strong>定义</strong>：方差是指模型在不同训练数据集下预测结果的波动程度。它衡量了模型对训练数据中噪声或微小变化的敏感性。</li>
</ul>
<p><strong>3. 如何降低偏差与方差</strong></p>
<table>
<colgroup>
<col style="width: 11%">
<col style="width: 56%">
<col style="width: 32%">
</colgroup>
<thead>
<tr class="header">
<th><strong>目标</strong></th>
<th><strong>方法</strong></th>
<th><strong>示例</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>降低偏差</strong></td>
<td>增加模型复杂度（如更多特征、更深的神经网络）、减少正则化强度</td>
<td>使用多项式回归替代线性回归</td>
</tr>
<tr class="even">
<td><strong>降低方差</strong></td>
<td>增加训练数据、引入正则化（L1/L2）、使用集成方法（如
Bagging、Boosting）</td>
<td>随机森林（Bagging）降低决策树的方差</td>
</tr>
</tbody>
</table>
<p><strong>4. 总结</strong></p>
<ul>
<li><strong>偏差</strong>关注模型是否能准确拟合数据（<strong>学习能力</strong>），而<strong>方差</strong>关注模型对数据波动的稳定性（<strong>泛化能力</strong>）。</li>
<li>实际应用中需通过交叉验证、正则化或集成学习等技术平衡两者的关系。</li>
</ul>
<h3 id="监督学习与无监督学习">监督学习与无监督学习</h3>
<p>以下是关于监督学习与无监督学习的核心区别总结：</p>
<p><strong>1. 监督学习（Supervised Learning）</strong></p>
<p><strong>任务类型</strong>：<br>
-
<strong>分类（Classification）</strong>：预测离散类别标签（如垃圾邮件/非垃圾邮件）。<br>
-
<strong>回归（Regression）</strong>：预测连续数值标签（如房价预测）。</p>
<p><strong>特点</strong>：<br>
- 需要<strong>带标签的样本</strong>（Labeled
Data），即每个训练样本都有明确的输入 $ x $ 和输出 $ y $。<br>
- 模型通过学习输入与标签之间的映射关系进行预测。</p>
<p><strong>2. 无监督学习（Unsupervised Learning）</strong></p>
<p><strong>任务类型</strong>：</p>
<ul>
<li><strong>聚类（Clustering）</strong>：将样本划分为具有相似特征的群体（如客户分群）。<br>
</li>
<li><strong>降维（Dimensionality
Reduction）</strong>：压缩数据维度同时保留关键信息（如PCA）。</li>
</ul>
<p><strong>特点</strong>：<br>
- 仅需<strong>无标签的样本</strong>（Unlabeled
Data），无需预先定义输出目标。<br>
- 模型自主挖掘数据内在结构或分布规律。</p>
<h3 id="贝叶斯分类">贝叶斯分类</h3>
<h4 id="贝叶斯分类器">贝叶斯分类器</h4>
<h5 id="贝叶斯决策论">贝叶斯决策论</h5>
<p>本质思想：寻找合适的参数使得「当前的样本情况发生的概率」最大。</p>
<p>又由于假设每一个样本相互独立（概率条件理想的情况下），因此可以用连乘的形式表示上述概率，当然由于概率较小导致连乘容易出现浮点数精度损失，因此尝尝采用取对数的方式来避免「下溢」问题。也就是所谓的「对数似然估计」方法。</p>
<p>在已知样本特征 $ $ 的条件下，选择分类结果 $ c_i
$，使得分类的期望损失（Risk）最小<strong>。</strong></p>
<p>**(1) 损失函数 $ _{ij} $**</p>
<ul>
<li><strong>定义</strong>：$ _{ij} $ 是将真实类别为 $ c_j $
的样本误分类为 $ c_i $ 所产生的损失。
<ul>
<li>例如：
<ul>
<li>在医学诊断中，若 $ c_1 $ 表示“患病”，$ c_2 $ 表示“未患病”：
<ul>
<li>$ _{21} <span class="math inline">：<em>将</em><em>实</em><em>际</em><em>患</em><em>病</em>（</span>
c_1 <span class="math inline">）<em>误</em><em>判</em><em>为</em><em>未</em><em>患</em><em>病</em>（</span>
c_2 $）的损失（可能更高）。</li>
<li>$ _{12} <span class="math inline">：<em>将</em><em>实</em><em>际</em><em>未</em><em>患</em><em>病</em>（</span>
c_2 <span class="math inline">）<em>误</em><em>判</em><em>为</em><em>患</em><em>病</em>（</span>
c_1 $）的损失（可能较低）。</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<p><strong>(2) 条件风险（单个样本的期望损失）</strong></p>
<p>对于给定样本 $ $，若将其分类为 $ c_i
$，则其<strong>条件风险</strong>为： <span class="math display">$$
R(c_i | \mathbf{x}) = \sum_{j=1}^N \lambda_{ij} P(c_j | \mathbf{x})
$$</span> - <strong>含义</strong>：在已知 $ $ 的情况下，分类为 $ c_i $
的平均损失。 - <strong>推导</strong>： - $ P(c_j | ) $：样本 $ $
真实属于 $ c_j $ 的后验概率。 - $ <em>{ij} $：若真实类别是 $ c_j
$，但被分到 $ c_i $，则产生损失 $ </em>{ij} $。 -
因此，总期望损失是所有可能真实类别的加权和（权重为后验概率）。</p>
<p><strong>(3) 总体风险</strong></p>
<p>对于整个数据集，分类器 $ h() $ 的<strong>总体风险</strong>为： <span class="math display"><em>R</em>(<em>h</em>) = 𝔼<sub><strong>x</strong></sub>[<em>R</em>(<em>h</em>(<strong>x</strong>)|<strong>x</strong>)] = ∫<em>R</em>(<em>h</em>(<strong>x</strong>)|<strong>x</strong>)<em>p</em>(<strong>x</strong>)<em>d</em><strong>x</strong></span>
- <strong>含义</strong>：所有样本的平均条件风险。h为分类器（模型） -
<strong>目标</strong>：找到使 $ R(h) $ 最小的分类器 $ h() $。</p>
<h5 id="贝叶斯决策规则"><strong>贝叶斯决策规则</strong></h5>
<p>根据上述定义，贝叶斯决策论的分类规则是： &gt; <strong>对于样本 $
$，选择使其条件风险 $ R(c_i | ) $ 最小的类别 $ c_i $
作为预测结果。</strong></p>
<p>即： <span class="math display">$$
h^*(\mathbf{x}) = \arg\min_{c_i} R(c_i | \mathbf{x}) = \arg\min_{c_i}
\sum_{j=1}^N \lambda_{ij} P(c_j | \mathbf{x})
$$</span></p>
<h5 id="特殊情况0-1-损失函数"><strong>特殊情况：0-1
损失函数</strong></h5>
<p>当所有误分类的损失相同（即 $ <em>{ij} = 1 $ 对于 $ i j <span class="math inline">，</span> </em>{ii} = 0 $）<strong>0-1
损失函数</strong>： <span class="math display">$$
\lambda_{ij} =
\begin{cases}
0, &amp; \text{if } i = j \\
1, &amp; \text{otherwise}
\end{cases}
$$</span> 此时条件风险简化为： <span class="math display"><em>R</em>(<em>c</em><sub><em>i</em></sub>|<strong>x</strong>) = ∑<sub><em>j</em> ≠ <em>i</em></sub><em>P</em>(<em>c</em><sub><em>j</em></sub>|<strong>x</strong>) = 1 − <em>P</em>(<em>c</em><sub><em>i</em></sub>|<strong>x</strong>)</span>
原因：概率之和为 1：$ <em>{j=1}^N P(c_j | ) = 1 $，因此 $ </em>{j i}
P(c_j | ) = 1 - P(c_i | ) $。</p>
<p>此时，最小化风险等价于<strong>最大化后验概率</strong>，即： <span class="math display"><em>h</em><sup>*</sup>(<strong>x</strong>) = arg max<sub><em>c</em><sub><em>i</em></sub></sub><em>P</em>(<em>c</em><sub><em>i</em></sub>|<strong>x</strong>)</span>
这正是传统贝叶斯分类器的决策规则。</p>
<blockquote>
<p>即在x样本的情况下，分类正确的概率最大</p>
</blockquote>
<h4 id="后验概率与先验概率">后验概率与先验概率</h4>
<h5 id="后验概率">后验概率</h5>
<p>后验概率（Posterior
Probability）是贝叶斯理论中的核心概念，指的是<strong>在观察到新证据（数据）后，对事件发生概率的修正</strong>
。 其本质是：</p>
<blockquote>
<p><strong>“已知结果（数据），反推原因（类别或参数）的概率”</strong>
。</p>
</blockquote>
<p>已知结果（数据）B，反推最可能的原因A（后验概率
<em>P</em>(<em>A</em>∣<em>B</em>) ）</p>
<h5 id="先验概率prior-probability"><strong>先验概率（Prior
Probability）</strong></h5>
<p>先验概率是贝叶斯统计中的核心概念，指的是在<strong>观察到新数据之前</strong>，对某一事件或假设的概率估计。它是基于<strong>已有知识、经验或假设</strong>得出的初始概率，后续会通过新数据更新为更准确的<strong>后验概率</strong>。</p>
<p><strong>1. 核心定义</strong></p>
<ul>
<li><strong>数学表达</strong>：<br>
<span class="math display"><em>P</em>(<em>A</em>)</span>
<ul>
<li>$ P(A) $：事件 $ A $ 的先验概率。</li>
<li>例如：$ A $ 表示“某人患有某种疾病”，则 $ P(A) $
是该疾病的已知发病率（在未进行检测前的概率）。</li>
</ul></li>
<li><strong>与后验概率的区别</strong>：
<ul>
<li><strong>先验概率</strong>：$ P(A) $，在无新数据时的概率。<br>
</li>
<li><strong>后验概率</strong>：$ P(A|B) $，在观察到数据 $ B $
后更新的概率（通过贝叶斯定理计算）。</li>
</ul></li>
</ul>
<p><strong>2. 直观理解</strong></p>
<p><strong>(1) 类比：医学诊断</strong></p>
<ul>
<li><strong>先验概率</strong>：某种疾病的已知发病率（如 1%）。<br>
</li>
<li><strong>新数据</strong>：患者接受检测，结果为阳性。<br>
</li>
<li><strong>后验概率</strong>：结合发病率和检测结果，计算实际患病的概率（如
8.7%，参考贝叶斯定理的经典医学测试案例）。</li>
</ul>
<h4 id="生成式模型和判别式模型">生成式模型和判别式模型</h4>
<h5 id="核心区别"><strong>核心区别</strong></h5>
<table>
<colgroup>
<col style="width: 10%">
<col style="width: 44%">
<col style="width: 44%">
</colgroup>
<thead>
<tr class="header">
<th><strong>模型类型</strong></th>
<th><strong>建模目标</strong></th>
<th><strong>数学表达</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>判别式模型</strong></td>
<td>直接建模 $ P(c</td>
<td>) $</td>
</tr>
<tr class="even">
<td><strong>生成式模型</strong></td>
<td>先建模联合概率 $ P(, c) $，再推导 $ P(c</td>
<td>) $</td>
</tr>
</tbody>
</table>
<h5 id="详细解释"><strong>详细解释</strong></h5>
<p><strong>1. 判别式模型（Discriminative Model）</strong></p>
<ul>
<li><strong>目标</strong>：直接学习从输入 $ $ 到标签 $ c $
的映射关系。</li>
<li><strong>数学本质</strong>：建模条件概率 $ P(c|) $，即“已知特征 $
$，预测类别 $ c $”。</li>
<li><strong>特点</strong>：
<ul>
<li>不关心数据本身的分布，只关注分类边界。</li>
<li>例如：逻辑回归、支持向量机（SVM）、神经网络等。</li>
</ul></li>
</ul>
<p><strong>2. 生成式模型（Generative Model）</strong></p>
<ul>
<li><p><strong>目标</strong>：先学习数据的生成过程，即联合概率 $ P(, c)
$，再通过贝叶斯定理推导条件概率 $ P(c|) $。</p></li>
<li><p><strong>数学步骤</strong>：</p>
<ol type="1">
<li>建模 $ P(|c) $（特征在类别 $ c $ 下的分布）和 $ P(c)
$（类别先验）。</li>
<li>根据贝叶斯定理计算后验概率： <span class="math display">$$
P(c|\mathbf{x}) = \frac{P(\mathbf{x}|c)P(c)}{P(\mathbf{x})}
$$</span></li>
<li>选择使 $ P(c|) $ 最大的类别作为预测结果。</li>
</ol></li>
</ul>
<h5 id="示例二分类问题"><strong>示例：二分类问题</strong></h5>
<p>假设我们要判断一封邮件是否为垃圾邮件（$ c=spam $ 或 $ ham $）。</p>
<p><strong>判别式模型（逻辑回归）</strong></p>
<p>直接建模： <span class="math display">$$
P(spam|\mathbf{x}) = \frac{1}{1 + e^{-(w^T \mathbf{x} + b)}}
$$</span> 若 $ P(spam|) &gt; 0.5 $，则判定为垃圾邮件。</p>
<p><strong>生成式模型（朴素贝叶斯）</strong></p>
<ol type="1">
<li>建模联合概率： <span class="math display"><em>P</em>(<strong>x</strong>,<em>s</em><em>p</em><em>a</em><em>m</em>) = <em>P</em>(<em>s</em><em>p</em><em>a</em><em>m</em>)∏<sub><em>i</em></sub><em>P</em>(<em>w</em><em>o</em><em>r</em><em>d</em><sub><em>i</em></sub>|<em>s</em><em>p</em><em>a</em><em>m</em>)</span>
<span class="math display"><em>P</em>(<strong>x</strong>,<em>h</em><em>a</em><em>m</em>) = <em>P</em>(<em>h</em><em>a</em><em>m</em>)∏<sub><em>i</em></sub><em>P</em>(<em>w</em><em>o</em><em>r</em><em>d</em><sub><em>i</em></sub>|<em>h</em><em>a</em><em>m</em>)</span></li>
<li>计算后验概率： <span class="math display">$$
P(spam|\mathbf{x}) = \frac{P(\mathbf{x}|spam)P(spam)}{P(\mathbf{x})}
$$</span> <span class="math display">$$
P(ham|\mathbf{x}) = \frac{P(\mathbf{x}|ham)P(ham)}{P(\mathbf{x})}
$$</span></li>
<li>选择概率更大的类别。</li>
</ol>
<h4 id="生成式模型的建模思路">生成式模型的建模思路</h4>
<p>根据概率论的基本定义： <span class="math display">$$
P(c|\mathbf{x}) = \frac{P(\mathbf{x}, c)}{P(\mathbf{x})}
$$</span> - <strong>含义</strong>： - $ P(, c) $：联合概率，表示特征 $ $
和类别 $ c $ 同时发生的概率。 - $ P() $：边缘概率（证据），表示特征 $ $
出现的概率，用于归一化。</p>
<p>根据贝叶斯定理，联合概率 $ P(, c) $ 可以分解为： <span class="math display"><em>P</em>(<strong>x</strong>,<em>c</em>) = <em>P</em>(<em>c</em>) ⋅ <em>P</em>(<strong>x</strong>|<em>c</em>)</span>
其中： - $ P(c) $：类先验概率（Prior Probability），表示类别 $ c $
在数据中的整体占比。 - $ P(|c) $：似然度（Likelihood），表示在类别 $ c $
下，特征 $ $ 出现的概率。</p>
<p>将上述分解代入条件概率公式，得到： <span class="math display">$$
P(c|\mathbf{x}) = \frac{P(c) \cdot P(\mathbf{x}|c)}{P(\mathbf{x})}
$$</span> 产生问题：</p>
<p>在贝叶斯分类中，需要计算联合概率
<em>P</em>(<strong>x</strong>∣<em>c</em>) ，即在类别 <em>c</em>
下，特征向量 <strong>x</strong>=(<em>x</em>1,<em>x</em>2,…,*x**d<em>)
的条件概率。 若直接建模联合概率，需估计 </em>d*
个特征的所有可能组合的概率。例如：</p>
<ul>
<li>若每个特征有 <em>k</em> 个取值，类别数为 <em>K</em> ，则需要估计
<em>K</em>⋅*k**d* 个参数。</li>
<li>当特征维度 <em>d</em>
很大时（如文本分类中成千上万的词汇），参数数量呈指数级增长，导致计算不可行（<strong>维度灾难</strong>
）。</li>
</ul>
<p>举例：</p>
<ul>
<li><strong>低维空间</strong> ：假设只有 2
个特征（如“免费”和“中奖”），每个特征取值为 0 或 1，则特征空间共有 22=4
个可能的组合（即四个格子）。
<ul>
<li>如果有 100 封邮件，每个格子平均有 25 封邮件（数据较密集）。</li>
</ul></li>
<li><strong>高维空间</strong> ： 当特征维度增加到 <em>d</em>=10,000
时，特征空间的组合数是 210,000 ，远大于宇宙中原子的数量（约 1080 ）。
<ul>
<li>即使有 100 万封邮件，每个组合几乎都是空的（数据极度稀疏）。</li>
</ul></li>
</ul>
<p><strong>结果</strong> ：
在高维空间中，训练数据无法覆盖所有可能的特征组合，导致模型无法可靠估计联合概率
<em>P</em>(x∣c) 。</p>
<p>因此产生<strong>属性条件独立性假设</strong></p>
<h4 id="朴素贝叶斯分类器">朴素贝叶斯分类器</h4>
<p>朴素贝叶斯分类器的核心思想是通过<strong>贝叶斯定理</strong>和<strong>属性条件独立性假设</strong>来简化计算，从而高效地进行分类。</p>
<h5 id="属性条件独立性假设">属性条件独立性假设</h5>
<p>朴素贝叶斯的核心假设是：<strong>在已知类别 $ c $
的条件下，所有属性（特征）之间相互独立</strong>。<br>
因此，联合概率 $ P(|c) $ 可以分解为各属性独立概率的乘积： <span class="math display">$$
P(\mathbf{x}|c) = \prod_{i=1}^d P(x_i|c)
$$</span> 其中 $ d $ 是特征的数量，$ x_i $ 是第 $ i $ 个特征的取值。</p>
<p>将此代入贝叶斯公式： <span class="math display">$$
P(c|\mathbf{x}) = \frac{P(c) \cdot \prod_{i=1}^d
P(x_i|c)}{P(\mathbf{x})}
$$</span></p>
<h5 id="为何可以忽略-p"><strong>为何可以忽略 $ P() $?</strong></h5>
<p>在分类任务中，我们的目标是比较不同类别 $ c $ 的后验概率 $ P(c|)
$，并选择最大值。由于 $ P() $
对所有类别来说是相同的常量（与类别无关），因此在最大化过程中可以忽略：
<span class="math display">$$
\arg\max_{c} P(c|\mathbf{x}) = \arg\max_{c} \left[ \frac{P(c) \cdot
\prod_{i=1}^d P(x_i|c)}{P(\mathbf{x})} \right] = \arg\max_{c} \left[
P(c) \cdot \prod_{i=1}^d P(x_i|c) \right]
$$</span> 这就是公式中 $ P() $ 被省略的原因。</p>
<blockquote>
<p>在比较的过程中，分母相同，可以忽略</p>
</blockquote>
<h5 id="朴素贝叶斯的最终决策规则"><strong>朴素贝叶斯的最终决策规则</strong></h5>
<p>简化后的决策规则为： <span class="math display">$$
h_{nb}(\mathbf{x}) = \arg\max_{c} \left[ P(c) \cdot \prod_{i=1}^d
P(x_i|c) \right]
$$</span> 即： - 计算每个类别的先验概率 $ P(c) $。 -
计算每个特征在该类别下的条件概率 $ P(x_i|c) $。 -
将这些概率相乘，选择乘积最大的类别作为预测结果。</p>
<h5 id="类先验概率-pc-的估计方法"><strong>类先验概率 $ P(c) $
的估计方法</strong></h5>
<p>基于<strong>大数定律</strong> <span class="math display">$$
P(c) = \frac{|D_c|}{|D|}
$$</span> - <strong>符号含义</strong>： - $ D $：训练集，包含所有样本。
- $ D_c $：训练集中类别为 $ c $ 的样本子集。 - $ |D_c| $：类别 $ c $
的样本数量。 - $ |D| $：训练集总样本数量。</p>
<ul>
<li><strong>直观解释</strong>：
类先验概率等于该类别样本数占总样本数的比例。</li>
</ul>
<h5 id="条件概率-px_i-c-的估计方法"><strong>条件概率 $ P(x_i | c) $
的估计方法</strong></h5>
<p>在生成式模型（如朴素贝叶斯分类器）中，<strong>条件概率 $ P(x_i | c)
$</strong> 表示在类别 $ c $ 下，第 $ i $ 个属性取值为 $ x_i $
的概率。根据属性类型（离散或连续），其估计方法不同：</p>
<p><strong>1. 离散属性的条件概率估计</strong></p>
<p><strong>公式</strong>： <span class="math display">$$
P(x_i | c) = \frac{|D_{c,x_i}|}{|D_c|}
$$</span> - <strong>符号含义</strong>： - $ D_c $：训练集中类别为 $ c $
的样本集合。 - $ D_{c,x_i} <span class="math inline">：</span> D_c $
中第 $ i $ 个属性取值为 $ x_i $ 的样本子集。 - $ |D_{c,x_i}| <span class="math inline">：</span> D_{c,x_i} $ 的样本数量。 - $ |D_c| $：类别
$ c $ 的总样本数量。</p>
<p><strong>直观解释</strong>：</p>
<ul>
<li>在类别 $ c $ 的样本中，统计第 $ i $ 个属性取值为 $ x_i $
的频率，作为 $ P(x_i | c) $ 的估计。</li>
<li><strong>示例</strong>：<br>
若类别 $ c=spam $（垃圾邮件）有 200 封，其中 150 封包含“免费”一词，则：
<span class="math display">$$
P(\text{“免费”} | spam) = \frac{150}{200} = 0.75
$$</span></li>
</ul>
<p><strong>注意事项</strong>：</p>
<ul>
<li><strong>零概率问题</strong>：若某属性值在类别 $ c $ 中未出现，则 $
P(x_i | c) = 0 $，可能导致后续计算失效。<br>
<strong>解决方案</strong>：使用<strong>拉普拉斯平滑（Laplace
Smoothing）</strong>，将公式改为： <span class="math display">$$
P(x_i | c) = \frac{|D_{c,x_i}| + 1}{|D_c| + K}
$$</span> 其中 $ K $ 是该属性的取值总数。</li>
</ul>
<p><strong>2. 连续属性的条件概率估计</strong></p>
<p><strong>假设</strong>：属性服从正态分布（高斯分布） <span class="math display">$$
p(x_i | c) = \frac{1}{\sqrt{2\pi}\sigma_{c,i}} \exp\left( -\frac{(x_i -
\mu_{c,i})^2}{2\sigma_{c,i}^2} \right)
$$</span> - <strong>符号含义</strong>： - $ <em>{c,i} $：类别 $ c $ 在第
$ i $ 个属性上的均值。 - $ </em>{c,i}^2 $：类别 $ c $ 在第 $ i $
个属性上的方差。</p>
<p><strong>直观解释</strong>：</p>
<ul>
<li>假设在类别 $ c $ 下，属性 $ x_i $ 服从均值为 $ <em>{c,i} $、方差为 $
</em>{c,i}^2 $ 的正态分布。</li>
<li><strong>示例</strong>：<br>
若类别 $ c=spam $ 的“字数”属性均值 $ <em>{spam, } = 500 $，方差 $
</em>{spam, }^2 = 100 $，则： <span class="math display">$$
p(600 | spam) = \frac{1}{\sqrt{2\pi \cdot 100}} \exp\left( -\frac{(600 -
500)^2}{2 \cdot 100} \right) \approx 0.004
$$</span></li>
</ul>
<p><strong>注意事项</strong>：</p>
<ul>
<li><strong>分布假设</strong>：若实际数据不符合正态分布，需调整假设（如使用核密度估计、对数变换等）。</li>
<li><strong>参数估计</strong>：均值和方差通过训练数据计算： <span class="math display">$$
\mu_{c,i} = \frac{1}{|D_c|} \sum_{x \in D_c} x_i, \quad \sigma_{c,i}^2 =
\frac{1}{|D_c|} \sum_{x \in D_c} (x_i - \mu_{c,i})^2
$$</span></li>
</ul>
<h4 id="半朴素贝叶斯分类器">半朴素贝叶斯分类器</h4>
<p>半朴素贝叶斯分类器是对传统<strong>朴素贝叶斯</strong>的改进，它在保留计算效率的同时，<strong>适当引入部分属性间的依赖关系</strong>，从而在分类性能和计算复杂度之间取得平衡。</p>
<h5 id="独依赖估计ode方法"><strong>独依赖估计（ODE）方法</strong></h5>
<p><strong>(1) 定义</strong></p>
<p>独依赖估计（One-Dependent Estimator,
ODE）是半朴素贝叶斯的一种实现方式，其核心假设是： &gt; <strong>每个属性
$ x_i $ 在类别 $ c $ 之外最多依赖于一个其他属性（称为父属性 $ pa_i
$）</strong>。</p>
<p>数学表达式为： <span class="math display">$$
P(c|\mathbf{x}) \propto P(c) \prod_{i=1}^d P(x_i | c, pa_i)
$$</span> 其中： - $ pa_i $：属性 $ x_i $ 的父属性（依赖的单一属性）。 -
$ P(x_i | c, pa_i) $：在类别 $ c $ 和父属性 $ pa_i $ 下，属性 $ x_i $
的条件概率。</p>
<p><strong>(2) 直观理解</strong></p>
<ul>
<li>每个属性 $ x_i $ 的分布不仅受类别 $ c $ 影响，还受其父属性 $ pa_i $
的影响。</li>
<li>例如，在文本分类中，若属性 $ x_1 $ 是“免费”，$ x_2 $
是“中奖”，可设定 $ pa_2 = x_1
$，表示“中奖”在类别和“免费”的共同作用下出现。</li>
</ul>
<h5 id="超父独依赖估计spode"><strong>超父独依赖估计（SPODE）</strong></h5>
<p>超父独依赖估计（Super Parent One-Dependent Estimator,
SPODE）是<strong>半朴素贝叶斯分类器</strong>的一种扩展，其核心思想是：
&gt; <strong>所有属性都依赖于同一个“超父”属性 $ x_i
$</strong>，从而在保留部分依赖关系的同时避免完全联合概率的计算。</p>
<p><strong>(1) 贝叶斯定理展开</strong> <span class="math display">$$
P(c|\mathbf{x}) = \frac{P(\mathbf{x}, c)}{P(\mathbf{x})}
$$</span> 其中： - $ P(, c) $：联合概率，表示特征 $ $ 和类别 $ c $
同时发生的概率。 - $ P() $：证据（归一化因子）。</p>
<p><strong>(2) 引入“超父”属性 $ x_i $</strong></p>
<p>假设所有属性 $ x_j (j i) $ 在类别 $ c $ 下仅依赖于 $ x_i $，则：
<span class="math display"><em>P</em>(<strong>x</strong>,<em>c</em>) = <em>P</em>(<em>c</em>,<em>x</em><sub><em>i</em></sub>) ⋅ <em>P</em>(<em>x</em><sub>1</sub>,…,<em>x</em><sub><em>i</em> − 1</sub>,<em>x</em><sub><em>i</em> + 1</sub>,…,<em>x</em><sub><em>d</em></sub>|<em>c</em>,<em>x</em><sub><em>i</em></sub>)</span>
进一步分解为： <span class="math display"><em>P</em>(<strong>x</strong>,<em>c</em>) = <em>P</em>(<em>c</em>,<em>x</em><sub><em>i</em></sub>) ⋅ ∏<sub><em>j</em> ≠ <em>i</em></sub><em>P</em>(<em>x</em><sub><em>j</em></sub>|<em>c</em>,<em>x</em><sub><em>i</em></sub>)</span></p>
<p><strong>(3) 最终形式</strong></p>
<p>由于 $ P() $ 对所有类别相同，可忽略，最终决策规则为： <span class="math display">$$
P(c|\mathbf{x}) \propto P(c, x_i) \cdot \prod_{j=1}^d P(x_j | c, x_i)
$$</span> 其中： - $ P(c, x_i) $：类别 $ c $ 和属性 $ x_i $ 的联合概率。
- $ P(x_j | c, x_i) $：在类别 $ c $ 和 $ x_i $ 的条件下，属性 $ x_j $
的概率。</p>
<h5 id="树增强朴素贝叶斯tan-tree-augmented-naive-bayes"><strong>树增强朴素贝叶斯（TAN:
Tree-Augmented Naive Bayes）</strong></h5>
<p><strong>TAN</strong>（Tree-Augmented Naive
Bayes）是<strong>半朴素贝叶斯分类器</strong>的一种扩展，旨在通过引入属性间的<strong>树状依赖关系</strong>，在保留计算效率的同时，显著提升分类性能。它结合了<strong>贝叶斯网络</strong>的建模能力和<strong>生成式模型</strong>的概率推理优势。</p>
<p><strong>1. 核心思想</strong></p>
<p>TAN 的核心假设是： &gt; <strong>所有属性（特征）在类别 $ c $
的基础上，形成一个以属性为节点的树状依赖结构</strong>，即每个属性最多依赖一个其他属性（父属性），且整个依赖图是一棵无环的树。</p>
<p><strong>数学表达</strong>： <span class="math display">$$
P(c|\mathbf{x}) \propto P(c) \cdot \prod_{i=1}^d P(x_i | c, pa_i)
$$</span> 其中： - $ pa_i $：属性 $ x_i $ 的父属性（依赖的单一属性）。 -
$ P(x_i | c, pa_i) $：在类别 $ c $ 和父属性 $ pa_i $ 的条件下，属性 $
x_i $ 的条件概率。</p>
<p><strong>2. TAN 的构建步骤</strong></p>
<p>TAN 通过以下步骤构建属性间的依赖结构：</p>
<p><strong>(1) 计算互信息（Mutual Information）</strong></p>
<p>互信息衡量两个属性之间的相关性： <span class="math display">$$
I(x_i, x_j) = \sum_{x_i, x_j} P(x_i, x_j) \log \frac{P(x_i,
x_j)}{P(x_i)P(x_j)}
$$</span> -
<strong>含义</strong>：互信息越大，两个属性之间的依赖关系越强。</p>
<p><strong>(2) 构建带权图</strong></p>
<ul>
<li>将所有属性视为图中的节点。</li>
<li>每对属性间的边权重设为互信息 $ I(x_i, x_j) $。</li>
</ul>
<p><strong>(3) 最大带权生成树（Maximum Weight Spanning Tree,
MWST）</strong></p>
<p>使用克鲁斯卡尔（Kruskal）算法或普里姆（Prim）算法，选择一棵连接所有属性节点的树，使得：
- 树的边权重（互信息）总和最大。 - 树中无环。</p>
<p><strong>(4) 确定依赖方向</strong></p>
<ul>
<li>随机选择一个根节点（或根据领域知识指定）。</li>
<li>从根节点出发，确定每条边的方向（父属性 → 子属性）。</li>
</ul>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250605223036362.png" alt="image-20250605223036362">
<figcaption aria-hidden="true">image-20250605223036362</figcaption>
</figure>
<h4 id="贝叶斯网">贝叶斯网</h4>
<p>待学习</p>
<h4 id="em算法">EM算法</h4>
<p>EM算法（Expectation-Maximization
Algorithm）是一种<strong>迭代优化算法</strong>，用于处理<strong>含有隐变量</strong>（Hidden
Variables）或<strong>缺失数据</strong>的概率模型参数估计问题。它的核心思想是通过交替执行<strong>期望（E）步</strong>和<strong>最大化（M）步</strong>，逐步逼近模型参数的最大似然估计。</p>
<h5 id="核心思想解决隐变量问题"><strong>1.
核心思想：解决隐变量问题</strong></h5>
<p><strong>(1) 什么是隐变量？</strong></p>
<p>隐变量（Latent
Variables）是模型中<strong>不可观测但影响观测数据</strong>的变量。例如：
-
<strong>混合高斯模型（GMM）</strong>：每个样本属于哪个高斯分布是隐变量。
- <strong>聚类任务</strong>：样本所属的聚类标签是隐变量。</p>
<p><strong>(2) 问题挑战</strong></p>
<p>当存在隐变量时，直接最大化似然函数变得困难。例如： <span class="math display">log <em>P</em>(<strong>x</strong>|<em>θ</em>) = log ∑<sub><em>z</em></sub><em>P</em>(<strong>x</strong>,<em>z</em>|<em>θ</em>)</span>
其中 $ z $ 是隐变量，$ $
是模型参数。由于对数中包含求和，直接求导无法分离参数。</p>
<p><strong>(3) EM算法的解决方案</strong></p>
<p>EM算法通过以下步骤迭代求解： 1.
<strong>E步（期望）</strong>：用当前参数估计隐变量的后验分布（即“责任”分配）。
2.
<strong>M步（最大化）</strong>：基于隐变量的后验分布，最大化期望似然函数以更新参数。</p>
<h5 id="算法流程"><strong>2. 算法流程</strong></h5>
<p><strong>(1) 初始化参数</strong></p>
<p>选择初始参数 $ ^{(0)} $，例如随机初始化或通过启发式方法设定。</p>
<p><strong>(2) E步：计算隐变量后验分布</strong></p>
<p>给定当前参数 $ ^{(t)} $，计算隐变量 $ z $ 的后验概率： <span class="math display"><em>Q</em><sup>(<em>t</em>)</sup>(<em>z</em>) = <em>P</em>(<em>z</em>|<strong>x</strong>,<em>θ</em><sup>(<em>t</em>)</sup>)</span>
这一步为每个样本分配隐变量的概率分布（如样本属于某个聚类的概率）。</p>
<p><strong>(3) M步：最大化期望似然</strong></p>
<p>基于 $ Q^{(t)}(z) $，构造期望似然函数并最大化： <span class="math display"><em>θ</em><sup>(<em>t</em>+1)</sup> = arg max<sub><em>θ</em></sub>∑<sub><em>z</em></sub><em>Q</em><sup>(<em>t</em>)</sup>(<em>z</em>)log <em>P</em>(<strong>x</strong>,<em>z</em>|<em>θ</em>)</span>
这一步更新参数 $ $，使得期望似然最大。</p>
<p><strong>(4) 收敛判断</strong></p>
<p>重复E步和M步直到参数收敛（如 $ |^{(t+1)} - ^{(t)}| &lt;
$）或达到最大迭代次数。</p>
<h5 id="示例混合高斯模型gmm"><strong>3.
示例：混合高斯模型（GMM）</strong></h5>
<p>假设数据由多个高斯分布生成，但不知道每个样本属于哪个分布。</p>
<p><strong>(1) 模型定义</strong></p>
<ul>
<li>观测变量 $ x_i ^d $：第 $ i $ 个样本。</li>
<li>隐变量 $ z_i {1, …, K} $：样本 $ x_i $ 所属的高斯分布。</li>
<li>参数 $ = {_k, _k, <em>k}</em>{k=1}^K $：
<ul>
<li>$ _k $：第 $ k $ 个高斯分布的均值。</li>
<li>$ _k $：第 $ k $ 个高斯分布的协方差矩阵。</li>
<li>$ _k $：第 $ k $ 个高斯分布的权重（先验概率）。</li>
</ul></li>
</ul>
<p><strong>(2) E步：计算责任分配</strong></p>
<p>对于每个样本 $ x_i $ 和类别 $ k $，计算责任（responsibility）： <span class="math display">$$
\gamma_{ik}^{(t)} = P(z_i=k|x_i, \theta^{(t)}) = \frac{\pi_k^{(t)}
\mathcal{N}(x_i|\mu_k^{(t)}, \Sigma_k^{(t)})}{\sum_{j=1}^K \pi_j^{(t)}
\mathcal{N}(x_i|\mu_j^{(t)}, \Sigma_j^{(t)})}
$$</span> 含义：在当前参数下，样本 $ x_i $ 属于类别 $ k $ 的概率。</p>
<p><strong>(3) M步：更新参数</strong></p>
<p>根据责任 $ _{ik} $ 更新参数： - <strong>均值更新</strong>： <span class="math display">$$
  \mu_k^{(t+1)} = \frac{\sum_{i=1}^N \gamma_{ik}^{(t)} x_i}{\sum_{i=1}^N
\gamma_{ik}^{(t)}}
  $$</span> - <strong>协方差更新</strong>： <span class="math display">$$
  \Sigma_k^{(t+1)} = \frac{\sum_{i=1}^N \gamma_{ik}^{(t)} (x_i -
\mu_k^{(t+1)})(x_i - \mu_k^{(t+1)})^T}{\sum_{i=1}^N \gamma_{ik}^{(t)}}
  $$</span> - <strong>权重更新</strong>： <span class="math display">$$
  \pi_k^{(t+1)} = \frac{\sum_{i=1}^N \gamma_{ik}^{(t)}}{N}
  $$</span></p>
<p><strong>(4) 迭代终止</strong></p>
<p>当参数变化小于阈值或达到最大迭代次数时停止。</p>
<h4 id="作业">作业</h4>
<h5 id="section">1</h5>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250606143758565.png" alt="image-20250606143758565">
<figcaption aria-hidden="true">image-20250606143758565</figcaption>
</figure>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250606143819352.png" alt="image-20250606143819352">
<figcaption aria-hidden="true">image-20250606143819352</figcaption>
</figure>
<h5 id="section-1">2</h5>
<p>已知观测数据-67，-48，6，8，14，16，23，24，28，29，41，49，56，60，75，试估计两个分量的高斯混合模型的5个参数。</p>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250606150830535.png" alt="image-20250606150830535">
<figcaption aria-hidden="true">image-20250606150830535</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.mixture <span class="keyword">import</span> GaussianMixture</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化观测数据</span></span><br><span class="line">data = np.array([-<span class="number">67</span>, -<span class="number">48</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">14</span>, <span class="number">16</span>, <span class="number">23</span>, <span class="number">24</span>, <span class="number">28</span>, <span class="number">29</span>, <span class="number">41</span>, <span class="number">49</span>, <span class="number">56</span>, <span class="number">60</span>,</span><br><span class="line">                 <span class="number">75</span>]).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 聚类</span></span><br><span class="line">gmmModel = GaussianMixture(n_components=<span class="number">2</span>)</span><br><span class="line">gmmModel.fit(data)</span><br><span class="line">labels = gmmModel.predict(data)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;labels =&quot;</span>, labels)</span><br><span class="line">labels = [<span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(labels)):</span><br><span class="line">    <span class="keyword">if</span> labels[i] == <span class="number">0</span>:</span><br><span class="line">        plt.scatter(i, data.take(i), s=<span class="number">15</span>, c=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">    <span class="keyword">elif</span> labels[i] == <span class="number">1</span>:</span><br><span class="line">        plt.scatter(i, data.take(i), s=<span class="number">15</span>, c=<span class="string">&#x27;blue&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Gaussian Mixture Model&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;y&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;means =&quot;</span>, gmmModel.means_.reshape(<span class="number">1</span>, -<span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;covariances =&quot;</span>, gmmModel.covariances_.reshape(<span class="number">1</span>, -<span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;weights = &quot;</span>, gmmModel.weights_.reshape(<span class="number">1</span>, -<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250606150906475.png" alt="image-20250606150906475">
<figcaption aria-hidden="true">image-20250606150906475</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># means = [[ 32.98489643 -57.51107027]]</span></span><br><span class="line"><span class="comment"># covariances = [[429.45764867  90.24987882]]</span></span><br><span class="line"><span class="comment"># weights =  [[0.86682762 0.13317238]]</span></span><br></pre></td></tr></table></figure>
<h5 id="section-2">3</h5>
<p>简要阐述下EM算法的原理，并给出EM算法对高斯混合模型GMM进行求解的具体过程。</p>
<h6 id="em算法的原理">EM算法的原理</h6>
<p>EM算法（期望最大化算法）是一种用于含有隐变量的概率模型参数估计的迭代优化方法。其核心思想是通过交替执行两个步骤来最大化观测数据的似然函数：</p>
<ol type="1">
<li><strong>E步（期望步）</strong>：计算隐变量的后验期望（即责任），给定当前参数估计。</li>
<li><strong>M步（最大化步）</strong>：基于责任，最大化完全数据的期望似然函数以更新参数。</li>
</ol>
<p>EM算法通过不断优化似然函数的下界，最终收敛到局部最优解。以下具体阐述EM算法对高斯混合模型（GMM）的求解过程。</p>
<h6 id="em算法对gmm的具体求解过程"><strong>EM算法对GMM的具体求解过程</strong></h6>
<p><strong>1. GMM模型定义</strong></p>
<p>GMM假设数据由 $ K $ 个高斯分布线性组合生成，其概率密度函数为： <span class="math display">$$
p(\mathbf{x}|\theta) = \sum_{k=1}^K \alpha_k \cdot
\mathcal{N}(\mathbf{x}|\mu_k, \Sigma_k)
$$</span> 其中： - $ <em>k $：第 $ k $ 个高斯分布的权重（$ </em>{k=1}^K
_k = 1 $）。 - $ _k $：第 $ k $ 个高斯分布的均值向量。 - $ _k $：第 $ k
$ 个高斯分布的协方差矩阵。 - $ = {_k, _k, <em>k}</em>{k=1}^K
$：模型参数。</p>
<p>隐变量 $ z_i {1,,K} $ 表示样本 $ _i $ 的类别标签（未知）。</p>
<p><strong>2. EM算法步骤</strong></p>
<p><strong>(1) 初始化参数</strong></p>
<p>随机或通过K-means初始化： - 每个高斯分布的均值 $ _k^{(0)} $、协方差 $
_k^{(0)} $、权重 $ _k^{(0)} $。</p>
<p><strong>(2) 迭代优化（E步与M步）</strong></p>
<p><strong>E步：计算责任（后验概率）</strong> 对每个样本 <span class="math inline"><strong>x</strong><sub><em>i</em></sub></span>
和每个簇 $ k $，计算其属于第 $ k $ 个高斯分布的后验概率 <span class="math display">$$
\gamma(z_{ik}) = \frac{\alpha_k \cdot \mathcal{N}(\mathbf{x}_i | \mu_k,
\Sigma_k)}{\sum_{j=1}^K \alpha_j \cdot \mathcal{N}(\mathbf{x}_i | \mu_j,
\Sigma_j)}
$$</span> 此概率表示在当前参数下，样本 $ _i $ 属于第 $ k $
个高斯分布的“责任”。</p>
<p><strong>M步：更新参数</strong><br>
基于责任 $ (z_{ik}) $，最大化完全数据似然函数的期望，更新参数：</p>
<ul>
<li><strong>权重更新</strong>： <span class="math display">$$
\alpha_k^{(new)} = \frac{1}{N} \sum_{i=1}^N \gamma(z_{ik})
$$</span></li>
<li><strong>均值更新</strong>： <span class="math display">$$
\mu_k^{(new)} = \frac{\sum_{i=1}^N \gamma(z_{ik})
\mathbf{x}_i}{\sum_{i=1}^N \gamma(z_{ik})}
$$</span></li>
<li><strong>协方差更新</strong>： <span class="math display">$$
\Sigma_k^{(new)} = \frac{\sum_{i=1}^N \gamma(z_{ik}) (\mathbf{x}_i -
\mu_k^{(new)})(\mathbf{x}_i - \mu_k^{(new)})^\top}{\sum_{i=1}^N
\gamma(z_{ik})}
$$</span> 若为单变量高斯分布，则更新方差： <span class="math display">$$
\sigma_k^{(new)} = \frac{\sum_{i=1}^N \gamma(z_{ik}) (x_i -
\mu_k^{(new)})^2}{\sum_{i=1}^N \gamma(z_{ik})}
$$</span></li>
</ul>
<p><strong>(3) 收敛判断</strong></p>
<p>计算对数似然函数： <span class="math display">$$
\log p(\mathbf{X}|\theta) = \sum_{i=1}^N \log \left( \sum_{k=1}^K
\alpha_k \cdot \mathcal{N}(\mathbf{x}_i|\mu_k, \Sigma_k) \right)
$$</span>
若对数似然的变化量小于阈值或达到最大迭代次数，则停止；否则重复E步和M步。。</p>
<h4 id="参考资料">参考资料</h4>
<p>[<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1RT411G7jJ/?spm_id_from=333.788.recommend_more_video.0&amp;vd_source=bacf29bd4bb51f2ecf08a1ac7c7d8f11">5分钟学算法]
#06 EM算法 你到底是哪个班级的_哔哩哔哩_bilibili</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/396007256">《统计学习方法_第二版》学习笔记第九章
- 知乎</a></p>
<h3 id="集成学习">集成学习</h3>
<h4 id="个体与集成">个体与集成</h4>
<h5 id="集成学习的基本概念"><strong>集成学习的基本概念</strong></h5>
<p>集成学习（Ensemble
Learning）通过构建并结合<strong>多个学习器（基模型）</strong>来完成学习任务，其核心思想是“<strong>优而不同</strong>”，即<strong>通过多个弱学习器的协作提升整体性能</strong>，通常能获得比单一学习器更优的泛化能力
。</p>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250606154731476.png" alt="image-20250606154731476">
<figcaption aria-hidden="true">image-20250606154731476</figcaption>
</figure>
<p>在上图的集成模型中，若个体学习器都属于同一类别，例如都是决策树或都是神经网络，则称该集成为同质的（homogeneous）;若个体学习器包含多种类型的学习算法，例如既有决策树又有神经网络，则称该集成为异质的（heterogenous）。</p>
<blockquote>
<p><strong>同质集成</strong>：个体学习器称为“基学习器”（base
learner），对应的学习算法为“基学习算法”（base learning algorithm）。</p>
<p><strong>异质集成</strong>：个体学习器称为“组件学习器”（component
learner）或直称为“个体学习器”。</p>
</blockquote>
<p>集成学习的两个重要概念：<strong>准确性</strong>和<strong>多样性</strong>（diversity）。准确性指的是个体学习器不能太差，要有一定的准确度；多样性则是个体学习器之间的输出要具有差异性。</p>
<p>通过下面的这三个例子可以很容易看出这一点，准确度较高，差异度也较高，可以较好地提升集成性能。</p>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250606155939884.png" alt="image-20250606155939884">
<figcaption aria-hidden="true">image-20250606155939884</figcaption>
</figure>
<p><strong>集成策略</strong>：如何结合多个基模型的预测结果，例如：</p>
<ul>
<li><strong>投票法</strong>（Voting）：多数投票（硬投票）或概率加权（软投票）。<br>
</li>
<li><strong>加权平均法</strong>：对基模型的输出赋予不同权重 。<br>
</li>
<li><strong>Stacking</strong>：用元模型（Meta-Model）学习基模型的输出作为新特征
。</li>
</ul>
<h5 id="基于投票法的集成个体学习器的收敛性保证"><strong>基于投票法的集成个体学习器的收敛性保证</strong>：</h5>
<p><strong>公式解析</strong> <span class="math display">$$
P(H(\boldsymbol{x}) \neq f(\boldsymbol{x})) = \sum_{k=0}^{\lfloor T/2
\rfloor} \binom{T}{k} (1-\epsilon)^k \epsilon^{T-k} \leq
\exp\left(-\frac{1}{2} T (1 - 2\epsilon)^2\right)
$$</span></p>
<p><strong>1. 公式含义</strong></p>
<ul>
<li><strong><span class="math inline"><em>H</em>(<strong>x</strong>)</span></strong>：集成学习器的最终预测结果（如多数投票结果）。</li>
<li><strong><span class="math inline"><em>f</em>(<strong>x</strong>)</span></strong>：真实标记。</li>
<li><strong><span class="math inline"><em>ϵ</em></span></strong>：单个弱学习器的错误率（即
<span class="math inline"><em>P</em>(<em>h</em><sub><em>t</em></sub>(<strong>x</strong>)≠<em>f</em>(<strong>x</strong>))</span>），默认小于0.5。</li>
<li><strong><span class="math inline"><em>T</em></span></strong>：基学习器的数量。</li>
<li><strong>左边</strong>：集成学习器预测错误的概率（即至少有超过 <span class="math inline"><em>T</em>/2</span>
个基学习器预测错误的概率）。</li>
<li><strong>右边</strong>：对左边概率的指数级上限估计。</li>
</ul>
<p><strong>2. 推导思路</strong></p>
<ul>
<li>假设每个基学习器独立且错误率为 <span class="math inline"><em>ϵ</em></span>，则错误次数服从<strong>二项分布</strong>
<span class="math inline"><em>B</em>(<em>T</em>,<em>ϵ</em>)</span>。</li>
<li>集成错误的条件是“超过半数基学习器错误”，即错误次数 <span class="math inline"><em>k</em> ≤ ⌊<em>T</em>/2⌋</span>。</li>
</ul>
<p><strong>两个基本结论</strong></p>
<p><strong>1. 收敛速率随个体学习器数量 <span class="math inline"><em>T</em></span> 指数下降</strong></p>
<ul>
<li><strong>数学体现</strong>：错误概率的上界是 <span class="math inline">exp (−<em>c</em><em>T</em>)</span> 形式，其中 <span class="math inline">$c = \frac{1}{2}(1 - 2\epsilon)^2$</span>。</li>
</ul>
<p><strong>2. <span class="math inline"><em>ϵ</em> = 0.5</span>
的个体学习器对收敛没有作用</strong></p>
<ul>
<li><strong>数学原因</strong>：当 <span class="math inline"><em>ϵ</em> = 0.5</span> 时，<span class="math inline">(1−2<em>ϵ</em>)<sup>2</sup> = 0</span>，指数项变为
0，错误概率上界为 <span class="math inline">exp (0) = 1</span>，即错误概率无法降低。</li>
</ul>
<p>根据个体学习器的<strong>生成方式</strong>，目前集成学习可分为两类，代表作如下：</p>
<ol type="1">
<li>个体学习器直接存在强依赖关系，必须串行生成的序列化方法：<strong>Boosting</strong>；</li>
<li>个体学习器间不存在强依赖关系，可以同时生成的并行化方法：<strong>Bagging</strong>
和 <strong>随机森林 (Random Forest)</strong>。</li>
</ol>
<h4 id="boosting"><strong>Boosting</strong></h4>
<p>Boosting是一种<strong>串行</strong>的工作机制，即<strong>个体学习器的训练存在依赖关系</strong>，必须一步一步序列化进行。</p>
<p>其<strong>基本思想</strong>是：<strong>增加前一个基学习器在训练过程中预测错误样本的权重，使得后续基学习器更加关注这些打标错误的训练样本，尽可能纠正这些错误，然后基于调整后的样本分布训练下一个基学习器</strong>，如此重复，一直向下串行直至产生需要的T个基学习器，Boosting最终对这T个学习器进行加权结合，产生学习器委员会。</p>
<p>Boosting族算法最著名、使用最为广泛的就是<strong>AdaBoost</strong>，因此下面主要是对AdaBoost算法进行介绍。</p>
<p>AdaBoost使用的是<strong>指数损失函数</strong>，因此AdaBoost的权值与样本分布的更新都是围绕着最小化指数损失函数进行的。</p>
<blockquote>
<p>看到这里回想一下之前的机器学习算法，<strong>不难发现机器学习的大部分带参模型只是改变了最优化目标中的损失函数</strong>：如果是Square
loss，那就是最小二乘了；如果是Hinge
Loss，那就是著名的SVM了；如果是log-Loss，那就是Logistic
Regression了。</p>
</blockquote>
<h5 id="adaboost">AdaBoost</h5>
<h5 id="公式解析"><strong>公式解析</strong></h5>
<p><span class="math display">$$
H(\boldsymbol{x}) = \sum_{t=1}^T \alpha_t h_t(\boldsymbol{x})
$$</span> <span class="math display">ℓ<sub>exp</sub>(<em>H</em>|𝒟) = 𝔼<sub><strong>x</strong> ∼ 𝒟</sub>[<em>e</em><sup>−<em>f</em>(<strong>x</strong>)<em>H</em>(<strong>x</strong>)</sup>]</span></p>
<p><strong>1. 符号含义</strong></p>
<ul>
<li><strong><span class="math inline"><em>H</em>(<strong>x</strong>)</span></strong>：最终集成模型的预测结果，是
<span class="math inline"><em>T</em></span> 个基学习器 <span class="math inline"><em>h</em><sub><em>t</em></sub>(<strong>x</strong>)</span>
的加权和。</li>
<li><strong><span class="math inline"><em>α</em><sub><em>t</em></sub></span></strong>：第
<span class="math inline"><em>t</em></span>
个基学习器的权重，表示其在集成中的重要性。</li>
<li><strong><span class="math inline"><em>h</em><sub><em>t</em></sub>(<strong>x</strong>)</span></strong>：第
<span class="math inline"><em>t</em></span>
个基学习器（如决策树、感知机等）。</li>
<li><strong><span class="math inline"><em>f</em>(<strong>x</strong>)</span></strong>：真实标签，通常取值为
<span class="math inline">{ − 1,  + 1}</span>（二分类问题）。</li>
<li><strong><span class="math inline">𝒟</span></strong>：训练数据分布。</li>
<li><strong><span class="math inline">ℓ<sub>exp</sub></span></strong>：指数损失函数（Exponential
Loss）。</li>
</ul>
<p><strong>2. 指数损失函数的意义</strong></p>
<p>指数损失函数的形式为： <span class="math display">ℓ<sub>exp</sub>(<em>H</em>|𝒟) = 𝔼<sub><strong>x</strong> ∼ 𝒟</sub>[<em>e</em><sup>−<em>f</em>(<strong>x</strong>)<em>H</em>(<strong>x</strong>)</sup>]</span>
- <strong>直观解释</strong>： - 当 <span class="math inline"><em>H</em>(<strong>x</strong>)</span> 与 <span class="math inline"><em>f</em>(<strong>x</strong>)</span>
同号时（预测正确），指数项 <span class="math inline"><em>e</em><sup>−<em>f</em>(<strong>x</strong>)<em>H</em>(<strong>x</strong>)</sup></span>
接近 0，损失小。 - 当 <span class="math inline"><em>H</em>(<strong>x</strong>)</span> 与 <span class="math inline"><em>f</em>(<strong>x</strong>)</span>
异号时（预测错误），指数项趋近于正无穷，损失极大。 -
因此，该损失函数对错误样本的惩罚非常严格，迫使模型优先修正错误。</p>
<h5 id="adaboost的优化目标"><strong>AdaBoost的优化目标</strong></h5>
<p>AdaBoost的目标是选择基学习器 <span class="math inline"><em>h</em><sub><em>t</em></sub></span> 和权重 <span class="math inline"><em>α</em><sub><em>t</em></sub></span>，使得集成模型
<span class="math inline"><em>H</em>(<strong>x</strong>)</span>
能够<strong>最小化指数损失函数</strong>： <span class="math display">$$
\min_{\alpha_1, h_1, \dots, \alpha_T, h_T} \mathbb{E}_{\boldsymbol{x}
\sim \mathcal{D}} \left[ e^{-f(\boldsymbol{x}) \sum_{t=1}^T \alpha_t
h_t(\boldsymbol{x})} \right]
$$</span></p>
<p><strong>优化策略</strong></p>
<p>AdaBoost采用<strong>前向分步算法（Forward Stagewise
Algorithm）</strong>，逐轮迭代优化： 1.
<strong>初始化样本权重</strong>：初始时所有样本权重相等。 2.
<strong>训练基学习器 <span class="math inline"><em>h</em><sub><em>t</em></sub></span></strong>：在当前样本权重分布下，训练一个弱学习器
<span class="math inline"><em>h</em><sub><em>t</em></sub></span>。 3.
<strong>计算权重 <span class="math inline"><em>α</em><sub><em>t</em></sub></span></strong>：根据
<span class="math inline"><em>h</em><sub><em>t</em></sub></span>
的错误率 <span class="math inline"><em>ϵ</em><sub><em>t</em></sub></span> 计算其权重：
<span class="math display">$$
   \alpha_t = \frac{1}{2} \ln \left( \frac{1 - \epsilon_t}{\epsilon_t}
\right)
   $$</span> 4. <strong>更新样本权重</strong>：提高被 <span class="math inline"><em>h</em><sub><em>t</em></sub></span>
错分类样本的权重，降低正确分类样本的权重。 5. <strong>重复步骤
2-4</strong>，直到训练完成 <span class="math inline"><em>T</em></span>
轮。</p>
<h5 id="示例二分类问题-1"><strong>示例：二分类问题</strong></h5>
<p>假设一个二分类任务，真实标签 <span class="math inline"><em>f</em>(<strong>x</strong>) ∈ { − 1,  + 1}</span>，集成模型预测值
<span class="math inline">$H(\boldsymbol{x}) = \sum_{t=1}^T \alpha_t
h_t(\boldsymbol{x})$</span>： - 若 <span class="math inline"><em>H</em>(<strong>x</strong>) &gt; 0</span>，预测为
<span class="math inline"> + 1</span>； - 若 <span class="math inline"><em>H</em>(<strong>x</strong>) &lt; 0</span>，预测为
<span class="math inline"> − 1</span>。</p>
<p>此时，指数损失函数的值反映了模型对错误样本的惩罚程度： -
正确预测时，<span class="math inline"><em>e</em><sup>−<em>f</em>(<strong>x</strong>)<em>H</em>(<strong>x</strong>)</sup> ≈ 0</span>；
- 错误预测时，<span class="math inline"><em>e</em><sup>−<em>f</em>(<strong>x</strong>)<em>H</em>(<strong>x</strong>)</sup> ≫ 1</span>。</p>
<h5 id="adaboost的算法流程">AdaBoost的算法流程</h5>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250606172851748.png" alt="image-20250606172851748">
<figcaption aria-hidden="true">image-20250606172851748</figcaption>
</figure>
<h5 id="重赋权法与重采样法">重赋权法与重采样法</h5>
<p>在集成学习中，<strong>Boosting
算法的核心在于动态调整样本权重</strong> ，以逐步聚焦难分类样本。Boosting
主要通过两种方法实现样本权重的更新：<strong>重赋权法（re-weighting）</strong>
和 <strong>重采样法（re-sampling）</strong> 。</p>
<blockquote>
<p><strong>重赋权法</strong> :
对每个样本附加一个权重，这时涉及到样本属性与标签的计算，都需要乘上一个权值。
<strong>重采样法</strong> :
对于一些无法接受带权样本的及学习算法，适合用“重采样法”进行处理。方法大致过程是，根据各个样本的权重，对训练数据进行重采样，初始时样本权重一样，每个样本被采样到的概率一致，每次从N个原始的训练样本中按照权重有放回采样N个样本作为训练集，然后计算训练集错误率，然后调整权重，重复采样，集成多个基学习器。</p>
</blockquote>
<p>从偏差-方差分解来看：Boosting算法主要关注于降低偏差，每轮的迭代都关注于训练过程中预测错误的样本，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成学习器。</p>
<h5 id="拓展gradient-boosting">拓展：Gradient Boosting</h5>
<p>任务分为分类，回归，聚类，降维等，而分类中还分为二分类和多分类</p>
<p>从AdaBoost的算法流程来看，标准的AdaBoost只适用于二分类问题。</p>
<p>通过改造AdaBoost对样本分类的限制和损失函数，可以实现多分类或回归问题，这样改造出来的算法框架成为<strong>Gradient
Boosting</strong></p>
<h6 id="gbdtgradient-boosting-decision-tree与xgboost"><strong>GBDT（Gradient
Boosting Decision Tree）与XGBoost</strong></h6>
<p><strong>1. GBDT 的核心思想</strong></p>
<p>GBDT 是基于<strong>梯度提升（Gradient
Boosting）</strong>框架的集成学习方法，其特点包括： -
<strong>基学习器</strong>：使用<strong>CART（分类与回归树）</strong>作为个体学习器。
- <strong>损失函数</strong>： -
<strong>回归问题</strong>：平方损失（Squared Loss）： <span class="math display">err(<em>H</em><sub><em>t</em></sub>(<strong>x</strong>),<em>f</em>(<strong>x</strong>)) = (<em>H</em><sub><em>t</em></sub>(<strong>x</strong>)−<em>f</em>(<strong>x</strong>))<sup>2</sup></span>
- <strong>二分类问题</strong>：对数似然损失（Log-Likelihood
Loss，类似逻辑回归）： <span class="math display">err(<em>H</em><sub><em>t</em></sub>(<strong>x</strong>),<em>f</em>(<strong>x</strong>)) = log (1+exp(−<em>f</em>(<strong>x</strong>)<em>H</em><sub><em>t</em></sub>(<strong>x</strong>)))</span>
- <strong>多分类问题</strong>：扩展为多分类对数损失。</p>
<p><strong>2. XGBoost 的定位</strong></p>
<p>XGBoost（eXtreme Gradient Boosting）是 GBDT
的一种<strong>高效实现和改进</strong>，类似于 LIBSVM 对 SVM
的优化关系。其核心目标是： -
<strong>提升训练速度</strong>：通过<strong>并行计算</strong>、<strong>内存优化</strong>等工程技巧。
-
<strong>增强模型性能</strong>：引入<strong>正则化项</strong>、<strong>缺失值处理</strong>、<strong>自适应学习率</strong>等改进。</p>
<blockquote>
<p>XGBoost即eXtremeGradient
Boosting的缩写，XGBoost与GBDT的关系可以类比为
LIBSVM和SVM的关系，即XGBoOst是GBDT的一种高效实现和改进。</p>
<p>它并非一个全新的算法框架，而是对标准 GBDT
进行了<strong>大量的工程优化和算法增强</strong>。</p>
</blockquote>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250606175536310.png" alt="image-20250606175536310">
<figcaption aria-hidden="true">image-20250606175536310</figcaption>
</figure>
<h4 id="bagging">Bagging</h4>
<p>Bagging是一种<strong>并行式</strong>的集成学习方法，即<strong>基学习器的训练之间没有前后顺序可以同时进行</strong></p>
<p>Bagging使用<strong>“有放回”采样的方式选取训练集</strong>，对于包含m个样本的训练集，进行m次有放回的随机采样操作，从而得到m个样本的采样集，这样训练集中有<strong>接近36.8%</strong>的样本没有被采到，可用作验证集来对泛化性能进行“包外估计”(out-of-bag
estimate)。</p>
<p>按照相同的方式重复进行，我们就可以采集到T个包含m个样本的数据集，从而训练出<strong>T个基学习器</strong>，最终对<strong>这T个基学习器的输出进行结合</strong>。</p>
<h5 id="bagging与boosting的差异">Bagging与Boosting的差异</h5>
<p>Boosting算法一大特点是串行，这样诚然可以降低模型的偏差，增强拟合能力，但是当数据过大时，一大缺点就是会降低学习效率</p>
<p>Bagging作为并行式的集成学习方法，通过综合多个基学习器的结果，可以增加学习效率</p>
<p>二者差异性：</p>
<p>1.对目标的拟合程度：Boosting对目标有更好的拟合能力（偏差小）；Bagging则偏差相对大一些</p>
<p>2.运行效率：由于并行的特点，Bagging的运行效率是大于Boosting的</p>
<p>3.泛化能力：由于Bagging每个学习器不会受其他学习器的影响，泛化能力（方差大）相对于Boosting</p>
<p>更好</p>
<h5 id="bagging的算法流程">Bagging的算法流程</h5>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250606182733022.png" alt="image-20250606182733022">
<figcaption aria-hidden="true">image-20250606182733022</figcaption>
</figure>
<p>可以看出Bagging主要通过<strong>样本的扰动</strong>来增加基学习器之间的多样性，因此Bagging的基学习器应为那些对训练集十分敏感的不稳定学习算法，例如：神经网络与决策树等。</p>
<p>从偏差-方差分解来看，Bagging算法主要关注于降低方差，即通过多次重复训练提高稳定性。</p>
<p>不同于AdaBoost的是，Bagging可以十分简单地移植到多分类、回归等问题。总的说起来则是：<strong>AdaBoost关注于降低偏差，而Bagging关注于降低方差。</strong></p>
<h5 id="自助采样法bootstrap-sampling">自助采样法（Bootstrap
Sampling）</h5>
<p>在机器学习中，<strong>自助采样法（Bootstrap Sampling）</strong> 是
Bagging
算法的核心技术之一。其核心思想是从原始数据集中有放回地随机抽取样本，形成新的训练子集。这一过程的一个重要数学性质是：当样本量
<span class="math inline"><em>n</em></span> 趋近于无穷大时，每个样本在
Bootstrap 样本集中<strong>未被抽中</strong>的概率趋近于 <span class="math inline">$\frac{1}{e} \approx
36.6\%$</span>。以下是详细解析：</p>
<p><strong>1. 公式推导</strong></p>
<p>假设我们从 <span class="math inline"><em>n</em></span>
个样本中<strong>有放回地</strong>抽取 <span class="math inline"><em>n</em></span> 次，形成一个 Bootstrap
样本集。对于任意一个特定样本（如第 <span class="math inline"><em>i</em></span>
个样本），它在某次抽样中<strong>未被选中</strong>的概率为： <span class="math display">$$
1 - \frac{1}{n}
$$</span> 因此，它在整个 <span class="math inline"><em>n</em></span>
次抽样中<strong>从未被选中</strong>的概率为： <span class="math display">$$
\left(1 - \frac{1}{n}\right)^n
$$</span> 当 <span class="math inline"><em>n</em> → ∞</span>
时，该概率的极限为： <span class="math display">$$
\lim_{n \to \infty} \left(1 - \frac{1}{n}\right)^n = \frac{1}{e} \approx
0.3679 \quad (\text{即 } 36.6\%)
$$</span> 在每次 Bootstrap 采样中，约有 <strong>36.6%
的样本未被选中</strong> ，这些样本称为
<strong>Out-of-Bag（OOB，包外估计）样本</strong> 。</p>
<p><strong>2. OOB 样本的应用</strong></p>
<p>在 Bagging 算法中，OOB 样本具有以下重要作用： 1.
<strong>无偏验证</strong>：<br>
每个基学习器的训练数据不包含其对应的 OOB
样本，因此可以用这些样本直接评估模型性能（即 OOB
误差），无需额外的交叉验证。 2. <strong>特征重要性评估</strong>：<br>
在随机森林中，通过比较 OOB
样本在打乱某个特征后的预测误差变化，可以衡量该特征的重要性。</p>
<p><strong>3. 与其他采样方法的对比</strong></p>
<table>
<colgroup>
<col style="width: 25%">
<col style="width: 17%">
<col style="width: 32%">
<col style="width: 24%">
</colgroup>
<thead>
<tr class="header">
<th><strong>采样方法</strong></th>
<th><strong>是否放回</strong></th>
<th><strong>样本覆盖范围</strong></th>
<th><strong>典型应用场景</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Bootstrap 采样</strong></td>
<td>是</td>
<td>约 63.4% 样本被重复使用</td>
<td>Bagging、随机森林</td>
</tr>
<tr class="even">
<td><strong>简单随机采样</strong></td>
<td>否</td>
<td>所有样本唯一出现</td>
<td>传统交叉验证</td>
</tr>
</tbody>
</table>
<h5 id="随机森林">随机森林</h5>
<p>随机森林（Random
Forest）是Bagging的一个拓展体，它的基学习器固定为<strong>决策树</strong>，多棵树也就组成了森林，而<strong>“随机”则在于选择划分属性的随机</strong>，随机森林在训练基学习器时，也采用有放回采样的方式添加样本扰动，同时它还引入了一种<strong>属性扰动</strong>，即在基决策树的训练过程中，在选择划分属性时，RF先从候选属性集中随机挑选出一个包含K个属性的子集，再从这个子集中选择最优划分属性
。</p>
<p>这样随机森林中基学习器的<strong>多样性不仅来自样本扰动，还来自属性扰动</strong>，从而进一步提升了基学习器之间的差异度。相比决策树的Bagging集成，随机森林的起始性能较差（由于属性扰动，基决策树的准确度有所下降），但随着基学习器数目的增多，随机森林往往会收敛到更低的泛化误差。同时不同于Bagging中决策树从所有属性集中选择最优划分属性，<strong>随机森林只在属性集的一个子集中选择划分属性，因此训练效率更高</strong>。</p>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250606184958951.png" alt="image-20250606184958951">
<figcaption aria-hidden="true">image-20250606184958951</figcaption>
</figure>
<h4 id="结合策略">结合策略</h4>
<p>在集成学习中，结合策略是将多个基学习器的输出整合为最终预测结果的关键步骤。以下是针对回归和分类问题的不同结合策略及其核心要点：</p>
<p><strong>定义</strong>：在训练好多个基学习器后，如何将其输出组合成集成模型的最终输出。</p>
<h5 id="平均法回归问题"><strong>1.平均法（回归问题）</strong></h5>
<ol type="1">
<li><p><strong>简单平均法（Simple Averaging）</strong></p>
<ul>
<li><strong>公式</strong>：<br>
<span class="math display">$$
H(x) = \frac{1}{T} \sum_{i=1}^{T} h_i(x)
$$</span></li>
<li><strong>特点</strong>：
<ul>
<li>直接对所有基学习器的预测结果取算术平均。<br>
</li>
<li>计算简单，适合基学习器性能相近的场景。<br>
</li>
<li>若部分基学习器表现较差，可能拖累整体性能。</li>
</ul></li>
</ul></li>
<li><p><strong>加权平均法（Weighted Averaging）</strong></p>
<ul>
<li><strong>公式</strong>：<br>
<span class="math display">$$
H(x) = \sum_{i=1}^{T} w_i h_i(x)
$$</span> 其中，$ w_i $ 且 $ _{i=1}^{T} w_i = 1 $。<br>
</li>
<li><strong>特点</strong>：
<ul>
<li>通过权重 $ w_i $ 调节各基学习器的贡献，灵活性更高。<br>
</li>
<li>适用于基学习器性能差异较大的情况，可提升鲁棒性。<br>
</li>
<li>权重可通过验证集性能（如RMSE、MAE）或优化算法（如梯度下降）确定。</li>
</ul></li>
</ul></li>
</ol>
<h5 id="投票法分类问题"><strong>2.投票法（分类问题）</strong></h5>
<ol type="1">
<li><strong>简单投票法（Majority Voting）</strong>
<ul>
<li><strong>原理</strong>：<br>
每个基学习器对样本进行分类投票，最终结果由得票最多的类别决定。<br>
</li>
<li><strong>公式</strong>（二分类示例）：<br>
<span class="math display">$$
H(x) =
\begin{cases}
1 &amp; \text{若} \sum_{i=1}^{T} I(h_i(x) = 1) &gt; T/2 \\
0 &amp; \text{否则}
\end{cases}
$$</span> 其中，$ I() $ 为指示函数。<br>
</li>
<li><strong>特点</strong>：
<ul>
<li>简单高效，适合基学习器性能相近的场景。<br>
</li>
<li>对异常分类器的鲁棒性较弱。</li>
</ul></li>
</ul></li>
<li><strong>加权投票法（Weighted Voting）</strong>
<ul>
<li><strong>原理</strong>：<br>
给不同基学习器分配权重，最终结果由加权票数最高的类别决定。<br>
</li>
<li><strong>公式</strong>（二分类示例）：<br>
<span class="math display">$$
H(x) =
\begin{cases}
1 &amp; \text{若} \sum_{i=1}^{T} w_i I(h_i(x) = 1) &gt; 0.5
\sum_{i=1}^{T} w_i \\
0 &amp; \text{否则}
\end{cases}
$$</span></li>
<li><strong>特点</strong>：
<ul>
<li>权重可根据基学习器的验证集准确率或领域知识设定。<br>
</li>
<li>更适合处理性能差异较大的基学习器。</li>
</ul></li>
</ul></li>
</ol>
<p>绝对多数投票法（majority
voting）提供了拒绝选项，这在可靠性要求很高的学习任务中是一个很好的机制。同时，对于分类任务，各个基学习器的输出值有两种类型，分别为类标记和类概率。</p>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250606195241433.png" alt="image-20250606195241433">
<figcaption aria-hidden="true">image-20250606195241433</figcaption>
</figure>
<p>一些在产生类别标记的同时也生成置信度的学习器，置信度可转化为类概率使用，<strong>一般基于类概率进行结合往往比基于类标记进行结合的效果更好</strong>，需要注意的是对于异质集成，其类概率不能直接进行比较，此时需要将类概率转化为类标记输出，然后再投票。</p>
<h5 id="学习法stacking"><strong>3.学习法（Stacking）</strong></h5>
<p><strong>学习法</strong>是一种更高级的结合策略，其核心思想是通过训练一个<strong>次级学习器（Meta-Learner）</strong>
来动态融合多个基学习器的输出。其中，<strong>Stacking（堆叠泛化）</strong>
是学习法的典型代表，它通过将基学习器的预测结果作为新特征，进一步训练一个次级模型，最终实现更优的泛化性能。</p>
<p><strong>Stacking 的基本原理</strong></p>
<p><strong>步骤概述</strong>：</p>
<ul>
<li><strong>训练基学习器</strong>：使用原始数据训练 $ T $
个基学习器（如决策树、SVM、神经网络等）。<br>
</li>
<li><strong>生成新特征</strong>：对于每个样本，将 $ T $
个基学习器的输出（预测结果）作为该样本的新特征，形成一个 $ m T $
的数据集（$ m $ 为样本数量）。<br>
</li>
<li><strong>训练次级学习器</strong>：使用新数据集（基学习器输出 +
真实标签）训练一个次级学习器（如逻辑回归、梯度提升树等），该学习器负责融合基学习器的预测结果。</li>
</ul>
<p><strong>Stacking 的优势</strong></p>
<ol type="1">
<li><strong>动态权重分配</strong>：<br>
次级学习器可以自动学习基学习器的权重，无需人工设定。例如，若某个基学习器表现优异，次级学习器会赋予其更高的权重。<br>
</li>
<li><strong>异质模型融合</strong>：<br>
可以混合不同类型的基学习器（如线性模型与树模型），充分利用各自的特性。<br>
</li>
<li><strong>提升泛化能力</strong>：<br>
次级学习器通过学习基学习器的输出模式，能够捕捉更复杂的决策边界。</li>
</ol>
<p><strong>Stacking 的实现细节</strong></p>
<ol type="1">
<li><strong>数据划分</strong>：
<ul>
<li>通常需将原始数据分为两部分：
<ul>
<li><strong>训练集</strong>：用于训练基学习器。<br>
</li>
<li><strong>验证集</strong>：用于生成基学习器的输出（避免过拟合）。<br>
</li>
</ul></li>
<li>或采用交叉验证（如 $ k
$-折）生成基学习器的预测结果，确保次级学习器的训练数据不被污染。</li>
</ul></li>
<li><strong>基学习器输出类型</strong>：
<ul>
<li><strong>分类任务</strong>：基学习器输出类概率（Soft
Voting），而非类别标签（Hard Voting）。例如，逻辑回归输出 $ P(c_j | x)
$，随机森林输出节点样本的类别分布。<br>
</li>
<li><strong>回归任务</strong>：基学习器直接输出预测值（如线性回归的
<span class="math inline"><em>ŷ</em></span>）。</li>
</ul></li>
<li><strong>次级学习器选择</strong>：
<ul>
<li><strong>多响应线性回归（MLR）</strong>：适用于基学习器输出可加权平均的情况，计算简单且鲁棒。<br>
<span class="math display">$$
H(x) = \sum_{i=1}^{T} w_i h_i(x)
$$</span></li>
<li><strong>复杂模型</strong>：如梯度提升树、神经网络，可捕捉基学习器输出之间的非线性关系。</li>
</ul></li>
</ol>
<h4 id="多样性">多样性</h4>
<p>在集成学习中，<strong>多样性增强（Diversity Enhancement）</strong>
是提升模型性能的关键策略。通过引入多样性，可以降低基学习器之间的相关性，从而减少误差传递和过拟合风险。以下是四种常见的多样性增强方法及其核心要点：</p>
<p><strong>1. 数据样本扰动（Data Perturbation）</strong></p>
<p><strong>原理</strong>：通过修改训练数据的分布或采样方式，使每个基学习器看到不同的数据子集。</p>
<p><strong>实现方式</strong>：<br>
- <strong>Bagging（如随机森林）</strong>：<br>
- 随机有放回地采样（Bootstrap），生成多个不同的训练集。<br>
- 对输入扰动敏感的基学习器（如决策树、神经网络）效果显著。<br>
- <strong>示例</strong>：<br>
- 决策树对数据扰动敏感，Bagging 可有效提升其泛化能力。<br>
- 线性模型（如线性回归、SVM）对数据扰动不敏感，Bagging 效果有限。</p>
<p><strong>2. 输入属性扰动（Input Attribute Perturbation）</strong></p>
<p><strong>原理</strong>：通过改变输入特征的表示或选择，增加基学习器间的差异。</p>
<p><strong>实现方式</strong>：<br>
-
<strong>特征子集采样</strong>：每次随机选择部分特征进行训练（如随机森林中的列扰动）。<br>
- <strong>特征变换</strong>：对特征进行缩放、旋转或加噪声等操作。<br>
- <strong>适用场景</strong>：<br>
- 数据包含大量冗余属性时，可大幅加速训练并提升多样性。<br>
- 对高维数据（如图像、文本）尤其有效。</p>
<p><strong>3. 输出属性扰动（Output Attribute Perturbation）</strong></p>
<p><strong>原理</strong>：通过修改训练样本的标签，间接影响基学习器的学习过程。</p>
<p><strong>实现方式</strong>：<br>
-
<strong>随机翻转标签</strong>：对部分样本的标记进行随机更改（需谨慎使用，避免干扰模型）。<br>
- <strong>Dropout（神经网络）</strong>：<br>
- 在训练过程中随机“关闭”部分神经元，强制网络学习更鲁棒的特征。<br>
- 类似于对输出属性的随机扰动，可提升模型泛化能力。</p>
<p><strong>4. 算法参数扰动（Algorithm Parameter
Perturbation）</strong></p>
<p><strong>原理</strong>：通过调整基学习器的超参数，生成不同的模型行为。</p>
<p><strong>实现方式</strong>：</p>
<ul>
<li><strong>正则化方法</strong>：L1/L2 正则化（如
Ridge、Lasso）限制模型复杂度，降低过拟合风险。</li>
<li><strong>随机初始化</strong>：
神经网络的随机权重初始化可能导致收敛到不同局部最优解。</li>
</ul>
<h4 id="作业-1">作业</h4>
<h5 id="section-3">1</h5>
<p>集成学习中常见的两种方法是什么？请分别介绍它们的原理和特点。集成学习相比于单个模型有什么优势和应用场景？</p>
<p><strong>集成学习常见方法、原理、特点及优势</strong></p>
<p><strong>常见方法</strong>：Bagging 和 Boosting<br>
<strong>原理与特点</strong>：</p>
<table>
<colgroup>
<col style="width: 9%">
<col style="width: 45%">
<col style="width: 45%">
</colgroup>
<thead>
<tr class="header">
<th><strong>方法</strong></th>
<th><strong>原理</strong></th>
<th><strong>特点</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Bagging</strong></td>
<td>1. <strong>自助采样</strong>：从训练集有放回抽取多个子集<br>2.
<strong>并行训练</strong>基模型<br>3.
<strong>聚合预测</strong>（投票/平均）</td>
<td>- 降低方差<br>- 适合高方差模型（如未剪枝决策树）<br>-
并行化，训练快<br>- 代表：随机森林</td>
</tr>
<tr class="even">
<td><strong>Boosting</strong></td>
<td>1. <strong>顺序训练</strong>：后一个模型修正前一个模型的错误<br>2.
<strong>加权困难样本</strong><br>3. <strong>加权组合</strong>模型</td>
<td>- 降低偏差<br>- 需弱学习器（如树桩）<br>- 易过拟合（需正则化）<br>-
代表：AdaBoost, GBDT, XGBoost</td>
</tr>
</tbody>
</table>
<p><strong>集成学习的优势</strong>：<br>
-
<strong>提升泛化能力</strong>：降低过拟合（Bagging）或欠拟合（Boosting）风险<br>
- <strong>增强鲁棒性</strong>：减少异常值/噪声影响（如投票机制）<br>
- <strong>突破性能上限</strong>：组合多个弱模型达到强模型效果</p>
<p><strong>应用场景</strong>：<br>
- <strong>分类任务</strong>：医疗诊断（整合多模型减少误诊）<br>
- <strong>回归任务</strong>：房价预测（融合不同树模型提升精度）<br>
- <strong>不平衡数据</strong>：Boosting 加权少数类样本<br>
- <strong>高维数据</strong>：随机森林自动特征选择</p>
<h5 id="section-4">2</h5>
<p>如果在完全相同的训练集上训练了五个不同的模型，并且它们都达到了95%的准确率，是否还有机会通过结合这些模型来获得更好的结果？如果可以，该怎么做？如果不行，为什么？</p>
<p><strong>模型结合提升性能的可能性与方法</strong></p>
<p><strong>是否可能提升</strong>：<strong>是</strong>，但需满足条件：<strong>模型错误不相关</strong>（即犯错样本不同）。</p>
<p><strong>如何实现</strong>：</p>
<ol type="1">
<li><strong>投票法（分类）</strong>：
<ul>
<li>多数投票：5个模型对样本 (x) 的预测为 ([A, A, B, A, C]) → 最终输出
(A)<br>
</li>
<li><strong>关键要求</strong>：模型存在<strong>多样性</strong>（如使用SVM、决策树等不同算法）<br>
</li>
</ul></li>
<li><strong>加权平均（回归）</strong>：
<ul>
<li>若模型精度不同，分配权重：$ y_{} = w_1 y_1 + w_2 y_2 + + w_5
y_5$</li>
<li>权重可通过验证集性能确定</li>
</ul></li>
</ol>
<p><strong>若无法提升的情况</strong>：<br>
-
<strong>原因</strong>：模型高度相关（如相同算法、相同特征、相同超参）<br>
- <strong>数学解释</strong>：误差相关性 <span class="math inline"><em>r</em><em>h</em><em>o</em> ≈ 1</span>
时，集成误差 <span class="math inline">≈</span>单一模型误差</p>
<h5 id="section-5">3</h5>
<p>是否可以通过在多个服务器上并行来加速随机森林的训练？AdaBoost集成呢？为什么？</p>
<table>
<colgroup>
<col style="width: 13%">
<col style="width: 18%">
<col style="width: 68%">
</colgroup>
<thead>
<tr class="header">
<th><strong>算法</strong></th>
<th><strong>是否支持并行</strong></th>
<th><strong>原因</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>随机森林</strong></td>
<td>✅ <strong>是</strong></td>
<td>1. 树之间独立训练<br>2. 可分布式分配Bootstrap样本到不同服务器<br>3.
特征分裂也可并行（如选特征子集）</td>
</tr>
<tr class="even">
<td><strong>AdaBoost</strong></td>
<td>❌ <strong>否</strong></td>
<td>1.
模型必须<strong>顺序训练</strong>：后一个模型依赖前一个模型的样本权重更新<br>2.
无法解耦迭代过程</td>
</tr>
</tbody>
</table>
<h3 id="聚类">聚类</h3>
<h4 id="聚类任务"><strong>聚类任务</strong></h4>
<blockquote>
<p>我们之前学习的分类/回归任务都属于 有监督学习
需要我们提供样本与标签</p>
<p>而马上要学习的聚类任务和后续学习的降维则属于 无监督学习
仅需提供样本</p>
</blockquote>
<p>聚类是一种经典的<strong>无监督学习</strong>(unsupervised
learning)方法，<strong>无监督学习的目标是通过对无标记训练样本的学习，发掘和揭示数据集本身潜在的结构与规律</strong>，即不依赖于训练数据集的类标记信息。</p>
<p>聚类试图将数据集中的样本划分为若干个通常是不相交的子集,<strong>每个子集称为一个“簇”(
cluster)</strong>。通过这样的划分,每簇可能对应于一些潜在的概念(类别),如“浅色瓜”“深色瓜”,“有籽瓜”“无籽瓜”,甚至“本地瓜”“外地瓜”等;需说明的是,这些概念对聚类算法而言事先是未知的,聚类过程仅能自动形成簇结构,
<strong>簇所对应的概念语义需由使用者来把握和命名</strong>。</p>
<p>直观上来说，聚类是将相似的样本聚在一起，从而形成一个<strong>类簇（cluster）</strong>。涉及两个问题</p>
<ul>
<li>如何<strong>度量相似性</strong>（similarity
measure），这便是<strong>距离度量</strong>(distance
measure)，在生活中我们说差别小则相似，对应到多维样本，每个样本可以对应于高维空间中的一个数据点，若它们的距离相近，我们便可以称它们相似。</li>
<li>如何<strong>评价聚类结果</strong>，这便是<strong>性能度量</strong>(validity
index)</li>
</ul>
<h4 id="距离度量">距离度量</h4>
<h5 id="连续离散有序">连续/离散有序</h5>
<p><strong>明可夫斯基距离（Minkowski Distance）</strong></p>
<p>明可夫斯基距离是一组常用的<strong>连续型距离度量</strong>，通过调整参数
$ p $ 可以统一表示多种距离形式，是欧氏距离和曼哈顿距离的推广。</p>
<p><strong>1. 公式定义</strong></p>
<p>对于两个 $ n $ 维向量 $ <em>i = (x</em>{i1}, x_{i2}, , x_{in}) $ 和 $
<em>j = (x</em>{j1}, x_{j2}, , x_{jn}) $，明可夫斯基距离的计算公式为：
<span class="math display">$$
\text{dist}_{\text{mk}}(\boldsymbol{x}_i, \boldsymbol{x}_j) = \left(
\sum_{u=1}^{n} |x_{iu} - x_{ju}|^p \right)^{\frac{1}{p}}
$$</span> 其中，$ p $ 是一个可调节的参数。</p>
<p><strong>2. 特殊情况</strong></p>
<ul>
<li><strong>当 $ p = 2 $</strong>：退化为<strong>欧氏距离（Euclidean
Distance）</strong><br>
<span class="math display">$$
\text{dist}_{\text{ed}}(\boldsymbol{x}_i, \boldsymbol{x}_j) =
\sqrt{\sum_{u=1}^{n} |x_{iu} - x_{ju}|^2}
$$</span>
<ul>
<li><strong>几何意义</strong>：两点之间的直线距离。<br>
</li>
<li><strong>适用场景</strong>：大多数机器学习算法（如KNN、PCA）默认使用欧氏距离。</li>
</ul></li>
<li><strong>当 $ p = 1 $</strong>：退化为<strong>曼哈顿距离（Manhattan
Distance）</strong><br>
<span class="math display">$$
\text{dist}_{\text{man}}(\boldsymbol{x}_i, \boldsymbol{x}_j) =
\sum_{u=1}^{n} |x_{iu} - x_{ju}|
$$</span>
<ul>
<li><strong>几何意义</strong>：沿坐标轴移动的总距离（如棋盘格路径）。<br>
</li>
<li><strong>适用场景</strong>：高维稀疏数据（如文本特征）、计算资源受限的场景。</li>
</ul></li>
<li><strong>当 $ p $</strong>：退化为<strong>切比雪夫距离（Chebyshev
Distance）</strong><br>
<span class="math display">dist<sub>che</sub>(<strong>x</strong><sub><em>i</em></sub>,<strong>x</strong><sub><em>j</em></sub>) = max<sub><em>u</em></sub>|<em>x</em><sub><em>i</em><em>u</em></sub>−<em>x</em><sub><em>j</em><em>u</em></sub>|</span>
<ul>
<li><strong>几何意义</strong>：各维度差值的最大值。<br>
</li>
<li><strong>适用场景</strong>：关注最坏情况下的误差（如游戏AI路径规划）。</li>
</ul></li>
</ul>
<p><strong>3. 参数 $ p $ 的影响</strong></p>
<ul>
<li><strong>$ p $
越小</strong>：距离计算越关注较小的维度差异（如曼哈顿距离对单个维度的扰动更敏感）。<br>
</li>
<li><strong>$ p $
越大</strong>：距离计算越关注较大的维度差异（如切比雪夫距离仅关注最大差值）。<br>
</li>
<li><strong>选择依据</strong>：
<ul>
<li>数据分布是否均匀：若某些维度差异显著，可增大 $ p $。<br>
</li>
<li>算法需求：如KNN中，高维数据可能更适合曼哈顿距离（缓解“维度灾难”）。</li>
</ul></li>
</ul>
<h5 id="离散无序">离散无序</h5>
<p>我们知道属性分为两种：<strong>连续属性</strong>(continuous
attribute)和<strong>离散属性</strong>（catergorical
attribute有限个取值）。对于连续值的属性，一般都可以被学习器所用，有时会根据具体的情形作相应的预处理，例如：归一化等；而对于离散值的属性，需要作下面进一步的处理：</p>
<blockquote>
<p>若属性值之间<strong>存在序关系</strong>(ordinal
attribute)，则可以将其转化为连续值，例如：身高属性“高”“中等”“矮”，可转化为{1,
0.5, 0}。</p>
<p>若属性值之间<strong>不存在序关系</strong>(non-ordinal
attribute)，则通常将其转化为向量的形式，例如：性别属性“男”“女”，可转化为{（1,0）,（0,1）}。</p>
</blockquote>
<p><strong>连续属性和存在序关系的离散属性都可以直接参与计算</strong>，而不存在序关系的<strong>无序属性，我们一般采用VDM（Value
Difference Metric）进行距离的计算</strong></p>
<p>VDM
是一种专门用于<strong>离散无序属性</strong>的距离度量方法，通过统计信息量化不同类别间的差异。其核心思想是：<strong>若两个类别的样本在目标变量上的分布差异越大，则它们的距离越大</strong>。</p>
<p><strong>1. 公式解析</strong> <span class="math display">$$
\text{VDM}_p(a, b) = \sum_{i=1}^{k} \left| \frac{m_{u,a,i}}{m_{u,a}} -
\frac{m_{u,b,i}}{m_{u,b}} \right|^p
$$</span> - <strong>符号含义</strong>：<br>
- $ a, b $：两个不同的类别值（如性别“男”和“女”）。<br>
- $ m_{u,a,i} $：在属性 $ u $ 的第 $ i $ 个取值下，类别 $ a $
的样本数量。<br>
- $ m_{u,a} $：类别 $ a $ 的总样本数量。<br>
- $ k $：属性 $ u $ 的不同取值数目（如颜色属性有红、蓝、绿三种取值，则 $
k=3 $）。<br>
- $ p $：距离幂指数（通常取 $ p=1 $ 或 $ p=2 $）。</p>
<p><strong>2. 核心思想</strong></p>
<ul>
<li><strong>统计分布差异</strong>：<br>
对于每个属性取值 $ i $，计算类别 $ a $ 和 $ b $ 的样本比例差异：<br>
<span class="math display">$$
\left| \frac{m_{u,a,i}}{m_{u,a}} - \frac{m_{u,b,i}}{m_{u,b}} \right|
$$</span> 该值越大，说明两个类别在该取值上的分布差异越大。<br>
</li>
<li><strong>加权求和</strong>：<br>
将所有属性取值的差异按 $ p $ 次方加权求和，得到最终的距离。</li>
</ul>
<p><strong>3. 示例说明</strong></p>
<p>假设我们有一个“颜色”属性（红、蓝、绿），目标变量是“是否购买商品”（0/1）。统计结果如下：</p>
<table>
<thead>
<tr class="header">
<th>颜色</th>
<th>购买（1）</th>
<th>不购买（0）</th>
<th>总计</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>红</td>
<td>10</td>
<td>5</td>
<td>15</td>
</tr>
<tr class="even">
<td>蓝</td>
<td>8</td>
<td>12</td>
<td>20</td>
</tr>
<tr class="odd">
<td>绿</td>
<td>3</td>
<td>7</td>
<td>10</td>
</tr>
</tbody>
</table>
<p>计算“红”与“蓝”之间的 VDM 距离（$ p=1 <span class="math inline">）：1.<em>计</em><em>算</em><em>每</em><em>个</em><em>颜</em><em>色</em><em>在</em><em>购</em><em>买</em>/<em>不</em><em>购</em><em>买</em><em>的</em><em>比</em><em>例</em>： − <em>红</em>：</span>
P(1) = 10/15 <span class="math inline">，</span> P(0) = 5/15 $<br>
- 蓝：$ P(1) = 8/20 = 0.4 <span class="math inline">，</span> P(0) =
12/20 = 0.6 $<br>
2. 计算差异并求和：<br>
<span class="math display">VDM<sub>1</sub>(红,蓝) = |0.67−0.4| + |0.33−0.6| = 0.27 + 0.27 = 0.54</span></p>
<h4 id="性能度量">性能度量</h4>
<p>于聚类算法不依赖于样本的真实类标，就不能像监督学习的分类那般，通过计算分对分错（即精确度或错误率）来评价学习器的好坏或作为学习过程中的优化目标。</p>
<p>直观上看,我们希望<strong>“物以类聚”</strong>,即同一簇的样本尽可能彼此相似,不同簇的样本尽可能不同换言之,聚类结果的<strong>“簇内相似度”(
intra-cluster similarity)高且“簇间相似度” inter-cluster
similarity)低</strong></p>
<p><strong>聚类性能度量有两类</strong></p>
<ul>
<li>“外部指标”(external
index)：所谓外部指标就是已经有一个“参考模型”存在了，将当前模型与参考模型的比对结果作为指标。</li>
<li>“内部指标”( internal
index)：所谓内部指标就是仅仅考虑当前模型的聚类结果。</li>
</ul>
<h5 id="外部指标">外部指标</h5>
<p><strong>1.基本概念</strong></p>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250607110103570.png" alt="image-20250607110103570">
<figcaption aria-hidden="true">image-20250607110103570</figcaption>
</figure>
<p>显然，$ a + b + c + d = $ 。</p>
<p><strong>2. 常用外部指标</strong></p>
<p><strong>（1）Jaccard系数（JC）</strong> <span class="math display">$$
\text{JC} = \frac{a}{a + b + c}
$$</span> -
<strong>含义</strong>：衡量两个划分的重叠程度，仅考虑正确匹配（$ a <span class="math inline">）<em>与</em><em>矛</em><em>盾</em><em>情</em><em>况</em>（</span>
b + c <span class="math inline">）。 −  *  * <em>范</em><em>围</em> *  * ：</span>
[0, 1] $，值越大越好。<br>
- <strong>特点</strong>：对称性差，对噪声敏感 。</p>
<p><strong>（2）Fowlkes-Mallows指数（FMI）</strong> <span class="math display">$$
\text{FMI} = \sqrt{\frac{a}{a + b} \cdot \frac{a}{a + c}}
$$</span> - <strong>含义</strong>：结合查准率（$ <span class="math inline">）<em>和</em><em>查</em><em>全</em><em>率</em>（</span>
<span class="math inline">），<em>反</em><em>映</em><em>正</em><em>确</em><em>匹</em><em>配</em><em>的</em><em>综</em><em>合</em><em>能</em><em>力</em>。 −  *  * <em>范</em><em>围</em> *  * ：</span>
[0, 1] $，值越大越好。<br>
- <strong>特点</strong>：平衡性较好，适合小样本 。</p>
<p><strong>（3）Rand指数（RI）</strong> <span class="math display">$$
\text{RI} = \frac{2(a + d)}{m(m - 1)}
$$</span> - <strong>含义</strong>：同时考虑正确匹配（$ a + d <span class="math inline">）<em>与</em><em>总</em><em>样</em><em>本</em><em>对</em><em>数</em>，<em>适</em><em>用</em><em>于</em><em>大</em><em>规</em><em>模</em><em>数</em><em>据</em>。 −  *  * <em>范</em><em>围</em> *  * ：</span>
[0, 1] $，值越大越好。<br>
- <strong>特点</strong>：计算简单，但对噪声较鲁棒 。</p>
<p><strong>常用指标</strong></p>
<ul>
<li><strong>调整兰德指数（Adjusted Rand Index, ARI）</strong>
<ul>
<li><strong>定义</strong>：衡量聚类结果与真实标签的匹配程度，调整随机聚类的影响，取值范围
[-1, 1]，值越大越好。<br>
</li>
<li><strong>公式</strong>：<br>
<span class="math display">$$
\text{ARI} = \frac{\text{RI} - \mathbb{E}[\text{RI}]}{\max(\text{RI}) -
\mathbb{E}[\text{RI}]}
$$</span> 其中 RI 是兰德指数（匹配样本对的比例）。</li>
</ul></li>
<li><strong>归一化互信息（Normalized Mutual Information, NMI）</strong>
<ul>
<li><strong>定义</strong>：衡量聚类结果与真实标签的信息共享程度，值越大越好。<br>
</li>
<li><strong>公式</strong>：<br>
<span class="math display">$$
\text{NMI} = \frac{I(C; K)}{\sqrt{H(C) H(K)}}
$$</span> 其中 $ I(C; K) $ 是互信息，$ H(C) $ 和 $ H(K) $ 是熵。</li>
</ul></li>
<li><strong>Fowlkes-Mallows 指数（FMI）</strong>
<ul>
<li><strong>定义</strong>：基于聚类结果与真实标签的 TP、FP、TN、FN
计算，值越大越好。<br>
</li>
<li><strong>公式</strong>：<br>
<span class="math display">$$
\text{FMI} = \sqrt{\frac{\text{TP}}{\text{TP} + \text{FP}} \cdot
\frac{\text{TP}}{\text{TP} + \text{FN}}}
$$</span></li>
</ul></li>
</ul>
<p><strong>优点</strong></p>
<ul>
<li>在有真实标签时，能更客观地评估聚类效果。</li>
<li>适用于验证聚类结果的业务意义（如客户分群是否符合预期）。</li>
</ul>
<p><strong>局限性</strong></p>
<ul>
<li>需要真实标签，不适用于纯无监督任务。</li>
<li>对标签噪声敏感（如标签错误会误导 $ K $ 的选择）。</li>
</ul>
<p><strong>3. 应用示例</strong></p>
<p>假设一个包含4个样本的数据集，参考标签为 <span class="math inline">{<em>A</em>, <em>A</em>, <em>B</em>, <em>B</em>}</span>，聚类结果为
<span class="math inline">{<em>C</em>, <em>C</em>, <em>D</em>, <em>D</em>}</span>：
- <strong>计算样本对</strong>：<br>
- $ a = 2 $（样本1-2同簇，参考与聚类均同类）。<br>
- $ b = 0 $（参考同类但聚类不同类）。<br>
- $ c = 0 $（参考不同类但聚类同类）。<br>
- $ d = 2 $（参考不同类且聚类不同类）。<br>
- <strong>指标结果</strong>：<br>
- JC = $ = 1 $（完美匹配）。<br>
- FMI = $ = 1 $。<br>
- RI = $ = $。</p>
<h5 id="内部指标">内部指标</h5>
<p>内部指标不依赖任何外部参考模型，直接通过<strong>簇内紧凑性</strong>和<strong>簇间分离性</strong>评估聚类结果。其核心思想是：
- <strong>簇内高内聚</strong>：同一簇的样本尽可能相似（距离小）。<br>
- <strong>簇间低耦合</strong>：不同簇的样本尽可能不同（距离大）。</p>
<p><strong>1. 基本定义</strong></p>
<p>设聚类结果为 $ C = {C_1, C_2, , C_k} $，定义以下四个关键距离：</p>
<p><strong>（1）簇内平均距离（avg(C)）</strong> <span class="math display">$$
\text{avg}(C) = \frac{2}{|C|(|C| - 1)} \sum_{1 \leq i &lt; j \leq |C|}
\text{dist}(\boldsymbol{x}_i, \boldsymbol{x}_j)
$$</span> - <strong>含义</strong>：簇内所有样本对的平均距离。<br>
- <strong>目标</strong>：越小越好，表示簇内样本更紧密。</p>
<p><strong>（2）簇内最大距离（diam(C)）</strong> <span class="math display">diam(<em>C</em>) = max<sub>1 ≤ <em>i</em> &lt; <em>j</em> ≤ |<em>C</em>|</sub>dist(<strong>x</strong><sub><em>i</em></sub>,<strong>x</strong><sub><em>j</em></sub>)</span>
- <strong>含义</strong>：簇内最远的两个样本之间的距离。<br>
- <strong>目标</strong>：越小越好，避免簇内存在离群点。</p>
<p><strong>（3）簇间最小距离（$ d_{}(C_i, C_j) $）</strong> <span class="math display"><em>d</em><sub>min</sub>(<em>C</em><sub><em>i</em></sub>,<em>C</em><sub><em>j</em></sub>) = min<sub><strong>x</strong><sub><em>i</em></sub> ∈ <em>C</em><sub><em>i</em></sub>, <strong>x</strong><sub><em>j</em></sub> ∈ <em>C</em><sub><em>j</em></sub></sub>dist(<strong>x</strong><sub><em>i</em></sub>,<strong>x</strong><sub><em>j</em></sub>)</span>
- <strong>含义</strong>：簇 $ C_i $ 和 $ C_j $
之间最近的两个样本的距离。<br>
- <strong>目标</strong>：越大越好，表示簇间分离度高。</p>
<p><strong>（4）簇中心距离（$ d_{}(C_i, C_j) $）</strong> <span class="math display"><em>d</em><sub>cen</sub>(<em>C</em><sub><em>i</em></sub>,<em>C</em><sub><em>j</em></sub>) = dist(<strong>μ</strong><sub><em>i</em></sub>,<strong>μ</strong><sub><em>j</em></sub>)</span>
- <strong>含义</strong>：簇 $ C_i $ 和 $ C_j $
的中心点（均值向量）之间的距离。<br>
- <strong>目标</strong>：越大越好，表示簇中心相隔较远。</p>
<p><strong>2. 常用内部指标</strong></p>
<p><strong>1. DB指数（Davies-Bouldin Index, DBI）</strong></p>
<ul>
<li><strong>公式</strong>：<br>
<span class="math display">$$
\text{DBI} = \frac{1}{k} \sum_{i=1}^{k} \max_{j \neq i} \left(
\frac{\text{avg}(C_i) + \text{avg}(C_j)}{d_{\text{cen}}(\mu_i, \mu_j)}
\right)
$$</span>
<ul>
<li><strong>符号含义</strong>：
<ul>
<li>$ k $：簇的数量。<br>
</li>
<li>$ (C_i) $：簇 $ C_i $ 内部样本的平均距离。<br>
</li>
<li>$ d_{}(_i, _j) $：簇 $ C_i $ 和 $ C_j $
的中心点（均值向量）之间的距离。<br>
</li>
</ul></li>
<li><strong>目标</strong>：越小越好。<br>
</li>
<li><strong>核心思想</strong>：对于每个簇 $ C_i $，找到与其“最竞争”的簇
$ C_j $（即 $ $ 最大的簇），并取所有簇的平均值。</li>
</ul></li>
<li><strong>示例</strong>：<br>
若簇 $ C_1 $ 和 $ C_2 $ 的平均距离分别为 2 和 3，中心距离为
5，则它们的比值为 $ = 1 $。若这是 $ C_1 $ 的最大比值，则 $ C_1 $ 对 DBI
的贡献为 1。最终 DBI 是所有簇贡献的平均值。</li>
</ul>
<p><strong>2. Dunn指数（Dunn Index, DI）</strong></p>
<ul>
<li><strong>公式</strong>：<br>
<span class="math display">$$
\text{DI} = \min_{1 \leq i \leq k} \left\{ \frac{\min_{j \neq i}
d_{\min}(C_i, C_j)}{\max_{1 \leq l \leq k} \text{diam}(C_l)} \right\}
$$</span>
<ul>
<li><strong>符号含义</strong>：
<ul>
<li>$ d_{}(C_i, C_j) $：簇 $ C_i $ 和 $ C_j $
之间的最小距离（最近样本对的距离）。<br>
</li>
<li>$ (C_l) $：簇 $ C_l $ 内的最大距离（最远样本对的距离）。<br>
</li>
</ul></li>
<li><strong>目标</strong>：越大越好。<br>
</li>
<li><strong>核心思想</strong>：
<ul>
<li>分子：所有簇对之间的最小距离中的最小值（即最“脆弱”的簇间分离度）。<br>
</li>
<li>分母：所有簇中的最大直径（最“松散”的簇内紧凑度）。<br>
</li>
<li>指数越大，表示簇间分离度高且簇内紧凑。</li>
</ul></li>
</ul></li>
<li><strong>示例</strong>：<br>
假设簇对 $ (C_1, C_2) $ 的最小距离为 5，簇 $ C_3 $ 的最大直径为 10，则
DI 为 $ = 0.5 $。</li>
</ul>
<p><strong>3. 轮廓系数（Silhouette Coefficient）</strong></p>
<ul>
<li><p><strong>单一样本的轮廓系数</strong>：<br>
<span class="math display">$$
s = \frac{b - a}{\max(a, b)}
$$</span></p>
<ul>
<li><strong>符号含义</strong>：
<ul>
<li>$ a $：样本到同簇其他样本的平均距离（簇内凝聚度）。<br>
</li>
<li>$ b $：样本到最近簇中样本的平均距离（簇间分离度）。<br>
</li>
</ul></li>
<li><strong>取值范围</strong>：$ [-1, 1] $，越接近 1
表示聚类效果越好。<br>
</li>
<li><strong>核心思想</strong>：
<ul>
<li>若 $ a &lt; b $（同簇紧密，异簇疏远），则 $ s &gt; 0
$，样本分类合理。<br>
</li>
<li>若 $ a &gt; b $（同簇松散，异簇更近），则 $ s &lt; 0
$，样本可能被错误分类。</li>
</ul></li>
</ul></li>
<li><p><strong>整体轮廓系数</strong>：所有样本轮廓系数的平均值。</p></li>
<li><p><strong>示例</strong>：<br>
若某样本 $ a = 2 <span class="math inline">，</span> b = 5 $，则 $ s = =
0.6 $，表明该样本分类合理。</p></li>
</ul>
<p><strong>4.肘部法则（Elbow Method）</strong></p>
<p>肘部法则是一种<strong>经验性方法</strong>，常用于确定K-means等聚类算法的最优簇数（$
K $）。其核心思想是通过观察误差平方和（SSE, Sum of Squared Errors）随 $
K $ 值变化的趋势，寻找“肘部点”（即 SSE
下降速度明显减缓的拐点），从而选择最优的 $ K $ 值</p>
<ul>
<li><p><strong>SSE（误差平方和）</strong>：衡量每个样本到其所属簇中心的距离平方和，公式为：
<span class="math display">$$
\text{SSE} = \sum_{i=1}^n \|x_i - \mu_{c_i}\|^2
$$</span> 其中 $ x_i $ 是样本点，$ _{c_i} $ 是其所属簇中心。</p></li>
<li><p><strong>趋势分析</strong>：</p>
<ul>
<li>当 $ K $ 增大时，SSE
会不断减小（因为簇越多，每个簇的样本越密集）。</li>
<li>但当 $ K $ 增加到某个值后，SSE
的下降速度会显著放缓，形成“肘部”形状。</li>
</ul></li>
<li><p><strong>肘部点的意义</strong>：<br>
肘部点对应的 $ K $
值是<strong>模型复杂度</strong>（簇数）与<strong>聚类效果</strong>（SSE）之间的平衡点。</p></li>
</ul>
<p><strong>指标对比与选择</strong></p>
<table style="width:100%;">
<colgroup>
<col style="width: 8%">
<col style="width: 32%">
<col style="width: 13%">
<col style="width: 22%">
<col style="width: 22%">
</colgroup>
<thead>
<tr class="header">
<th><strong>指标</strong></th>
<th><strong>计算方式</strong></th>
<th><strong>目标</strong></th>
<th><strong>适用场景</strong></th>
<th><strong>局限性</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>DBI</td>
<td>簇内平均距离与簇中心距离的比值</td>
<td>越小越好</td>
<td>球形簇，需指定 $ k $</td>
<td>对离群点敏感</td>
</tr>
<tr class="even">
<td>Dunn指数</td>
<td>簇间最小距离与簇内最大直径的比值</td>
<td>越大越好</td>
<td>强调簇间分离与簇内紧凑</td>
<td>计算复杂，受离群点影响</td>
</tr>
<tr class="odd">
<td>轮廓系数</td>
<td>样本到同簇/异簇的平均距离差</td>
<td>越接近 1 越好</td>
<td>快速评估，适合 K-Means</td>
<td>对非球形簇不敏感</td>
</tr>
</tbody>
</table>
<h4 id="原型聚类与kmeans">原型聚类与kmeans</h4>
<h5 id="原型聚类">原型聚类</h5>
<p>原型聚类即“<strong>基于原型的聚类</strong>”（prototype-based
clustering），原型表示模板的意思，就是通过参考一个模板向量或模板分布的方式来完成聚类的过程，通常情形下算法先对原型进行初始化,然后对原型进行迭代更新求解。采用不同的原型表、不同的求解方式,将产生不同的算法。</p>
<p>常见的K-Means便是基于簇中心（原型向量）来实现聚类，混合高斯聚类则是基于簇分布（概率模型）来实现聚类。</p>
<h5 id="k-means-聚类算法详解"><strong>K-Means 聚类算法详解</strong></h5>
<p><strong>目标函数</strong>：最小化所有样本到其所属簇中心的平方距离之和：<br>
<span class="math display">$$
E = \sum_{i=1}^{k} \sum_{\boldsymbol{x} \in C_i} \|\boldsymbol{x} -
\boldsymbol{\mu}_i\|_2^2
$$</span> 其中，$ <em>i = </em>{ C_i} $ 是簇 $ C_i $ 的均值向量。</p>
<p><strong>算法步骤</strong></p>
<ol type="1">
<li><strong>初始化簇中心</strong>：随机选择 $ k $ 个样本作为初始簇中心。
<ul>
<li><strong>改进方法</strong>：K-Means++
算法可提升初始中心的质量。<br>
</li>
</ul></li>
<li><strong>分配样本到最近簇</strong>：对每个样本 $
$，计算其到所有簇中心的距离，将其分配到距离最近的簇 $ C_i $。<br>
</li>
<li><strong>更新簇中心</strong>：重新计算每个簇的均值向量 $ _i $。<br>
</li>
<li><strong>迭代终止条件</strong>：
<ul>
<li>达到预设的最大迭代次数；<br>
</li>
<li>簇中心不再显著变化（如变化幅度小于阈值 $ $）；<br>
</li>
<li>样本分配不再改变。</li>
</ul></li>
</ol>
<p><strong>如何选择 $ k $ 值？</strong></p>
<ul>
<li><strong>肘部法则（Elbow Method）</strong>：绘制 $ k $ 与误差 $ E $
的关系曲线，选择误差下降显著变缓的 $ k $ 值。<br>
</li>
<li><strong>轮廓系数（Silhouette
Coefficient）</strong>：计算每个样本的轮廓系数，选择平均轮廓系数最大的 $
k $。</li>
</ul>
<h5 id="k-means的算法流程">K-Means的算法流程</h5>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250607114743211.png" alt="image-20250607114743211">
<figcaption aria-hidden="true">image-20250607114743211</figcaption>
</figure>
<h5 id="k-means">K-means++</h5>
<p>此法相对于 K-means 做出了一个小的改进。在一开始选择 k
个聚类中心时，并不是随机初始化 k 个，而是首先随机出 1 个，然后循环
k−1<em>k</em>−1 次选择剩下的 k-1
个聚类中心。选择的规则是：每次选择最不可能成为新的聚类中心的样本，或者是到所有聚类中心的最小距离最大的样本。</p>
<h5 id="优势"><strong>优势</strong></h5>
<p><strong>避免不良初始化</strong>
：传统K-means随机初始化可能导致中心过于集中，而K-means++通过“最大化最小距离”策略，使初始中心分布更均匀。</p>
<h5 id="bisecting-k-means">Bisecting K-means</h5>
<p>此法叫做二分 K-means
算法。具体的，在一开始将所有的样本划分为一个簇，然后每次选择一个误差最大的簇进行二分裂，不断分裂直到收敛。这种方法不能使得
Loss 最小，但是可以作为 K-means
算法的一个预热，比如可以通过这种方法得到一个相对合理的簇中心，然后再利用
K-means 算法进行聚类。</p>
<h5 id="优势-1"><strong>优势</strong></h5>
<p><strong>降低计算复杂度</strong>
：每次仅对一个簇进行二分，时间复杂度为
<em>O</em>(<em>k</em>⋅<em>m</em>⋅<em>n</em>) ，适合大规模数据。</p>
<p><strong>提供合理初始中心</strong>
：可作为传统K-means的预处理，减少随机初始化的影响。</p>
<h5 id="lvq学习向量量化"><strong>LVQ（学习向量量化）</strong></h5>
<p><strong>核心思想</strong>：<br>
LVQ
是一种<strong>有监督的原型聚类算法</strong>，结合了神经网络与向量量化技术。它通过维护一组<strong>原型向量</strong>（Prototype
Vectors）来代表不同类别，并利用这些原型对数据进行分类或聚类。与 K-Means
类似，LVQ
会为每个簇分配一个原型向量，但其更新规则受类别标签的指导，因此更适用于分类任务
。</p>
<p><strong>算法特点</strong>：</p>
<ul>
<li><strong>有监督学习</strong>：需要已知类别标签来调整原型向量，使同类样本更接近对应原型，异类样本远离原型。<br>
</li>
<li><strong>拓扑结构建模</strong>：通过原型向量捕捉数据的局部特征，类似于自组织映射（SOM），但更具针对性。<br>
</li>
<li><strong>硬聚类</strong>：每个样本最终被分配到最近的原型对应的类别，不提供概率输出
。</li>
</ul>
<h5 id="高斯混合聚类gaussian-mixture-model-gmm"><strong>高斯混合聚类（Gaussian
Mixture Model, GMM）</strong></h5>
<p>一句话概述算法：高斯混合聚类算法是一种概率模型，假设数据由多个高斯分布混合而成，通过迭代优化参数以拟合数据分布，常用于无监督学习中的聚类任务。</p>
<p>算法过程：</p>
<p>初始化参数： 随机初始化每个分量的均值、协方差矩阵和混合系数。</p>
<p>E 步（Expectation）：
对每个数据点，计算它属于每个分量的后验概率，即计算每个分量的权重。</p>
<p>M 步（Maximization）：
使用E步计算得到的后验概率，更新每个分量的均值、协方差矩阵和混合系数。</p>
<p>迭代： 重复执行E步和M步，直到模型参数收敛或达到预定的迭代次数。</p>
<p>GMM的优点包括对各种形状和方向的聚类簇建模能力，以及对数据分布的灵活性。它在许多领域，如模式识别、图像处理和自然语言处理等，都有广泛的应用。
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250611180451162.png" alt="image-20250611180451162"></p>
<p>以下是高斯混合聚类（GMM）算法的详细步骤及EM算法中E步与M步的解释：</p>
<p><strong>算法流程解析</strong></p>
<p><strong>输入</strong>：样本集 $ D = {x_1, x_2, , x_m} $，混合成分个数
$ k $。<br>
<strong>输出</strong>：簇划分 $ C = {C_1, C_2, , C_k} $。</p>
<p><strong>步骤详解</strong></p>
<ol type="1">
<li><p><strong>初始化模型参数</strong><br>
随机初始化或通过K-means初步估计以下参数：</p>
<ul>
<li><strong>混合系数</strong> $ <em>i $（满足 $ </em>{i=1}^k _i = 1
$）。</li>
<li><strong>均值向量</strong> $ _i $。</li>
<li><strong>协方差矩阵</strong> $ _i $。</li>
</ul></li>
<li><p><strong>迭代优化参数（EM循环）</strong><br>
重复以下步骤直到收敛（如对数似然变化小于阈值）：</p>
<ul>
<li><p><strong>E步（期望步）</strong>：<br>
对每个样本 $ x_j $，计算其由第 $ i $
个高斯分布生成的<strong>后验概率</strong>（责任度 $ _{ji} $）： <span class="math display">$$
\gamma_{ji} = p(z_j = i | x_j) = \frac{\alpha_i \mathcal{N}(x_j | \mu_i,
\Sigma_i)}{\sum_{l=1}^k \alpha_l \mathcal{N}(x_j | \mu_l, \Sigma_l)}
$$</span> 其中 $ (x | , ) $ 是高斯分布的概率密度函数。</p></li>
<li><p><strong>M步（最大化步）</strong>：<br>
根据当前的责任度 $ _{ji} $，更新模型参数：</p>
<ol type="1">
<li><strong>新均值向量</strong>： <span class="math display">$$
\mu_i' = \frac{\sum_{j=1}^m \gamma_{ji} x_j}{\sum_{j=1}^m \gamma_{ji}}
$$</span></li>
<li><strong>新协方差矩阵</strong>： <span class="math display">$$
\Sigma_i' = \frac{\sum_{j=1}^m \gamma_{ji} (x_j - \mu_i')(x_j -
\mu_i')^\top}{\sum_{j=1}^m \gamma_{ji}}
$$</span></li>
<li><strong>新混合系数</strong>： <span class="math display">$$
\alpha_i' = \frac{\sum_{j=1}^m \gamma_{ji}}{m}
$$</span></li>
</ol></li>
</ul></li>
<li><p><strong>簇划分</strong></p>
<ul>
<li>初始化空簇 $ C_i = $。</li>
<li>对每个样本 $ x_j $，计算其属于各簇的后验概率 $ _j = <em>i </em>{ji}
$。</li>
<li>将 $ x_j $ 分配到簇 $ C_{_j} $ 中。</li>
</ul></li>
</ol>
<p><strong>E步与M步的核心作用</strong></p>
<p><strong>E步（期望步）</strong></p>
<ul>
<li><strong>目标</strong>：基于当前参数 $ (_i, _i, <em>i)
$，计算每个样本 $ x_j $ 属于各高斯分布的<strong>责任度</strong> $
</em>{ji} $。</li>
<li><strong>意义</strong>：
<ul>
<li>责任度反映了在当前模型下，样本 $ x_j $ 由第 $ i $
个高斯分布生成的概率。</li>
<li><strong>软分配</strong>：允许样本部分属于多个簇，而非硬划分。</li>
</ul></li>
</ul>
<p><strong>M步（最大化步）</strong></p>
<ul>
<li><strong>目标</strong>：根据责任度 $ _{ji} $，重新估计模型参数 $
(_i’, _i’, _i’) $，以最大化数据的对数似然。</li>
<li><strong>关键公式</strong>：
<ul>
<li><strong>均值更新</strong>：加权平均样本点，权重为责任度。</li>
<li><strong>协方差更新</strong>：加权样本点的方差，反映簇内数据分布。</li>
<li><strong>混合系数更新</strong>：各簇样本的“有效数量”占总样本的比例。</li>
</ul></li>
</ul>
<h4 id="参考资料-1">参考资料</h4>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/smileyan9/article/details/135398479">西瓜书读书笔记整理（九）
—— 第九章 聚类_西瓜书笔记第9章-CSDN博客</a></p>
<h4 id="密度聚类与dbscan">密度聚类与DBSCAN</h4>
<blockquote>
<p>若样本分布为同心的两个环，kmeans则无法做到良好的聚类效果，因此引出密度聚类</p>
</blockquote>
<p>密度聚类是一种基于<strong>样本分布密集程度</strong>的无监督学习方法，其核心思想是：<strong>将高密度区域划分为同一簇，低密度区域视为噪声或边界</strong>。</p>
<p>DBSCAN（Density-Based Spatial Clustering of Applications with
Noise）是密度聚类的典型代表，通过两个关键参数 $ $ 和 $ MinPts $
描述样本分布的紧密性。</p>
<h5 id="核心概念"><strong>1. 核心概念</strong></h5>
<ol type="1">
<li><strong>$ $-邻域</strong>
<ul>
<li>定义：与样本 $ x $ 距离不超过 $ $ 的所有样本集合。<br>
</li>
<li>作用：衡量样本周围的局部密度。<br>
</li>
</ul></li>
<li><strong>核心对象（Core Object）</strong>
<ul>
<li>定义：若样本 $ x $ 的 $ $-邻域内包含至少 $ MinPts $ 个样本，则 $ x $
是核心对象。<br>
</li>
<li>作用：作为簇的生长起点，确保簇的最小密度要求。<br>
</li>
</ul></li>
<li><strong>密度直达（Directly Density-Reachable）</strong>
<ul>
<li>定义：若样本 $ x_j $ 位于核心对象 $ x_i $ 的 $ $-邻域内，则称 $ x_i
$ 可密度直达 $ x_j $。<br>
</li>
<li>作用：建立核心对象与邻近样本的直接连接。<br>
</li>
</ul></li>
<li><strong>密度可达（Density-Reachable）</strong>
<ul>
<li>定义：若存在样本序列 $ x_i, p_1, p_2, , p_n, x_j $，其中 $ p_i $
密度直达 $ p_{i+1} $，则称 $ x_i $ 可密度可达 $ x_j $。<br>
</li>
<li>作用：通过链式传递扩展簇的范围。<br>
</li>
</ul></li>
<li><strong>密度相连（Density-Connected）</strong>
<ul>
<li>定义：若样本 $ x_i $ 和 $ x_j $ 均可密度可达某个公共样本 $ x_k
$，则称 $ x_i $ 和 $ x_j $ 密度相连。<br>
</li>
<li>作用：确保簇的连通性，避免碎片化。</li>
</ul></li>
</ol>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250607124326529.png" alt="image-20250607124326529">
<figcaption aria-hidden="true">image-20250607124326529</figcaption>
</figure>
<p><strong>DBSCN定义的簇</strong></p>
<ul>
<li>定义：最大密度相连的样本集合为一个簇</li>
<li>有两个性质：1.连接性：同一个簇内任意两样本，必然密度相连2.最大性：密度可达的两个样本必
定属于同一个簇</li>
</ul>
<h5 id="dbscan-算法流程"><strong>2. DBSCAN 算法流程</strong></h5>
<p>简单来理解DBSCAN：<strong>找出一个核心对象所有密度可达的样本集合形成簇</strong>。首先从数据集中任选一个核心对象A，找出所有A密度可达的样本集合，将这些样本形成一个密度相连的类簇，直到所有的核心对象都遍历完。DBSCAN算法的流程如下图所示：</p>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250607124446432.png" alt="image-20250607124446432">
<figcaption aria-hidden="true">image-20250607124446432</figcaption>
</figure>
<h5 id="参数选择与影响"><strong>3. 参数选择与影响</strong></h5>
<ul>
<li><strong>$ $（邻域半径）</strong>：
<ul>
<li>过小：可能导致多数样本被标记为噪声，簇数量增加。<br>
</li>
<li>过大：可能导致不同簇合并，簇数量减少。<br>
</li>
<li><strong>选择方法</strong>：通过<strong>K-Distance图</strong>（排序后的第
$ k $ 近邻距离）观察“拐点”。</li>
</ul></li>
<li><strong>$ MinPts $（最小样本数）</strong>：
<ul>
<li>控制簇的最小密度阈值。<br>
</li>
<li>通常取 $ d+1 <span class="math inline">（</span> d $
为特征维度），避免在高维空间中误判噪声。</li>
</ul></li>
</ul>
<h4 id="层次聚类与agnes">层次聚类与AGNES</h4>
<p>层次聚类是一种通过构建<strong>树状结构（Dendrogram）</strong>将数据划分为不同层次的聚类方法。其核心思想是：<br>
-
<strong>凝聚型（Agglomerative）</strong>：从每个样本作为一个独立簇开始，逐步合并最相似的簇，直到达到预设的簇数或形成一个唯一簇。<br>
-
<strong>分裂型（Divisive）</strong>：与凝聚型相反，从整个数据集作为一个簇开始，逐步分裂为更小的簇。</p>
<p>本节重点介绍<strong>AGNES（Agglomerative
Nesting）</strong>，一种经典的自底向上的层次聚类算法。</p>
<h5 id="agnes-算法流程"><strong>1. AGNES 算法流程</strong></h5>
<ol type="1">
<li><strong>初始化</strong>：每个样本作为一个独立簇。<br>
</li>
<li><strong>迭代合并</strong>：
<ul>
<li>计算所有簇对之间的距离。<br>
</li>
<li>合并距离最近的两个簇。<br>
</li>
</ul></li>
<li><strong>终止条件</strong>：
<ul>
<li>达到预设的簇数 $ k $；<br>
</li>
<li>所有簇之间的距离大于阈值。</li>
</ul></li>
</ol>
<h5 id="簇间距离的定义"><strong>2. 簇间距离的定义</strong></h5>
<p>AGNES
的关键在于如何定义<strong>簇间距离</strong>，常见的三种方法如下：</p>
<p><strong>（1）最小距离（Single Linkage）</strong> <span class="math display"><em>d</em><sub>min</sub>(<em>C</em><sub><em>i</em></sub>,<em>C</em><sub><em>j</em></sub>) = min<sub><strong>x</strong> ∈ <em>C</em><sub><em>i</em></sub>, <strong>z</strong> ∈ <em>C</em><sub><em>j</em></sub></sub>dist(<strong>x</strong>,<strong>z</strong>)</span>
- <strong>含义</strong>：两个簇之间最近的两个样本的距离。</p>
<p><strong>（2）最大距离（Complete Linkage）</strong> <span class="math display"><em>d</em><sub>max</sub>(<em>C</em><sub><em>i</em></sub>,<em>C</em><sub><em>j</em></sub>) = max<sub><strong>x</strong> ∈ <em>C</em><sub><em>i</em></sub>, <strong>z</strong> ∈ <em>C</em><sub><em>j</em></sub></sub>dist(<strong>x</strong>,<strong>z</strong>)</span>
- <strong>含义</strong>：两个簇之间最远的两个样本的距离。</p>
<p><strong>（3）平均距离（Average Linkage）</strong> <span class="math display">$$
d_{\text{avg}}(C_i, C_j) = \frac{1}{|C_i| |C_j|} \sum_{\boldsymbol{x}
\in C_i} \sum_{\boldsymbol{z} \in C_j} \text{dist}(\boldsymbol{x},
\boldsymbol{z})
$$</span> - <strong>含义</strong>：两个簇所有样本对距离的平均值。</p>
<h5 id="层次聚类法的算法流程如下所示">层次聚类法的算法流程如下所示：</h5>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250607125338029.png" alt="image-20250607125338029">
<figcaption aria-hidden="true">image-20250607125338029</figcaption>
</figure>
<h4 id="作业-2">作业</h4>
<h5 id="section-6">1</h5>
<p>假设任务是将下面8个点聚类成3个簇：A1(2,10), A2(2,5), A3(8,4),
B1(5,8), B2(7,5), B3(6,4), C1(1,2),
C3(4,9)，距离函数是欧式距离。假设初始选择A1，B1，C1分别作为每个聚类的中心，用Kmeans算法给出计算过程。</p>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250607125506436.png" alt="image-20250607125506436">
<figcaption aria-hidden="true">image-20250607125506436</figcaption>
</figure>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250607125606040.png" alt="image-20250607125606040">
<figcaption aria-hidden="true">image-20250607125606040</figcaption>
</figure>
<h5 id="section-7">2</h5>
<p>Kmeans初始类簇中心如何选取？K值如何确定？请简要阐述。</p>
<p><strong>一、初始类簇中心的选取 (如何选好的起始点？)</strong></p>
<p>传统K-means随机选择初始中心点，容易导致结果不稳定（多次运行结果不同）或陷入局部最优（效果差）。改进方法主要有：</p>
<ol type="1">
<li><strong>K-means++ (最常用且推荐)：</strong>
<ul>
<li><strong>核心思想：</strong> 让初始中心点彼此尽量远离。</li>
<li><strong>步骤：</strong>
<ol type="1">
<li>随机选择<strong>第一个</strong>中心点。</li>
<li>计算每个数据点到<strong>当前已选中心点</strong>的最短距离（即离最近中心的距离）。</li>
<li>以<strong>与这个最短距离平方成正比</strong>的概率，随机选择下一个中心点（距离越大的点，被选中的概率越大）。</li>
<li>重复步骤2和3，直到选出K个中心点。</li>
</ol></li>
<li><strong>优点：</strong>
显著提高聚类质量和稳定性，计算开销增加不大。</li>
</ul></li>
<li><strong>多次运行+选取最优：</strong>
<ul>
<li>独立运行K-means算法多次（每次随机初始化）。</li>
<li>每次运行完成后，计算所有数据点与其所属簇中心的距离平方和（SSE, Sum
of Squared Errors）。</li>
<li>选择SSE最小的那次运行结果作为最终结果。</li>
<li><strong>优点：</strong> 简单，增加找到更好解的机会。</li>
<li><strong>缺点：</strong> 计算开销随运行次数增加。</li>
</ul></li>
<li><strong>基于样本密度/距离：</strong>
<ul>
<li>选择数据空间中样本密度高的区域点作为中心。</li>
<li>或选择相互之间距离较远的点作为中心（类似K-means++的思想，但实现方式可能不同）。</li>
</ul></li>
</ol>
<p><strong>二、K值（簇数量）的确定 (如何知道分几类？)</strong></p>
<p>K值通常需要预先指定，但没有绝对正确的答案。常用方法基于评估不同K值下聚类结果的“质量”，寻找拐点或最优值：</p>
<ol type="1">
<li><strong>肘部法则：</strong>
<ul>
<li><strong>核心思想：</strong>
随着K增大，簇内样本聚合更紧密，簇内平方和误差（SSE）会下降，但下降幅度会逐渐变缓。找到SSE下降速率发生显著变化的“肘点”。</li>
<li><strong>做法：</strong> 计算不同K值（如K=1, 2, 3, …,
max）对应的SSE。绘制<code>K值 - SSE</code>曲线图。观察曲线，寻找SSE下降幅度突然变得平缓的那个K值（形如手臂的“肘关节”）。</li>
<li><strong>优点：</strong> 直观。</li>
<li><strong>缺点：</strong>
“肘点”有时不明显或不存在，需要主观判断。</li>
</ul></li>
<li><strong>轮廓系数：</strong>
<ul>
<li><strong>核心思想：</strong>
综合衡量一个样本与其自身簇的紧密度(<code>a</code>)和与其他簇的分离度(<code>b</code>)。</li>
<li><strong>计算：</strong> 对于每个样本i：
<ul>
<li><code>a(i)</code> = i
到同簇内所有其他点的平均距离（簇内不相似度）。</li>
<li><code>b(i)</code> = i
到所有<strong>其他簇</strong>中点的平均距离的最小值（最近邻簇的不相似度）。</li>
<li>样本i的轮廓系数：<code>s(i) = (b(i) - a(i)) / max(a(i), b(i))</code>。值在[-1,
1]之间。</li>
</ul></li>
<li><strong>整体评估：</strong>
计算所有样本轮廓系数的平均值，作为该K值下聚类的整体轮廓系数。</li>
<li><strong>选择K：</strong>
尝试不同K值，选择<strong>平均轮廓系数最大</strong>对应的K值。轮廓系数越接近1，表示聚类效果越好（簇内紧凑，簇间分离）。</li>
<li><strong>优点：</strong> 量化评估，结果在[-1, 1]之间有界。</li>
<li><strong>缺点：</strong> 计算量较大，尤其对于大数据集。</li>
</ul></li>
</ol>
<h4 id="参考资料-2">参考资料</h4>
<p><a target="_blank" rel="noopener" href="https://cloud.tencent.com.cn/developer/article/1802143">《机器学习》–
第九章 聚类-腾讯云开发者社区-腾讯云</a></p>
<h3 id="降维与度量学习">降维与度量学习</h3>
<h4 id="knn">KNN</h4>
<p>k近邻算法简称<strong>kNN（k-Nearest
Neighbor）</strong>，是一种经典的监督学习方法，是数据挖掘十大算法之一。其工作机制十分简单：给定某个测试样本，kNN基于某种<strong>距离度量</strong>在训练集中找出与其距离最近的k个带有真实标记的训练样本，然后基于这k个邻居的真实标记来进行预测，类似于集成学习中的基学习器结合策略：分类任务采用投票法，回归任务则采用平均法。</p>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250607150256290.png" alt="image-20250607150256290">
<figcaption aria-hidden="true">image-20250607150256290</figcaption>
</figure>
<p><strong>核心思想</strong></p>
<p>1NN 分类器通过将测试样本 $ $ 分配到其最近邻样本 $ $
的类别来完成预测。其错误概率取决于两个关键因素： - <strong>$ $
的真实类别</strong>：$ P(c | ) $，即给定 $ $ 属于类别 $ c $
的概率。<br>
- <strong>$ $ 的类别</strong>：$ P(c | ) $，即 $ $ 属于类别 $ c $
的概率。</p>
<p><strong>错误概率公式</strong></p>
<p>若测试样本 $ $ 的最近邻为 $ $，则 1NN 分类器出错的概率为： <span class="math display"><em>P</em>(err) = 1 − <em>P</em>(correct) = 1 − ∑<sub><em>c</em> ∈ 𝒞</sub><em>P</em>(<em>c</em>|<strong>x</strong>)<em>P</em>(<em>c</em>|<strong>z</strong>)</span>
其中： - $ $ 是所有可能的类别集合。<br>
- $ P(c | ) <span class="math inline">：</span> $ 属于类别 $ c $
的条件概率。<br>
- $ P(c | ) <span class="math inline">：</span> $ 属于类别 $ c $
的条件概率。</p>
<p>通过证明可以发现一个令人震惊的结论：<strong>最近邻分类器的错误率不超过贝叶斯最优分类器错误率的两倍</strong>。</p>
<p>对于距离度量，<strong>不同的度量方法得到的k个近邻不尽相同，从而对最终的投票结果产生了影响</strong>，因此选择一个合适的距离度量方法也十分重要。</p>
<p>在上一篇聚类算法中，在度量样本相似性时介绍了常用的几种距离计算方法，包括<strong>闵可夫斯基距离，曼哈顿距离，VDM</strong>等。在实际应用中，<strong>kNN的距离度量函数一般根据样本的特性来选择合适的距离度量，同时应对数据进行去量纲/归一化处理来消除大量纲属性的强权政治影响</strong>。</p>
<h4 id="低维嵌入">低维嵌入</h4>
<p><strong>使用knn的前提是样本空间的密度要一定大，但是这个条件在现实中很难满足，因此引出降维操作</strong></p>
<blockquote>
<p>kNN的重要假设: 任意测试样本 附近任意小的
距离范围内总能找到一个训练样本，即训练样本的采样密度足够大，或称为
<strong>“密采样”( dense sample)</strong>
。然而，这个假设在现实任务中通常很难满足</p>
</blockquote>
<p>样本的<strong>特征数</strong>也称为<strong>维数</strong>（dimensionality），当维数非常大时，也就是通常所说的“<strong>维数灾难</strong>”(curse
of
dimensionality)，具体表现在：在高维情形下，<strong>数据样本变得十分稀疏</strong>，因为此时要满足训练样本为“<strong>密采样</strong>”的总体样本数目是一个触不可及的天文数字。<strong>训练样本的稀疏使得其代表总体分布的能力大大减弱，从而消减了学习器的泛化能力</strong>；同时当维数很高时，<strong>计算距离也变得十分复杂</strong>，甚至连计算内积都不再容易</p>
<p>缓解维数灾难的一个重要途径就是<strong>降维（dimension
reduction），即通过某种数学变换将原始高维空间转变到一个低维的子空间</strong>。在这个子空间中，样本的密度将大幅提高，同时距离计算也变得容易。这</p>
<p>时也许会有疑问，降维之后不是会丢失原始数据的一部分信息吗？</p>
<p>实际上，在很多实际问题中，虽然训练数据是高维的，但是与学习任务相关也许仅仅是其中的一个低维子空间，也称为一个<strong>低维嵌入</strong>，例如：数据属性中存在噪声属性、相似属性或冗余属性等，<strong>对高维数据进行降维能在一定程度上达到提炼低维优质属性或降噪的效果</strong>。</p>
<h4 id="mds算法"><strong>MDS算法</strong></h4>
<p>MDS（Multidimensional
Scaling，多维尺度分析）是一种经典的<strong>降维技术</strong>，其核心目标是将高维数据映射到低维空间（如二维或三维），同时<strong>尽可能保留原始数据中样本点之间的距离关系</strong>。以下是其核心原理与应用要点：</p>
<p><strong>1. 核心思想</strong></p>
<ul>
<li><strong>输入</strong>：一个样本点之间的距离矩阵 $ D
$（如欧氏距离、余弦距离等）。<br>
</li>
<li><strong>输出</strong>：低维空间中样本点的坐标矩阵 $ Z
$，使得低维空间中的距离与原始距离尽可能一致 。<br>
</li>
<li><strong>关键假设</strong>：高维数据的内在结构可通过样本间的距离关系描述，降维后需最小化这种关系的失真。</li>
</ul>
<p><strong>2. 算法步骤</strong></p>
<p>MDS 的核心是通过<strong>矩阵分解</strong>从距离矩阵推导低维坐标： 1.
<strong>构建距离矩阵 $ D $</strong>：<br>
对于 $ r $ 个样本，计算两两之间的距离，形成 $ r r $ 的矩阵 $ D $，其中 $
D_{ij} $ 表示样本 $ i $ 和 $ j $ 的距离 。</p>
<ol start="2" type="1">
<li><p><strong>双中心化（Double Centering）</strong>：<br>
构造矩阵 $ B = - H D^{(2)} H $，其中 $ D^{(2)} $ 是距离的平方矩阵，$ H =
I - ^$ 是中心化矩阵 。</p></li>
<li><p><strong>特征值分解</strong>：<br>
对 $ B $ 进行特征值分解，得到 $ B = V V^$，其中 $ $
是按降序排列的特征值对角矩阵，$ V $ 是对应的特征向量矩阵 。</p></li>
<li><p><strong>构造低维坐标</strong>：<br>
选择前 $ d’ $ 个最大特征值（$ d’ $
为目标维度）和对应的特征向量，计算低维坐标矩阵：<br>
<span class="math display"><em>Z</em> = <em>Λ</em><sup>1/2</sup><em>V</em><sup>⊤</sup></span>
其中 $ ^{1/2} $ 是特征值矩阵的平方根 。</p></li>
</ol>
<p><strong>3. 关键特性</strong></p>
<ul>
<li><strong>保留距离关系</strong>：MDS
直接优化低维空间中的距离与原始距离的一致性，适用于需精确保留样本相似性的场景（如生物信息学中的基因关系分析）。<br>
</li>
<li><strong>非线性适应性</strong>：与 PCA 不同，MDS
不要求数据线性分布，更适合处理非线性结构（如环形、流形数据）。<br>
</li>
<li><strong>灵活性</strong>：支持任意距离度量（如自定义的相似性指标），而
PCA 仅适用于欧氏距离 。</li>
</ul>
<h4 id="线性降维方法"><strong>线性降维方法</strong></h4>
<p>线性降维通过<strong>线性变换</strong>将高维数据 $ ^{d m} $
投影到低维空间 $ ^{d’ m} <span class="math inline">（</span> d’ d
$），保留数据的主要信息。其数学表达为： <span class="math display"><strong>Z</strong> = <strong>W</strong><sup>⊤</sup><strong>X</strong></span></p>
<ul>
<li><p><strong>变换矩阵 $ ^{d d’} $</strong>：<br>
每一列是正交的基向量，构成低维子空间的坐标系。<br>
</p></li>
<li><p><strong>目标</strong>：选择 $ $ 使得低维表示 $ $
最大化保留原始数据的信息（如方差、距离等）。</p></li>
<li><p><strong>MDS</strong>：<br>
直接以<strong>保留高维空间中样本点之间的距离关系</strong>为目标。降维后的低维空间需尽可能保持原始样本两两之间的距离（如欧氏距离、自定义相似性距离）。</p>
<ul>
<li><strong>示例</strong>：在基因数据分析中，MDS可确保基因表达相似的样本在低维空间中仍紧密分布。</li>
</ul></li>
<li><p><strong>其他线性方法（如PCA、LDA）</strong>：</p>
<ul>
<li><strong>PCA</strong>：最大化数据在低维空间的方差，强调保留全局结构而非具体距离。<br>
</li>
<li><strong>LDA</strong>：在监督学习中最大化类间分离度，忽略类内距离。</li>
</ul></li>
</ul>
<h4 id="主成分分析">主成分分析</h4>
<p>不同于MDS采用距离保持的方法，主成分分析（Principal Component Analysis
,PCA）是一种经典的<strong>无监督降维算法</strong>
，其核心目标是通过线性变换将高维数据映射到低维空间，同时保留数据的<strong>最大方差信息</strong>
（即信息损失最小）</p>
<p>直接通过一个<strong>线性变换</strong>，将原始空间中的样本<strong>投影</strong>到新的低维空间中。</p>
<p>简单来理解这一过程便是：<strong>PCA采用一组新的基（向量）来表示样本点，其中每一个基向量都是原始空间基向量的线性组合，通过使用尽可能少的新基向量来表出样本，从而达到降维的目的。</strong></p>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250607155733314.png" alt="image-20250607155733314">
<figcaption aria-hidden="true">image-20250607155733314</figcaption>
</figure>
<p>假设使用d’个新基向量来表示原来样本，实质上是将样本投影到一个由d’个基向量确定的一个<strong>超平面</strong>上（<strong>即舍弃了一些维度</strong>），要用一个超平面对空间中所有高维样本进行恰当的表达，最理想的情形是：<strong>若这些样本点都能在超平面上表出且这些表出在超平面上都能够很好地分散开来</strong>。但是一般使用较原空间低一些维度的超平面来做到这两点十分不容易，因此我们退一步海阔天空，要求这个超平面应具有如下两个性质：</p>
<blockquote>
<p><strong>最近重构性</strong>：样本点到超平面的距离足够近，即尽可能在超平面附近；</p>
<p><strong>最大可分性</strong>：样本点在超平面上的投影尽可能地分散开来，即投影后的坐标具有区分性。</p>
</blockquote>
<p>这里十分神奇的是：<strong>最近重构性与最大可分性虽然从不同的出发点来定义优化问题中的目标函数，但最终这两种特性得到了完全相同的优化问题</strong>：</p>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250607165159235.png" alt="image-20250607165159235">
<figcaption aria-hidden="true">image-20250607165159235</figcaption>
</figure>
<h5 id="协方差矩阵与优化求解"><strong>协方差矩阵与优化求解</strong></h5>
<p>若数据已<strong>中心化</strong>（均值为零），则 $ ^$
是<strong>样本协方差矩阵</strong>的 $ m $
倍。此时，PCA的优化问题转化为： <span class="math display">$$
\begin{aligned}
&amp; \underset{\mathbf{W}}{\text{maximize}}
&amp; &amp; \text{tr}\left( \mathbf{W}^\top \mathbf{X} \mathbf{X}^\top
\mathbf{W} \right) \\
&amp; \text{subject to}
&amp; &amp; \mathbf{W}^\top \mathbf{W} = \mathbf{I}
\end{aligned}
$$</span> 通过拉格朗日乘数法，该问题的解为 $ ^$ 的前 $ d’ $
个最大特征值对应的特征向量</p>
<h5 id="pca的数学推导"><strong>PCA的数学推导</strong></h5>
<ul>
<li><p><strong>优化目标</strong>：<br>
<span class="math display">max<sub><strong>W</strong></sub>  tr(<strong>W</strong><sup>⊤</sup><strong>X</strong><strong>X</strong><sup>⊤</sup><strong>W</strong>)  s.t.  <strong>W</strong><sup>⊤</sup><strong>W</strong> = <strong>I</strong></span>
其中，$ ^{d m} $ 是中心化后的数据矩阵（均值为零）。</p></li>
<li><p><strong>拉格朗日乘数法</strong>：<br>
引入拉格朗日乘子 $ $，构造拉格朗日函数： <span class="math display">ℒ(<strong>W</strong>,<em>Λ</em>) = tr(<strong>W</strong><sup>⊤</sup><strong>X</strong><strong>X</strong><sup>⊤</sup><strong>W</strong>) − tr(<em>Λ</em>(<strong>W</strong><sup>⊤</sup><strong>W</strong>−<strong>I</strong>))</span>
对 $ $ 求导并令导数为零，得到： <span class="math display"><strong>X</strong><strong>X</strong><sup>⊤</sup><strong>W</strong> = <em>Λ</em><strong>W</strong></span>
即 $ ^$ 的特征向量 $ _i $ 满足： <span class="math display"><strong>X</strong><strong>X</strong><sup>⊤</sup><strong>w</strong><sub><em>i</em></sub> = <em>λ</em><sub><em>i</em></sub><strong>w</strong><sub><em>i</em></sub></span></p></li>
</ul>
<h5 id="pca特征向量选择">PCA特征向量选择</h5>
<p><strong>1. 核心问题</strong></p>
<p>在PCA中，我们希望找到一个 $ d’ d $ 的变换矩阵 $
$，其列向量是协方差矩阵 $ ^$ 的特征向量，且满足正交约束 $ ^ =
$。关键问题是：<strong>如何从 $ d $ 个特征向量中选择 $ d’ $
个最优的？</strong></p>
<p><strong>2. 数学推导</strong></p>
<ol type="1">
<li><p><strong>特征值分解</strong>：<br>
协方差矩阵 $ <sup></sup>{d d} $ 可分解为： <span class="math display"><strong>X</strong><strong>X</strong><sup>⊤</sup><strong>W</strong> = <strong>W</strong><strong>Λ</strong></span>
其中，$ = (_1, _2, , _d) $ 是特征值对角矩阵，$ $
是特征向量矩阵。</p></li>
<li><p><strong>优化目标转化</strong>：<br>
PCA的目标是最大化 $ (^ ^) $。利用特征值分解，可得： <span class="math display"><strong>W</strong><sup>⊤</sup><strong>X</strong><strong>X</strong><sup>⊤</sup><strong>W</strong> = <strong>W</strong><sup>⊤</sup>(<strong>W</strong><strong>Λ</strong>) = <strong>Λ</strong></span>
因此，优化目标变为： <span class="math display">$$
\max_{\mathbf{W}} \quad \text{tr}(\boldsymbol{\Lambda}) =
\sum_{i=1}^{d'} \lambda_i
$$</span> 即选择 $ d’ $ 个最大的特征值 $ _i $ 对应的特征向量组成 $
$。</p></li>
</ol>
<p><strong>3. 特征向量选择策略</strong></p>
<ul>
<li><strong>按特征值排序</strong>：<br>
特征值 $ _i $ 表示数据沿特征向量 $ _i $ 方向的方差。选择前 $ d’ $
个最大特征值对应的特征向量，可保留最多信息。<br>
</li>
<li><strong>正交性保证</strong>：<br>
特征向量矩阵 $ $ 的列自动满足 $ ^ = $，无需额外正交化。</li>
</ul>
<h5 id="pca算法的整个流程如下图所示">PCA算法的整个流程如下图所示：</h5>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250607170020467.png" alt="image-20250607170020467">
<figcaption aria-hidden="true">image-20250607170020467</figcaption>
</figure>
<h4 id="核化线性降维"><strong>核化线性降维</strong></h4>
<p>待学习</p>
<h4 id="流形学习">流形学习</h4>
<p><strong>流形学习（manifold
learning）</strong>是一种借助拓扑流形概念的降维方法，流形是指在<strong>局部与欧式空间同胚的空间</strong>，即在局部与欧式空间具有相同的性质，能用欧氏距离计算样本之间的距离。这样即使高维空间的分布十分复杂，但是在局部上依然满足欧式空间的性质，基于流形学习的降维正是这种
<strong>“邻域保持”</strong> 的思想。其中
<strong>等度量映射（Isomap）试图在降维前后保持邻域内样本之间的距离，而局部线性嵌入（LLE）则是保持邻域内样本之间的线性关系</strong>
。</p>
<h5 id="等度量映射isomap">等度量映射Isomap</h5>
<p>等度量映射的基本出发点是：高维空间中的直线距离具有误导性，因为有时高维空间中的直线距离在低维空间中是不可达的。<strong>因此利用流形在局部上与欧式空间同胚的性质，可以使用近邻距离来逼近测地线距离</strong>，即对于一个样本点，它与近邻内的样本点之间是可达的，且距离使用欧式距离计算，这样整个样本空间就形成了一张近邻图，高维空间中两个样本之间的距离就转为最短路径问题。可采用著名的<strong>Dijkstra算法</strong>或<strong>Floyd算法</strong>计算最短距离，得到高维空间中任意两点之间的距离后便可以使用
MDS 算法来其计算低维空间中的坐标。</p>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250607171119645.png" alt="image-20250607171119645">
<figcaption aria-hidden="true">image-20250607171119645</figcaption>
</figure>
<p>Isomap算法流程如下图：</p>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250607171258284.png" alt="image-20250607171258284">
<figcaption aria-hidden="true">image-20250607171258284</figcaption>
</figure>
<p>对于近邻图的构建，常用的有两种方法：<strong>一种是指定近邻点个数</strong>，像kNN一样选取k个最近的邻居；<strong>另一种是指定邻域半径</strong>，距离小于该阈值的被认为是它的近邻点。但两种方法均会出现下面的问题：</p>
<blockquote>
<p>若<strong>邻域范围指定过大，则会造成“短路问题”</strong>，即本身距离很远却成了近邻，将距离近的那些样本扼杀在摇篮。</p>
<p>若<strong>邻域范围指定过小，则会造成“断路问题”</strong>，即有些样本点无法可达了，整个世界村被划分为互不可达的小部落。</p>
</blockquote>
<h5 id="局部线性嵌入">局部线性嵌入</h5>
<p>待学习</p>
<h4 id="度量学习">度量学习</h4>
<p><strong>1. 核心思想</strong></p>
<p>度量学习（Metric
Learning）的核心目标是<strong>学习一个合理的距离度量</strong>，使得相似样本距离更近，不相似样本距离更远。传统欧式距离（Euclidean
Distance）虽然简单，但其固定权重无法反映不同特征的实际重要性。因此，我们引入<strong>加权欧式距离</strong>，通过可调节的参数（权重）优化距离计算。</p>
<p><strong>2. 欧式距离与加权欧式距离</strong></p>
<ul>
<li><p><strong>标准欧式距离</strong>：<br>
<span class="math display">$$
\text{dist}_{\text{ed}}^2(\boldsymbol{x}_i, \boldsymbol{x}_j) =
\|\boldsymbol{x}_i - \boldsymbol{x}_j\|_2^2 = \sum_{k=1}^d
(\boldsymbol{x}_{i,k} - \boldsymbol{x}_{j,k})^2
$$</span>
每个特征维度对距离的贡献相同，未考虑特征的重要性差异。</p></li>
<li><p><strong>加权欧式距离</strong>：<br>
<span class="math display">dist<sub>wed</sub><sup>2</sup>(<strong>x</strong><sub><em>i</em></sub>,<strong>x</strong><sub><em>j</em></sub>) = (<strong>x</strong><sub><em>i</em></sub>−<strong>x</strong><sub><em>j</em></sub>)<sup>⊤</sup><strong>W</strong>(<strong>x</strong><sub><em>i</em></sub>−<strong>x</strong><sub><em>j</em></sub>)</span>
其中，$ = () $ 是对角权重矩阵，$ w_k $ 表示第 $ k $ 个特征的权重。<br>
展开后为： <span class="math display">$$
\text{dist}_{\text{wed}}^2(\boldsymbol{x}_i, \boldsymbol{x}_j) =
\sum_{k=1}^d w_k (\boldsymbol{x}_{i,k} - \boldsymbol{x}_{j,k})^2
$$</span></p></li>
</ul>
<p><strong>3. 权重的作用</strong></p>
<ul>
<li><strong>特征重要性调节</strong>：
<ul>
<li>高权重 $ w_k $：强调第 $ k $
维特征对距离的影响（如图像的颜色通道比位置更重要）。<br>
</li>
<li>低权重 $ w_k $：弱化噪声或冗余特征的影响。<br>
</li>
</ul></li>
<li><strong>几何意义</strong>：<br>
加权欧式距离相当于在各特征维度上进行缩放，将数据映射到一个新的空间，使得关键特征的差异更显著。</li>
</ul>
<p><strong>4. 度量学习的目标</strong></p>
<p>通过学习最优权重 $ <span class="math inline">，<em>使</em><em>以</em><em>下</em><em>目</em><em>标</em><em>成</em><em>立</em>： −  *  * <em>相</em><em>似</em><em>样</em><em>本</em> *  * ：<em>加</em><em>权</em><em>距</em><em>离</em><em>小</em>（</span>
_{}^2(_i, <em>j) <span class="math inline">）。 −  *  * <em>不</em><em>相</em><em>似</em><em>样</em><em>本</em> *  * ：<em>加</em><em>权</em><em>距</em><em>离</em><em>大</em>（</span>
</em>{}^2(_i, _j) $）。</p>
<p>典型优化问题形式： <span class="math display">min<sub><strong>w</strong></sub>  ∑<sub>(<strong>x</strong><sub><em>i</em></sub>,<strong>x</strong><sub><em>j</em></sub>) ∈ <em>S</em></sub>dist<sub>wed</sub><sup>2</sup>(<strong>x</strong><sub><em>i</em></sub>,<strong>x</strong><sub><em>j</em></sub>) + <em>λ</em>∥<strong>w</strong>∥<sub>2</sub><sup>2</sup></span>
其中，$ S $ 是相似样本对集合，$ $ 是正则化项防止过拟合。</p>
<blockquote>
<p>总结来说，</p>
<ul>
<li><strong>降维是将原高维空间嵌入到一个合适的低维子空间中，接着在低维空间中进行学习任务</strong></li>
<li><strong>度量学习则是试图去学习出一个 *距离度量*
来等效降维的效果</strong></li>
</ul>
</blockquote>
<h5 id="lmnnlarge-margin-nearest-neighbors详解"><strong>LMNN（Large
Margin Nearest Neighbors）详解</strong></h5>
<p><strong>1. 核心思想</strong></p>
<p>LMNN
是一种<strong>监督度量学习方法</strong>，其目标是通过学习一个线性变换矩阵
$
$，使<strong>同类样本在变换后的空间中更紧密</strong>，<strong>不同类样本被推开</strong>，从而提升KNN等基于距离的算法性能。其核心是引入<strong>最大边距（Large
Margin）</strong>的概念，类似于SVM的分类边界。</p>
<p><strong>2. 损失函数</strong></p>
<p>LMNN 的优化目标由两部分组成： - <strong>Pull
Loss（拉力损失）</strong>：<br>
使同类样本对的距离尽可能小，公式为： <span class="math display">$$
  \varepsilon_{\text{pull}}(\mathbf{L}) = \sum_{j \sim i}
\|\mathbf{L}(\bar{\boldsymbol{x}}_i - \bar{\boldsymbol{x}}_j)\|^2
  $$</span> 其中，$ j i $ 表示与样本 $ i $ 同类的最近邻样本。</p>
<ul>
<li><p><strong>Push Loss（推力损失）</strong>：<br>
使不同类样本对的距离至少保持一个固定边距 $ <em>{ijl} $，公式为： <span class="math display">$$
\varepsilon_{\text{push}}(\mathbf{L}) = \sum_{i,j,l} (1 - y_{il})
\left[1 + \|\mathbf{L}(\bar{\boldsymbol{x}}_i -
\bar{\boldsymbol{x}}_j)\|^2 - \|\mathbf{L}(\bar{\boldsymbol{x}}_i -
\bar{\boldsymbol{x}}_l)\|^2\right]_+
$$</span> 其中，$ y</em>{il} = 1 $ 表示样本 $ i $ 和 $ l $
属于同一类，否则为0；$ []_+ $ 表示取正值部分。</p></li>
<li><p><strong>总损失函数</strong>：<br>
<span class="math display"><em>ε</em>(<strong>L</strong>) = (1−<em>μ</em>)<em>ε</em><sub>pull</sub>(<strong>L</strong>) + <em>μ</em><em>ε</em><sub>push</sub>(<strong>L</strong>)</span>
参数 $ $ 控制两类损失的权重。</p></li>
</ul>
<p><strong>3. 优化问题</strong></p>
<p>LMNN 的目标是最小化总损失函数，同时满足以下约束： <span class="math display">$$
\begin{aligned}
&amp; \min_{\mathbf{M}, \boldsymbol{\xi}} \quad (1 - \mu) \sum_{i,j \sim
i} (\bar{\boldsymbol{x}}_i - \bar{\boldsymbol{x}}_j)^\top \mathbf{M}
(\bar{\boldsymbol{x}}_i - \bar{\boldsymbol{x}}_j) + \mu \sum_{i,j \sim
i,l} (1 - y_{il}) \xi_{ijl} \\
&amp; \text{s.t.} \quad (\bar{\boldsymbol{x}}_i -
\bar{\boldsymbol{x}}_l)^\top \mathbf{M} (\bar{\boldsymbol{x}}_i -
\bar{\boldsymbol{x}}_l) - (\bar{\boldsymbol{x}}_i -
\bar{\boldsymbol{x}}_j)^\top \mathbf{M} (\bar{\boldsymbol{x}}_i -
\bar{\boldsymbol{x}}_j) \geq 1 - \xi_{ijl}, \\
&amp; \quad \quad \quad \xi_{ijl} \geq 0, \quad \mathbf{M} \succeq 0.
\end{aligned}
$$</span> -
<strong>约束（1）</strong>：确保不同类样本对的距离比同类样本对大至少 $ 1
- <em>{ijl} $。<br>
- <strong>约束（2）</strong>：松弛变量 $ </em>{ijl} $
允许部分样本对违反约束。<br>
- <strong>约束（3）</strong>：$ $
必须是半正定矩阵，保证距离的非负性和三角不等式。</p>
<h4 id="作业-3">作业</h4>
<h5 id="section-8">1</h5>
<p>数据降维有哪些常用的方法？阐述主成分分析（PCA）算法的计算流程，并讨论PCA
降维之后的维度如何确定？</p>
<p><strong>（1）常用数据降维方法</strong></p>
<ol type="1">
<li><strong>主成分分析（PCA）</strong>：通过线性变换保留最大方差方向，适用于去噪和压缩数据
。<br>
</li>
<li><strong>线性判别分析（LDA）</strong>：在监督学习中最大化类间分离度，适用于分类任务
。</li>
</ol>
<p><strong>（2）主成分分析（PCA）的计算流程</strong></p>
<ol type="1">
<li><strong>数据标准化</strong>：对原始数据去均值、方差归一化，消除量纲影响
。<br>
</li>
<li><strong>计算协方差矩阵</strong>：<br>
<span class="math display">$$
\mathbf{\Sigma} = \frac{1}{m} \mathbf{X} \mathbf{X}^\top
$$</span> 其中 $ $ 是中心化后的数据矩阵 。<br>
</li>
<li><strong>特征值分解</strong>：对协方差矩阵进行特征值分解，得到特征值
$ _i $ 和单位正交特征向量 $ _i $ 。<br>
</li>
<li><strong>选择主成分</strong>：按特征值大小排序，选择前 $ d’ $
个最大特征值对应的特征向量构成变换矩阵 $ = [_1, <em>2, , </em>{d’}]
$。<br>
</li>
<li><strong>降维投影</strong>：计算低维表示 $ = ^ $，其中 $ ^{d’ m} $
。</li>
</ol>
<p><strong>（3）PCA降维后维度的确定</strong></p>
<ul>
<li><strong>累积方差贡献率</strong>：选择前 $ d’ $
个主成分，使累计方差占比达到阈值（如95%）。<br>
</li>
<li><strong>肘部法则（Elbow
Method）</strong>：绘制特征值随维度变化的曲线，选择“拐点”作为 $ d’
$。</li>
</ul>
<h5 id="section-9">2</h5>
<p>度量学习的目标是什么？LMNN算法中三元组损失是什么？如何计算？</p>
<p><strong>（1）度量学习的目标</strong></p>
<p>度量学习旨在学习一个合理的距离度量，使得： -
<strong>相似样本</strong>：距离尽可能小（如同类样本）。<br>
- <strong>不相似样本</strong>：距离尽可能大（如异类样本）。<br>
典型应用包括推荐系统（优化用户-商品相似性）、图像检索（提升匹配精度）和生物识别（增强类间可分性）。</p>
<p><strong>（2）LMNN中的三元组损失</strong></p>
<p>LMNN（Large Margin Nearest
Neighbor）是一种监督度量学习方法，其核心思想是通过优化距离度量来提升KNN的分类性能。虽然LMNN本身主要使用对比损失（Contrastive
Loss），但三元组损失（Triplet
Loss）是深度度量学习中常见的损失函数，其计算方式如下：<strong>三元组损失的定义</strong></p>
<p>三元组损失基于锚点（Anchor）、正例（Positive）和负例（Negative）三个样本，目标是使锚点与正例的距离小于锚点与负例的距离，公式为：
<span class="math display">ℒ = ∑<sub><em>i</em>, <em>j</em>, <em>l</em></sub>max (0,∥<strong>z</strong><sub><em>i</em></sub>−<strong>z</strong><sub><em>j</em></sub>∥<sup>2</sup>−∥<strong>z</strong><sub><em>i</em></sub>−<strong>z</strong><sub><em>l</em></sub>∥<sup>2</sup>+<em>m</em>)</span>
- $ _i $：锚点样本的嵌入表示。<br>
- $ _j $：与锚点同类的正例样本。<br>
- $ _l $：与锚点不同类的负例样本。<br>
- $ m $：预设的边界值（Margin），控制正负样本距离的最小差距 。</p>
<p><strong>LMNN的损失函数</strong></p>
<p>LMNN 的损失函数包含两部分： 1. <strong>拉力损失（Pull
Loss）</strong>：最小化同类样本对的距离：<br>
<span class="math display">$$
   \varepsilon_{\text{pull}} = \sum_{i,j \sim i}
\|\mathbf{L}(\bar{\boldsymbol{x}}_i - \bar{\boldsymbol{x}}_j)\|^2
   $$</span> 2. <strong>推力损失（Push
Loss）</strong>：最大化异类样本对的距离：<br>
<span class="math display">$$
   \varepsilon_{\text{push}} = \sum_{i,j \sim i,l} (1 - y_{il}) \left[1
+ \|\mathbf{L}(\bar{\boldsymbol{x}}_i - \bar{\boldsymbol{x}}_j)\|^2 -
\|\mathbf{L}(\bar{\boldsymbol{x}}_i -
\bar{\boldsymbol{x}}_l)\|^2\right]_+
   $$</span> 其中 $ $ 是线性变换矩阵，$ y_{il} $ 表示样本对是否同类，$
[]_+ $ 表示取正值部分 。</p>
<p><strong>优化目标</strong></p>
<p>LMNN 的总损失为拉力和推力损失的加权和： <span class="math display"><em>ε</em>(<strong>L</strong>) = (1−<em>μ</em>)<em>ε</em><sub>pull</sub> + <em>μ</em><em>ε</em><sub>push</sub></span>
参数 $ $ 平衡两类损失的权重，最终通过优化 $ $ 得到最优距离度量 。</p>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250607175825520.png" alt="image-20250607175825520">
<figcaption aria-hidden="true">image-20250607175825520</figcaption>
</figure>
<h3 id="半监督学习">半监督学习</h3>
<p>监督学习解决现实问题有哪些难点?
1.标记数据获取成本高：在许多领域如医疗，获取标记数据是昂贵且耗时的。
2.未标记数据大量存在且易得：相对而言，未标记数据大量存在且容易获取。
3.提升模型的泛化能力：通过利用未标记数据，可以增强模型的泛化能力。
举例：在医疗领域，获取医生标记的诊断数据非常昂贵，但有大量未标记的病人记录。
半监督学习可以帮助利用这些未标记数据，提高疾病预测模型的准确性。</p>
<p><img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250607181345721.png" alt="image-20250607181345721">半监督学习结合了有监督学习和无监督学习，半监督学习使用<strong>少量的标记数据</strong>和<strong>大量的未标记数据</strong>来训练模型，主要目标是提升模型在未标记数据上的表现。</p>
<h5 id="基于生成模型的方法">基于生成模型的方法</h5>
<p>假设所有数据（无论是否有标记）都是由一个<strong>潜在的模型</strong>“生成”的。那么无标记的数据可以帮助更准确的估计潜在模型的参数。
比如右图中可以看到数据可以由两个高斯分布近似，则无监督的数据可以被用来更好得做高斯分布的参数估计</p>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250607183201926.png" alt="image-20250607183201926">
<figcaption aria-hidden="true">image-20250607183201926</figcaption>
</figure>
<h5 id="半监督svm"><strong>半监督SVM</strong></h5>
<p>监督学习中的SVM试图找到一个划分超平面，使得两侧支持向量之间的间隔最大，即
<strong>最大划分间隔</strong> 思想。对于半监督SVM (Semi-Supervised
Support Vector Machine, S3VM)
则考虑超平面在能将两类标记样本分隔的同时，<strong>穿过数据低密度的区域</strong>。</p>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250607183349866.png" alt="image-20250607183349866">
<figcaption aria-hidden="true">image-20250607183349866</figcaption>
</figure>
<h6 id="tsvmtransductive-support-vector-machine">TSVM(Transductive
Support Vector Machine)</h6>
<p><strong>1. 核心思想</strong></p>
<p>TSVM 是一种<strong>半监督学习方法</strong>，通过结合有标记数据 $ D_l
$ 和未标记数据 $ D_u
$，利用伪标签（Pseudo-labels）和迭代优化策略，最大化分类超平面的间隔。其损失函数需同时考虑：
- <strong>有标记样本</strong>：最小化分类错误（Hinge Loss）。<br>
- <strong>未标记样本</strong>：通过伪标签引入约束，逐步调整超平面。</p>
<p><strong>2. 损失函数推导</strong></p>
<p>TSVM 的目标是找到一个超平面 $ ^ + b = 0 $，使得： 1.
<strong>有标记样本</strong>的分类误差最小。<br>
2. <strong>未标记样本</strong>的伪标签与超平面预测结果一致。</p>
<p><strong>标准SVM的损失函数</strong>为： <span class="math display">$$
\min_{\boldsymbol{w}, b, \xi} \quad \frac{1}{2} \|\boldsymbol{w}\|^2 + C
\sum_{i=1}^l \xi_i
$$</span> 其中，$ _i $ 是松弛变量，表示样本 $ (_i, y_i) $
的分类误差。</p>
<p><strong>TSVM的扩展</strong>：<br>
引入未标记样本 $ D_u $ 的伪标签 $ _j <span class="math inline">（</span>
j = l+1, , l+u $），并赋予其较小的惩罚系数 $ C_u $（初始阶段 $ C_u C_l
$）： <span class="math display">$$
\min_{\boldsymbol{w}, b, \xi} \quad \frac{1}{2} \|\boldsymbol{w}\|^2 +
C_l \sum_{i=1}^l \xi_i + C_u \sum_{j=l+1}^{l+u} \xi_j
$$</span> 其中： - $ C_l $：有标记样本的惩罚系数。<br>
- $ C_u
$：未标记样本的惩罚系数，初始值很小，逐步增大以增强伪标签的影响。</p>
<p><strong>3. 迭代优化流程</strong></p>
<ol type="1">
<li><strong>初始化</strong>：
<ul>
<li>用有标记数据 $ D_l $ 训练初始 SVM，得到 $ _0, b_0 $。<br>
</li>
<li>对未标记数据 $ D_u $ 预测伪标签 $ _j = (_0^_j + b_0) $。</li>
</ul></li>
<li><strong>伪标签调整</strong>：
<ul>
<li>若存在冲突（如 $ _i _j &lt; 0 $ 且 $ _i + _j &gt; 2
$），翻转其中一个伪标签（如 $ _i -_i $）。<br>
</li>
<li>重新求解优化问题，更新 $ , b $。</li>
</ul></li>
<li><strong>参数调整</strong>：
<ul>
<li>逐步增大 $ C_u $（如 $ C_u {2C_u, C_l}
$），增强未标记样本的影响。</li>
</ul></li>
</ol>
<p><strong>4. 关键数学细节</strong></p>
<ul>
<li><p><strong>Hinge Loss</strong>：<br>
对每个样本 $ (_i, y_i) $，损失为： <span class="math display"><em>ξ</em><sub><em>i</em></sub> = max (0,1−<em>y</em><sub><em>i</em></sub>(<strong>w</strong><sup>⊤</sup><strong>x</strong><sub><em>i</em></sub>+<em>b</em>))</span>
未标记样本的伪标签 $ _j $ 同样代入此公式，但惩罚系数为 $ C_u
$。</p></li>
<li><p><strong>正则化项</strong>：<br>
$ ||^2 $ 确保超平面的泛化能力，防止过拟合。</p></li>
<li><p><strong>伪标签翻转条件</strong>：<br>
当两个未标记样本 $ i, j $ 满足： <span class="math display"><em>ŷ</em><sub><em>i</em></sub><em>ŷ</em><sub><em>j</em></sub> &lt; 0  且  <em>ξ</em><sub><em>i</em></sub> &gt; 0, <em>ξ</em><sub><em>j</em></sub> &gt; 0,  <em>ξ</em><sub><em>i</em></sub> + <em>ξ</em><sub><em>j</em></sub> &gt; 2</span>
表示它们被错误分类且距离超平面较近，需翻转其中一个标签以减少冲突。</p></li>
</ul>
<h5 id="图半监督学习"><strong>图半监督学习</strong></h5>
<p>给定一个数据集，我们可将其映射为一个图，数据集中每个样本对应于图结点，若两个样本之间的相似度很高(或相关性很强)，则对应的结点之间存在一条边，边的“强度”(strength)
正比于样本之间的相似度(或相关性)。</p>
<p>可将有标记样本所对应的结点想象为染过色，标记样本所对应的结点尚未染色。半监督学习就对应于“颜色”在图上扩散或传播的过程。由于个图对应了一个矩阵，我们就能基于矩阵运算来进行半监督学习算法的推导与分析。</p>
<figure>
<img src="/2025/06/06/college/%E5%A4%A7%E4%BA%8C%E4%B8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%8B%EF%BC%89/image-20250607184534217.png" alt="image-20250607184534217">
<figcaption aria-hidden="true">image-20250607184534217</figcaption>
</figure>
<p><strong>图半监督学习中的能量函数推导详解</strong></p>
<p><strong>1. 图结构与亲和矩阵</strong></p>
<p>给定有标记数据集 $ D_l = {(<em>1, y_1), (<em>2, y_2), , (<em>l, y_l)}
$ 和未标记数据集 $ D_u = {</em>{l+1}, </em>{l+2}, , </em>{l+u}}
$，构建图 $ G = (V, E) <span class="math inline">： −  *  * <em>结</em><em>点</em><em>集</em> *  * ：</span>
V = {<em>1, , <em>l, </em>{l+1}, , </em>{l+u}} $，包含所有样本。<br>
- <strong>边集</strong>：通过亲和矩阵 $ $ 表示，元素定义为： <span class="math display">$$
  (\mathbf{W})_{ij} =
  \begin{cases}
  \exp\left(-\frac{\|\boldsymbol{x}_i -
\boldsymbol{x}_j\|^2}{2\sigma^2}\right), &amp; i \neq j \\
  0, &amp; \text{otherwise}
  \end{cases}
  $$</span> 其中，$ $ 是高斯核的带宽参数，控制邻接关系的敏感性。</p>
<p><strong>2. 能量函数的定义与推导</strong></p>
<p>假设分类模型的输出标记为 $ f(_i) $（取值为类别标签，如 $
$），定义能量函数 $ E(f) $ 为： <span class="math display">$$
E(f) = \frac{1}{2} \sum_{i=1}^m \sum_{j=1}^m (\mathbf{W})_{ij}
(f(\boldsymbol{x}_i) - f(\boldsymbol{x}_j))^2
$$</span> 其中 $ m = l + u $ 是总样本数。</p>
<p><strong>3. 能量函数的展开与简化</strong></p>
<ol type="1">
<li><strong>展开平方项</strong> <span class="math display">$$
E(f) = \frac{1}{2} \sum_{i=1}^m \sum_{j=1}^m (\mathbf{W})_{ij} \left[
f^2(\boldsymbol{x}_i) - 2 f(\boldsymbol{x}_i) f(\boldsymbol{x}_j) +
f^2(\boldsymbol{x}_j) \right]
$$</span></li>
<li><strong>利用对称性简化</strong> 由于 $ $ 是对称矩阵（<span class="math inline">(<strong>W</strong>)<sub><em>i</em><em>j</em></sub> = (<strong>W</strong>)<sub><em>j</em><em>i</em></sub></span>），可交换求和顺序：
<span class="math display">$$
\sum_{i=1}^m \sum_{j=1}^m (\mathbf{W})_{ij} f^2(\boldsymbol{x}_j) =
\sum_{j=1}^m \sum_{i=1}^m (\mathbf{W})_{ji} f^2(\boldsymbol{x}_j) =
\sum_{i=1}^m \sum_{j=1}^m (\mathbf{W})_{ij} f^2(\boldsymbol{x}_i)
$$</span> 因此，能量函数变为 <span class="math display">$$
E(f) = \frac{1}{2} \left( 2 \sum_{i=1}^m \sum_{j=1}^m (\mathbf{W})_{ij}
f^2(\boldsymbol{x}_i) - 2 \sum_{i=1}^m \sum_{j=1}^m (\mathbf{W})_{ij}
f(\boldsymbol{x}_i) f(\boldsymbol{x}_j) \right)
$$</span></li>
<li><strong>引入度矩阵</strong> 定义度矩阵 $ $
为对角矩阵，其对角线元素为： <span class="math display">$$
d_i = \sum_{j=1}^m (\mathbf{W})_{ij}
$$</span> 最终能量函数可表示为： <span class="math display">$$
E(f) = \sum_{i=1}^m d_i f^2(\boldsymbol{x}_i) - \sum_{i=1}^m
\sum_{j=1}^m (\mathbf{W})_{ij} f(\boldsymbol{x}_i) f(\boldsymbol{x}_j) =
\boldsymbol{f}^\top (\mathbf{D} - \mathbf{W}) \boldsymbol{f}
$$</span> 其中，$ = [f(_1), f(_2), , f(_m)]^$。</li>
</ol>
<p><strong>图半监督学习方法推导详解</strong></p>
<p><strong>1. 分块矩阵表示</strong></p>
<p>将亲和矩阵 $ $ 和度矩阵 $ $ 按有标记数据（前 $ l $
行列）和未标记数据（后 $ u $ 行列）分块： <span class="math display">$$
\mathbf{W} =
\begin{bmatrix}
\mathbf{W}_{ll} &amp; \mathbf{W}_{lu} \\
\mathbf{W}_{ul} &amp; \mathbf{W}_{uu}
\end{bmatrix}, \quad
\mathbf{D} =
\begin{bmatrix}
\mathbf{D}_{ll} &amp; \mathbf{0}_{lu} \\
\mathbf{0}_{ul} &amp; \mathbf{D}_{uu}
\end{bmatrix}
$$</span> 其中： - $ <em>{ll} $：有标记数据间的亲和度。<br>
- $ </em>{lu} $：有标记与未标记数据间的亲和度。<br>
- $ <em>{uu} $：未标记数据间的亲和度。<br>
- $ </em>{ll}, _{uu} $：对应子图的度矩阵。</p>
<p><strong>2. 能量函数的分块展开</strong></p>
<p>能量函数 $ E(f) = ^( - ) $ 可展开为</p>
<p>展开后得到： <span class="math display"><em>E</em>(<em>f</em>) = <strong>f</strong><sub><em>l</em></sub><sup>⊤</sup>(<strong>D</strong><sub><em>l</em><em>l</em></sub>−<strong>W</strong><sub><em>l</em><em>l</em></sub>)<strong>f</strong><sub><em>l</em></sub> − 2<strong>f</strong><sub><em>u</em></sub><sup>⊤</sup><strong>W</strong><sub><em>u</em><em>l</em></sub><strong>f</strong><sub><em>l</em></sub> + <strong>f</strong><sub><em>u</em></sub><sup>⊤</sup>(<strong>D</strong><sub><em>u</em><em>u</em></sub>−<strong>W</strong><sub><em>u</em><em>u</em></sub>)<strong>f</strong><sub><em>u</em></sub></span></p>
<p>**3. 对未标记数据 $ _u $ 求偏微分**</p>
<p>目标是最小化 $ E(f) $，对 $ _u $ 求偏导并令其为零： <span class="math display">$$
\frac{\partial E(f)}{\partial \boldsymbol{f}_u} = -2 \mathbf{W}_{ul}
\boldsymbol{f}_l + 2 (\mathbf{D}_{uu} - \mathbf{W}_{uu})
\boldsymbol{f}_u = 0
$$</span> 解得： <span class="math display"><strong>f</strong><sub><em>u</em></sub> = (<strong>D</strong><sub><em>u</em><em>u</em></sub>−<strong>W</strong><sub><em>u</em><em>u</em></sub>)<sup>−1</sup><strong>W</strong><sub><em>u</em><em>l</em></sub><strong>f</strong><sub><em>l</em></sub></span></p>
<h4 id="协同训练">协同训练</h4>
<p>协同训练（Co-training）是一种经典的<strong>半监督学习方法</strong>，由Blum和Mitchell于1998年首次提出，主要用于处理<strong>多视图数据</strong>（Multi-view
Data）。其核心思想是通过多个分类器的协作，利用少量标记数据和大量未标记数据提升模型性能。以下是详细解析：</p>
<p><strong>1. 核心思想与假设</strong></p>
<p><strong>（1）多视图数据</strong></p>
<ul>
<li><strong>定义</strong>：每个样本可被划分为多个<strong>充分冗余且条件独立</strong>的视图（View）。
<ul>
<li><strong>充分冗余</strong>：每个视图本身包含足够信息，可独立完成学习任务。<br>
</li>
<li><strong>条件独立性</strong>：在给定类别标签的条件下，不同视图的特征相互独立。<br>
例如，网页数据可划分为“文本内容”和“超链接结构”两个视图，它们共同描述网页内容。</li>
</ul></li>
</ul>
<p><strong>（2）协作机制</strong></p>
<ul>
<li><strong>双分类器设计</strong>：使用两个分类器 $ h_1 $ 和 $ h_2
$，分别基于视图 $ V_1 $ 和 $ V_2 $ 进行训练。<br>
</li>
<li><strong>伪标签生成</strong>：分类器 $ h_1 $
对未标记数据的高置信度预测结果会被 $ h_2 $
使用，反之亦然，形成迭代优化。<br>
</li>
<li><strong>目标</strong>：通过分类器间的互补性，逐步扩展标记数据集，提升模型泛化能力。</li>
</ul>
<p><strong>2. 算法流程</strong></p>
<ol type="1">
<li><strong>初始化阶段</strong>：
<ul>
<li>使用少量标记数据 $ D_l $，分别训练分类器 $ h_1 $（基于视图 $ V_1
$）和 $ h_2 $（基于视图 $ V_2 $）。<br>
</li>
</ul></li>
<li><strong>伪标签生成</strong>：
<ul>
<li>对未标记数据 $ D_u <span class="math inline">，</span> h_1 $
预测视图 $ V_1 $ 的伪标签，$ h_2 $ 预测视图 $ V_2 $ 的伪标签。<br>
</li>
<li>选择置信度高于阈值的样本加入训练集（如 $ h_1 $ 的预测结果用于更新 $
h_2 $ 的训练数据，反之亦然）。<br>
</li>
</ul></li>
<li><strong>迭代优化</strong>：
<ul>
<li>重复伪标签生成和模型训练，直到未标记数据耗尽或模型收敛。</li>
</ul></li>
</ol>
<p><strong>3. 核心优势</strong></p>
<ul>
<li><strong>减少对标注数据的依赖</strong>：仅需少量标记数据即可训练高性能模型，尤其适合标注成本高的场景（如医疗影像分析）。<br>
</li>
<li><strong>提升模型鲁棒性</strong>：分类器间的协作可纠正彼此的错误，降低单一模型过拟合风险。<br>
</li>
<li><strong>多视图互补性</strong>：不同视图的信息融合能捕捉更全面的特征（如图像的RGB通道与纹理特征）。</li>
</ul>
<h4 id="作业-4">作业</h4>
<h5 id="section-10">1</h5>
<p>什么是半监督学习？请简要描述其基本思想。半监督学习相比于监督学习和无监督学习有什么优势和应用场景？</p>
<p><strong>（1）定义与基本思想</strong></p>
<p>半监督学习（Semi-Supervised
Learning）是结合<strong>监督学习</strong>（利用标记数据）和<strong>无监督学习</strong>（利用未标记数据）的机器学习方法，其核心思想是通过少量标记数据与大量未标记数据的联合训练，提升模型的泛化能力和鲁棒性。<br>
-
<strong>监督学习</strong>：依赖大量人工标注数据（如分类、回归）。<br>
-
<strong>无监督学习</strong>：仅利用数据分布规律（如聚类、降维）。<br>
-
<strong>半监督学习</strong>：在标记数据稀缺时，通过未标记数据挖掘潜在结构，降低标注成本
。</p>
<p><strong>（2）优势</strong></p>
<ul>
<li><strong>减少标注依赖</strong>：仅需少量标记数据即可训练高性能模型，适用于标注成本高的场景（如医疗影像分析）。<br>
</li>
<li><strong>提升模型性能</strong>：利用未标记数据增强数据多样性，缓解过拟合风险。<br>
</li>
<li><strong>平衡效率与精度</strong>：在资源有限时，兼顾监督学习的准确性与无监督学习的高效性
。</li>
</ul>
<p><strong>（3）应用场景</strong></p>
<ul>
<li><strong>医学诊断</strong>：利用少量标注的病理图像和大量未标注数据训练疾病预测模型。<br>
</li>
<li><strong>推荐系统</strong>：结合用户行为（有标记）与商品属性（未标记）优化排序模型。<br>
</li>
<li><strong>自然语言处理</strong>：通过预训练模型（如GPT）的“预训练+微调”框架，减少人工标注需求
。</li>
</ul>
<h5 id="section-11">2</h5>
<p>协同训练算法的作用是什么？请简述算法主要流程和所需条件。</p>
<p><strong>（1）作用与核心思想</strong></p>
<p>协同训练是一种典型的半监督学习方法，适用于<strong>多视图数据</strong>（Multi-view
Data）。其核心思想是通过多个分类器的协作，利用未标记数据扩展训练集，最终提升模型性能。</p>
<ul>
<li><strong>多视图条件</strong>：
<ul>
<li><strong>充分冗余</strong>：每个视图本身包含足够信息，可独立完成任务。<br>
</li>
<li><strong>条件独立性</strong>：在给定类别标签的条件下，不同视图的特征相互独立
。</li>
</ul></li>
</ul>
<p><strong>（2）算法流程</strong></p>
<ol type="1">
<li><strong>初始化阶段</strong>：
<ul>
<li>使用少量标记数据 $ D_l $，分别训练两个分类器 $ h_1 $（基于视图 $ V_1
$）和 $ h_2 $（基于视图 $ V_2 $）。<br>
</li>
</ul></li>
<li><strong>伪标签生成</strong>：
<ul>
<li>对未标记数据 $ D_u <span class="math inline">，</span> h_1 $ 预测 $
V_2 $ 的伪标签，$ h_2 $ 预测 $ V_1 $ 的伪标签。<br>
</li>
<li>选择置信度高于阈值的样本加入训练集（如 $ h_1 $ 的预测结果用于更新 $
h_2 $ 的训练数据，反之亦然）。<br>
</li>
</ul></li>
<li><strong>迭代优化</strong>：
<ul>
<li>重复伪标签生成和模型训练，直到未标记数据耗尽或模型收敛 。</li>
</ul></li>
</ol>
<p><strong>（3）所需条件</strong></p>
<ul>
<li><strong>多视图划分</strong>：数据需满足“充分冗余”和“条件独立性”（如图像的RGB通道与纹理特征）。<br>
</li>
<li><strong>分类器多样性</strong>：选择差异较大的分类器（如SVM +
决策树），增强互补性。<br>
</li>
<li><strong>伪标签可靠性</strong>：初始模型需有一定性能，避免错误伪标签污染训练集
。</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/06/01/%E5%AD%A6%E4%B9%A0/weatherweb%E5%BC%80%E5%8F%91%E6%97%A5%E5%BF%97/weatherweb%E5%BC%80%E5%8F%91%E6%97%A5%E5%BF%97/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/06/01/%E5%AD%A6%E4%B9%A0/weatherweb%E5%BC%80%E5%8F%91%E6%97%A5%E5%BF%97/weatherweb%E5%BC%80%E5%8F%91%E6%97%A5%E5%BF%97/" class="post-title-link" itemprop="url">weatherweb开发学习记录</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-06-01 00:00:00" itemprop="dateCreated datePublished" datetime="2025-06-01T00:00:00+08:00">2025-06-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-06-12 18:48:30" itemprop="dateModified" datetime="2025-06-12T18:48:30+08:00">2025-06-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/weatherweb%E5%BC%80%E5%8F%91%E6%97%A5%E5%BF%97/" itemprop="url" rel="index"><span itemprop="name">weatherweb开发日志</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="项目要求">项目要求</h3>
<p>智能天气提醒助手</p>
<p>描述：开发一款web应用，实时获取天气数据并支持个性化提醒（如雨天带伞）。</p>
<p>要求：</p>
<p>调用天气API获取实时数据（如OpenWeatherMap，每天1000次免费调用）</p>
<p>使用前端三件套设计交互界面，展示当前及未来天气信息，空气质量、体感温度、日出日落、月相等信息；</p>
<p>使用fastapi做后端</p>
<p>支持地点设置和天气提醒条件配置，在预设的提醒条件下提醒用户，并且将用户偏好保存至本地文件。</p>
<p>多城市切换、历史天气查询、全球地图展示等额外功能（可选*）。</p>
<h3 id="技术栈">技术栈</h3>
<p>fastapi，前端三件套(fetchapi)，apifox</p>
<h3 id="fetchapi">fetchapi</h3>
<p><strong>Fetch API</strong>
是现代浏览器提供的标准网络请求接口，允许开发者通过 JavaScript 发起异步
HTTP 请求（如 GET、POST、PUT、DELETE 等），并处理响应数据（如
JSON、文本、图片等）。它是传统
<code>XMLHttpRequest</code>（AJAX）的替代方案，语法更简洁，且支持
Promise 异步编程。</p>
<p><strong>简单来说，就是用作给后端发送请求，实现前后端分离</strong></p>
<figure>
<img src="/2025/06/01/%E5%AD%A6%E4%B9%A0/weatherweb%E5%BC%80%E5%8F%91%E6%97%A5%E5%BF%97/weatherweb%E5%BC%80%E5%8F%91%E6%97%A5%E5%BF%97/image-20250609163335375.png" alt="image-20250609163335375">
<figcaption aria-hidden="true">image-20250609163335375</figcaption>
</figure>
<h4 id="用法学习">用法学习</h4>
<p>在使用 <code>fetch</code> 发起 HTTP
请求时，<code>method</code>、<code>headers</code> 和 <code>body</code>
是配置请求的核心参数，它们共同决定了请求的行为和数据格式。以下是每个参数的具体作用及示例：</p>
<h5 id="method-post"><strong>1.
<code>method: 'POST'</code></strong></h5>
<p><strong>作用</strong></p>
<p>指定 HTTP 请求的方法（动词），<code>POST</code>
表示向服务器提交数据（如创建资源）。 - <strong>常见方法</strong>： -
<code>GET</code>：获取数据（默认方法，无需显式声明）。 -
<code>POST</code>：提交数据（如新增记录）。 -
<code>PUT</code>：更新数据。 - <code>DELETE</code>：删除数据。 -
<strong>与后端交互</strong>：FastAPI 的路由通过
<code>@app.post()</code>、<code>@app.get()</code>
等装饰器匹配请求方法。</p>
<p><strong>示例</strong></p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="title function_">fetch</span>(<span class="string">&#x27;https://api.example.com/submit&#x27;</span>, &#123;</span><br><span class="line">  <span class="attr">method</span>: <span class="string">&#x27;POST&#x27;</span>, <span class="comment">// 告诉服务器这是一个提交请求</span></span><br><span class="line">  <span class="comment">// ...其他配置</span></span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>
<h5 id="headers-请求头"><strong>2. <code>headers</code>
请求头</strong></h5>
<p><strong>作用</strong></p>
<p>定义请求的元信息，用于告知服务器如何处理请求和数据格式。 -
<strong>关键字段</strong>： -
<strong><code>Content-Type</code></strong>：指定请求体（<code>body</code>）的数据格式。
- <code>application/json</code>：表示发送 JSON 数据。 -
<code>application/x-www-form-urlencoded</code>：表示表单数据（键值对）。
- <code>multipart/form-data</code>：用于上传文件。 -
<strong><code>Authorization</code></strong>：携带身份凭证（如 Token）。
- <strong><code>Accept</code></strong>：声明客户端期望的响应格式（如
JSON、XML）。</p>
<p><strong>示例</strong></p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">headers</span>: &#123;</span><br><span class="line">  <span class="string">&#x27;Content-Type&#x27;</span>: <span class="string">&#x27;application/json&#x27;</span>, <span class="comment">// 告诉服务器请求体是 JSON</span></span><br><span class="line">  <span class="string">&#x27;Authorization&#x27;</span>: <span class="string">&#x27;Bearer your_token_here&#x27;</span> <span class="comment">// 身份验证（可选）</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="body-json.stringifyitem"><strong>3.
<code>body: JSON.stringify(item)</code></strong></h5>
<p><strong>作用</strong></p>
<p>定义请求体（即发送给服务器的数据），需根据 <code>Content-Type</code>
的类型进行格式化。 -
<strong><code>JSON.stringify(item)</code></strong>：将 JavaScript
对象转换为 JSON 字符串。 - 因为 HTTP
协议只能传输文本，不能直接传输对象。 - <strong>注意事项</strong>： -
若未设置
<code>Content-Type: application/json</code>，服务器可能无法正确解析数据。
- 若使用 <code>FormData</code> 上传文件，需使用
<code>multipart/form-data</code> 格式。</p>
<p><strong>示例</strong></p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> item = &#123; <span class="attr">name</span>: <span class="string">&quot;Apple&quot;</span>, <span class="attr">price</span>: <span class="number">1.99</span> &#125;;</span><br><span class="line"></span><br><span class="line"><span class="attr">body</span>: <span class="title class_">JSON</span>.<span class="title function_">stringify</span>(item) <span class="comment">// 转换为 &#x27;&#123;&quot;name&quot;:&quot;Apple&quot;,&quot;price&quot;:1.99&#125;&#x27;</span></span><br></pre></td></tr></table></figure>
<h5 id="完整示例向-fastapi-提交数据"><strong>完整示例：向 FastAPI
提交数据</strong></h5>
<p><strong>FastAPI 后端定义</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastapi <span class="keyword">import</span> FastAPI</span><br><span class="line"><span class="keyword">from</span> pydantic <span class="keyword">import</span> BaseModel</span><br><span class="line"></span><br><span class="line">app = FastAPI()</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Item</span>(<span class="title class_ inherited__">BaseModel</span>):</span><br><span class="line">    name: <span class="built_in">str</span></span><br><span class="line">    price: <span class="built_in">float</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.post(<span class="params"><span class="string">&quot;/items/&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">create_item</span>(<span class="params">item: Item</span>):</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&quot;message&quot;</span>: <span class="string">&quot;Item created&quot;</span>, <span class="string">&quot;item&quot;</span>: item&#125;</span><br></pre></td></tr></table></figure>
<p><strong>前端调用</strong></p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> item = &#123; <span class="attr">name</span>: <span class="string">&quot;Banana&quot;</span>, <span class="attr">price</span>: <span class="number">0.99</span> &#125;;</span><br><span class="line"></span><br><span class="line"><span class="title function_">fetch</span>(<span class="string">&#x27;http://localhost:8000/items/&#x27;</span>, &#123;</span><br><span class="line">  <span class="attr">method</span>: <span class="string">&#x27;POST&#x27;</span>,</span><br><span class="line">  <span class="attr">headers</span>: &#123;</span><br><span class="line">    <span class="string">&#x27;Content-Type&#x27;</span>: <span class="string">&#x27;application/json&#x27;</span> <span class="comment">// 必须与数据格式匹配</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">body</span>: <span class="title class_">JSON</span>.<span class="title function_">stringify</span>(item) <span class="comment">// 将对象转为 JSON 字符串</span></span><br><span class="line">&#125;)</span><br><span class="line">  .<span class="title function_">then</span>(<span class="function"><span class="params">response</span> =&gt;</span> response.<span class="title function_">json</span>())</span><br><span class="line">  .<span class="title function_">then</span>(<span class="function"><span class="params">data</span> =&gt;</span> <span class="variable language_">console</span>.<span class="title function_">log</span>(data))</span><br><span class="line">  .<span class="title function_">catch</span>(<span class="function"><span class="params">error</span> =&gt;</span> <span class="variable language_">console</span>.<span class="title function_">error</span>(<span class="string">&#x27;Error:&#x27;</span>, error));</span><br></pre></td></tr></table></figure>
<h5 id="总结"><strong>总结</strong></h5>
<table>
<colgroup>
<col style="width: 13%">
<col style="width: 45%">
<col style="width: 40%">
</colgroup>
<thead>
<tr class="header">
<th>参数</th>
<th>作用</th>
<th>必填性</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>method</code></td>
<td>定义请求类型（如 <code>POST</code>）</td>
<td>必填（非 <code>GET</code> 时）</td>
</tr>
<tr class="even">
<td><code>headers</code></td>
<td>声明数据格式、身份凭证等</td>
<td>必填（尤其 <code>Content-Type</code>）</td>
</tr>
<tr class="odd">
<td><code>body</code></td>
<td>发送的数据（需格式化为字符串）</td>
<td>必填（<code>POST</code>/<code>PUT</code> 时）</td>
</tr>
</tbody>
</table>
<p><strong>关键点</strong>：<br>
- <code>POST</code> 请求必须设置 <code>headers['Content-Type']</code> 和
<code>body</code>。 - <code>JSON.stringify()</code> 是发送 JSON
数据的关键步骤。 - FastAPI 会根据 <code>Content-Type</code>
自动解析请求体并进行数据校验（通过 Pydantic 模型）。</p>
<h3 id="cors">CORS</h3>
<h4 id="cors-是什么"><strong>CORS 是什么？</strong></h4>
<p><strong>CORS（Cross-Origin Resource Sharing）</strong>
是一种浏览器安全机制，用于解决 <strong>跨域请求</strong>
的问题。它允许服务器明确授权某些跨域请求，从而在保障安全的前提下，实现前后端分离架构中的跨域通信。</p>
<h4 id="为什么需要-cors"><strong>为什么需要 CORS？</strong></h4>
<p><strong>1. 同源策略（Same-Origin Policy）</strong></p>
<p>浏览器默认遵循 <strong>同源策略</strong>，即网页只能请求与自身
<strong>同源（相同域名、协议、端口）</strong> 的资源。<br>
<strong>例如</strong>：</p>
<ul>
<li>前端地址：<code>http://localhost:3000</code></li>
<li>后端地址：<code>http://localhost:8000</code><br>
此时，前端向后端发起的请求会被浏览器
<strong>拦截</strong>，因为端口不同（3000 vs 8000）。</li>
</ul>
<p><strong>2. 跨域场景</strong></p>
<p>跨域是前后端分离架构中的常见问题，例如： - 前端部署在
<code>https://example.com</code>，后端 API 在
<code>https://api.example.com</code>。 -
前端本地开发（<code>localhost:3000</code>）调用后端服务（<code>localhost:8000</code>）。</p>
<p><strong>3. CORS 的作用</strong></p>
<p>CORS 通过 <strong>服务器响应头</strong>
告诉浏览器：“这个跨域请求是安全的，允许它通过”。<br>
浏览器根据这些响应头决定是否放行请求。</p>
<h4 id="如何配置-cors"><strong>如何配置 CORS？</strong></h4>
<p>以 <strong>FastAPI</strong> 为例，配置允许跨域请求的步骤如下：</p>
<p><strong>启用 CORS 中间件</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastapi <span class="keyword">import</span> FastAPI</span><br><span class="line"><span class="keyword">from</span> fastapi.middleware.cors <span class="keyword">import</span> CORSMiddleware</span><br><span class="line"></span><br><span class="line">app = FastAPI()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置 CORS</span></span><br><span class="line">app.add_middleware(</span><br><span class="line">    CORSMiddleware,</span><br><span class="line">    allow_origins=[<span class="string">&quot;http://localhost:3000&quot;</span>],  <span class="comment"># 允许的源</span></span><br><span class="line">    allow_credentials=<span class="literal">True</span>,                    <span class="comment"># 允许携带凭证</span></span><br><span class="line">    allow_methods=[<span class="string">&quot;*&quot;</span>],                       <span class="comment"># 允许所有方法（GET、POST 等）</span></span><br><span class="line">    allow_headers=[<span class="string">&quot;*&quot;</span>],                       <span class="comment"># 允许所有头信息</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h4 id="cors-的实际应用场景"><strong>CORS 的实际应用场景</strong></h4>
<p><strong>1. 前后端分离开发</strong></p>
<ul>
<li>前端（React/Vue）运行在
<code>localhost:3000</code>，后端（FastAPI）运行在
<code>localhost:8000</code>。</li>
<li>配置 <code>allow_origins=["http://localhost:3000"]</code>
允许跨域通信。</li>
</ul>
<p><strong>2. 第三方 API 调用</strong></p>
<ul>
<li>前端直接调用第三方服务（如天气 API），需服务器启用 CORS。</li>
<li>示例：<code>Access-Control-Allow-Origin: *</code>
表示允许所有来源。</li>
</ul>
<p><strong>3. 需要凭证的场景</strong></p>
<ul>
<li>前端需携带 Cookie 或 Token 访问后端接口： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">app.add_middleware(</span><br><span class="line">    CORSMiddleware,</span><br><span class="line">    allow_origins=[<span class="string">&quot;http://localhost:3000&quot;</span>],</span><br><span class="line">    allow_credentials=<span class="literal">True</span>,  <span class="comment"># 允许携带凭证</span></span><br><span class="line">    allow_methods=[<span class="string">&quot;*&quot;</span>],</span><br><span class="line">    allow_headers=[<span class="string">&quot;*&quot;</span>],</span><br><span class="line">)</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="总结-1"><strong>总结</strong></h4>
<table>
<colgroup>
<col style="width: 19%">
<col style="width: 45%">
<col style="width: 34%">
</colgroup>
<thead>
<tr class="header">
<th>概念</th>
<th>作用</th>
<th>配置示例</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>同源策略</strong></td>
<td>浏览器安全机制，阻止跨域请求</td>
<td>默认启用</td>
</tr>
<tr class="even">
<td><strong>CORS</strong></td>
<td>服务器通过响应头授权跨域请求</td>
<td><code>Access-Control-Allow-Origin</code></td>
</tr>
<tr class="odd">
<td><strong>预检请求</strong></td>
<td>OPTIONS 请求，验证复杂跨域请求的合法性</td>
<td>自动触发</td>
</tr>
<tr class="even">
<td><strong>FastAPI 配置</strong></td>
<td>使用 <code>CORSMiddleware</code> 中间件</td>
<td><code>app.add_middleware(...)</code></td>
</tr>
</tbody>
</table>
<p><strong>最佳实践</strong>： 1.
<strong>开发阶段</strong>：允许所有来源（<code>allow_origins=["*"]</code>），方便调试。
2.
<strong>生产环境</strong>：严格限制允许的源、方法、头信息，避免安全风险。
3. <strong>携带凭证</strong>：启用 <code>allow_credentials=True</code>
并明确指定允许的源（避免使用 <code>*</code>）。</p>
<h3 id="nginx">Nginx</h3>
<h4 id="nginx-是什么"><strong>Nginx 是什么？</strong></h4>
<p><strong>Nginx</strong>（发音为 “engine-x”）是一个高性能的开源
<strong>Web 服务器、反向代理服务器、负载均衡器和 HTTP
缓存</strong>，广泛用于现代 Web
架构中。它以轻量级、低资源消耗和高并发处理能力著称，常用于优化网站性能、管理流量和提升安全性。</p>
<h4 id="nginx-的核心功能"><strong>Nginx 的核心功能</strong></h4>
<p><strong>1. Web 服务器</strong></p>
<ul>
<li><strong>静态资源托管</strong>：直接提供
HTML、CSS、JS、图片等静态文件服务。</li>
<li><strong>动态请求转发</strong>：将动态请求（如
API）转发给后端应用（如 FastAPI、Django、Node.js）。</li>
</ul>
<p><strong>2. 反向代理</strong></p>
<ul>
<li><strong>作用</strong>：接收客户端请求，转发给后端服务器（如
FastAPI），隐藏真实服务器地址。</li>
<li><strong>优势</strong>：提高安全性、支持负载均衡、缓存和 SSL
终端。</li>
</ul>
<p><strong>3. 负载均衡</strong></p>
<ul>
<li><strong>作用</strong>：将请求分发到多个后端服务器（如多个 FastAPI
实例），避免单点故障。</li>
<li><strong>算法</strong>：轮询（Round Robin）、最少连接（Least
Connections）、IP 哈希（IP Hash）等。</li>
</ul>
<p><strong>4. SSL/TLS 终端</strong></p>
<ul>
<li><strong>作用</strong>：处理 HTTPS
加密和解密，减轻后端服务器的压力。</li>
<li><strong>配置</strong>：绑定证书和私钥，强制 HTTPS。</li>
</ul>
<p><strong>5. 缓存</strong></p>
<ul>
<li><strong>作用</strong>：缓存静态资源（如图片、CSS）或动态内容（如 API
响应），减少后端负载。</li>
</ul>
<p><strong>6. 高可用性和容错</strong></p>
<ul>
<li><strong>健康检查</strong>：自动检测后端服务器状态，故障时切换备用节点。</li>
</ul>
<h4 id="nginx-的典型应用场景"><strong>Nginx 的典型应用场景</strong></h4>
<p><strong>1. 反向代理 FastAPI 服务</strong></p>
<figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># /etc/nginx/sites-available/fastapi.conf</span></span><br><span class="line"><span class="section">server</span> &#123;</span><br><span class="line">    <span class="attribute">listen</span> <span class="number">80</span>;</span><br><span class="line">    <span class="attribute">server_name</span> example.com;</span><br><span class="line"></span><br><span class="line">    <span class="section">location</span> / &#123;</span><br><span class="line">        <span class="attribute">proxy_pass</span> http://127.0.0.1:8000;  <span class="comment"># FastAPI 服务地址</span></span><br><span class="line">        <span class="attribute">proxy_set_header</span> Host <span class="variable">$host</span>;</span><br><span class="line">        <span class="attribute">proxy_set_header</span> X-Real-IP <span class="variable">$remote_addr</span>;</span><br><span class="line">        <span class="attribute">proxy_set_header</span> X-Forwarded-For <span class="variable">$proxy_add_x_forwarded_for</span>;</span><br><span class="line">        <span class="attribute">proxy_set_header</span> X-Forwarded-Proto <span class="variable">$scheme</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>作用</strong>：将 <code>example.com</code>
的请求转发给运行在 <code>127.0.0.1:8000</code> 的 FastAPI 服务。</li>
</ul>
<p><strong>2. 静态文件托管</strong></p>
<figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">location</span> /static/ &#123;</span><br><span class="line">    <span class="attribute">alias</span> /var/www/static/;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>作用</strong>：直接提供 <code>/var/www/static/</code>
目录下的静态文件（如图片、CSS）。</li>
</ul>
<h4 id="nginx-与-fastapi-的协作流程"><strong>Nginx 与 FastAPI
的协作流程</strong></h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">客户端 -&gt; Nginx（反向代理） -&gt; FastAPI（处理业务逻辑） -&gt; 数据库/其他服务</span><br></pre></td></tr></table></figure>
<ol type="1">
<li><strong>静态资源</strong>：由 Nginx 直接返回（如
HTML、CSS、JS）。</li>
<li><strong>API 请求</strong>：Nginx 转发给 FastAPI，FastAPI 处理后返回
JSON 数据。</li>
<li><strong>HTTPS</strong>：Nginx 处理加密和解密，FastAPI 无需关心
SSL。</li>
</ol>
<p><strong>最佳实践</strong>：</p>
<ol type="1">
<li><strong>开发阶段</strong>：直接运行
FastAPI（<code>uvicorn main:app --reload</code>）。</li>
<li><strong>生产环境</strong>：Nginx + FastAPI（Gunicorn/Uvicorn） +
数据库。</li>
<li><strong>性能优化</strong>：启用 Gzip
压缩、HTTP/2、缓存静态资源。</li>
</ol>
<p>通过 Nginx 的反向代理和负载均衡，可以显著提升 FastAPI
应用的性能、安全性和可扩展性。</p>
<h3 id="反向代理是什么">反向代理是什么？</h3>
<p><strong>反向代理（Reverse Proxy）</strong>
是一种服务器角色，它位于客户端与服务器之间，接收客户端的请求后，将请求转发给后端服务器（如
FastAPI、Django、Node.js
等），并将后端服务器的响应返回给客户端。<strong>它的核心作用是隐藏后端服务器的真实地址，优化请求处理流程，并增强安全性</strong>。</p>
<p>反向代理是现代 Web
架构中不可或缺的组件，尤其在前后端分离、微服务、高并发场景下作用显著。通过
Nginx 等工具实现反向代理，可以： - 提升安全性（隐藏后端、过滤攻击）。 -
优化性能（负载均衡、缓存静态资源）。 - 简化运维（集中管理
SSL、日志）。</p>
<p>对于 FastAPI 项目，推荐在生产环境中使用 Nginx
作为反向代理，以充分发挥其高性能和灵活性优势。</p>
<h3 id="二级域名是什么">二级域名是什么？</h3>
<p><strong>二级域名（Second-Level Domain, SLD）</strong>
是域名系统（DNS）中的一个层级，通常位于顶级域名（TLD）之下，主域名（一级域名）之上。它是域名结构中的关键部分，用于标识网站或服务的主体。</p>
<p><strong>域名层级结构</strong></p>
<p>域名由多个层级组成，从右向左层级递增，具体如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mail.example.com</span><br><span class="line">|     |        |</span><br><span class="line">|     |        └── 顶级域名（TLD）：com/net/org</span><br><span class="line">|     └────────── 二级域名（SLD）：example</span><br><span class="line">└──────────────── 子域名（Subdomain）：mail</span><br></pre></td></tr></table></figure>
<p><strong>1. 顶级域名（TLD）</strong></p>
<ul>
<li><strong>定义</strong>：域名的最后一部分，表示域名的类别或国家/地区。</li>
<li><strong>示例</strong>：<code>.com</code>（商业）、<code>.org</code>（非营利组织）、<code>.net</code>（网络服务）、<code>.cn</code>（中国）、<code>.jp</code>（日本）。</li>
</ul>
<p><strong>2. 二级域名（SLD）</strong></p>
<ul>
<li><strong>定义</strong>：位于 TLD
之下的域名部分，是域名的主体，通常由用户注册并拥有。</li>
<li><strong>示例</strong>：在 <code>example.com</code>
中，<code>example</code> 是二级域名。</li>
</ul>
<p><strong>3. 子域名（Subdomain）</strong></p>
<ul>
<li><strong>定义</strong>：在二级域名前添加的前缀，用于进一步细分网站或服务。</li>
<li><strong>示例</strong>：在 <code>mail.example.com</code>
中，<code>mail</code> 是子域名。</li>
</ul>
<p><strong>二级域名的常见用途</strong></p>
<ol type="1">
<li><strong>品牌标识</strong>：<br>
二级域名是品牌的核心标识，如
<code>google.com</code>、<code>apple.com</code>。</li>
<li><strong>服务划分</strong>：<br>
通过子域名区分不同服务，例如：
<ul>
<li><code>mail.google.com</code>：邮件服务</li>
<li><code>drive.google.com</code>：云存储服务</li>
<li><code>maps.google.com</code>：地图服务</li>
</ul></li>
<li><strong>多语言或地区支持</strong>：<br>
通过二级域名提供本地化内容，例如：
<ul>
<li><code>fr.wikipedia.org</code>（法语版）</li>
<li><code>zh.wikipedia.org</code>（中文版）</li>
</ul></li>
</ol>
<h3 id="dom元素">DOM元素</h3>
<p><strong>DOM（Document Object Model，文档对象模型）</strong>
是浏览器将 HTML 或 XML 文档解析为树状结构的编程接口。<strong>DOM
元素</strong> 是构成这棵树的节点（如
<code>&lt;div&gt;</code>、<code>&lt;p&gt;</code>、<code>&lt;button&gt;</code>
等），它们不仅是页面内容的载体，更是实现动态交互的核心工具。</p>
<h3 id="开发日志">开发日志</h3>
<h4 id="api">api</h4>
<p>获取apihttps://home.openweathermap.org/api_keys</p>
<p>api文档<a target="_blank" rel="noopener" href="https://openweathermap.org/api">Weather API -
OpenWeatherMap</a></p>
<h4 id="版本1.0">版本1.0</h4>
<figure>
<img src="/2025/06/01/%E5%AD%A6%E4%B9%A0/weatherweb%E5%BC%80%E5%8F%91%E6%97%A5%E5%BF%97/weatherweb%E5%BC%80%E5%8F%91%E6%97%A5%E5%BF%97/image-20250602191448931.png" alt="image-20250602191448931">
<figcaption aria-hidden="true">image-20250602191448931</figcaption>
</figure>
<p>完成基本天气功能的开发</p>
<h4 id="版本2.0">版本2.0</h4>
<figure>
<img src="/2025/06/01/%E5%AD%A6%E4%B9%A0/weatherweb%E5%BC%80%E5%8F%91%E6%97%A5%E5%BF%97/weatherweb%E5%BC%80%E5%8F%91%E6%97%A5%E5%BF%97/image-20250604095743649.png" alt="image-20250604095743649">
<figcaption aria-hidden="true">image-20250604095743649</figcaption>
</figure>
<p>完成ai建议功能</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/05/21/diary/%E8%84%9A%E8%B8%8F%E5%AE%9E%E5%9C%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/05/21/diary/%E8%84%9A%E8%B8%8F%E5%AE%9E%E5%9C%B0/" class="post-title-link" itemprop="url">脚踏实地</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2025-05-21 00:00:00 / 修改时间：20:23:38" itemprop="dateCreated datePublished" datetime="2025-05-21T00:00:00+08:00">2025-05-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E9%9A%8F%E7%AC%94/" itemprop="url" rel="index"><span itemprop="name">随笔</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="脚踏实地">脚踏实地</h3>
<p>没有一夜暴富的美梦，天下掉馅饼的事情只有可能是诱惑，在得到某些好处前先想一想你配不配。</p>
<p>杜绝心浮气躁，用双手制造财富。.</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/05/14/%E5%AD%A6%E4%B9%A0/kaggle/titanic/%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E7%AC%AC%E4%B8%89%E8%8A%82%E6%8E%A2%E7%B4%A2%E6%80%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E8%AF%BE%E7%A8%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/05/14/%E5%AD%A6%E4%B9%A0/kaggle/titanic/%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E7%AC%AC%E4%B8%89%E8%8A%82%E6%8E%A2%E7%B4%A2%E6%80%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E8%AF%BE%E7%A8%8B/" class="post-title-link" itemprop="url">数据分析——第一章：第三节探索性数据分析</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-05-14 00:00:00" itemprop="dateCreated datePublished" datetime="2025-05-14T00:00:00+08:00">2025-05-14</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-06-11 17:11:02" itemprop="dateModified" datetime="2025-06-11T17:11:02+08:00">2025-06-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/kaggle/" itemprop="url" rel="index"><span itemprop="name">kaggle</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/kaggle/titanic/" itemprop="url" rel="index"><span itemprop="name">titanic</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><strong>复习：</strong>在前面我们已经学习了Pandas基础，知道利用Pandas读取csv数据的增删查改，今天我们要学习的就是<strong>探索性数据分析</strong>，主要介绍如何利用Pandas进行排序、算术计算以及计算描述函数describe()的使用。</p>
<h1 id="第一章探索性数据分析">1 第一章：探索性数据分析</h1>
<h4 id="开始之前导入numpypandas包和数据">开始之前，导入numpy、pandas包和数据</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#加载所需的库</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#载入之前保存的train_chinese.csv数据，关于泰坦尼克号的任务，我们就使用这个数据</span></span><br><span class="line">train_chinese = pd.read_csv(<span class="string">&#x27;./titanic/train_chinese.csv&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="了解你的数据吗">1.6 了解你的数据吗？</h3>
<p>教材《Python for Data Analysis》第五章</p>
<h4 id="任务一利用pandas对示例数据进行排序要求升序">1.6.1
任务一：利用Pandas对示例数据进行排序，要求升序</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 具体请看《利用Python进行数据分析》第五章 排序和排名 部分</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#自己构建一个都为数字的DataFrame数据</span></span><br><span class="line">frame = pd.DataFrame(np.arange(<span class="number">8</span>).reshape((<span class="number">2</span>, <span class="number">4</span>)), </span><br><span class="line">                     index=[<span class="string">&#x27;2&#x27;</span>, <span class="string">&#x27;1&#x27;</span>], </span><br><span class="line">                     columns=[<span class="string">&#x27;d&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>])</span><br><span class="line">frame</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
d
</th>
<th>
a
</th>
<th>
b
</th>
<th>
c
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
2
</th>
<td>
0
</td>
<td>
1
</td>
<td>
2
</td>
<td>
3
</td>
</tr>
<tr>
<th>
1
</th>
<td>
4
</td>
<td>
5
</td>
<td>
6
</td>
<td>
7
</td>
</tr>
</tbody>
</table>
<p>【代码解析】</p>
<p>pd.DataFrame() ：创建一个DataFrame对象</p>
<p>np.arange(8).reshape((2, 4)) :
生成一个二维数组（2*4）,第一列：0，1，2，3 第二列：4，5，6，7</p>
<p>index=[’2, 1] ：DataFrame 对象的索引列</p>
<p>columns=[‘d’, ‘a’, ‘b’, ‘c’] ：DataFrame 对象的索引行</p>
<p>【问题】：大多数时候我们都是想根据列的值来排序,所以将你构建的DataFrame中的数据根据某一列，升序排列</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#回答代码</span></span><br><span class="line"><span class="comment">#指定按列名 &#x27;b&#x27; 的值进行排序，ascending=False设置降序排列（默认是升序）</span></span><br><span class="line">frame.sort_values(by=<span class="string">&#x27;b&#x27;</span>,ascending=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
d
</th>
<th>
a
</th>
<th>
b
</th>
<th>
c
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
1
</th>
<td>
4
</td>
<td>
5
</td>
<td>
6
</td>
<td>
7
</td>
</tr>
<tr>
<th>
2
</th>
<td>
0
</td>
<td>
1
</td>
<td>
2
</td>
<td>
3
</td>
</tr>
</tbody>
</table>
<p>【思考】通过书本你能说出Pandas对DataFrame数据的其他排序方式吗？</p>
<p>【总结】下面将不同的排序方式做一个总结</p>
<p>1.让行索引升序排序</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#代码</span></span><br><span class="line">frame.sort_index(ascending=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
d
</th>
<th>
a
</th>
<th>
b
</th>
<th>
c
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
1
</th>
<td>
4
</td>
<td>
5
</td>
<td>
6
</td>
<td>
7
</td>
</tr>
<tr>
<th>
2
</th>
<td>
0
</td>
<td>
1
</td>
<td>
2
</td>
<td>
3
</td>
</tr>
</tbody>
</table>
<p>2.让列索引升序排序</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#代码</span></span><br><span class="line"><span class="comment">#axis=1指定对 列索引（columns） 进行排序（默认 axis=0 是对行索引排序）。</span></span><br><span class="line">frame.sort_index(axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
a
</th>
<th>
b
</th>
<th>
c
</th>
<th>
d
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
2
</th>
<td>
1
</td>
<td>
2
</td>
<td>
3
</td>
<td>
0
</td>
</tr>
<tr>
<th>
1
</th>
<td>
5
</td>
<td>
6
</td>
<td>
7
</td>
<td>
4
</td>
</tr>
</tbody>
</table>
<p>3.让列索引降序排序</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#代码</span></span><br><span class="line">frame.sort_index(axis=<span class="number">1</span>, ascending=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
d
</th>
<th>
c
</th>
<th>
b
</th>
<th>
a
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
2
</th>
<td>
0
</td>
<td>
3
</td>
<td>
2
</td>
<td>
1
</td>
</tr>
<tr>
<th>
1
</th>
<td>
4
</td>
<td>
7
</td>
<td>
6
</td>
<td>
5
</td>
</tr>
</tbody>
</table>
<p>4.让任选两列数据同时降序排序</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#代码</span></span><br><span class="line">frame.sort_values(by=[<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;b&#x27;</span>],ascending=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
d
</th>
<th>
a
</th>
<th>
b
</th>
<th>
c
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
1
</th>
<td>
4
</td>
<td>
5
</td>
<td>
6
</td>
<td>
7
</td>
</tr>
<tr>
<th>
2
</th>
<td>
0
</td>
<td>
1
</td>
<td>
2
</td>
<td>
3
</td>
</tr>
</tbody>
</table>
<h4 id="任务二对泰坦尼克号数据trian.csv按票价和年龄两列进行综合排序降序排列从这个数据中你可以分析出什么">1.6.2
任务二：对泰坦尼克号数据（trian.csv）按票价和年龄两列进行综合排序（降序排列），从这个数据中你可以分析出什么？</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">在开始我们已经导入了train_chinese.csv数据，而且前面我们也学习了导入数据过程，根据上面学习，我们直接对目标列进行排序即可</span></span><br><span class="line"><span class="string">head(20) : 读取前20条数据</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">train_chinese.sort_values(by=[<span class="string">&#x27;票价&#x27;</span>,<span class="string">&#x27;年龄&#x27;</span>], ascending=<span class="literal">False</span>).head(<span class="number">20</span>)</span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
乘客ID
</th>
<th>
是否幸存
</th>
<th>
仓位等级
</th>
<th>
姓名
</th>
<th>
性别
</th>
<th>
年龄
</th>
<th>
兄弟姐妹个数
</th>
<th>
父母子女个数
</th>
<th>
船票信息
</th>
<th>
票价
</th>
<th>
客舱
</th>
<th>
登船港口
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
679
</th>
<td>
680
</td>
<td>
1
</td>
<td>
1
</td>
<td>
Cardeza, Mr. Thomas Drake Martinez
</td>
<td>
male
</td>
<td>
36.0
</td>
<td>
0
</td>
<td>
1
</td>
<td>
PC 17755
</td>
<td>
512.3292
</td>
<td>
B51 B53 B55
</td>
<td>
C
</td>
</tr>
<tr>
<th>
258
</th>
<td>
259
</td>
<td>
1
</td>
<td>
1
</td>
<td>
Ward, Miss. Anna
</td>
<td>
female
</td>
<td>
35.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
PC 17755
</td>
<td>
512.3292
</td>
<td>
NaN
</td>
<td>
C
</td>
</tr>
<tr>
<th>
737
</th>
<td>
738
</td>
<td>
1
</td>
<td>
1
</td>
<td>
Lesurer, Mr. Gustave J
</td>
<td>
male
</td>
<td>
35.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
PC 17755
</td>
<td>
512.3292
</td>
<td>
B101
</td>
<td>
C
</td>
</tr>
<tr>
<th>
438
</th>
<td>
439
</td>
<td>
0
</td>
<td>
1
</td>
<td>
Fortune, Mr. Mark
</td>
<td>
male
</td>
<td>
64.0
</td>
<td>
1
</td>
<td>
4
</td>
<td>
19950
</td>
<td>
263.0000
</td>
<td>
C23 C25 C27
</td>
<td>
S
</td>
</tr>
<tr>
<th>
341
</th>
<td>
342
</td>
<td>
1
</td>
<td>
1
</td>
<td>
Fortune, Miss. Alice Elizabeth
</td>
<td>
female
</td>
<td>
24.0
</td>
<td>
3
</td>
<td>
2
</td>
<td>
19950
</td>
<td>
263.0000
</td>
<td>
C23 C25 C27
</td>
<td>
S
</td>
</tr>
<tr>
<th>
88
</th>
<td>
89
</td>
<td>
1
</td>
<td>
1
</td>
<td>
Fortune, Miss. Mabel Helen
</td>
<td>
female
</td>
<td>
23.0
</td>
<td>
3
</td>
<td>
2
</td>
<td>
19950
</td>
<td>
263.0000
</td>
<td>
C23 C25 C27
</td>
<td>
S
</td>
</tr>
<tr>
<th>
27
</th>
<td>
28
</td>
<td>
0
</td>
<td>
1
</td>
<td>
Fortune, Mr. Charles Alexander
</td>
<td>
male
</td>
<td>
19.0
</td>
<td>
3
</td>
<td>
2
</td>
<td>
19950
</td>
<td>
263.0000
</td>
<td>
C23 C25 C27
</td>
<td>
S
</td>
</tr>
<tr>
<th>
742
</th>
<td>
743
</td>
<td>
1
</td>
<td>
1
</td>
<td>
Ryerson, Miss. Susan Parker “Suzette”
</td>
<td>
female
</td>
<td>
21.0
</td>
<td>
2
</td>
<td>
2
</td>
<td>
PC 17608
</td>
<td>
262.3750
</td>
<td>
B57 B59 B63 B66
</td>
<td>
C
</td>
</tr>
<tr>
<th>
311
</th>
<td>
312
</td>
<td>
1
</td>
<td>
1
</td>
<td>
Ryerson, Miss. Emily Borie
</td>
<td>
female
</td>
<td>
18.0
</td>
<td>
2
</td>
<td>
2
</td>
<td>
PC 17608
</td>
<td>
262.3750
</td>
<td>
B57 B59 B63 B66
</td>
<td>
C
</td>
</tr>
<tr>
<th>
299
</th>
<td>
300
</td>
<td>
1
</td>
<td>
1
</td>
<td>
Baxter, Mrs. James (Helene DeLaudeniere Chaput)
</td>
<td>
female
</td>
<td>
50.0
</td>
<td>
0
</td>
<td>
1
</td>
<td>
PC 17558
</td>
<td>
247.5208
</td>
<td>
B58 B60
</td>
<td>
C
</td>
</tr>
<tr>
<th>
118
</th>
<td>
119
</td>
<td>
0
</td>
<td>
1
</td>
<td>
Baxter, Mr. Quigg Edmond
</td>
<td>
male
</td>
<td>
24.0
</td>
<td>
0
</td>
<td>
1
</td>
<td>
PC 17558
</td>
<td>
247.5208
</td>
<td>
B58 B60
</td>
<td>
C
</td>
</tr>
<tr>
<th>
380
</th>
<td>
381
</td>
<td>
1
</td>
<td>
1
</td>
<td>
Bidois, Miss. Rosalie
</td>
<td>
female
</td>
<td>
42.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
PC 17757
</td>
<td>
227.5250
</td>
<td>
NaN
</td>
<td>
C
</td>
</tr>
<tr>
<th>
716
</th>
<td>
717
</td>
<td>
1
</td>
<td>
1
</td>
<td>
Endres, Miss. Caroline Louise
</td>
<td>
female
</td>
<td>
38.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
PC 17757
</td>
<td>
227.5250
</td>
<td>
C45
</td>
<td>
C
</td>
</tr>
<tr>
<th>
700
</th>
<td>
701
</td>
<td>
1
</td>
<td>
1
</td>
<td>
Astor, Mrs. John Jacob (Madeleine Talmadge Force)
</td>
<td>
female
</td>
<td>
18.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
PC 17757
</td>
<td>
227.5250
</td>
<td>
C62 C64
</td>
<td>
C
</td>
</tr>
<tr>
<th>
557
</th>
<td>
558
</td>
<td>
0
</td>
<td>
1
</td>
<td>
Robbins, Mr. Victor
</td>
<td>
male
</td>
<td>
NaN
</td>
<td>
0
</td>
<td>
0
</td>
<td>
PC 17757
</td>
<td>
227.5250
</td>
<td>
NaN
</td>
<td>
C
</td>
</tr>
<tr>
<th>
527
</th>
<td>
528
</td>
<td>
0
</td>
<td>
1
</td>
<td>
Farthing, Mr. John
</td>
<td>
male
</td>
<td>
NaN
</td>
<td>
0
</td>
<td>
0
</td>
<td>
PC 17483
</td>
<td>
221.7792
</td>
<td>
C95
</td>
<td>
S
</td>
</tr>
<tr>
<th>
377
</th>
<td>
378
</td>
<td>
0
</td>
<td>
1
</td>
<td>
Widener, Mr. Harry Elkins
</td>
<td>
male
</td>
<td>
27.0
</td>
<td>
0
</td>
<td>
2
</td>
<td>
113503
</td>
<td>
211.5000
</td>
<td>
C82
</td>
<td>
C
</td>
</tr>
<tr>
<th>
779
</th>
<td>
780
</td>
<td>
1
</td>
<td>
1
</td>
<td>
Robert, Mrs. Edward Scott (Elisabeth Walton Mc…
</td>
<td>
female
</td>
<td>
43.0
</td>
<td>
0
</td>
<td>
1
</td>
<td>
24160
</td>
<td>
211.3375
</td>
<td>
B3
</td>
<td>
S
</td>
</tr>
<tr>
<th>
730
</th>
<td>
731
</td>
<td>
1
</td>
<td>
1
</td>
<td>
Allen, Miss. Elisabeth Walton
</td>
<td>
female
</td>
<td>
29.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
24160
</td>
<td>
211.3375
</td>
<td>
B5
</td>
<td>
S
</td>
</tr>
<tr>
<th>
689
</th>
<td>
690
</td>
<td>
1
</td>
<td>
1
</td>
<td>
Madill, Miss. Georgette Alexandra
</td>
<td>
female
</td>
<td>
15.0
</td>
<td>
0
</td>
<td>
1
</td>
<td>
24160
</td>
<td>
211.3375
</td>
<td>
B5
</td>
<td>
S
</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#代码</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>【思考】排序后，如果我们仅仅关注年龄和票价两列。根据常识我知道发现票价越高的应该客舱越好，所以我们会明显看出，票价前20的乘客中存活的有14人，这是相当高的一个比例，那么我们后面是不是可以进一步分析一下票价和存活之间的关系，年龄和存活之间的关系呢？当你开始发现数据之间的关系了，数据分析就开始了。</p>
<p>当然，这只是我的想法，你还可以有更多想法，欢迎写在你的学习笔记中。</p>
<p><strong>多做几个数据的排序</strong></p>
<h4 id="任务三利用pandas进行算术计算计算两个dataframe数据相加结果">1.6.3
任务三：利用Pandas进行算术计算，计算两个DataFrame数据相加结果</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 具体请看《利用Python进行数据分析》第五章 算术运算与数据对齐 部分</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#自己构建两个都为数字的DataFrame数据</span></span><br><span class="line"></span><br><span class="line">frame1_a = pd.DataFrame(np.arange(<span class="number">9.</span>).reshape(<span class="number">3</span>, <span class="number">3</span>),</span><br><span class="line">                     columns=[<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>],</span><br><span class="line">                     index=[<span class="string">&#x27;one&#x27;</span>, <span class="string">&#x27;two&#x27;</span>, <span class="string">&#x27;three&#x27;</span>])</span><br><span class="line">frame1_b = pd.DataFrame(np.arange(<span class="number">12.</span>).reshape(<span class="number">4</span>, <span class="number">3</span>),</span><br><span class="line">                     columns=[<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;e&#x27;</span>, <span class="string">&#x27;c&#x27;</span>],</span><br><span class="line">                     index=[<span class="string">&#x27;first&#x27;</span>, <span class="string">&#x27;one&#x27;</span>, <span class="string">&#x27;two&#x27;</span>, <span class="string">&#x27;second&#x27;</span>])</span><br><span class="line">frame1_a, frame1_b</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<pre><code>(         a    b    c
 one    0.0  1.0  2.0
 two    3.0  4.0  5.0
 three  6.0  7.0  8.0,
           a     e     c
 first   0.0   1.0   2.0
 one     3.0   4.0   5.0
 two     6.0   7.0   8.0
 second  9.0  10.0  11.0)</code></pre>
<p>将frame_a和frame_b进行相加</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#代码</span></span><br><span class="line">frame1_a+frame1_b</span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
a
</th>
<th>
b
</th>
<th>
c
</th>
<th>
e
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
first
</th>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
</tr>
<tr>
<th>
one
</th>
<td>
3.0
</td>
<td>
NaN
</td>
<td>
7.0
</td>
<td>
NaN
</td>
</tr>
<tr>
<th>
second
</th>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
</tr>
<tr>
<th>
three
</th>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
<td>
NaN
</td>
</tr>
<tr>
<th>
two
</th>
<td>
9.0
</td>
<td>
NaN
</td>
<td>
13.0
</td>
<td>
NaN
</td>
</tr>
</tbody>
</table>
<p>【提醒】两个DataFrame相加后，会返回一个新的DataFrame，对应的行和列的值会相加，没有对应的会变成空值NaN。<br>
当然，DataFrame还有很多算术运算，如减法，除法等，有兴趣的同学可以看《利用Python进行数据分析》第五章
算术运算与数据对齐 部分，多在网络上查找相关学习资料。</p>
<h4 id="任务四通过泰坦尼克号数据如何计算出在船上最大的家族有多少人">1.6.4
任务四：通过泰坦尼克号数据如何计算出在船上最大的家族有多少人？</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">还是用之前导入的chinese_train.csv如果我们想看看在船上，最大的家族有多少人（‘兄弟姐妹个数’+‘父母子女个数’），我们该怎么做呢？</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="built_in">max</span>(train_chinese[<span class="string">&#x27;兄弟姐妹个数&#x27;</span>] + train_chinese[<span class="string">&#x27;父母子女个数&#x27;</span>])</span><br></pre></td></tr></table></figure>
<pre><code>10</code></pre>
<p>【提醒】我们只需找出”兄弟姐妹个数“和”父母子女个数“之和最大的数，当然你还可以想出很多方法和思考角度，欢迎你来说出你的看法。</p>
<p><strong>多做几个数据的相加，看看你能分析出什么？</strong></p>
<h4 id="任务五学会使用pandas-describe函数查看数据基本统计信息">1.6.5
任务五：学会使用Pandas describe()函数查看数据基本统计信息</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#(1) 关键知识点示例做一遍（简单数据）</span></span><br><span class="line"><span class="comment"># 具体请看《利用Python进行数据分析》第五章 汇总和计算描述统计 部分</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#自己构建一个有数字有空值的DataFrame数据</span></span><br><span class="line"></span><br><span class="line">frame2 = pd.DataFrame([[<span class="number">1.4</span>, np.nan], </span><br><span class="line">                       [<span class="number">7.1</span>, -<span class="number">4.5</span>],</span><br><span class="line">                       [np.nan, np.nan], </span><br><span class="line">                       [<span class="number">0.75</span>, -<span class="number">1.3</span>]</span><br><span class="line">                      ], index=[<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;d&#x27;</span>], columns=[<span class="string">&#x27;one&#x27;</span>, <span class="string">&#x27;two&#x27;</span>])</span><br><span class="line">frame2</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
one
</th>
<th>
two
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
a
</th>
<td>
1.40
</td>
<td>
NaN
</td>
</tr>
<tr>
<th>
b
</th>
<td>
7.10
</td>
<td>
-4.5
</td>
</tr>
<tr>
<th>
c
</th>
<td>
NaN
</td>
<td>
NaN
</td>
</tr>
<tr>
<th>
d
</th>
<td>
0.75
</td>
<td>
-1.3
</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#代码</span></span><br><span class="line">frame2.describe()  <span class="comment">#描述统计</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">count : 样本数据大小</span></span><br><span class="line"><span class="string">mean : 样本数据的平均值</span></span><br><span class="line"><span class="string">std : 样本数据的标准差</span></span><br><span class="line"><span class="string">min : 样本数据的最小值</span></span><br><span class="line"><span class="string">25% : 样本数据25%的时候的值</span></span><br><span class="line"><span class="string">50% : 样本数据50%的时候的值</span></span><br><span class="line"><span class="string">75% : 样本数据75%的时候的值</span></span><br><span class="line"><span class="string">max : 样本数据的最大值</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
one
</th>
<th>
two
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
count
</th>
<td>
3.000000
</td>
<td>
2.000000
</td>
</tr>
<tr>
<th>
mean
</th>
<td>
3.083333
</td>
<td>
-2.900000
</td>
</tr>
<tr>
<th>
std
</th>
<td>
3.493685
</td>
<td>
2.262742
</td>
</tr>
<tr>
<th>
min
</th>
<td>
0.750000
</td>
<td>
-4.500000
</td>
</tr>
<tr>
<th>
25%
</th>
<td>
1.075000
</td>
<td>
-3.700000
</td>
</tr>
<tr>
<th>
50%
</th>
<td>
1.400000
</td>
<td>
-2.900000
</td>
</tr>
<tr>
<th>
75%
</th>
<td>
4.250000
</td>
<td>
-2.100000
</td>
</tr>
<tr>
<th>
max
</th>
<td>
7.100000
</td>
<td>
-1.300000
</td>
</tr>
</tbody>
</table>

<p>调用 describe 函数，观察frame2的数据基本信息</p>
<h4 id="任务六分别看看泰坦尼克号数据集中-票价父母子女-这列数据的基本统计数据你能发现什么">1.6.6
任务六：分别看看泰坦尼克号数据集中 票价、父母子女
这列数据的基本统计数据，你能发现什么？</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">看看泰坦尼克号数据集中 票价 这列数据的基本统计数据</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<pre><code>&#39;\n看看泰坦尼克号数据集中 票价 这列数据的基本统计数据\n&#39;</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#代码</span></span><br><span class="line">train_chinese[<span class="string">&#x27;票价&#x27;</span>].describe()</span><br></pre></td></tr></table></figure>
<pre><code>count    891.000000
mean      32.204208
std       49.693429
min        0.000000
25%        7.910400
50%       14.454200
75%       31.000000
max      512.329200
Name: 票价, dtype: float64</code></pre>
<p>【思考】从上面数据我们可以看出， 一共有891个票价数据，
平均值约为：32.20， 标准差约为49.69，说明票价波动特别大，
25%的人的票价是低于7.91的，50%的人的票价低于14.45，75%的人的票价低于31.00，
票价最大值约为512.33，最小值为0。
当然，答案只是我的想法，你还可以有更多想法，欢迎写在你的学习笔记中。</p>
<p><strong>多做几个组数据的统计，看看你能分析出什么？</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 写下你的其他分析</span></span><br><span class="line">train_chinese[<span class="string">&#x27;父母子女个数&#x27;</span>].describe()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<pre><code>count    891.000000
mean       0.381594
std        0.806057
min        0.000000
25%        0.000000
50%        0.000000
75%        0.000000
max        6.000000
Name: 父母子女个数, dtype: float64</code></pre>
<p>【思考】有更多想法，欢迎写在你的学习笔记中。</p>
<p>【总结】本节中我们通过Pandas的一些内置函数对数据进行了初步统计查看，这个过程最重要的不是大家得掌握这些函数，而是看懂从这些函数出来的数据，构建自己的数据分析思维，这也是第一章最重要的点，希望大家学完第一章能对数据有个基本认识，了解自己在做什么，为什么这么做，后面的章节我们将开始对数据进行清洗，进一步分析。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/05/14/%E5%AD%A6%E4%B9%A0/kaggle/titanic/%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E7%AC%AC%E4%B8%80%E8%8A%82%E6%95%B0%E6%8D%AE%E8%BD%BD%E5%85%A5%E5%8F%8A%E5%88%9D%E6%AD%A5%E8%A7%82%E5%AF%9F-%E8%AF%BE%E7%A8%8B-checkpoint/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/05/14/%E5%AD%A6%E4%B9%A0/kaggle/titanic/%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E7%AC%AC%E4%B8%80%E8%8A%82%E6%95%B0%E6%8D%AE%E8%BD%BD%E5%85%A5%E5%8F%8A%E5%88%9D%E6%AD%A5%E8%A7%82%E5%AF%9F-%E8%AF%BE%E7%A8%8B-checkpoint/" class="post-title-link" itemprop="url">数据分析——第一章：第一节数据载入及初步观察</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2025-05-14 00:00:00 / 修改时间：16:56:27" itemprop="dateCreated datePublished" datetime="2025-05-14T00:00:00+08:00">2025-05-14</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/kaggle/" itemprop="url" rel="index"><span itemprop="name">kaggle</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/kaggle/titanic/" itemprop="url" rel="index"><span itemprop="name">titanic</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><strong>复习</strong>:这门课程得主要目的是通过真实的数据，以实战的方式了解数据分析的流程和熟悉数据分析python的基本操作。知道了课程的目的之后，我们接下来我们要正式的开始数据分析的实战教学，完成kaggle上<a target="_blank" rel="noopener" href="https://www.kaggle.com/c/titanic/overview">泰坦尼克的任务</a>，实战数据分析全流程。
这里有两份资料： 教材《Python for Data Analysis》和 baidu.com &amp;
google.com（善用搜索引擎）</p>
<h2 id="第一章数据载入及初步观察">1 第一章：数据载入及初步观察</h2>
<h3 id="载入数据">1.1 载入数据</h3>
<p>数据集下载 https://www.kaggle.com/c/titanic/overview</p>
<h4 id="任务一导入numpy和pandas">1.1.1 任务一：导入numpy和pandas</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>【提示】如果加载失败，学会如何在你的python环境下安装numpy和pandas这两个库</p>
<h4 id="任务二载入数据">1.1.2 任务二：载入数据</h4>
<ol type="1">
<li>使用相对路径载入数据<br>
</li>
<li>使用绝对路径载入数据</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.getcwd()</span><br><span class="line">test=pd.read_csv(<span class="string">&#x27;./titanic/test.csv&#x27;</span>)</span><br><span class="line">train=pd.read_csv(<span class="string">&#x27;./titanic/train.csv&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line">abs_path_test=os.path.abspath(<span class="string">&#x27;./titanic/test.csv&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(abs_path_test)</span><br><span class="line">test=pd.read_csv(abs_path_test)</span><br><span class="line">abs_path_train=os.path.abspath(<span class="string">&#x27;./titanic/train.csv&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(abs_path_train)</span><br><span class="line">train=pd.read_csv(abs_path_train)</span><br><span class="line">train.head()</span><br></pre></td></tr></table></figure>
<pre><code>/workspace/WuTeachingAI/hands-on-data-analysis/myself/titanic/test.csv
/workspace/WuTeachingAI/hands-on-data-analysis/myself/titanic/train.csv</code></pre>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
PassengerId
</th>
<th>
Survived
</th>
<th>
Pclass
</th>
<th>
Name
</th>
<th>
Sex
</th>
<th>
Age
</th>
<th>
SibSp
</th>
<th>
Parch
</th>
<th>
Ticket
</th>
<th>
Fare
</th>
<th>
Cabin
</th>
<th>
Embarked
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
1
</td>
<td>
0
</td>
<td>
3
</td>
<td>
Braund, Mr. Owen Harris
</td>
<td>
male
</td>
<td>
22.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
A/5 21171
</td>
<td>
7.2500
</td>
<td>
NaN
</td>
<td>
S
</td>
</tr>
<tr>
<th>
1
</th>
<td>
2
</td>
<td>
1
</td>
<td>
1
</td>
<td>
Cumings, Mrs. John Bradley (Florence Briggs Th…
</td>
<td>
female
</td>
<td>
38.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
PC 17599
</td>
<td>
71.2833
</td>
<td>
C85
</td>
<td>
C
</td>
</tr>
<tr>
<th>
2
</th>
<td>
3
</td>
<td>
1
</td>
<td>
3
</td>
<td>
Heikkinen, Miss. Laina
</td>
<td>
female
</td>
<td>
26.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
STON/O2. 3101282
</td>
<td>
7.9250
</td>
<td>
NaN
</td>
<td>
S
</td>
</tr>
<tr>
<th>
3
</th>
<td>
4
</td>
<td>
1
</td>
<td>
1
</td>
<td>
Futrelle, Mrs. Jacques Heath (Lily May Peel)
</td>
<td>
female
</td>
<td>
35.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
113803
</td>
<td>
53.1000
</td>
<td>
C123
</td>
<td>
S
</td>
</tr>
<tr>
<th>
4
</th>
<td>
5
</td>
<td>
0
</td>
<td>
3
</td>
<td>
Allen, Mr. William Henry
</td>
<td>
male
</td>
<td>
35.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
373450
</td>
<td>
8.0500
</td>
<td>
NaN
</td>
<td>
S
</td>
</tr>
</tbody>
</table>
<p>【提示】相对路径载入报错时，尝试使用os.getcwd()查看当前工作目录。<br>
【思考】知道数据加载的方法后，试试pd.read_csv()和pd.read_table()的不同，如果想让他们效果一样，需要怎么做？了解一下’.tsv’和’.csv’的不同，如何加载这两个数据集？<br>
【总结】加载的数据是所有工作的第一步，我们的工作会接触到不同的数据格式（eg:.csv;.tsv;.xlsx）,但是加载的方法和思路都是一样的，在以后工作和做项目的过程中，遇到之前没有碰到的问题，要多多查资料吗，使用googel，了解业务逻辑，明白输入和输出是什么。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pd.read_csv() 和 pd.read_table() 本质上非常相似，主要区别在于默认的分隔符参数。</span></span><br><span class="line"><span class="comment"># 通过显式设置 `sep` 参数，可以让它们处理各种以不同字符分隔的文本文件。</span></span><br><span class="line"><span class="comment"># &#x27;.csv&#x27; 文件用逗号分隔，&#x27;.tsv&#x27; 文件用制表符分隔。选择合适的pandas读取函数或正确设置`sep`参数即可加载。</span></span><br></pre></td></tr></table></figure>
<h4 id="任务三每1000行为一个数据模块逐块读取">1.1.3
任务三：每1000行为一个数据模块，逐块读取</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line">chunker=pd.read_csv(<span class="string">&#x27;./titanic/train.csv&#x27;</span>,chunksize=<span class="number">1000</span>)</span><br><span class="line"><span class="keyword">for</span> chunk <span class="keyword">in</span> chunker:</span><br><span class="line">    <span class="built_in">print</span>(chunk)</span><br></pre></td></tr></table></figure>
<pre><code>     PassengerId  Survived  Pclass  \
0              1         0       3   
1              2         1       1   
2              3         1       3   
3              4         1       1   
4              5         0       3   
..           ...       ...     ...   
886          887         0       2   
887          888         1       1   
888          889         0       3   
889          890         1       1   
890          891         0       3   

                                                  Name     Sex   Age  SibSp  \
0                              Braund, Mr. Owen Harris    male  22.0      1   
1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   
2                               Heikkinen, Miss. Laina  female  26.0      0   
3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   
4                             Allen, Mr. William Henry    male  35.0      0   
..                                                 ...     ...   ...    ...   
886                              Montvila, Rev. Juozas    male  27.0      0   
887                       Graham, Miss. Margaret Edith  female  19.0      0   
888           Johnston, Miss. Catherine Helen &quot;Carrie&quot;  female   NaN      1   
889                              Behr, Mr. Karl Howell    male  26.0      0   
890                                Dooley, Mr. Patrick    male  32.0      0   

     Parch            Ticket     Fare Cabin Embarked  
0        0         A/5 21171   7.2500   NaN        S  
1        0          PC 17599  71.2833   C85        C  
2        0  STON/O2. 3101282   7.9250   NaN        S  
3        0            113803  53.1000  C123        S  
4        0            373450   8.0500   NaN        S  
..     ...               ...      ...   ...      ...  
886      0            211536  13.0000   NaN        S  
887      0            112053  30.0000   B42        S  
888      2        W./C. 6607  23.4500   NaN        S  
889      0            111369  30.0000  C148        C  
890      0            370376   7.7500   NaN        Q  

[891 rows x 12 columns]</code></pre>
<p>【思考】什么是逐块读取？为什么要逐块读取呢？</p>
<p>【提示】大家可以chunker(数据块)是什么类型？用<code>for</code>循环打印出来出处具体的样子是什么？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># **什么是逐块读取？**</span></span><br><span class="line"><span class="comment"># 逐块读取（Chunking）是指在读取大型数据集时，不一次性将整个文件加载到内存中，而是将文件分成若干个小的数据块（chunks），每次只加载和处理一个数据块。</span></span><br><span class="line"><span class="comment"># 在pandas中，可以通过在 `pd.read_csv()` 或类似的读取函数中设置 `chunksize` 参数来实现逐块读取。`chunksize` 定义了每个数据块包含的行数。</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># **为什么要逐块读取呢？**</span></span><br><span class="line"><span class="comment"># 1.  **处理内存不足的大文件**：</span></span><br><span class="line"><span class="comment">#     *   当数据集非常大，其大小超过了计算机可用内存时，一次性加载整个文件会导致内存溢出错误（MemoryError）。逐块读取允许我们分批处理数据，每次只在内存中保留一小部分数据，从而有效避免内存问题。</span></span><br><span class="line"><span class="comment"># 2.  **提高处理效率（特定场景下）**：</span></span><br><span class="line"><span class="comment">#     *   对于某些类型的操作，例如对数据进行迭代处理、过滤或聚合，如果不需要同时访问所有数据，逐块处理可以使得程序更快地开始处理数据，而不是等待整个大文件加载完毕。</span></span><br><span class="line"><span class="comment">#     *   可以边读取边处理，实现流式数据处理的效果。</span></span><br><span class="line"><span class="comment"># 3.  **数据清洗和预处理**：</span></span><br><span class="line"><span class="comment">#     *   在对大型原始数据进行初步的清洗、转换或特征工程时，可以逐块进行，将处理后的数据块追加到新的存储中，或者在每个块上计算统计量并逐步累积。</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="任务四将表头改成中文索引改为乘客id-对于某些英文资料我们可以通过翻译来更直观的熟悉我们的数据">1.1.4
任务四：将表头改成中文，索引改为乘客ID
[对于某些英文资料，我们可以通过翻译来更直观的熟悉我们的数据]</h4>
<p>PassengerId =&gt; 乘客ID<br>
Survived =&gt; 是否幸存<br>
Pclass =&gt; 乘客等级(1/2/3等舱位)<br>
Name =&gt; 乘客姓名<br>
Sex =&gt; 性别<br>
Age =&gt; 年龄<br>
SibSp =&gt; 堂兄弟/妹个数<br>
Parch =&gt; 父母与小孩个数<br>
Ticket =&gt; 船票信息<br>
Fare =&gt; 票价<br>
Cabin =&gt; 客舱<br>
Embarked =&gt; 登船港口</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line"><span class="comment">#将&quot;乘客ID&quot;列作为行索引</span></span><br><span class="line">df=pd.read_csv(<span class="string">&#x27;./titanic/train.csv&#x27;</span>,names=[<span class="string">&#x27;乘客ID&#x27;</span>,<span class="string">&#x27;是否幸存&#x27;</span>,<span class="string">&#x27;仓位等级&#x27;</span>,<span class="string">&#x27;姓名&#x27;</span>,<span class="string">&#x27;性别&#x27;</span>,<span class="string">&#x27;年龄&#x27;</span>,<span class="string">&#x27;兄弟姐妹个数&#x27;</span>,<span class="string">&#x27;父母子女个数&#x27;</span>,<span class="string">&#x27;船票信息&#x27;</span>,<span class="string">&#x27;票价&#x27;</span>,<span class="string">&#x27;客舱&#x27;</span>,<span class="string">&#x27;登船港口&#x27;</span>],index_col=<span class="string">&#x27;乘客ID&#x27;</span>,header=<span class="number">0</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
是否幸存
</th>
<th>
仓位等级
</th>
<th>
姓名
</th>
<th>
性别
</th>
<th>
年龄
</th>
<th>
兄弟姐妹个数
</th>
<th>
父母子女个数
</th>
<th>
船票信息
</th>
<th>
票价
</th>
<th>
客舱
</th>
<th>
登船港口
</th>
</tr>
<tr>
<th>
乘客ID
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
1
</th>
<td>
0
</td>
<td>
3
</td>
<td>
Braund, Mr. Owen Harris
</td>
<td>
male
</td>
<td>
22.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
A/5 21171
</td>
<td>
7.2500
</td>
<td>
NaN
</td>
<td>
S
</td>
</tr>
<tr>
<th>
2
</th>
<td>
1
</td>
<td>
1
</td>
<td>
Cumings, Mrs. John Bradley (Florence Briggs Th…
</td>
<td>
female
</td>
<td>
38.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
PC 17599
</td>
<td>
71.2833
</td>
<td>
C85
</td>
<td>
C
</td>
</tr>
<tr>
<th>
3
</th>
<td>
1
</td>
<td>
3
</td>
<td>
Heikkinen, Miss. Laina
</td>
<td>
female
</td>
<td>
26.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
STON/O2. 3101282
</td>
<td>
7.9250
</td>
<td>
NaN
</td>
<td>
S
</td>
</tr>
<tr>
<th>
4
</th>
<td>
1
</td>
<td>
1
</td>
<td>
Futrelle, Mrs. Jacques Heath (Lily May Peel)
</td>
<td>
female
</td>
<td>
35.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
113803
</td>
<td>
53.1000
</td>
<td>
C123
</td>
<td>
S
</td>
</tr>
<tr>
<th>
5
</th>
<td>
0
</td>
<td>
3
</td>
<td>
Allen, Mr. William Henry
</td>
<td>
male
</td>
<td>
35.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
373450
</td>
<td>
8.0500
</td>
<td>
NaN
</td>
<td>
S
</td>
</tr>
</tbody>
</table>
<p>【思考】所谓将表头改为中文其中一个思路是：将英文列名表头替换成中文。还有其他的方法吗？</p>
<h3 id="初步观察">1.2 初步观察</h3>
<p>导入数据后，你可能要对数据的整体结构和样例进行概览，比如说，数据大小、有多少列，各列都是什么格式的，是否包含null等</p>
<h4 id="任务一查看数据的基本信息">1.2.1 任务一：查看数据的基本信息</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line">df.info()</span><br><span class="line">df.describe()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
Index: 891 entries, 1 to 891
Data columns (total 11 columns):
 #   Column  Non-Null Count  Dtype  
---  ------  --------------  -----  
 0   是否幸存    891 non-null    int64  
 1   仓位等级    891 non-null    int64  
 2   姓名      891 non-null    object 
 3   性别      891 non-null    object 
 4   年龄      714 non-null    float64
 5   兄弟姐妹个数  891 non-null    int64  
 6   父母子女个数  891 non-null    int64  
 7   船票信息    891 non-null    object 
 8   票价      891 non-null    float64
 9   客舱      204 non-null    object 
 10  登船港口    889 non-null    object 
dtypes: float64(2), int64(4), object(5)
memory usage: 83.5+ KB</code></pre>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
是否幸存
</th>
<th>
仓位等级
</th>
<th>
年龄
</th>
<th>
兄弟姐妹个数
</th>
<th>
父母子女个数
</th>
<th>
票价
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
count
</th>
<td>
891.000000
</td>
<td>
891.000000
</td>
<td>
714.000000
</td>
<td>
891.000000
</td>
<td>
891.000000
</td>
<td>
891.000000
</td>
</tr>
<tr>
<th>
mean
</th>
<td>
0.383838
</td>
<td>
2.308642
</td>
<td>
29.699118
</td>
<td>
0.523008
</td>
<td>
0.381594
</td>
<td>
32.204208
</td>
</tr>
<tr>
<th>
std
</th>
<td>
0.486592
</td>
<td>
0.836071
</td>
<td>
14.526497
</td>
<td>
1.102743
</td>
<td>
0.806057
</td>
<td>
49.693429
</td>
</tr>
<tr>
<th>
min
</th>
<td>
0.000000
</td>
<td>
1.000000
</td>
<td>
0.420000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
</tr>
<tr>
<th>
25%
</th>
<td>
0.000000
</td>
<td>
2.000000
</td>
<td>
20.125000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
7.910400
</td>
</tr>
<tr>
<th>
50%
</th>
<td>
0.000000
</td>
<td>
3.000000
</td>
<td>
28.000000
</td>
<td>
0.000000
</td>
<td>
0.000000
</td>
<td>
14.454200
</td>
</tr>
<tr>
<th>
75%
</th>
<td>
1.000000
</td>
<td>
3.000000
</td>
<td>
38.000000
</td>
<td>
1.000000
</td>
<td>
0.000000
</td>
<td>
31.000000
</td>
</tr>
<tr>
<th>
max
</th>
<td>
1.000000
</td>
<td>
3.000000
</td>
<td>
80.000000
</td>
<td>
8.000000
</td>
<td>
6.000000
</td>
<td>
512.329200
</td>
</tr>
</tbody>
</table>
<p>【提示】有多个函数可以这样做，你可以做一下总结</p>
<h4 id="任务二观察表格前10行的数据和后15行的数据">1.2.2
任务二：观察表格前10行的数据和后15行的数据</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line">df.head(<span class="number">15</span>)</span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
是否幸存
</th>
<th>
仓位等级
</th>
<th>
姓名
</th>
<th>
性别
</th>
<th>
年龄
</th>
<th>
兄弟姐妹个数
</th>
<th>
父母子女个数
</th>
<th>
船票信息
</th>
<th>
票价
</th>
<th>
客舱
</th>
<th>
登船港口
</th>
</tr>
<tr>
<th>
乘客ID
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
1
</th>
<td>
0
</td>
<td>
3
</td>
<td>
Braund, Mr. Owen Harris
</td>
<td>
male
</td>
<td>
22.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
A/5 21171
</td>
<td>
7.2500
</td>
<td>
NaN
</td>
<td>
S
</td>
</tr>
<tr>
<th>
2
</th>
<td>
1
</td>
<td>
1
</td>
<td>
Cumings, Mrs. John Bradley (Florence Briggs Th…
</td>
<td>
female
</td>
<td>
38.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
PC 17599
</td>
<td>
71.2833
</td>
<td>
C85
</td>
<td>
C
</td>
</tr>
<tr>
<th>
3
</th>
<td>
1
</td>
<td>
3
</td>
<td>
Heikkinen, Miss. Laina
</td>
<td>
female
</td>
<td>
26.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
STON/O2. 3101282
</td>
<td>
7.9250
</td>
<td>
NaN
</td>
<td>
S
</td>
</tr>
<tr>
<th>
4
</th>
<td>
1
</td>
<td>
1
</td>
<td>
Futrelle, Mrs. Jacques Heath (Lily May Peel)
</td>
<td>
female
</td>
<td>
35.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
113803
</td>
<td>
53.1000
</td>
<td>
C123
</td>
<td>
S
</td>
</tr>
<tr>
<th>
5
</th>
<td>
0
</td>
<td>
3
</td>
<td>
Allen, Mr. William Henry
</td>
<td>
male
</td>
<td>
35.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
373450
</td>
<td>
8.0500
</td>
<td>
NaN
</td>
<td>
S
</td>
</tr>
<tr>
<th>
6
</th>
<td>
0
</td>
<td>
3
</td>
<td>
Moran, Mr. James
</td>
<td>
male
</td>
<td>
NaN
</td>
<td>
0
</td>
<td>
0
</td>
<td>
330877
</td>
<td>
8.4583
</td>
<td>
NaN
</td>
<td>
Q
</td>
</tr>
<tr>
<th>
7
</th>
<td>
0
</td>
<td>
1
</td>
<td>
McCarthy, Mr. Timothy J
</td>
<td>
male
</td>
<td>
54.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
17463
</td>
<td>
51.8625
</td>
<td>
E46
</td>
<td>
S
</td>
</tr>
<tr>
<th>
8
</th>
<td>
0
</td>
<td>
3
</td>
<td>
Palsson, Master. Gosta Leonard
</td>
<td>
male
</td>
<td>
2.0
</td>
<td>
3
</td>
<td>
1
</td>
<td>
349909
</td>
<td>
21.0750
</td>
<td>
NaN
</td>
<td>
S
</td>
</tr>
<tr>
<th>
9
</th>
<td>
1
</td>
<td>
3
</td>
<td>
Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)
</td>
<td>
female
</td>
<td>
27.0
</td>
<td>
0
</td>
<td>
2
</td>
<td>
347742
</td>
<td>
11.1333
</td>
<td>
NaN
</td>
<td>
S
</td>
</tr>
<tr>
<th>
10
</th>
<td>
1
</td>
<td>
2
</td>
<td>
Nasser, Mrs. Nicholas (Adele Achem)
</td>
<td>
female
</td>
<td>
14.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
237736
</td>
<td>
30.0708
</td>
<td>
NaN
</td>
<td>
C
</td>
</tr>
<tr>
<th>
11
</th>
<td>
1
</td>
<td>
3
</td>
<td>
Sandstrom, Miss. Marguerite Rut
</td>
<td>
female
</td>
<td>
4.0
</td>
<td>
1
</td>
<td>
1
</td>
<td>
PP 9549
</td>
<td>
16.7000
</td>
<td>
G6
</td>
<td>
S
</td>
</tr>
<tr>
<th>
12
</th>
<td>
1
</td>
<td>
1
</td>
<td>
Bonnell, Miss. Elizabeth
</td>
<td>
female
</td>
<td>
58.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
113783
</td>
<td>
26.5500
</td>
<td>
C103
</td>
<td>
S
</td>
</tr>
<tr>
<th>
13
</th>
<td>
0
</td>
<td>
3
</td>
<td>
Saundercock, Mr. William Henry
</td>
<td>
male
</td>
<td>
20.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
A/5. 2151
</td>
<td>
8.0500
</td>
<td>
NaN
</td>
<td>
S
</td>
</tr>
<tr>
<th>
14
</th>
<td>
0
</td>
<td>
3
</td>
<td>
Andersson, Mr. Anders Johan
</td>
<td>
male
</td>
<td>
39.0
</td>
<td>
1
</td>
<td>
5
</td>
<td>
347082
</td>
<td>
31.2750
</td>
<td>
NaN
</td>
<td>
S
</td>
</tr>
<tr>
<th>
15
</th>
<td>
0
</td>
<td>
3
</td>
<td>
Vestrom, Miss. Hulda Amanda Adolfina
</td>
<td>
female
</td>
<td>
14.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
350406
</td>
<td>
7.8542
</td>
<td>
NaN
</td>
<td>
S
</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line">df.tail()</span><br></pre></td></tr></table></figure>
<h4 id="任务三判断数据是否为空为空的地方返回true其余地方返回false">1.2.4
任务三：判断数据是否为空，为空的地方返回True，其余地方返回False</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line">df.isnull().head()</span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
是否幸存
</th>
<th>
仓位等级
</th>
<th>
姓名
</th>
<th>
性别
</th>
<th>
年龄
</th>
<th>
兄弟姐妹个数
</th>
<th>
父母子女个数
</th>
<th>
船票信息
</th>
<th>
票价
</th>
<th>
客舱
</th>
<th>
登船港口
</th>
</tr>
<tr>
<th>
乘客ID
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
<th>
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
1
</th>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
<td>
False
</td>
</tr>
<tr>
<th>
2
</th>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
</tr>
<tr>
<th>
3
</th>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
<td>
False
</td>
</tr>
<tr>
<th>
4
</th>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
</tr>
<tr>
<th>
5
</th>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
<td>
False
</td>
</tr>
</tbody>
</table>
<p>【总结】上面的操作都是数据分析中对于数据本身的观察</p>
<p>【思考】对于一个数据，还可以从哪些方面来观察？找找答案，这个将对下面的数据分析有很大的帮助</p>
<h3 id="保存数据">1.3 保存数据</h3>
<h4 id="任务一将你加载并做出改变的数据在工作目录下保存为一个新文件train_chinese.csv">1.3.1
任务一：将你加载并做出改变的数据，在工作目录下保存为一个新文件train_chinese.csv</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line"><span class="comment"># 注意：不同的操作系统保存下来可能会有乱码。大家可以加入`encoding=&#x27;GBK&#x27; 或者 ’encoding = ’utf-8‘‘`</span></span><br><span class="line">df.to_csv(<span class="string">&#x27;./titanic/train_chinese.csv&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>【总结】数据的加载以及入门，接下来就要接触数据本身的运算，我们将主要掌握numpy和pandas在工作和项目场景的运用。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/05/14/%E5%AD%A6%E4%B9%A0/kaggle/titanic/%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E7%AC%AC%E4%BA%8C%E8%8A%82pandas%E5%9F%BA%E7%A1%80-%E8%AF%BE%E7%A8%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/05/14/%E5%AD%A6%E4%B9%A0/kaggle/titanic/%E7%AC%AC%E4%B8%80%E7%AB%A0%EF%BC%9A%E7%AC%AC%E4%BA%8C%E8%8A%82pandas%E5%9F%BA%E7%A1%80-%E8%AF%BE%E7%A8%8B/" class="post-title-link" itemprop="url">数据分析——第一章：第一节数据载入及初步观察</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2025-05-14 00:00:00 / 修改时间：16:57:42" itemprop="dateCreated datePublished" datetime="2025-05-14T00:00:00+08:00">2025-05-14</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/kaggle/" itemprop="url" rel="index"><span itemprop="name">kaggle</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/kaggle/titanic/" itemprop="url" rel="index"><span itemprop="name">titanic</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><strong>复习：</strong>数据分析的第一步，加载数据我们已经学习完毕了。当数据展现在我们面前的时候，我们所要做的第一步就是认识他，今天我们要学习的就是<strong>了解字段含义以及初步观察数据</strong>。</p>
<h2 id="第一章数据载入及初步观察">1 第一章：数据载入及初步观察</h2>
<h3 id="知道你的数据叫什么">1.4 知道你的数据叫什么</h3>
<p>我们学习pandas的基础操作，那么上一节通过pandas加载之后的数据，其数据类型是什么呢？</p>
<p><strong>开始前导入numpy和pandas</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure>
<h4 id="任务一pandas中有两个数据类型dateframe和series通过查找简单了解他们然后自己写一个关于这两个数据类型的小例子开放题">1.4.1
任务一：pandas中有两个数据类型DateFrame和Series，通过查找简单了解他们。然后自己写一个关于这两个数据类型的小例子🌰[开放题]</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#我们举的例子</span></span><br><span class="line">sdata = &#123;<span class="string">&#x27;Ohio&#x27;</span>: <span class="number">35000</span>, <span class="string">&#x27;Texas&#x27;</span>: <span class="number">71000</span>, <span class="string">&#x27;Oregon&#x27;</span>: <span class="number">16000</span>, <span class="string">&#x27;Utah&#x27;</span>: <span class="number">5000</span>&#125;</span><br><span class="line">example_1 = pd.Series(sdata)</span><br><span class="line">example_1</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<pre><code>Ohio      35000
Texas     71000
Oregon    16000
Utah       5000
dtype: int64</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#我们举的例子</span></span><br><span class="line">data = &#123;<span class="string">&#x27;state&#x27;</span>: [<span class="string">&#x27;Ohio&#x27;</span>, <span class="string">&#x27;Ohio&#x27;</span>, <span class="string">&#x27;Ohio&#x27;</span>, <span class="string">&#x27;Nevada&#x27;</span>, <span class="string">&#x27;Nevada&#x27;</span>, <span class="string">&#x27;Nevada&#x27;</span>],</span><br><span class="line">        <span class="string">&#x27;year&#x27;</span>: [<span class="number">2000</span>, <span class="number">2001</span>, <span class="number">2002</span>, <span class="number">2001</span>, <span class="number">2002</span>, <span class="number">2003</span>],<span class="string">&#x27;pop&#x27;</span>: [<span class="number">1.5</span>, <span class="number">1.7</span>, <span class="number">3.6</span>, <span class="number">2.4</span>, <span class="number">2.9</span>, <span class="number">3.2</span>]&#125;</span><br><span class="line">example_2 = pd.DataFrame(data)</span><br><span class="line">example_2</span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
state
</th>
<th>
year
</th>
<th>
pop
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
Ohio
</td>
<td>
2000
</td>
<td>
1.5
</td>
</tr>
<tr>
<th>
1
</th>
<td>
Ohio
</td>
<td>
2001
</td>
<td>
1.7
</td>
</tr>
<tr>
<th>
2
</th>
<td>
Ohio
</td>
<td>
2002
</td>
<td>
3.6
</td>
</tr>
<tr>
<th>
3
</th>
<td>
Nevada
</td>
<td>
2001
</td>
<td>
2.4
</td>
</tr>
<tr>
<th>
4
</th>
<td>
Nevada
</td>
<td>
2002
</td>
<td>
2.9
</td>
</tr>
<tr>
<th>
5
</th>
<td>
Nevada
</td>
<td>
2003
</td>
<td>
3.2
</td>
</tr>
</tbody>
</table>
<h4 id="任务二根据上节课的方法载入train.csv文件">1.4.2
任务二：根据上节课的方法载入”train.csv”文件</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line">df=pd.read_csv(<span class="string">&#x27;./titanic/train.csv&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>也可以加载上一节课保存的”train_chinese.csv”文件。通过翻译版train_chinese.csv熟悉了这个数据集，然后我们对trian.csv来进行操作
#### 1.4.3 任务三：查看DataFrame数据的每列的名称</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line">df.columns</span><br></pre></td></tr></table></figure>
<pre><code>Index([&#39;PassengerId&#39;, &#39;Survived&#39;, &#39;Pclass&#39;, &#39;Name&#39;, &#39;Sex&#39;, &#39;Age&#39;, &#39;SibSp&#39;,
       &#39;Parch&#39;, &#39;Ticket&#39;, &#39;Fare&#39;, &#39;Cabin&#39;, &#39;Embarked&#39;],
      dtype=&#39;object&#39;)</code></pre>
<h4 id="任务四查看cabin这列的所有值有多种方法">1.4.4任务四：查看”Cabin”这列的所有值[有多种方法]</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line">df[<span class="string">&#x27;Cabin&#x27;</span>].head(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<pre><code>0     NaN
1     C85
2     NaN
3    C123
4     NaN
5     NaN
6     E46
7     NaN
8     NaN
9     NaN
Name: Cabin, dtype: object</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line">df.Cabin.values[:<span class="number">10</span>]</span><br><span class="line">df.Cabin.head(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<pre><code>0     NaN
1     C85
2     NaN
3    C123
4     NaN
5     NaN
6     E46
7     NaN
8     NaN
9     NaN
Name: Cabin, dtype: object</code></pre>
<h4 id="任务五加载文件test_1.csv然后对比train.csv看看有哪些多出的列然后将多出的列删除">1.4.5
任务五：加载文件”test_1.csv”，然后对比”train.csv”，看看有哪些多出的列，然后将多出的列删除</h4>
<p>经过我们的观察发现一个测试集test_1.csv有一列是多余的，我们需要将这个多余的列删去</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line">test_1=pd.read_csv(<span class="string">&#x27;../第一单元项目集合/test_1.csv&#x27;</span>)</span><br><span class="line">test_1.head()</span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
Unnamed: 0
</th>
<th>
PassengerId
</th>
<th>
Survived
</th>
<th>
Pclass
</th>
<th>
Name
</th>
<th>
Sex
</th>
<th>
Age
</th>
<th>
SibSp
</th>
<th>
Parch
</th>
<th>
Ticket
</th>
<th>
Fare
</th>
<th>
Cabin
</th>
<th>
Embarked
</th>
<th>
a
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
3
</td>
<td>
Braund, Mr. Owen Harris
</td>
<td>
male
</td>
<td>
22.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
A/5 21171
</td>
<td>
7.2500
</td>
<td>
NaN
</td>
<td>
S
</td>
<td>
100
</td>
</tr>
<tr>
<th>
1
</th>
<td>
1
</td>
<td>
2
</td>
<td>
1
</td>
<td>
1
</td>
<td>
Cumings, Mrs. John Bradley (Florence Briggs Th…
</td>
<td>
female
</td>
<td>
38.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
PC 17599
</td>
<td>
71.2833
</td>
<td>
C85
</td>
<td>
C
</td>
<td>
100
</td>
</tr>
<tr>
<th>
2
</th>
<td>
2
</td>
<td>
3
</td>
<td>
1
</td>
<td>
3
</td>
<td>
Heikkinen, Miss. Laina
</td>
<td>
female
</td>
<td>
26.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
STON/O2. 3101282
</td>
<td>
7.9250
</td>
<td>
NaN
</td>
<td>
S
</td>
<td>
100
</td>
</tr>
<tr>
<th>
3
</th>
<td>
3
</td>
<td>
4
</td>
<td>
1
</td>
<td>
1
</td>
<td>
Futrelle, Mrs. Jacques Heath (Lily May Peel)
</td>
<td>
female
</td>
<td>
35.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
113803
</td>
<td>
53.1000
</td>
<td>
C123
</td>
<td>
S
</td>
<td>
100
</td>
</tr>
<tr>
<th>
4
</th>
<td>
4
</td>
<td>
5
</td>
<td>
0
</td>
<td>
3
</td>
<td>
Allen, Mr. William Henry
</td>
<td>
male
</td>
<td>
35.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
373450
</td>
<td>
8.0500
</td>
<td>
NaN
</td>
<td>
S
</td>
<td>
100
</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line"><span class="comment">#test_1.columns</span></span><br><span class="line">test_1.drop(<span class="string">&#x27;Unnamed: 0&#x27;</span>,axis=<span class="number">1</span>,inplace=<span class="literal">True</span>)</span><br><span class="line">test_1.columns</span><br></pre></td></tr></table></figure>
<pre><code>Index([&#39;PassengerId&#39;, &#39;Survived&#39;, &#39;Pclass&#39;, &#39;Name&#39;, &#39;Sex&#39;, &#39;Age&#39;, &#39;SibSp&#39;,
       &#39;Parch&#39;, &#39;Ticket&#39;, &#39;Fare&#39;, &#39;Cabin&#39;, &#39;Embarked&#39;, &#39;a&#39;],
      dtype=&#39;object&#39;)</code></pre>
<p>【思考】还有其他的删除多余的列的方式吗？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 思考回答</span></span><br><span class="line"><span class="keyword">del</span> test_1[<span class="string">&#x27;Unnamed: 0&#x27;</span>]</span><br><span class="line">test_1.columns</span><br></pre></td></tr></table></figure>
<pre><code>Index([&#39;PassengerId&#39;, &#39;Survived&#39;, &#39;Pclass&#39;, &#39;Name&#39;, &#39;Sex&#39;, &#39;Age&#39;, &#39;SibSp&#39;,
       &#39;Parch&#39;, &#39;Ticket&#39;, &#39;Fare&#39;, &#39;Cabin&#39;, &#39;Embarked&#39;, &#39;a&#39;],
      dtype=&#39;object&#39;)</code></pre>
<h4 id="任务六-将passengeridnameageticket这几个列元素隐藏只观察其他几个列元素">1.4.6
任务六：
将[‘PassengerId’,‘Name’,‘Age’,‘Ticket’]这几个列元素隐藏，只观察其他几个列元素</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line">df.drop([<span class="string">&#x27;PassengerId&#x27;</span>, <span class="string">&#x27;Name&#x27;</span>, <span class="string">&#x27;Age&#x27;</span>,<span class="string">&#x27;Ticket&#x27;</span>], axis=<span class="number">1</span>).head(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
Survived
</th>
<th>
Pclass
</th>
<th>
Sex
</th>
<th>
SibSp
</th>
<th>
Parch
</th>
<th>
Fare
</th>
<th>
Cabin
</th>
<th>
Embarked
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
0
</td>
<td>
3
</td>
<td>
male
</td>
<td>
1
</td>
<td>
0
</td>
<td>
7.2500
</td>
<td>
NaN
</td>
<td>
S
</td>
</tr>
<tr>
<th>
1
</th>
<td>
1
</td>
<td>
1
</td>
<td>
female
</td>
<td>
1
</td>
<td>
0
</td>
<td>
71.2833
</td>
<td>
C85
</td>
<td>
C
</td>
</tr>
<tr>
<th>
2
</th>
<td>
1
</td>
<td>
3
</td>
<td>
female
</td>
<td>
0
</td>
<td>
0
</td>
<td>
7.9250
</td>
<td>
NaN
</td>
<td>
S
</td>
</tr>
<tr>
<th>
3
</th>
<td>
1
</td>
<td>
1
</td>
<td>
female
</td>
<td>
1
</td>
<td>
0
</td>
<td>
53.1000
</td>
<td>
C123
</td>
<td>
S
</td>
</tr>
<tr>
<th>
4
</th>
<td>
0
</td>
<td>
3
</td>
<td>
male
</td>
<td>
0
</td>
<td>
0
</td>
<td>
8.0500
</td>
<td>
NaN
</td>
<td>
S
</td>
</tr>
<tr>
<th>
5
</th>
<td>
0
</td>
<td>
3
</td>
<td>
male
</td>
<td>
0
</td>
<td>
0
</td>
<td>
8.4583
</td>
<td>
NaN
</td>
<td>
Q
</td>
</tr>
<tr>
<th>
6
</th>
<td>
0
</td>
<td>
1
</td>
<td>
male
</td>
<td>
0
</td>
<td>
0
</td>
<td>
51.8625
</td>
<td>
E46
</td>
<td>
S
</td>
</tr>
<tr>
<th>
7
</th>
<td>
0
</td>
<td>
3
</td>
<td>
male
</td>
<td>
3
</td>
<td>
1
</td>
<td>
21.0750
</td>
<td>
NaN
</td>
<td>
S
</td>
</tr>
<tr>
<th>
8
</th>
<td>
1
</td>
<td>
3
</td>
<td>
female
</td>
<td>
0
</td>
<td>
2
</td>
<td>
11.1333
</td>
<td>
NaN
</td>
<td>
S
</td>
</tr>
<tr>
<th>
9
</th>
<td>
1
</td>
<td>
2
</td>
<td>
female
</td>
<td>
1
</td>
<td>
0
</td>
<td>
30.0708
</td>
<td>
NaN
</td>
<td>
C
</td>
</tr>
</tbody>
</table>
<p>【思考】对比任务五和任务六，是不是使用了不一样的方法(函数)，如果使用一样的函数如何完成上面的不同的要求呢？</p>
<p>【思考回答】</p>
<p>如果想要完全的删除你的数据结构，使用inplace=True，因为使用inplace就将原数据覆盖了，所以这里没有用</p>
<h3 id="筛选的逻辑">1.5 筛选的逻辑</h3>
<p>表格数据中，最重要的一个功能就是要具有可筛选的能力，选出我所需要的信息，丢弃无用的信息。</p>
<p>下面我们还是用实战来学习pandas这个功能。</p>
<h4 id="任务一-我们以age为筛选条件显示年龄在10岁以下的乘客信息">1.5.1
任务一： 我们以”Age”为筛选条件，显示年龄在10岁以下的乘客信息。</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line"><span class="built_in">print</span>((df[<span class="string">&#x27;Age&#x27;</span>]&lt;<span class="number">10</span>).head(<span class="number">2</span>))</span><br><span class="line">df[df[<span class="string">&#x27;Age&#x27;</span>]&lt;<span class="number">10</span>].head(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<pre><code>0    False
1    False
Name: Age, dtype: bool</code></pre>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
PassengerId
</th>
<th>
Survived
</th>
<th>
Pclass
</th>
<th>
Name
</th>
<th>
Sex
</th>
<th>
Age
</th>
<th>
SibSp
</th>
<th>
Parch
</th>
<th>
Ticket
</th>
<th>
Fare
</th>
<th>
Cabin
</th>
<th>
Embarked
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
7
</th>
<td>
8
</td>
<td>
0
</td>
<td>
3
</td>
<td>
Palsson, Master. Gosta Leonard
</td>
<td>
male
</td>
<td>
2.0
</td>
<td>
3
</td>
<td>
1
</td>
<td>
349909
</td>
<td>
21.075
</td>
<td>
NaN
</td>
<td>
S
</td>
</tr>
<tr>
<th>
10
</th>
<td>
11
</td>
<td>
1
</td>
<td>
3
</td>
<td>
Sandstrom, Miss. Marguerite Rut
</td>
<td>
female
</td>
<td>
4.0
</td>
<td>
1
</td>
<td>
1
</td>
<td>
PP 9549
</td>
<td>
16.700
</td>
<td>
G6
</td>
<td>
S
</td>
</tr>
</tbody>
</table>
<h4 id="任务二-以age为条件将年龄在10岁以上和50岁以下的乘客信息显示出来并将这个数据命名为midage">1.5.2
任务二：
以”Age”为条件，将年龄在10岁以上和50岁以下的乘客信息显示出来，并将这个数据命名为midage</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line">midage=df[(df[<span class="string">&#x27;Age&#x27;</span>]&gt;<span class="number">10</span>) &amp; (df[<span class="string">&#x27;Age&#x27;</span>]&lt;<span class="number">50</span>)]</span><br><span class="line">midage</span><br></pre></td></tr></table></figure>
<p>【提示】了解pandas的条件筛选方式以及如何使用交集和并集操作</p>
<h4 id="任务三将midage的数据中第100行的pclass和sex的数据显示出来">1.5.3
任务三：将midage的数据中第100行的”Pclass”和”Sex”的数据显示出来</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line">midage = midage.reset_index(drop=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(midage)</span><br><span class="line">midage.loc[<span class="number">1</span>][[<span class="string">&#x27;Pclass&#x27;</span>, <span class="string">&#x27;Sex&#x27;</span>]]</span><br></pre></td></tr></table></figure>
<pre><code>     PassengerId  Survived  Pclass  \
0              1         0       3   
1              2         1       1   
2              3         1       3   
3              4         1       1   
4              5         0       3   
..           ...       ...     ...   
571          886         0       3   
572          887         0       2   
573          888         1       1   
574          890         1       1   
575          891         0       3   

                                                  Name     Sex   Age  SibSp  \
0                              Braund, Mr. Owen Harris    male  22.0      1   
1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   
2                               Heikkinen, Miss. Laina  female  26.0      0   
3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   
4                             Allen, Mr. William Henry    male  35.0      0   
..                                                 ...     ...   ...    ...   
571               Rice, Mrs. William (Margaret Norton)  female  39.0      0   
572                              Montvila, Rev. Juozas    male  27.0      0   
573                       Graham, Miss. Margaret Edith  female  19.0      0   
574                              Behr, Mr. Karl Howell    male  26.0      0   
575                                Dooley, Mr. Patrick    male  32.0      0   

     Parch            Ticket     Fare Cabin Embarked  
0        0         A/5 21171   7.2500   NaN        S  
1        0          PC 17599  71.2833   C85        C  
2        0  STON/O2. 3101282   7.9250   NaN        S  
3        0            113803  53.1000  C123        S  
4        0            373450   8.0500   NaN        S  
..     ...               ...      ...   ...      ...  
571      5            382652  29.1250   NaN        Q  
572      0            211536  13.0000   NaN        S  
573      0            112053  30.0000   B42        S  
574      0            111369  30.0000  C148        C  
575      0            370376   7.7500   NaN        Q  

[576 rows x 12 columns]



Pclass         1
Sex       female
Name: 1, dtype: object</code></pre>
<p>【提示】在抽取数据中，我们希望数据的相对顺序保持不变，用什么函数可以达到这个效果呢？</p>
<h4 id="任务四使用loc方法将midage的数据中第100105108行的pclassname和sex的数据显示出来">1.5.4
任务四：使用loc方法将midage的数据中第100，105，108行的”Pclass”，“Name”和”Sex”的数据显示出来</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line">midage.loc[[<span class="number">100</span>,<span class="number">105</span>,<span class="number">108</span>],[<span class="string">&#x27;Pclass&#x27;</span>,<span class="string">&#x27;Name&#x27;</span>,<span class="string">&#x27;Sex&#x27;</span>]] </span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
Pclass
</th>
<th>
Name
</th>
<th>
Sex
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
100
</th>
<td>
2
</td>
<td>
Byles, Rev. Thomas Roussel Davids
</td>
<td>
male
</td>
</tr>
<tr>
<th>
105
</th>
<td>
3
</td>
<td>
Cribb, Mr. John Hatfield
</td>
<td>
male
</td>
</tr>
<tr>
<th>
108
</th>
<td>
3
</td>
<td>
Calic, Mr. Jovo
</td>
<td>
male
</td>
</tr>
</tbody>
</table>
<h4 id="任务五使用iloc方法将midage的数据中第100105108行的pclassname和sex的数据显示出来">1.5.5
任务五：使用iloc方法将midage的数据中第100，105，108行的”Pclass”，“Name”和”Sex”的数据显示出来</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line">midage.iloc[[<span class="number">100</span>,<span class="number">105</span>,<span class="number">108</span>],[<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]] </span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
Pclass
</th>
<th>
Name
</th>
<th>
Sex
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
149
</th>
<td>
2
</td>
<td>
Byles, Rev. Thomas Roussel Davids
</td>
<td>
male
</td>
</tr>
<tr>
<th>
160
</th>
<td>
3
</td>
<td>
Cribb, Mr. John Hatfield
</td>
<td>
male
</td>
</tr>
<tr>
<th>
163
</th>
<td>
3
</td>
<td>
Calic, Mr. Jovo
</td>
<td>
male
</td>
</tr>
</tbody>
</table>
<p>【思考】对比<code>iloc</code>和<code>loc</code>的异同</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># *   当你需要根据**标签名称**（如行索引名或列名）来选取数据时，使用 `loc`。这使得代码更具可读性，因为你可以直接看到你正在操作的标签。</span></span><br><span class="line"><span class="comment"># *   当你需要根据**整数位置**来选取数据时（不关心标签名称，或者标签不是整数），使用 `iloc`。这在处理没有有意义标签的 DataFrame，或者需要进行与位置相关的操作时很有用。</span></span><br><span class="line"><span class="comment"># *   **注意**：如果 DataFrame 的索引是默认的整数索引 (0, 1, 2, ...)，那么 `loc` 和 `iloc` 在使用单个整数或整数切片进行行选择时，行为可能会相似，但这可能会导致混淆。</span></span><br><span class="line"><span class="comment">#     *   例如，如果 `df.index` 是 `[0, 1, 2, 5, 6]`：</span></span><br><span class="line"><span class="comment">#         *   `df.loc[0]` 会选择索引标签为 `0` 的行。</span></span><br><span class="line"><span class="comment">#         *   `df.iloc[0]` 也会选择第一行（即索引标签为 `0` 的行）。</span></span><br><span class="line"><span class="comment">#         *   `df.loc[3]` 会报错，因为没有索引标签为 `3`。</span></span><br><span class="line"><span class="comment">#         *   `df.iloc[3]` 会选择第四行（即索引标签为 `5` 的行）。</span></span><br><span class="line"><span class="comment"># *   为了避免混淆，最佳实践是：当你知道你正在使用标签时，明确使用 `loc`；当你知道你正在使用整数位置时，明确使用 `iloc`。</span></span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/05/14/%E5%AD%A6%E4%B9%A0/kaggle/titanic/%E7%AC%AC%E4%B8%89%E7%AB%A0%E6%A8%A1%E5%9E%8B%E5%BB%BA%E7%AB%8B%E5%92%8C%E8%AF%84%E4%BC%B0---%E8%AF%84%E4%BB%B7-%E8%AF%BE%E7%A8%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/05/14/%E5%AD%A6%E4%B9%A0/kaggle/titanic/%E7%AC%AC%E4%B8%89%E7%AB%A0%E6%A8%A1%E5%9E%8B%E5%BB%BA%E7%AB%8B%E5%92%8C%E8%AF%84%E4%BC%B0---%E8%AF%84%E4%BB%B7-%E8%AF%BE%E7%A8%8B/" class="post-title-link" itemprop="url">数据分析——第三章模型建立和评估---评价</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-05-14 00:00:00" itemprop="dateCreated datePublished" datetime="2025-05-14T00:00:00+08:00">2025-05-14</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-06-12 18:46:07" itemprop="dateModified" datetime="2025-06-12T18:46:07+08:00">2025-06-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/kaggle/" itemprop="url" rel="index"><span itemprop="name">kaggle</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/kaggle/titanic/" itemprop="url" rel="index"><span itemprop="name">titanic</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="第三章-模型搭建和评估-评估">第三章 模型搭建和评估-评估</h2>
<p>根据之前的模型的建模，我们知道如何运用sklearn这个库来完成建模，以及我们知道了的数据集的划分等等操作。那么一个模型我们怎么知道它好不好用呢？以至于我们能不能放心的使用模型给我的结果呢？那么今天的学习的评估，就会很有帮助。</p>
<p>加载下面的库</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = [<span class="string">&#x27;SimHei&#x27;</span>]  <span class="comment"># 用来正常显示中文标签</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="literal">False</span>  <span class="comment"># 用来正常显示负号</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;figure.figsize&#x27;</span>] = (<span class="number">10</span>, <span class="number">6</span>)  <span class="comment"># 设置输出图片大小</span></span><br></pre></td></tr></table></figure>
<p><strong>任务：加载数据并分割测试集和训练集</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line"><span class="comment"># 一般先取出X和y后再切割，有些情况会使用到未切割的，这时候X和y就可以用,x是清洗好的数据，y是我们要预测的存活数据&#x27;Survived&#x27;</span></span><br><span class="line">data = pd.read_csv(<span class="string">&#x27;clear_data.csv&#x27;</span>)</span><br><span class="line">train = pd.read_csv(<span class="string">&#x27;train.csv&#x27;</span>)</span><br><span class="line">X = data</span><br><span class="line">y = train[<span class="string">&#x27;Survived&#x27;</span>]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line"><span class="comment"># 对数据集进行切割</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line"><span class="comment"># 默认参数逻辑回归模型</span></span><br><span class="line">lr = LogisticRegression()</span><br><span class="line">lr.fit(X_train, y_train)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<pre><code>/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(</code></pre>
<h3 id="模型评估">模型评估</h3>
<ul>
<li>模型评估是为了知道模型的泛化能力。</li>
<li>交叉验证（cross-validation）是一种评估泛化性能的统计学方法，它比单次划分训练集和测试集的方法更加稳定、全面。</li>
<li>在交叉验证中，数据被多次划分，并且需要训练多个模型。</li>
<li>最常用的交叉验证是 k 折交叉验证（k-fold cross-validation），其中 k
是由用户指定的数字，通常取 5 或 10。</li>
<li>准确率（precision）度量的是被预测为正例的样本中有多少是真正的正例</li>
<li>召回率（recall）度量的是正类样本中有多少被预测为正类</li>
<li>f-分数是准确率与召回率的调和平均</li>
</ul>
<p>【思考】：将上面的概念进一步的理解，大家可以做一下总结</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#思考回答：</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="任务一交叉验证">任务一：交叉验证</h4>
<ul>
<li>用10折交叉验证来评估之前的逻辑回归模型</li>
<li>计算交叉验证精度的平均值</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#提示：交叉验证</span></span><br><span class="line">Image(<span class="string">&#x27;Snipaste_2020-01-05_16-37-56.png&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure>
<img src="/2025/05/14/%E5%AD%A6%E4%B9%A0/kaggle/titanic/%E7%AC%AC%E4%B8%89%E7%AB%A0%E6%A8%A1%E5%9E%8B%E5%BB%BA%E7%AB%8B%E5%92%8C%E8%AF%84%E4%BC%B0---%E8%AF%84%E4%BB%B7-%E8%AF%BE%E7%A8%8B/第三章模型建立和评估---评价-课程_16_0.png" alt="第三章模型建立和评估—评价-课程_16_0">
<figcaption aria-hidden="true">第三章模型建立和评估—评价-课程_16_0</figcaption>
</figure>
<h4 id="提示4">提示4</h4>
<ul>
<li>交叉验证在sklearn中的模块为<code>sklearn.model_selection</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line">lr = LogisticRegression(C=<span class="number">100</span>)</span><br><span class="line">scores = cross_val_score(lr, X_train, y_train, cv=<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line"><span class="comment"># 平均交叉验证分数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Average cross-validation score: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(scores.mean()))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<pre><code>Average cross-validation score: 0.78</code></pre>
<h4 id="思考4">思考4</h4>
<ul>
<li>k折越多的情况下会带来什么样的影响？</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#思考回答</span></span><br><span class="line"><span class="comment"># 当 k 越大时：</span></span><br><span class="line"><span class="comment"># 1. 每次训练使用的数据更多，评估偏差（bias）降低</span></span><br><span class="line"><span class="comment"># 2. 每次测试集样本更少，评估方差（variance）增大</span></span><br><span class="line"><span class="comment"># 3. 需要训练 k 个模型，计算开销显著增加</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="任务二混淆矩阵">任务二：混淆矩阵</h4>
<ul>
<li>计算二分类问题的混淆矩阵</li>
<li>计算精确率、召回率以及f-分数</li>
</ul>
<p>【思考】什么是二分类问题的混淆矩阵，理解这个概念，知道它主要是运算到什么任务中的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#思考回答</span></span><br><span class="line"><span class="comment"># 混淆矩阵（confusion matrix）是一个 2×2 的表格，用于二分类任务中展示模型预测结果与真实标签的对应关系：</span></span><br><span class="line"><span class="comment">#    - True Positive (TP)：真实为正类，预测也为正类</span></span><br><span class="line"><span class="comment">#    - False Positive (FP)：真实为负类，却被误预测为正类</span></span><br><span class="line"><span class="comment">#    - False Negative (FN)：真实为正类，却被误预测为负类</span></span><br><span class="line"><span class="comment">#    - True Negative (TN)：真实为负类，预测也为负类</span></span><br><span class="line"><span class="comment"># 通过混淆矩阵，可以进一步计算精确率（Precision）、召回率（Recall）、F1 分数等指标，</span></span><br><span class="line"><span class="comment"># 帮助我们评估模型在不同类型错误上的表现，常用于分类模型的性能评估和错误分析。</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#提示：混淆矩阵</span></span><br><span class="line">Image(<span class="string">&#x27;Snipaste_2020-01-05_16-38-26.png&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure>
<img src="/2025/05/14/%E5%AD%A6%E4%B9%A0/kaggle/titanic/%E7%AC%AC%E4%B8%89%E7%AB%A0%E6%A8%A1%E5%9E%8B%E5%BB%BA%E7%AB%8B%E5%92%8C%E8%AF%84%E4%BC%B0---%E8%AF%84%E4%BB%B7-%E8%AF%BE%E7%A8%8B/第三章模型建立和评估---评价-课程_27_0.png" alt="第三章模型建立和评估—评价-课程_27_0">
<figcaption aria-hidden="true">第三章模型建立和评估—评价-课程_27_0</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#提示：准确率 (Accuracy),精确度（Precision）,Recall,f-分数计算方法</span></span><br><span class="line">Image(<span class="string">&#x27;Snipaste_2020-01-05_16-39-27.png&#x27;</span>)</span><br><span class="line">​    </span><br></pre></td></tr></table></figure>
<figure>
<img src="/2025/05/14/%E5%AD%A6%E4%B9%A0/kaggle/titanic/%E7%AC%AC%E4%B8%89%E7%AB%A0%E6%A8%A1%E5%9E%8B%E5%BB%BA%E7%AB%8B%E5%92%8C%E8%AF%84%E4%BC%B0---%E8%AF%84%E4%BB%B7-%E8%AF%BE%E7%A8%8B/第三章模型建立和评估---评价-课程_28_0.png" alt="第三章模型建立和评估—评价-课程_28_0">
<figcaption aria-hidden="true">第三章模型建立和评估—评价-课程_28_0</figcaption>
</figure>
<h4 id="提示5">提示5</h4>
<ul>
<li>混淆矩阵的方法在sklearn中的<code>sklearn.metrics</code>模块</li>
<li>混淆矩阵需要输入真实标签和预测标签</li>
<li>精确率、召回率以及f-分数可使用<code>classification_report</code>模块</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">lr = LogisticRegression(C=<span class="number">100</span>)</span><br><span class="line">lr.fit(X_train, y_train)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<pre><code>/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line"><span class="comment"># 模型预测结果</span></span><br><span class="line">pred = lr.predict(X_train)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line"><span class="comment"># 混淆矩阵</span></span><br><span class="line">confusion_matrix(y_train, pred)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<pre><code>array([[355,  57],
       [ 82, 174]])</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"><span class="comment"># 精确率、召回率以及f1-score</span></span><br><span class="line"><span class="built_in">print</span>(classification_report(y_train, pred))</span><br></pre></td></tr></table></figure>
<pre><code>              precision    recall  f1-score   support

           0       0.81      0.86      0.84       412
           1       0.75      0.68      0.71       256

    accuracy                           0.79       668
   macro avg       0.78      0.77      0.78       668
weighted avg       0.79      0.79      0.79       668</code></pre>
<p>【思考】 * 如果自己实现混淆矩阵的时候该注意什么问题</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#思考回答</span></span><br><span class="line"><span class="comment"># 如果自己实现混淆矩阵，需要注意：</span></span><br><span class="line"><span class="comment"># 1. 明确行列含义：通常行是真实标签，列是预测标签，并保持一致。</span></span><br><span class="line"><span class="comment"># 2. 类别顺序要固定：最好指定 labels 列表，避免类别稀疏时错位。</span></span><br><span class="line"><span class="comment"># 3. 初始化大小为 n_classes×n_classes 的零矩阵。</span></span><br><span class="line"><span class="comment"># 4. 索引时使用整数或统一的类别映射，避免类型不一致。</span></span><br><span class="line"><span class="comment"># 5. 对每个样本累加到对应的 [真实, 预测] 单元格，最后矩阵元素之和应等于样本数。</span></span><br><span class="line"><span class="comment"># 6. 对于未出现的类别，矩阵对应行或列应保留 0。</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="任务三roc曲线">任务三：ROC曲线</h4>
<ul>
<li>绘制ROC曲线</li>
</ul>
<p>【思考】什么是ROC曲线，OCR曲线的存在是为了解决什么问题？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#思考</span></span><br><span class="line"><span class="comment"># ROC曲线（Receiver Operating Characteristic Curve）是一条以假正例率（FPR）为横坐标、</span></span><br><span class="line"><span class="comment"># 真正例率（TPR）为纵坐标绘制的曲线，展示模型在不同阈值下的分类性能变化。</span></span><br><span class="line"><span class="comment"># 它解决了单一阈值下评估不全面的问题，通过曲线下的面积（AUC）能够衡量模型整体区分正负样本的能力；</span></span><br><span class="line"><span class="comment"># 对类别不平衡更稳健，可在不同模型或参数设置间进行客观比较。</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="提示6">提示6</h4>
<ul>
<li>ROC曲线在sklearn中的模块为<code>sklearn.metrics</code></li>
<li>ROC曲线下面所包围的面积越大越好</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_curve</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(lr.decision_function(X_test))</span><br></pre></td></tr></table></figure>
<pre><code>[-1.7776276  -1.68901519 -2.9343385  -2.73339993 -0.7425476   0.1771919
  0.42300886 -0.95177507 -2.19297241 -2.09492243 -2.09876666 -2.24379328
 -0.72898893 -0.74448703  1.55206252  2.26736362 -3.0615053  -1.45551632
  1.82143942  1.10174703  2.80348253  2.20862227 -2.08595792 -1.98565326
 -2.62459231  2.61608127  2.52054836  0.46386814 -2.26805651 -1.89799476
 -4.40221097 -2.45118004 -2.11507984  0.25727282  1.56507901 -3.49922092
  0.09517543  3.1727335  -0.66659502 -2.16889122 -2.31738004 -0.75154631
  1.34173247 -0.68691348 -2.38317701 -1.48352807  3.30441868  0.37836543
  0.15120699 -2.39554116  0.71230509 -2.94049784  0.0526656  -0.12124772
  0.21937853 -0.95736671 -2.91315052  1.73227025 -2.30451919 -0.11949728
 -2.40406452 -1.23217853 -3.04709277 -2.51149884 -2.91275507  0.36741872
  1.88515182 -1.73344723  1.61180838 -2.64456699 -2.82671595 -1.32885535
 -1.89201447 -2.38194062  1.14830497  0.7324757   3.41575634 -0.04718518
  1.99047031  0.71098531 -2.5002286   2.11220527  1.35687779 -4.65208202
 -0.50164169 -2.21847127 -0.27744568 -2.1098023  -2.28203956 -2.24087733
  1.49913758 -0.46745632 -1.76590269 -3.13694507 -2.48969764 -2.52447108
 -0.31359417 -2.62456277  0.10812447 -3.22505518 -0.54301462 -1.31398633
 -2.45637232 -0.9392769  -1.99910791 -0.01952273  0.16386412  1.17043699
  0.83571934 -0.30892412 -2.56236834 -2.52630696 -2.15878988  3.38005162
 -1.63316112 -2.0470374   1.16802525  1.96428556 -0.85542758 -0.84711271
 -2.3923425  -2.27467461  1.27340371 -0.16738478  2.77379952 -0.91636487
  3.49337899  2.22265823 -1.03898765 -1.79576035  3.05405598 -1.72625544
 -2.08233698  0.14427761 -2.03826492 -1.87510703 -2.43040363  0.88364821
 -2.31722422  1.21479438 -2.19509856 -1.96948465  2.90456606  1.22909197
 -0.60993113 -2.40508898  1.79832298 -2.33619419 -1.76964851  0.54894164
  0.56920781 -1.65544357 -2.18783672 -2.51890544 -1.1167812   1.85506633
 -2.14366192  2.56003678  1.79741811  2.22038003 -0.93948297  2.11029939
  3.66773152  3.37255532 -1.62079149 -0.21922341 -2.93532548 -1.8851028
 -0.11223495 -0.89402373 -2.79168773  0.58319665 -1.20213471  2.11583429
 -1.78550619 -1.21648746 -2.91538781 -2.80005448 -2.74359191 -0.06775047
 -1.28645408 -1.17048578 -0.1176852  -1.59958242 -0.65901928 -2.40701243
  0.57575073 -3.0756839   1.53932753 -2.49031769 -3.03266822  0.30539932
 -0.05523861 -0.24431132 -2.36483723  3.25595248 -2.11664845 -1.97728592
 -2.04509461 -3.07727841 -1.11942703 -3.38920295 -2.59088459 -3.55978164
  0.22449105 -0.3214215   0.05735696  0.02061023 -3.01544378 -0.77973619
 -1.39798016 -3.10075724 -4.80621573 -3.01948006  3.44366918 -2.88193813
 -2.01992513 -0.09559774  0.91447527 -1.13270082 -2.45426968 -1.91415803
 -0.08403516]</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line"><span class="comment"># 计算ROC曲线的假正例率(FPR)、真正例率(TPR)和阈值</span></span><br><span class="line">fpr, tpr, thresholds = roc_curve(y_test, lr.decision_function(X_test))</span><br><span class="line">plt.plot(fpr, tpr, label=<span class="string">&quot;ROC Curve&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;FPR&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;TPR (recall)&quot;</span>)</span><br><span class="line"><span class="comment"># 找到最接近于0的阈值</span></span><br><span class="line">close_zero = np.argmin(np.<span class="built_in">abs</span>(thresholds))</span><br><span class="line">plt.plot(fpr[close_zero], tpr[close_zero], <span class="string">&#x27;o&#x27;</span>, markersize=<span class="number">10</span>, label=<span class="string">&quot;threshold zero&quot;</span>, fillstyle=<span class="string">&quot;none&quot;</span>, c=<span class="string">&#x27;k&#x27;</span>, mew=<span class="number">2</span>)</span><br><span class="line">plt.legend(loc=<span class="number">4</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure>
<img src="/2025/05/14/%E5%AD%A6%E4%B9%A0/kaggle/titanic/%E7%AC%AC%E4%B8%89%E7%AB%A0%E6%A8%A1%E5%9E%8B%E5%BB%BA%E7%AB%8B%E5%92%8C%E8%AF%84%E4%BC%B0---%E8%AF%84%E4%BB%B7-%E8%AF%BE%E7%A8%8B/第三章模型建立和评估---评价-课程_44_3.png" alt="第三章模型建立和评估—评价-课程_44_3">
<figcaption aria-hidden="true">第三章模型建立和评估—评价-课程_44_3</figcaption>
</figure>
<h4 id="思考6">思考6</h4>
<ul>
<li>对于多分类问题如何绘制ROC曲线</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#思考回答</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>【思考】你能从这条OCR曲线的到什么信息？这些信息可以做什么？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#思考回答</span></span><br><span class="line"><span class="comment"># 从ROC曲线中可以得到以下信息：</span></span><br><span class="line"><span class="comment"># 1. 模型整体性能：曲线下的面积 (AUC - Area Under the Curve) 是一个常用的评估指标。</span></span><br><span class="line"><span class="comment">#    - AUC = 1：完美分类器。</span></span><br><span class="line"><span class="comment">#    - AUC = 0.5：随机分类器（无区分能力，ROC曲线接近对角线）。</span></span><br><span class="line"><span class="comment">#    - AUC &gt; 0.5：模型优于随机猜测。AUC越大，模型区分正负样本的能力越强。</span></span><br><span class="line"><span class="comment">#    - AUC &lt; 0.5：模型表现差于随机猜测（可能标签反了或者模型非常差）。</span></span><br><span class="line"><span class="comment"># 2. 不同阈值下的权衡：ROC曲线展示了在所有可能的分类阈值下，真正例率 (TPR) 与假正例率 (FPR) 之间的关系。</span></span><br><span class="line"><span class="comment">#    - 曲线上的每个点代表一个特定的阈值。</span></span><br><span class="line"><span class="comment">#    - 曲线越靠近左上角 (FPR低, TPR高)，说明模型在较低的假正例率下能达到较高的真正例率，性能越好。</span></span><br><span class="line"><span class="comment"># 3. 模型的区分能力：曲线的形状可以反映模型区分正负样本的能力。如果曲线显著高于对角线，说明模型具有较好的区分能力。</span></span><br><span class="line"><span class="comment"># 4. 阈值选择的依据：可以根据业务需求，在ROC曲线上选择一个合适的平衡点（即选择一个阈值）。</span></span><br><span class="line"><span class="comment">#    - 例如，如果更关注减少漏报（提高TPR），可以选择曲线上TPR较高的点，即使FPR可能略高。</span></span><br><span class="line"><span class="comment">#    - 如果更关注减少误报（降低FPR），可以选择曲线上FPR较低的点，即使TPR可能略低。</span></span><br><span class="line"><span class="comment">#    - 图中标记的 &quot;threshold zero&quot; 点通常是模型默认的分类阈值对应的性能点。</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/05/14/%E5%AD%A6%E4%B9%A0/kaggle/titanic/%E7%AC%AC%E4%B8%89%E7%AB%A0%E6%A8%A1%E5%9E%8B%E5%BB%BA%E7%AB%8B%E5%92%8C%E8%AF%84%E4%BC%B0--%E5%BB%BA%E6%A8%A1-%E8%AF%BE%E7%A8%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/05/14/%E5%AD%A6%E4%B9%A0/kaggle/titanic/%E7%AC%AC%E4%B8%89%E7%AB%A0%E6%A8%A1%E5%9E%8B%E5%BB%BA%E7%AB%8B%E5%92%8C%E8%AF%84%E4%BC%B0--%E5%BB%BA%E6%A8%A1-%E8%AF%BE%E7%A8%8B/" class="post-title-link" itemprop="url">数据分析——第三章模型建立和评估--建模</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-05-14 00:00:00" itemprop="dateCreated datePublished" datetime="2025-05-14T00:00:00+08:00">2025-05-14</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-06-12 18:45:38" itemprop="dateModified" datetime="2025-06-12T18:45:38+08:00">2025-06-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/kaggle/" itemprop="url" rel="index"><span itemprop="name">kaggle</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/kaggle/titanic/" itemprop="url" rel="index"><span itemprop="name">titanic</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="第三章-模型搭建和评估建模">第三章 模型搭建和评估–建模</h2>
<p>经过前面的两章的知识点的学习，我可以对数数据的本身进行处理，比如数据本身的增删查补，还可以做必要的清洗工作。那么下面我们就要开始使用我们前面处理好的数据了。这一章我们要做的就是使用数据，我们做数据分析的目的也就是，运用我们的数据以及结合我的业务来得到某些我们需要知道的结果。那么分析的第一步就是建模，搭建一个预测模型或者其他模型；我们从这个模型的到结果之后，我们要分析我的模型是不是足够的可靠，那我就需要评估这个模型。今天我们学习建模，下一节我们学习评估。</p>
<p>我们拥有的泰坦尼克号的数据集，那么我们这次的目的就是，完成泰坦尼克号存活预测这个任务。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> Image</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = [<span class="string">&#x27;SimHei&#x27;</span>]  <span class="comment"># 用来正常显示中文标签</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="literal">False</span>  <span class="comment"># 用来正常显示负号</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;figure.figsize&#x27;</span>] = (<span class="number">10</span>, <span class="number">6</span>)  <span class="comment"># 设置输出图片大小</span></span><br></pre></td></tr></table></figure>
<p>载入这些库，如果缺少某些库，请安装他们</p>
<p>【思考】这些库的作用是什么呢？你需要查一查</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#思考题回答</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Image 是 IPython.display 模块的一部分，用于在 Jupyter Notebook 或 IPython 环境中直接显示图像。支持从本地路径或网络 URL 加载图像，并控制显示格式（如宽度、高度、格式类型）</span></span><br><span class="line"><span class="string">seaborn 是一个基于 Matplotlib 的高级数据可视化库，专注于统计图形的绘制（如分布图、相关性图、分类图等），能快速生成美观且信息丰富的图表</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<pre><code>&#39;\nImage 是 IPython.display 模块的一部分，用于在 Jupyter Notebook 或 IPython 环境中直接显示图像。支持从本地路径或网络 URL 加载图像，并控制显示格式（如宽度、高度、格式类型）\nseaborn 是一个基于 Matplotlib 的高级数据可视化库，专注于统计图形的绘制（如分布图、相关性图、分类图等），能快速生成美观且信息丰富的图表\n&#39;</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<p><strong>载入我们提供清洗之后的数据(clear_data.csv)，大家也将原始数据载入（train.csv），说说他们有什么不同</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line">train = pd.read_csv(<span class="string">&#x27;train.csv&#x27;</span>)</span><br><span class="line">train.shape</span><br></pre></td></tr></table></figure>
<pre><code>(891, 12)</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line">train.head()</span><br></pre></td></tr></table></figure>
<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;</code></pre>

<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
PassengerId
</th>
<th>
Survived
</th>
<th>
Pclass
</th>
<th>
Name
</th>
<th>
Sex
</th>
<th>
Age
</th>
<th>
SibSp
</th>
<th>
Parch
</th>
<th>
Ticket
</th>
<th>
Fare
</th>
<th>
Cabin
</th>
<th>
Embarked
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
1
</td>
<td>
0
</td>
<td>
3
</td>
<td>
Braund, Mr. Owen Harris
</td>
<td>
male
</td>
<td>
22.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
A/5 21171
</td>
<td>
7.2500
</td>
<td>
NaN
</td>
<td>
S
</td>
</tr>
<tr>
<th>
1
</th>
<td>
2
</td>
<td>
1
</td>
<td>
1
</td>
<td>
Cumings, Mrs. John Bradley (Florence Briggs Th…
</td>
<td>
female
</td>
<td>
38.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
PC 17599
</td>
<td>
71.2833
</td>
<td>
C85
</td>
<td>
C
</td>
</tr>
<tr>
<th>
2
</th>
<td>
3
</td>
<td>
1
</td>
<td>
3
</td>
<td>
Heikkinen, Miss. Laina
</td>
<td>
female
</td>
<td>
26.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
STON/O2. 3101282
</td>
<td>
7.9250
</td>
<td>
NaN
</td>
<td>
S
</td>
</tr>
<tr>
<th>
3
</th>
<td>
4
</td>
<td>
1
</td>
<td>
1
</td>
<td>
Futrelle, Mrs. Jacques Heath (Lily May Peel)
</td>
<td>
female
</td>
<td>
35.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
113803
</td>
<td>
53.1000
</td>
<td>
C123
</td>
<td>
S
</td>
</tr>
<tr>
<th>
4
</th>
<td>
5
</td>
<td>
0
</td>
<td>
3
</td>
<td>
Allen, Mr. William Henry
</td>
<td>
male
</td>
<td>
35.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
373450
</td>
<td>
8.0500
</td>
<td>
NaN
</td>
<td>
S
</td>
</tr>
</tbody>
</table>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line">data = pd.read_csv(<span class="string">&#x27;clear_data.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
PassengerId
</th>
<th>
Pclass
</th>
<th>
Age
</th>
<th>
SibSp
</th>
<th>
Parch
</th>
<th>
Fare
</th>
<th>
Sex_female
</th>
<th>
Sex_male
</th>
<th>
Embarked_C
</th>
<th>
Embarked_Q
</th>
<th>
Embarked_S
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
0
</td>
<td>
3
</td>
<td>
22.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
7.2500
</td>
<td>
0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
0
</td>
<td>
1
</td>
</tr>
<tr>
<th>
1
</th>
<td>
1
</td>
<td>
1
</td>
<td>
38.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
71.2833
</td>
<td>
1
</td>
<td>
0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
0
</td>
</tr>
<tr>
<th>
2
</th>
<td>
2
</td>
<td>
3
</td>
<td>
26.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
7.9250
</td>
<td>
1
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
1
</td>
</tr>
<tr>
<th>
3
</th>
<td>
3
</td>
<td>
1
</td>
<td>
35.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
53.1000
</td>
<td>
1
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
1
</td>
</tr>
<tr>
<th>
4
</th>
<td>
4
</td>
<td>
3
</td>
<td>
35.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
8.0500
</td>
<td>
0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
0
</td>
<td>
1
</td>
</tr>
</tbody>
</table>
<h3 id="模型搭建">模型搭建</h3>
<ul>
<li>处理完前面的数据我们就得到建模数据，下一步是选择合适模型</li>
<li>在进行模型选择之前我们需要先知道数据集最终是进行<strong>监督学习</strong>还是<strong>无监督学习</strong></li>
<li>模型的选择一方面是通过我们的任务来决定的。</li>
<li>除了根据我们任务来选择模型外，还可以根据数据样本量以及特征的稀疏性来决定</li>
<li>刚开始我们总是先尝试使用一个基本的模型来作为其baseline，进而再训练其他模型做对比，最终选择泛化能力或性能比较好的模型</li>
</ul>
<p>这里我的建模，并不是从零开始，自己一个人完成完成所有代码的编译。我们这里使用一个机器学习最常用的一个库（sklearn）来完成我们的模型的搭建</p>
<p><strong>下面给出sklearn的算法选择路径，供大家参考</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sklearn模型算法选择路径图</span></span><br><span class="line">Image(<span class="string">&#x27;sklearn.png&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure>
<img src="/2025/05/14/%E5%AD%A6%E4%B9%A0/kaggle/titanic/%E7%AC%AC%E4%B8%89%E7%AB%A0%E6%A8%A1%E5%9E%8B%E5%BB%BA%E7%AB%8B%E5%92%8C%E8%AF%84%E4%BC%B0--%E5%BB%BA%E6%A8%A1-%E8%AF%BE%E7%A8%8B/第三章模型建立和评估--建模-课程_17_0.png" alt="第三章模型建立和评估–建模-课程_17_0">
<figcaption aria-hidden="true">第三章模型建立和评估–建模-课程_17_0</figcaption>
</figure>
<p>【思考】数据集哪些差异会导致模型在拟合数据是发生变化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#思考回答</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="任务一切割训练集和测试集">任务一：切割训练集和测试集</h4>
<p>这里使用留出法划分数据集</p>
<ul>
<li>将数据集分为自变量和因变量</li>
<li>按比例切割训练集和测试集(一般测试集的比例有30%、25%、20%、15%和10%)</li>
<li>使用分层抽样</li>
<li>设置随机种子以便结果能复现</li>
</ul>
<p>【思考】 * 划分数据集的方法有哪些？ *
为什么使用分层抽样，这样的好处有什么？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 划分数据集的方法有哪些？</span></span><br><span class="line"><span class="comment"># 1.  **留出法 (Hold-out Cross Validation)**：</span></span><br><span class="line"><span class="comment">#     *   直接将数据集D划分为两个互斥的集合：训练集S和测试集T。</span></span><br><span class="line"><span class="comment">#     *   例如，70%的数据用于训练，30%用于测试。</span></span><br><span class="line"><span class="comment">#     *   优点：简单、计算开销小。</span></span><br><span class="line"><span class="comment">#     *   缺点：划分具有随机性，单次划分的结果可能不够稳定和准确，尤其是在数据集较小时。训练集和测试集的样本比例会影响评估结果。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.  **交叉验证法 (Cross Validation)**：</span></span><br><span class="line"><span class="comment">#     *   **k折交叉验证 (k-Fold Cross Validation)**：</span></span><br><span class="line"><span class="comment">#         *   将数据集D划分为k个大小相似的互斥子集 D1, D2, ..., Dk。</span></span><br><span class="line"><span class="comment">#         *   每次用k-1个子集的并集作为训练集，余下的那个子集作为测试集。</span></span><br><span class="line"><span class="comment">#         *   这样可以获得k组训练/测试集，从而可进行k次训练和测试，最终返回的是这k个测试结果的均值。</span></span><br><span class="line"><span class="comment">#         *   常用的k值为5或10。</span></span><br><span class="line"><span class="comment">#         *   优点：比留出法更稳定，更充分地利用了数据。</span></span><br><span class="line"><span class="comment">#         *   缺点：计算开销是k倍。</span></span><br><span class="line"><span class="comment">#     *   **留一法 (Leave-One-Out Cross Validation, LOOCV)**：</span></span><br><span class="line"><span class="comment">#         *   k折交叉验证的特例，当k等于样本数N时。</span></span><br><span class="line"><span class="comment">#         *   每次只留下一个样本作为测试集，其余N-1个样本作为训练集。</span></span><br><span class="line"><span class="comment">#         *   优点：评估结果通常被认为比较准确，因为几乎所有数据都用于训练。</span></span><br><span class="line"><span class="comment">#         *   缺点：计算开销非常大，尤其是在数据集很大时。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.  **自助法 (Bootstrapping)**：</span></span><br><span class="line"><span class="comment">#     *   以自助采样法为基础。给定包含m个样本的数据集D，对它进行采样产生数据集D&#x27;：每次随机从D中挑选一个样本，将其拷贝放入D&#x27;，然后再将该样本放回初始数据集D中，使得该样本在下次采样时仍有可能被采到。这个过程重复执行m次后，我们就得到了包含m个样本的数据集D&#x27;。</span></span><br><span class="line"><span class="comment">#     *   可以证明，初始数据集D中约有36.8%的样本未出现在采样数据集D&#x27;中。于是我们可将D&#x27;用作训练集，D\D&#x27;用作测试集。</span></span><br><span class="line"><span class="comment">#     *   优点：在数据集较小、难以有效划分训练/测试集时很有用；能从初始数据集中产生多个不同的训练集。</span></span><br><span class="line"><span class="comment">#     *   缺点：自助法产生的数据集改变了初始数据集的分布，会引入估计偏差。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 为什么使用分层抽样，这样的好处有什么？</span></span><br><span class="line"><span class="comment"># **分层抽样 (Stratified Sampling)** 是一种抽样技术，它将总体（数据集）划分为若干个互不重叠的子群（称为“层”），然后从每个层中独立地进行简单随机抽样。在划分训练集和测试集时，特别是对于分类任务，通常是根据目标变量的类别进行分层。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># **好处：**</span></span><br><span class="line"><span class="comment"># 1.  **保持类别比例一致性**：</span></span><br><span class="line"><span class="comment">#     *   确保训练集和测试集中的各个类别的样本比例与原始数据集中各个类别的样本比例大致相同。</span></span><br><span class="line"><span class="comment">#     *   这对于类别不平衡的数据集尤为重要。如果进行纯随机抽样，可能会导致训练集或测试集中某些少数类别的样本过少，甚至没有，从而影响模型的训练效果和评估的可靠性。</span></span><br><span class="line"><span class="comment"># 2.  **提高模型的泛化能力和评估的准确性**：</span></span><br><span class="line"><span class="comment">#     *   由于训练集和测试集都较好地代表了原始数据的类别分布，模型在训练时能学习到各个类别的特征，评估时也能更准确地反映模型在所有类别上的表现。</span></span><br><span class="line"><span class="comment">#     *   避免了因随机划分导致训练集和测试集在类别分布上产生较大差异，从而使得模型评估结果更加稳定和可信。</span></span><br><span class="line"><span class="comment"># 3.  **减少抽样误差**：</span></span><br><span class="line"><span class="comment">#     *   相比于简单随机抽样，分层抽样通常能得到更具代表性的样本，从而减少因抽样带来的误差，使得基于样本的推断更加精确。</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="任务提示1">任务提示1</h4>
<ul>
<li>切割数据集是为了后续能评估模型泛化能力</li>
<li>sklearn中切割数据集的方法为<code>train_test_split</code></li>
<li>查看函数文档可以在jupyter
noteboo里面使用<code>train_test_split?</code>后回车即可看到</li>
<li>分层和随机种子在参数里寻找</li>
</ul>
<p>要从clear_data.csv和train.csv中提取train_test_split()所需的参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="comment"># 一般先取出X和y后再切割，有些情况会使用到未切割的，这时候X和y就可以用,x是清洗好的数据，y是我们要预测的存活数据&#x27;Survived&#x27;</span></span><br><span class="line">X = data</span><br><span class="line">y = train[<span class="string">&#x27;Survived&#x27;</span>]</span><br><span class="line"><span class="comment"># 对数据集进行切割</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line"></span><br><span class="line">X_train.shape, X_test.shape</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<pre><code>((668, 11), (223, 11))</code></pre>
<p>【思考】 * 什么情况下切割数据集的时候不用进行随机选取</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#思考回答</span></span><br><span class="line"><span class="comment"># 在以下情况下切割数据集时可能不需要或不适合进行随机选取：</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.  **时间序列数据 (Time Series Data)**：</span></span><br><span class="line"><span class="comment">#     *   对于时间序列数据，数据的顺序至关重要，因为它包含了时间依赖性。随机打乱顺序会破坏这种依赖关系。</span></span><br><span class="line"><span class="comment">#     *   通常的做法是按时间顺序划分，例如，将较早的数据作为训练集，较晚的数据作为测试集（或验证集）。这更符合实际应用中用过去预测未来的场景。</span></span><br><span class="line"><span class="comment">#     *   例如，用前几年的股票数据训练模型，用最近一年的数据测试模型。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.  **数据已经预先排序或具有特定结构**：</span></span><br><span class="line"><span class="comment">#     *   如果数据集已经按照某种对分析有意义的顺序排列（例如，按地理区域、按实验批次等），并且你希望测试集来自与训练集不同的、特定的部分，那么可能需要按顺序或按特定规则划分，而不是随机划分。</span></span><br><span class="line"><span class="comment">#     *   例如，在一个全国性的调查数据中，你可能想用某些省份的数据做训练，用另一些省份的数据做测试，以检验模型的地域泛化能力。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.  **数据集非常大且分布均匀**：</span></span><br><span class="line"><span class="comment">#     *   当数据集非常庞大，并且可以合理假设数据是独立同分布 (i.i.d.) 且分布均匀时，简单地按顺序取一部分作为训练集，另一部分作为测试集，其效果可能与随机选取相差不大。随机选取的计算开销在这种情况下可能显得不必要。</span></span><br><span class="line"><span class="comment">#     *   然而，即使在这种情况下，随机选取通常仍然是更稳妥的做法，以避免潜在的未知偏差。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.  **特定的交叉验证策略**：</span></span><br><span class="line"><span class="comment">#     *   某些交叉验证方法本身就定义了非随机的划分方式。例如，在k折交叉验证中，虽然整体上数据被分成了k折，但每一折的选择是确定的（通常是按顺序分割）。留一法交叉验证更是每次只留一个特定的样本作为测试集。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 5.  **当需要完全复现特定的、非随机的划分结果时**：</span></span><br><span class="line"><span class="comment">#     *   如果之前的研究或实验使用了某种特定的非随机划分方式，为了比较或复现结果，也需要采用相同的划分方式。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 6.  **流式数据或在线学习场景**：</span></span><br><span class="line"><span class="comment">#     *   在数据持续不断流入的场景中，模型可能需要用新到达的数据进行测试或持续训练。这种情况下，测试集自然是最新的一部分数据，而不是从历史数据中随机抽取的。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="任务二模型创建">任务二：模型创建</h4>
<ul>
<li>创建基于线性模型的分类模型（逻辑回归）</li>
<li>创建基于树的分类模型（决策树、随机森林）</li>
<li>分别使用这些模型进行训练，分别的到训练集和测试集的得分</li>
<li>查看模型的参数，并更改参数值，观察模型变化</li>
</ul>
<h4 id="提示">提示</h4>
<ul>
<li>逻辑回归不是回归模型而是分类模型，不要与<code>LinearRegression</code>混淆</li>
<li>随机森林其实是决策树集成为了降低决策树过拟合的情况</li>
<li>线性模型所在的模块为<code>sklearn.linear_model</code></li>
<li>树模型所在的模块为<code>sklearn.ensemble</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line"><span class="comment"># 默认参数逻辑回归模型</span></span><br><span class="line">lr = LogisticRegression()</span><br><span class="line">lr.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<pre><code>/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line"><span class="comment"># 查看训练集和测试集score值</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Training set score: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(lr.score(X_train, y_train)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Testing set score: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(lr.score(X_test, y_test)))</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<pre><code>Training set score: 0.80
Testing set score: 0.79</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line"><span class="comment"># 调整参数后的逻辑回归模型</span></span><br><span class="line">lr2 = LogisticRegression(C=<span class="number">100</span>)</span><br><span class="line">lr2.fit(X_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Training set score: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(lr2.score(X_train, y_train)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Testing set score: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(lr2.score(X_test, y_test)))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<pre><code>Training set score: 0.79
Testing set score: 0.78


/root/.pyenv/versions/3.11.1/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 默认参数的随机森林分类模型</span></span><br><span class="line">rfc = RandomForestClassifier()</span><br><span class="line">rfc.fit(X_train, y_train)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Training set score: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(rfc.score(X_train, y_train)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Testing set score: &#123;:.2f&#125;&quot;</span>.<span class="built_in">format</span>(rfc.score(X_test, y_test)))</span><br></pre></td></tr></table></figure>
<pre><code>Training set score: 1.00
Testing set score: 0.82</code></pre>
<p>【思考】 * 为什么线性模型可以进行分类任务，背后是怎么的数学关系 *
对于多分类问题，线性模型是怎么进行分类的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#思考回答</span></span><br><span class="line"><span class="comment"># 为什么线性模型可以进行分类任务，背后是怎么的数学关系</span></span><br><span class="line"><span class="comment"># 线性模型（如逻辑回归 Logistic Regression，或者支持向量机 SVM 的线性核）之所以能用于分类任务，是因为它们通过以下方式将线性组合的输入特征映射到类别预测：</span></span><br><span class="line"><span class="comment"># 1.  **线性组合**：首先，模型计算输入特征的线性组合，形式通常为 `z = w_1*x_1 + w_2*x_2 + ... + w_n*x_n + b`，或者用向量表示为 `z = w^T * x + b`。</span></span><br><span class="line"><span class="comment">#     *   `x` 是输入特征向量。</span></span><br><span class="line"><span class="comment">#     *   `w` 是模型学习到的权重（或系数）。</span></span><br><span class="line"><span class="comment">#     *   `b` 是偏置项（或截距）。</span></span><br><span class="line"><span class="comment">#     这个 `z` 值可以看作是样本点到决策边界的某种度量。</span></span><br><span class="line"><span class="comment"># 2.  **决策函数/激活函数**：然后，这个线性组合的结果 `z` 会被传递给一个决策函数或激活函数，该函数将其转换为类别预测或类别概率。</span></span><br><span class="line"><span class="comment">#     *   **对于逻辑回归 (Logistic Regression)**：</span></span><br><span class="line"><span class="comment">#         *   它使用 Sigmoid (Logistic) 函数：`p = 1 / (1 + e^(-z))`。</span></span><br><span class="line"><span class="comment">#         *   Sigmoid 函数将任意实数值 `z` 映射到 (0, 1) 区间，这个输出 `p` 可以解释为样本属于正类（通常是类别1）的概率。</span></span><br><span class="line"><span class="comment">#         *   通过设定一个阈值（通常是0.5），如果 `p &gt; 0.5` (即 `z &gt; 0`)，则预测为正类；否则预测为负类。</span></span><br><span class="line"><span class="comment">#         *   因此，决策边界是 `z = 0`，即 `w^T * x + b = 0`，这是一个超平面。</span></span><br><span class="line"><span class="comment">#     *   **对于线性支持向量机 (Linear SVM)**：</span></span><br><span class="line"><span class="comment">#         *   它直接使用 `z` 的符号来决定类别。如果 `z &gt; 0`，预测为一类；如果 `z &lt; 0`，预测为另一类。</span></span><br><span class="line"><span class="comment">#         *   SVM 的目标是找到一个能最大化两类样本之间间隔（margin）的决策边界（超平面）。</span></span><br><span class="line"><span class="comment"># 总结来说，线性模型通过学习一个线性决策边界（直线、平面或超平面）来分隔不同类别的样本。它们首先计算一个线性得分，然后通过一个非线性函数（如Sigmoid）或直接根据得分的符号来做出分类决策。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对于多分类问题，线性模型是怎么进行分类的</span></span><br><span class="line"><span class="comment"># 当类别数量大于两个时（即多分类问题），线性模型通常采用以下两种主要策略之一将问题转化为多个二分类问题：</span></span><br><span class="line"><span class="comment"># 1.  **一对余 (One-vs-Rest, OvR) 或 一对所有 (One-vs-All, OvA)**：</span></span><br><span class="line"><span class="comment">#     *   **原理**：如果有个 `K` 个类别，OvR 策略会训练 `K` 个独立的二分类器。</span></span><br><span class="line"><span class="comment">#     *   第 `i` 个分类器 (`i` 从 1 到 `K`) 会将类别 `i` 的样本视为正类，而将所有其他 `K-1` 个类别的样本视为负类。</span></span><br><span class="line"><span class="comment">#     *   **预测**：对于一个新的样本，它会被输入到所有 `K` 个分类器中。每个分类器都会输出一个分数或概率，表示该样本属于其对应“正类”的置信度。最终，样本被分配给那个给出最高置信度分数的类别。</span></span><br><span class="line"><span class="comment">#     *   **优点**：直观，实现相对简单，计算效率较高（只需要训练K个分类器）。</span></span><br><span class="line"><span class="comment">#     *   **缺点**：当类别数量很多时，每个二分类器的负类可能包含非常多样化的样本，可能导致类别不平衡问题。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.  **一对一 (One-vs-One, OvO)**：</span></span><br><span class="line"><span class="comment">#     *   **原理**：如果有个 `K` 个类别，OvO 策略会为每一对类别 `(i, j)` 训练一个二分类器，其中 `i != j`。总共需要训练 `K * (K-1) / 2` 个分类器。</span></span><br><span class="line"><span class="comment">#     *   每个分类器只负责区分两个特定的类别。</span></span><br><span class="line"><span class="comment">#     *   **预测**：对于一个新的样本，它会被输入到所有 `K * (K-1) / 2` 个分类器中。每个分类器都会对样本属于其两个类别中的哪一个进行投票。最终，样本被分配给获得最多投票的类别。</span></span><br><span class="line"><span class="comment">#     *   **优点**：每个分类器只需要处理两个类别的数据，通常训练速度更快，且对于某些对类别不平衡不敏感的算法（如SVM）可能表现更好。</span></span><br><span class="line"><span class="comment">#     *   **缺点**：当类别数量 `K` 很大时，需要训练的分类器数量会急剧增加，导致计算成本和存储成本较高。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># **在 scikit-learn 中**：</span></span><br><span class="line"><span class="comment"># *   `LogisticRegression` 默认使用 OvR 策略进行多分类 (可以通过 `multi_class` 参数设置为 `&#x27;multinomial&#x27;` 来使用 Softmax 回归，这是一种直接处理多分类的方法，但其基础仍然是线性的)。</span></span><br><span class="line"><span class="comment"># *   `LinearSVC` (线性支持向量机) 默认使用 OvR 策略。</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="任务三输出模型预测结果">任务三：输出模型预测结果</h4>
<ul>
<li>输出模型预测分类标签</li>
<li>输出不同分类标签的预测概率</li>
</ul>
<h4 id="提示3">提示3</h4>
<ul>
<li>一般监督模型在sklearn里面有个<code>predict</code>能输出预测标签，<code>predict_proba</code>则可以输出标签概率</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line">pred = lr.predict(X_train)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line">pred[:<span class="number">10</span>]</span><br></pre></td></tr></table></figure>
<pre><code>array([0, 1, 1, 1, 0, 0, 1, 0, 1, 1])</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line">pred_proba = lr.predict_proba(X_train)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line">pred_proba[:<span class="number">10</span>]</span><br></pre></td></tr></table></figure>
<pre><code>array([[0.60887905, 0.39112095],
       [0.17668722, 0.82331278],
       [0.40624596, 0.59375404],
       [0.18896449, 0.81103551],
       [0.87984221, 0.12015779],
       [0.91385758, 0.08614242],
       [0.13282516, 0.86717484],
       [0.90555878, 0.09444122],
       [0.05280619, 0.94719381],
       [0.10934565, 0.89065435]])</code></pre>
<p>【思考】 * 预测标签的概率对我们有什么帮助</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#思考回答</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/05/14/%E5%AD%A6%E4%B9%A0/kaggle/titanic/%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E7%AC%AC%E4%B8%80%E8%8A%82%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97%E5%8F%8A%E7%89%B9%E5%BE%81%E5%A4%84%E7%90%86-%E8%AF%BE%E7%A8%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/05/14/%E5%AD%A6%E4%B9%A0/kaggle/titanic/%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E7%AC%AC%E4%B8%80%E8%8A%82%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97%E5%8F%8A%E7%89%B9%E5%BE%81%E5%A4%84%E7%90%86-%E8%AF%BE%E7%A8%8B/" class="post-title-link" itemprop="url">数据分析——第二章：第一节数据清洗及特征处理</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2025-05-14 00:00:00 / 修改时间：17:00:20" itemprop="dateCreated datePublished" datetime="2025-05-14T00:00:00+08:00">2025-05-14</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/kaggle/" itemprop="url" rel="index"><span itemprop="name">kaggle</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/kaggle/titanic/" itemprop="url" rel="index"><span itemprop="name">titanic</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>【回顾&amp;引言】前面一章的内容大家可以感觉到我们主要是对基础知识做一个梳理，让大家了解数据分析的一些操作，主要做了数据的各个角度的观察。那么在这里，我们主要是做数据分析的流程性学习，主要是包括了数据清洗以及数据的特征处理，数据重构以及数据可视化。这些内容是为数据分析最后的建模和模型评价做一个铺垫。</p>
<h4 id="开始之前导入numpypandas包和数据">开始之前，导入numpy、pandas包和数据</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#加载所需的库</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#加载数据train.csv</span></span><br><span class="line">df = pd.read_csv(<span class="string">&#x27;./titanic/train.csv&#x27;</span>)</span><br><span class="line">df.head(<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
PassengerId
</th>
<th>
Survived
</th>
<th>
Pclass
</th>
<th>
Name
</th>
<th>
Sex
</th>
<th>
Age
</th>
<th>
SibSp
</th>
<th>
Parch
</th>
<th>
Ticket
</th>
<th>
Fare
</th>
<th>
Cabin
</th>
<th>
Embarked
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
1
</td>
<td>
0
</td>
<td>
3
</td>
<td>
Braund, Mr. Owen Harris
</td>
<td>
male
</td>
<td>
22.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
A/5 21171
</td>
<td>
7.2500
</td>
<td>
NaN
</td>
<td>
S
</td>
</tr>
<tr>
<th>
1
</th>
<td>
2
</td>
<td>
1
</td>
<td>
1
</td>
<td>
Cumings, Mrs. John Bradley (Florence Briggs Th…
</td>
<td>
female
</td>
<td>
38.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
PC 17599
</td>
<td>
71.2833
</td>
<td>
C85
</td>
<td>
C
</td>
</tr>
<tr>
<th>
2
</th>
<td>
3
</td>
<td>
1
</td>
<td>
3
</td>
<td>
Heikkinen, Miss. Laina
</td>
<td>
female
</td>
<td>
26.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
STON/O2. 3101282
</td>
<td>
7.9250
</td>
<td>
NaN
</td>
<td>
S
</td>
</tr>
</tbody>
</table>
<h2 id="第二章数据清洗及特征处理">2 第二章：数据清洗及特征处理</h2>
<p>我们拿到的数据通常是不干净的，所谓的不干净，就是数据中有缺失值，有一些异常点等，需要经过一定的处理才能继续做后面的分析或建模，所以拿到数据的第一步是进行数据清洗，本章我们将学习缺失值、重复值、字符串和数据转换等操作，将数据清洗成可以分析或建模的亚子。</p>
<h3 id="缺失值观察与处理">2.1 缺失值观察与处理</h3>
<p>我们拿到的数据经常会有很多缺失值，比如我们可以看到Cabin列存在NaN，那其他列还有没有缺失值，这些缺失值要怎么处理呢</p>
<h4 id="任务一缺失值观察">2.1.1 任务一：缺失值观察</h4>
<ol type="1">
<li>请查看每个特征缺失值个数<br>
</li>
<li>请查看Age， Cabin， Embarked列的数据
以上方式都有多种方式，所以大家多多益善</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line">df.info()</span><br></pre></td></tr></table></figure>
<pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 891 entries, 0 to 890
Data columns (total 12 columns):
 #   Column       Non-Null Count  Dtype  
---  ------       --------------  -----  
 0   PassengerId  891 non-null    int64  
 1   Survived     891 non-null    int64  
 2   Pclass       891 non-null    int64  
 3   Name         891 non-null    object 
 4   Sex          891 non-null    object 
 5   Age          714 non-null    float64
 6   SibSp        891 non-null    int64  
 7   Parch        891 non-null    int64  
 8   Ticket       891 non-null    object 
 9   Fare         891 non-null    float64
 10  Cabin        204 non-null    object 
 11  Embarked     889 non-null    object 
dtypes: float64(2), int64(5), object(5)
memory usage: 83.7+ KB</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line">df.isnull().<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure>
<pre><code>PassengerId      0
Survived         0
Pclass           0
Name             0
Sex              0
Age            177
SibSp            0
Parch            0
Ticket           0
Fare             0
Cabin          687
Embarked         2
dtype: int64</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line">df[[<span class="string">&#x27;Age&#x27;</span>,<span class="string">&#x27;Cabin&#x27;</span>,<span class="string">&#x27;Embarked&#x27;</span>]].head(<span class="number">3</span>)</span><br><span class="line"><span class="comment">#df[&#x27;Age&#x27;].head(3)</span></span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
Age
</th>
<th>
Cabin
</th>
<th>
Embarked
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
22.0
</td>
<td>
NaN
</td>
<td>
S
</td>
</tr>
<tr>
<th>
1
</th>
<td>
38.0
</td>
<td>
C85
</td>
<td>
C
</td>
</tr>
<tr>
<th>
2
</th>
<td>
26.0
</td>
<td>
NaN
</td>
<td>
S
</td>
</tr>
</tbody>
</table>
<p>外层 [] 是 DataFrame 的索引操作符。 内层 [] 是 Python
原生的列表语法，用于传递多个列名。</p>
<h4 id="任务二对缺失值进行处理">2.1.2 任务二：对缺失值进行处理</h4>
<p>(1)处理缺失值一般有几种思路</p>
<ol start="2" type="1">
<li><p>请尝试对Age列的数据的缺失值进行处理</p></li>
<li><p>请尝试使用不同的方法直接对整张表的缺失值进行处理</p></li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#处理缺失值的一般思路：</span></span><br><span class="line"><span class="comment">#提醒：可使用的函数有---&gt;dropna函数与fillna函数</span></span><br><span class="line"><span class="comment">#print(df.head(3))</span></span><br><span class="line">df[df[<span class="string">&#x27;Age&#x27;</span>]==<span class="literal">None</span>]=<span class="number">0</span></span><br><span class="line">df.head(<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
PassengerId
</th>
<th>
Survived
</th>
<th>
Pclass
</th>
<th>
Name
</th>
<th>
Sex
</th>
<th>
Age
</th>
<th>
SibSp
</th>
<th>
Parch
</th>
<th>
Ticket
</th>
<th>
Fare
</th>
<th>
Cabin
</th>
<th>
Embarked
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
1
</td>
<td>
0
</td>
<td>
3
</td>
<td>
Braund, Mr. Owen Harris
</td>
<td>
male
</td>
<td>
22.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
A/5 21171
</td>
<td>
7.2500
</td>
<td>
NaN
</td>
<td>
S
</td>
</tr>
<tr>
<th>
1
</th>
<td>
2
</td>
<td>
1
</td>
<td>
1
</td>
<td>
Cumings, Mrs. John Bradley (Florence Briggs Th…
</td>
<td>
female
</td>
<td>
38.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
PC 17599
</td>
<td>
71.2833
</td>
<td>
C85
</td>
<td>
C
</td>
</tr>
<tr>
<th>
2
</th>
<td>
3
</td>
<td>
1
</td>
<td>
3
</td>
<td>
Heikkinen, Miss. Laina
</td>
<td>
female
</td>
<td>
26.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
STON/O2. 3101282
</td>
<td>
7.9250
</td>
<td>
NaN
</td>
<td>
S
</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line">df[df[<span class="string">&#x27;Age&#x27;</span>].isnull()]</span><br><span class="line">df[df[<span class="string">&#x27;Age&#x27;</span>].isnull()] = <span class="number">0</span> </span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line">df[df[<span class="string">&#x27;Age&#x27;</span>] == np.nan] = <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>【思考1】dropna和fillna有哪些参数，分别如何使用呢?</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#dropna() 是 Pandas 中用于删除包含缺失值（NaN 或 None）的行或列的函数。其核心作用是清理数据中的缺失值，适用于数据清洗阶段。</span></span><br><span class="line">df.dropna().head(<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
PassengerId
</th>
<th>
Survived
</th>
<th>
Pclass
</th>
<th>
Name
</th>
<th>
Sex
</th>
<th>
Age
</th>
<th>
SibSp
</th>
<th>
Parch
</th>
<th>
Ticket
</th>
<th>
Fare
</th>
<th>
Cabin
</th>
<th>
Embarked
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
1
</th>
<td>
2
</td>
<td>
1
</td>
<td>
1
</td>
<td>
Cumings, Mrs. John Bradley (Florence Briggs Th…
</td>
<td>
female
</td>
<td>
38.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
PC 17599
</td>
<td>
71.2833
</td>
<td>
C85
</td>
<td>
C
</td>
</tr>
<tr>
<th>
3
</th>
<td>
4
</td>
<td>
1
</td>
<td>
1
</td>
<td>
Futrelle, Mrs. Jacques Heath (Lily May Peel)
</td>
<td>
female
</td>
<td>
35.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
113803
</td>
<td>
53.1000
</td>
<td>
C123
</td>
<td>
S
</td>
</tr>
<tr>
<th>
5
</th>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0.0000
</td>
<td>
0
</td>
<td>
0
</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#fillna() 是 Pandas 中用于填充缺失值（NaN 或 None）的核心函数，常用于数据清洗阶段。其核心作用是将缺失值替换为合理值，以便后续分析或建模。</span></span><br><span class="line">df.fillna(<span class="number">0</span>).head(<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
PassengerId
</th>
<th>
Survived
</th>
<th>
Pclass
</th>
<th>
Name
</th>
<th>
Sex
</th>
<th>
Age
</th>
<th>
SibSp
</th>
<th>
Parch
</th>
<th>
Ticket
</th>
<th>
Fare
</th>
<th>
Cabin
</th>
<th>
Embarked
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
1
</td>
<td>
0
</td>
<td>
3
</td>
<td>
Braund, Mr. Owen Harris
</td>
<td>
male
</td>
<td>
22.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
A/5 21171
</td>
<td>
7.2500
</td>
<td>
0
</td>
<td>
S
</td>
</tr>
<tr>
<th>
1
</th>
<td>
2
</td>
<td>
1
</td>
<td>
1
</td>
<td>
Cumings, Mrs. John Bradley (Florence Briggs Th…
</td>
<td>
female
</td>
<td>
38.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
PC 17599
</td>
<td>
71.2833
</td>
<td>
C85
</td>
<td>
C
</td>
</tr>
<tr>
<th>
2
</th>
<td>
3
</td>
<td>
1
</td>
<td>
3
</td>
<td>
Heikkinen, Miss. Laina
</td>
<td>
female
</td>
<td>
26.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
STON/O2. 3101282
</td>
<td>
7.9250
</td>
<td>
0
</td>
<td>
S
</td>
</tr>
</tbody>
</table>
</div>
<p>【思考】检索空缺值用<code>np.nan</code>,<code>None</code>以及<code>.isnull()</code>哪个更好，这是为什么？如果其中某个方式无法找到缺失值，原因又是为什么？</p>
<p>【参考】https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html</p>
<p>【参考】https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html</p>
<h3 id="重复值观察与处理">2.2 重复值观察与处理</h3>
<p>由于这样那样的原因，数据中会不会存在重复值呢，如果存在要怎样处理呢</p>
<h4 id="任务一请查看数据中的重复值">2.2.1
任务一：请查看数据中的重复值</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line"><span class="comment">#df.duplicated()返回一个布尔序列 (Series)，标记每一行是否为重复行</span></span><br><span class="line">df[df.duplicated()]</span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
PassengerId
</th>
<th>
Survived
</th>
<th>
Pclass
</th>
<th>
Name
</th>
<th>
Sex
</th>
<th>
Age
</th>
<th>
SibSp
</th>
<th>
Parch
</th>
<th>
Ticket
</th>
<th>
Fare
</th>
<th>
Cabin
</th>
<th>
Embarked
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
17
</th>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0.0
</td>
<td>
0
</td>
<td>
0
</td>
</tr>
<tr>
<th>
19
</th>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0.0
</td>
<td>
0
</td>
<td>
0
</td>
</tr>
<tr>
<th>
26
</th>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0.0
</td>
<td>
0
</td>
<td>
0
</td>
</tr>
<tr>
<th>
28
</th>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0.0
</td>
<td>
0
</td>
<td>
0
</td>
</tr>
<tr>
<th>
29
</th>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0.0
</td>
<td>
0
</td>
<td>
0
</td>
</tr>
<tr>
<th>
…
</th>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
</tr>
<tr>
<th>
859
</th>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0.0
</td>
<td>
0
</td>
<td>
0
</td>
</tr>
<tr>
<th>
863
</th>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0.0
</td>
<td>
0
</td>
<td>
0
</td>
</tr>
<tr>
<th>
868
</th>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0.0
</td>
<td>
0
</td>
<td>
0
</td>
</tr>
<tr>
<th>
878
</th>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0.0
</td>
<td>
0
</td>
<td>
0
</td>
</tr>
<tr>
<th>
888
</th>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
0.0
</td>
<td>
0
</td>
<td>
0
</td>
</tr>
</tbody>
</table>
<p>
176 rows × 12 columns
</p>
<h4 id="任务二对重复值进行处理">2.2.2 任务二：对重复值进行处理</h4>
<p>(1)重复值有哪些处理方式呢？</p>
<p>(2)处理我们数据的重复值</p>
<p>方法多多益善</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#重复值有哪些处理方式：</span></span><br><span class="line"><span class="comment">#删除 DataFrame 中的重复行（完全相同的行只保留一次）。</span></span><br><span class="line">df = df.drop_duplicates()</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
PassengerId
</th>
<th>
Survived
</th>
<th>
Pclass
</th>
<th>
Name
</th>
<th>
Sex
</th>
<th>
Age
</th>
<th>
SibSp
</th>
<th>
Parch
</th>
<th>
Ticket
</th>
<th>
Fare
</th>
<th>
Cabin
</th>
<th>
Embarked
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
1
</td>
<td>
0
</td>
<td>
3
</td>
<td>
Braund, Mr. Owen Harris
</td>
<td>
male
</td>
<td>
22.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
A/5 21171
</td>
<td>
7.2500
</td>
<td>
NaN
</td>
<td>
S
</td>
</tr>
<tr>
<th>
1
</th>
<td>
2
</td>
<td>
1
</td>
<td>
1
</td>
<td>
Cumings, Mrs. John Bradley (Florence Briggs Th…
</td>
<td>
female
</td>
<td>
38.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
PC 17599
</td>
<td>
71.2833
</td>
<td>
C85
</td>
<td>
C
</td>
</tr>
<tr>
<th>
2
</th>
<td>
3
</td>
<td>
1
</td>
<td>
3
</td>
<td>
Heikkinen, Miss. Laina
</td>
<td>
female
</td>
<td>
26.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
STON/O2. 3101282
</td>
<td>
7.9250
</td>
<td>
NaN
</td>
<td>
S
</td>
</tr>
<tr>
<th>
3
</th>
<td>
4
</td>
<td>
1
</td>
<td>
1
</td>
<td>
Futrelle, Mrs. Jacques Heath (Lily May Peel)
</td>
<td>
female
</td>
<td>
35.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
113803
</td>
<td>
53.1000
</td>
<td>
C123
</td>
<td>
S
</td>
</tr>
<tr>
<th>
4
</th>
<td>
5
</td>
<td>
0
</td>
<td>
3
</td>
<td>
Allen, Mr. William Henry
</td>
<td>
male
</td>
<td>
35.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
373450
</td>
<td>
8.0500
</td>
<td>
NaN
</td>
<td>
S
</td>
</tr>
</tbody>
</table>
<h4 id="任务三将前面清洗的数据保存为csv格式">2.2.3
任务三：将前面清洗的数据保存为csv格式</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line"></span><br><span class="line">df.to_csv(<span class="string">&#x27;test_clear.csv&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="特征观察与处理">2.3 特征观察与处理</h3>
<p>我们对特征进行一下观察，可以把特征大概分为两大类：<br>
数值型特征：Survived ，Pclass， Age ，SibSp， Parch，
Fare，其中Survived， Pclass为离散型数值特征，Age，SibSp， Parch，
Fare为连续型数值特征<br>
文本型特征：Name， Sex， Cabin，Embarked， Ticket，其中Sex， Cabin，
Embarked，
Ticket为类别型文本特征，数值型特征一般可以直接用于模型的训练，但有时候为了模型的稳定性及鲁棒性会对连续变量进行离散化。文本型特征往往需要转换成数值型特征才能用于建模分析。</p>
<h4 id="任务一对年龄进行分箱离散化处理">2.3.1
任务一：对年龄进行分箱（离散化）处理</h4>
<ol type="1">
<li><p>分箱操作是什么？</p></li>
<li><p>将连续变量Age平均分箱成5个年龄段，并分别用类别变量12345表示</p></li>
<li><p>将连续变量Age划分为[0,5) [5,15) [15,30) [30,50)
[50,80)五个年龄段，并分别用类别变量12345表示</p></li>
<li><p>将连续变量Age按10% 30% 50% 70%
90%五个年龄段，并用分类变量12345表示</p></li>
<li><p>将上面的获得的数据分别进行保存，保存为csv格式</p></li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#分箱操作是什么：</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">分箱操作（Binning）是数据预处理中的一种常用技术，主要用于将连续型数值转换为离散的区间（即“箱子”或“分组”）</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line"><span class="comment">#将连续变量Age平均分箱成5个年龄段，并分别用类别变量12345表示</span></span><br><span class="line">df[<span class="string">&#x27;AgeBand&#x27;</span>] = pd.cut(df[<span class="string">&#x27;Age&#x27;</span>], <span class="number">5</span>,labels = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])</span><br><span class="line">df.head()</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
PassengerId
</th>
<th>
Survived
</th>
<th>
Pclass
</th>
<th>
Name
</th>
<th>
Sex
</th>
<th>
Age
</th>
<th>
SibSp
</th>
<th>
Parch
</th>
<th>
Ticket
</th>
<th>
Fare
</th>
<th>
Cabin
</th>
<th>
Embarked
</th>
<th>
AgeBand
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
1
</td>
<td>
0
</td>
<td>
3
</td>
<td>
Braund, Mr. Owen Harris
</td>
<td>
male
</td>
<td>
22.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
A/5 21171
</td>
<td>
7.2500
</td>
<td>
NaN
</td>
<td>
S
</td>
<td>
2
</td>
</tr>
<tr>
<th>
1
</th>
<td>
2
</td>
<td>
1
</td>
<td>
1
</td>
<td>
Cumings, Mrs. John Bradley (Florence Briggs Th…
</td>
<td>
female
</td>
<td>
38.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
PC 17599
</td>
<td>
71.2833
</td>
<td>
C85
</td>
<td>
C
</td>
<td>
3
</td>
</tr>
<tr>
<th>
2
</th>
<td>
3
</td>
<td>
1
</td>
<td>
3
</td>
<td>
Heikkinen, Miss. Laina
</td>
<td>
female
</td>
<td>
26.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
STON/O2. 3101282
</td>
<td>
7.9250
</td>
<td>
NaN
</td>
<td>
S
</td>
<td>
2
</td>
</tr>
<tr>
<th>
3
</th>
<td>
4
</td>
<td>
1
</td>
<td>
1
</td>
<td>
Futrelle, Mrs. Jacques Heath (Lily May Peel)
</td>
<td>
female
</td>
<td>
35.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
113803
</td>
<td>
53.1000
</td>
<td>
C123
</td>
<td>
S
</td>
<td>
3
</td>
</tr>
<tr>
<th>
4
</th>
<td>
5
</td>
<td>
0
</td>
<td>
3
</td>
<td>
Allen, Mr. William Henry
</td>
<td>
male
</td>
<td>
35.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
373450
</td>
<td>
8.0500
</td>
<td>
NaN
</td>
<td>
S
</td>
<td>
3
</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line"><span class="comment">#将连续变量Age划分为(0,5] (5,15] (15,30] (30,50] (50,80]五个年龄段，并分别用类别变量12345表示</span></span><br><span class="line">df[<span class="string">&#x27;AgeBand&#x27;</span>] = pd.cut(df[<span class="string">&#x27;Age&#x27;</span>],[<span class="number">0</span>,<span class="number">5</span>,<span class="number">15</span>,<span class="number">30</span>,<span class="number">50</span>,<span class="number">80</span>],labels = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])</span><br><span class="line">df.head(<span class="number">3</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
PassengerId
</th>
<th>
Survived
</th>
<th>
Pclass
</th>
<th>
Name
</th>
<th>
Sex
</th>
<th>
Age
</th>
<th>
SibSp
</th>
<th>
Parch
</th>
<th>
Ticket
</th>
<th>
Fare
</th>
<th>
Cabin
</th>
<th>
Embarked
</th>
<th>
AgeBand
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
1
</td>
<td>
0
</td>
<td>
3
</td>
<td>
Braund, Mr. Owen Harris
</td>
<td>
male
</td>
<td>
22.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
A/5 21171
</td>
<td>
7.2500
</td>
<td>
NaN
</td>
<td>
S
</td>
<td>
3
</td>
</tr>
<tr>
<th>
1
</th>
<td>
2
</td>
<td>
1
</td>
<td>
1
</td>
<td>
Cumings, Mrs. John Bradley (Florence Briggs Th…
</td>
<td>
female
</td>
<td>
38.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
PC 17599
</td>
<td>
71.2833
</td>
<td>
C85
</td>
<td>
C
</td>
<td>
4
</td>
</tr>
<tr>
<th>
2
</th>
<td>
3
</td>
<td>
1
</td>
<td>
3
</td>
<td>
Heikkinen, Miss. Laina
</td>
<td>
female
</td>
<td>
26.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
STON/O2. 3101282
</td>
<td>
7.9250
</td>
<td>
NaN
</td>
<td>
S
</td>
<td>
3
</td>
</tr>
</tbody>
</table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line"><span class="comment">#将连续变量Age按10% 30% 50 70% 90%五个年龄段，并用分类变量12345表示</span></span><br><span class="line">df[<span class="string">&#x27;AgeBand&#x27;</span>] = pd.qcut(df[<span class="string">&#x27;Age&#x27;</span>],[<span class="number">0</span>,<span class="number">0.1</span>,<span class="number">0.3</span>,<span class="number">0.5</span>,<span class="number">0.7</span>,<span class="number">0.9</span>],labels = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
PassengerId
</th>
<th>
Survived
</th>
<th>
Pclass
</th>
<th>
Name
</th>
<th>
Sex
</th>
<th>
Age
</th>
<th>
SibSp
</th>
<th>
Parch
</th>
<th>
Ticket
</th>
<th>
Fare
</th>
<th>
Cabin
</th>
<th>
Embarked
</th>
<th>
AgeBand
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
1
</td>
<td>
0
</td>
<td>
3
</td>
<td>
Braund, Mr. Owen Harris
</td>
<td>
male
</td>
<td>
22.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
A/5 21171
</td>
<td>
7.2500
</td>
<td>
NaN
</td>
<td>
S
</td>
<td>
2
</td>
</tr>
<tr>
<th>
1
</th>
<td>
2
</td>
<td>
1
</td>
<td>
1
</td>
<td>
Cumings, Mrs. John Bradley (Florence Briggs Th…
</td>
<td>
female
</td>
<td>
38.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
PC 17599
</td>
<td>
71.2833
</td>
<td>
C85
</td>
<td>
C
</td>
<td>
5
</td>
</tr>
<tr>
<th>
2
</th>
<td>
3
</td>
<td>
1
</td>
<td>
3
</td>
<td>
Heikkinen, Miss. Laina
</td>
<td>
female
</td>
<td>
26.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
STON/O2. 3101282
</td>
<td>
7.9250
</td>
<td>
NaN
</td>
<td>
S
</td>
<td>
3
</td>
</tr>
<tr>
<th>
3
</th>
<td>
4
</td>
<td>
1
</td>
<td>
1
</td>
<td>
Futrelle, Mrs. Jacques Heath (Lily May Peel)
</td>
<td>
female
</td>
<td>
35.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
113803
</td>
<td>
53.1000
</td>
<td>
C123
</td>
<td>
S
</td>
<td>
4
</td>
</tr>
<tr>
<th>
4
</th>
<td>
5
</td>
<td>
0
</td>
<td>
3
</td>
<td>
Allen, Mr. William Henry
</td>
<td>
male
</td>
<td>
35.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
373450
</td>
<td>
8.0500
</td>
<td>
NaN
</td>
<td>
S
</td>
<td>
4
</td>
</tr>
</tbody>
</table>
<p>【参考】https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.cut.html</p>
<p>【参考】https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.qcut.html</p>
<h4 id="任务二对文本变量进行转换">2.3.2 任务二：对文本变量进行转换</h4>
<ol type="1">
<li>查看文本变量名及种类<br>
</li>
<li>将文本变量Sex， Cabin ，Embarked用数值变量12345表示<br>
</li>
<li>将文本变量Sex， Cabin， Embarked用one-hot编码表示</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line"><span class="comment">#方法一: value_counts</span></span><br><span class="line"><span class="built_in">print</span>(</span><br><span class="line">df[<span class="string">&#x27;Sex&#x27;</span>].value_counts(),</span><br><span class="line">df[<span class="string">&#x27;Cabin&#x27;</span>].value_counts(),</span><br><span class="line">df[<span class="string">&#x27;Embarked&#x27;</span>].value_counts())</span><br></pre></td></tr></table></figure>
<pre><code>Sex
male      453
female    261
0           1
Name: count, dtype: int64 Cabin
B96 B98        4
G6             4
C23 C25 C27    4
F2             3
C22 C26        3
              ..
E36            1
D7             1
C118           1
C99            1
D37            1
Name: count, Length: 135, dtype: int64 Embarked
S    554
C    130
Q     28
0      1
Name: count, dtype: int64</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line">df[<span class="string">&#x27;Sex&#x27;</span>].unique()</span><br><span class="line">df[<span class="string">&#x27;Sex&#x27;</span>].nunique()</span><br></pre></td></tr></table></figure>
<pre><code>3</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line"><span class="comment">#将类别文本转换为12345</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#方法一: replace</span></span><br><span class="line">df[<span class="string">&#x27;Sex_num&#x27;</span>] = df[<span class="string">&#x27;Sex&#x27;</span>].replace([<span class="string">&#x27;male&#x27;</span>,<span class="string">&#x27;female&#x27;</span>],[<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line">df.head()</span><br><span class="line"><span class="comment">#方法二: map</span></span><br><span class="line">df[<span class="string">&#x27;Sex_num&#x27;</span>] = df[<span class="string">&#x27;Sex&#x27;</span>].<span class="built_in">map</span>(&#123;<span class="string">&#x27;male&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;female&#x27;</span>: <span class="number">2</span>&#125;)</span><br><span class="line">df.head()</span><br><span class="line"><span class="comment">#方法三: 使用sklearn.preprocessing的LabelEncoder</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"><span class="keyword">for</span> feat <span class="keyword">in</span> [<span class="string">&#x27;Cabin&#x27;</span>, <span class="string">&#x27;Ticket&#x27;</span>]:</span><br><span class="line">    lbl = LabelEncoder()  </span><br><span class="line">    label_dict = <span class="built_in">dict</span>(<span class="built_in">zip</span>(df[feat].unique(), <span class="built_in">range</span>(df[feat].nunique())))</span><br><span class="line">    <span class="comment">#print(label_dict)</span></span><br><span class="line">    df[feat + <span class="string">&quot;_labelEncode&quot;</span>] = df[feat].<span class="built_in">map</span>(label_dict)</span><br><span class="line">    df[feat + <span class="string">&quot;_labelEncode&quot;</span>] = lbl.fit_transform(df[feat].astype(<span class="built_in">str</span>))</span><br><span class="line"></span><br><span class="line">df.head()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<pre><code>/tmp/ipykernel_1400/2627332835.py:5: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option(&#39;future.no_silent_downcasting&#39;, True)`
  df[&#39;Sex_num&#39;] = df[&#39;Sex&#39;].replace([&#39;male&#39;,&#39;female&#39;],[1,2])</code></pre>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
PassengerId
</th>
<th>
Survived
</th>
<th>
Pclass
</th>
<th>
Name
</th>
<th>
Sex
</th>
<th>
Age
</th>
<th>
SibSp
</th>
<th>
Parch
</th>
<th>
Ticket
</th>
<th>
Fare
</th>
<th>
…
</th>
<th>
Age_66.0
</th>
<th>
Age_70.0
</th>
<th>
Age_70.5
</th>
<th>
Age_71.0
</th>
<th>
Age_74.0
</th>
<th>
Age_80.0
</th>
<th>
Embarked_0
</th>
<th>
Embarked_C
</th>
<th>
Embarked_Q
</th>
<th>
Embarked_S
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
1
</td>
<td>
0
</td>
<td>
3
</td>
<td>
Braund, Mr. Owen Harris
</td>
<td>
male
</td>
<td>
22.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
A/5 21171
</td>
<td>
7.2500
</td>
<td>
…
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
1
</th>
<td>
2
</td>
<td>
1
</td>
<td>
1
</td>
<td>
Cumings, Mrs. John Bradley (Florence Briggs Th…
</td>
<td>
female
</td>
<td>
38.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
PC 17599
</td>
<td>
71.2833
</td>
<td>
…
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
<td>
False
</td>
<td>
False
</td>
</tr>
<tr>
<th>
2
</th>
<td>
3
</td>
<td>
1
</td>
<td>
3
</td>
<td>
Heikkinen, Miss. Laina
</td>
<td>
female
</td>
<td>
26.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
STON/O2. 3101282
</td>
<td>
7.9250
</td>
<td>
…
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
3
</th>
<td>
4
</td>
<td>
1
</td>
<td>
1
</td>
<td>
Futrelle, Mrs. Jacques Heath (Lily May Peel)
</td>
<td>
female
</td>
<td>
35.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
113803
</td>
<td>
53.1000
</td>
<td>
…
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
<tr>
<th>
4
</th>
<td>
5
</td>
<td>
0
</td>
<td>
3
</td>
<td>
Allen, Mr. William Henry
</td>
<td>
male
</td>
<td>
35.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
373450
</td>
<td>
8.0500
</td>
<td>
…
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
False
</td>
<td>
True
</td>
</tr>
</tbody>
</table>
<p>
5 rows × 109 columns
</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将类别文本转换为one-hot编码</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#方法一: OneHotEncoder</span></span><br><span class="line"><span class="keyword">for</span> feat <span class="keyword">in</span> [<span class="string">&quot;Age&quot;</span>, <span class="string">&quot;Embarked&quot;</span>]:</span><br><span class="line">    x = pd.get_dummies(df[<span class="string">&quot;Age&quot;</span>] // <span class="number">6</span>)</span><br><span class="line"><span class="comment">#     x = pd.get_dummies(pd.cut(df[&#x27;Age&#x27;],5))</span></span><br><span class="line">    x = pd.get_dummies(df[feat], prefix=feat)</span><br><span class="line">    df = pd.concat([df, x], axis=<span class="number">1</span>)</span><br><span class="line">    <span class="comment">#df[feat] = pd.get_dummies(df[feat], prefix=feat)</span></span><br><span class="line">    </span><br><span class="line">df.head()</span><br><span class="line">df.to_csv(<span class="string">&#x27;temp.csv&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="任务三从纯文本name特征里提取出titles的特征所谓的titles就是mrmissmrs等">2.3.3
任务三：从纯文本Name特征里提取出Titles的特征(所谓的Titles就是Mr,Miss,Mrs等)</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#保存最终你完成的已经清理好的数据</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/05/14/%E5%AD%A6%E4%B9%A0/kaggle/titanic/%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E7%AC%AC%E4%BA%8C%E8%8A%82%E6%95%B0%E6%8D%AE%E9%87%8D%E6%9E%841-%E8%AF%BE%E7%A8%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zxjavatar.gif">
      <meta itemprop="name" content="张熙浚">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhang XiJun">
      <meta itemprop="description" content="zxj Blogs">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Zhang XiJun">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/05/14/%E5%AD%A6%E4%B9%A0/kaggle/titanic/%E7%AC%AC%E4%BA%8C%E7%AB%A0%EF%BC%9A%E7%AC%AC%E4%BA%8C%E8%8A%82%E6%95%B0%E6%8D%AE%E9%87%8D%E6%9E%841-%E8%AF%BE%E7%A8%8B/" class="post-title-link" itemprop="url">数据分析——第二章：第二节数据重构1</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2025-05-14 00:00:00 / 修改时间：17:06:05" itemprop="dateCreated datePublished" datetime="2025-05-14T00:00:00+08:00">2025-05-14</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/kaggle/" itemprop="url" rel="index"><span itemprop="name">kaggle</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/kaggle/titanic/" itemprop="url" rel="index"><span itemprop="name">titanic</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><strong>复习：</strong>在前面我们已经学习了Pandas基础，第二章我们开始进入数据分析的业务部分，在第二章第一节的内容中，我们学习了<strong>数据的清洗</strong>，这一部分十分重要，只有数据变得相对干净，我们之后对数据的分析才可以更有力。而这一节，我们要做的是数据重构，数据重构依旧属于数据理解（准备）的范围。</p>
<h4 id="开始之前导入numpypandas包和数据">开始之前，导入numpy、pandas包和数据</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入基本库</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 载入data文件中的:train-left-up.csv</span></span><br><span class="line">text=pd.read_csv(<span class="string">&#x27;../第二章项目集合/data/train-left-up.csv&#x27;</span>)</span><br><span class="line">text.head()</span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
PassengerId
</th>
<th>
Survived
</th>
<th>
Pclass
</th>
<th>
Name
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
1
</td>
<td>
0
</td>
<td>
3
</td>
<td>
Braund, Mr. Owen Harris
</td>
</tr>
<tr>
<th>
1
</th>
<td>
2
</td>
<td>
1
</td>
<td>
1
</td>
<td>
Cumings, Mrs. John Bradley (Florence Briggs Th…
</td>
</tr>
<tr>
<th>
2
</th>
<td>
3
</td>
<td>
1
</td>
<td>
3
</td>
<td>
Heikkinen, Miss. Laina
</td>
</tr>
<tr>
<th>
3
</th>
<td>
4
</td>
<td>
1
</td>
<td>
1
</td>
<td>
Futrelle, Mrs. Jacques Heath (Lily May Peel)
</td>
</tr>
<tr>
<th>
4
</th>
<td>
5
</td>
<td>
0
</td>
<td>
3
</td>
<td>
Allen, Mr. William Henry
</td>
</tr>
</tbody>
</table>
<h1 id="第二章数据重构">2 第二章：数据重构</h1>
<h3 id="数据的合并">2.4 数据的合并</h3>
<h4 id="任务一将data文件夹里面的所有数据都载入观察数据的之间的关系">2.4.1
任务一：将data文件夹里面的所有数据都载入，观察数据的之间的关系</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line">text_left_up=pd.read_csv(<span class="string">&#x27;../第二章项目集合/data/train-left-up.csv&#x27;</span>)</span><br><span class="line">text_left_down=pd.read_csv(<span class="string">&#x27;../第二章项目集合/data/train-left-down.csv&#x27;</span>)</span><br><span class="line">text_right_up=pd.read_csv(<span class="string">&#x27;../第二章项目集合/data/train-right-up.csv&#x27;</span>)</span><br><span class="line">text_right_down=pd.read_csv(<span class="string">&#x27;../第二章项目集合/data/train-right-down.csv&#x27;</span>)</span><br><span class="line">text_left_up.head()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
PassengerId
</th>
<th>
Survived
</th>
<th>
Pclass
</th>
<th>
Name
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
1
</td>
<td>
0
</td>
<td>
3
</td>
<td>
Braund, Mr. Owen Harris
</td>
</tr>
<tr>
<th>
1
</th>
<td>
2
</td>
<td>
1
</td>
<td>
1
</td>
<td>
Cumings, Mrs. John Bradley (Florence Briggs Th…
</td>
</tr>
<tr>
<th>
2
</th>
<td>
3
</td>
<td>
1
</td>
<td>
3
</td>
<td>
Heikkinen, Miss. Laina
</td>
</tr>
<tr>
<th>
3
</th>
<td>
4
</td>
<td>
1
</td>
<td>
1
</td>
<td>
Futrelle, Mrs. Jacques Heath (Lily May Peel)
</td>
</tr>
<tr>
<th>
4
</th>
<td>
5
</td>
<td>
0
</td>
<td>
3
</td>
<td>
Allen, Mr. William Henry
</td>
</tr>
</tbody>
</table>
<p>【提示】结合之前我们加载的train.csv数据，大致预测一下上面的数据是什么</p>
<h4 id="任务二使用concat方法将数据train-left-up.csv和train-right-up.csv横向合并为一张表并保存这张表为result_up">2.4.2：任务二：使用concat方法：将数据train-left-up.csv和train-right-up.csv横向合并为一张表，并保存这张表为result_up</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line"><span class="comment">#pandas.concat 是 Pandas 中用于连接 Series 或 DataFrame 对象的核心方法，支持横向（列方向）或纵向（行方向）拼接</span></span><br><span class="line">list_up = [text_left_up,text_right_up]</span><br><span class="line">result_up = pd.concat(list_up,axis=<span class="number">1</span>)</span><br><span class="line">result_up.head()</span><br></pre></td></tr></table></figure>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
PassengerId
</th>
<th>
Survived
</th>
<th>
Pclass
</th>
<th>
Name
</th>
<th>
Sex
</th>
<th>
Age
</th>
<th>
SibSp
</th>
<th>
Parch
</th>
<th>
Ticket
</th>
<th>
Fare
</th>
<th>
Cabin
</th>
<th>
Embarked
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
1
</td>
<td>
0
</td>
<td>
3
</td>
<td>
Braund, Mr. Owen Harris
</td>
<td>
male
</td>
<td>
22.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
A/5 21171
</td>
<td>
7.2500
</td>
<td>
NaN
</td>
<td>
S
</td>
</tr>
<tr>
<th>
1
</th>
<td>
2
</td>
<td>
1
</td>
<td>
1
</td>
<td>
Cumings, Mrs. John Bradley (Florence Briggs Th…
</td>
<td>
female
</td>
<td>
38.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
PC 17599
</td>
<td>
71.2833
</td>
<td>
C85
</td>
<td>
C
</td>
</tr>
<tr>
<th>
2
</th>
<td>
3
</td>
<td>
1
</td>
<td>
3
</td>
<td>
Heikkinen, Miss. Laina
</td>
<td>
female
</td>
<td>
26.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
STON/O2. 3101282
</td>
<td>
7.9250
</td>
<td>
NaN
</td>
<td>
S
</td>
</tr>
<tr>
<th>
3
</th>
<td>
4
</td>
<td>
1
</td>
<td>
1
</td>
<td>
Futrelle, Mrs. Jacques Heath (Lily May Peel)
</td>
<td>
female
</td>
<td>
35.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
113803
</td>
<td>
53.1000
</td>
<td>
C123
</td>
<td>
S
</td>
</tr>
<tr>
<th>
4
</th>
<td>
5
</td>
<td>
0
</td>
<td>
3
</td>
<td>
Allen, Mr. William Henry
</td>
<td>
male
</td>
<td>
35.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
373450
</td>
<td>
8.0500
</td>
<td>
NaN
</td>
<td>
S
</td>
</tr>
</tbody>
</table>
<h4 id="任务三使用concat方法将train-left-down和train-right-down横向合并为一张表并保存这张表为result_down然后将上边的result_up和result_down纵向合并为result">2.4.3
任务三：使用concat方法：将train-left-down和train-right-down横向合并为一张表，并保存这张表为result_down。然后将上边的result_up和result_down纵向合并为result。</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line">list_down=[text_left_down,text_right_down]</span><br><span class="line">result_down = pd.concat(list_down,axis=<span class="number">1</span>)</span><br><span class="line">result = pd.concat([result_up,result_down])</span><br><span class="line">result.head()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
PassengerId
</th>
<th>
Survived
</th>
<th>
Pclass
</th>
<th>
Name
</th>
<th>
Sex
</th>
<th>
Age
</th>
<th>
SibSp
</th>
<th>
Parch
</th>
<th>
Ticket
</th>
<th>
Fare
</th>
<th>
Cabin
</th>
<th>
Embarked
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
1
</td>
<td>
0
</td>
<td>
3
</td>
<td>
Braund, Mr. Owen Harris
</td>
<td>
male
</td>
<td>
22.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
A/5 21171
</td>
<td>
7.2500
</td>
<td>
NaN
</td>
<td>
S
</td>
</tr>
<tr>
<th>
1
</th>
<td>
2
</td>
<td>
1
</td>
<td>
1
</td>
<td>
Cumings, Mrs. John Bradley (Florence Briggs Th…
</td>
<td>
female
</td>
<td>
38.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
PC 17599
</td>
<td>
71.2833
</td>
<td>
C85
</td>
<td>
C
</td>
</tr>
<tr>
<th>
2
</th>
<td>
3
</td>
<td>
1
</td>
<td>
3
</td>
<td>
Heikkinen, Miss. Laina
</td>
<td>
female
</td>
<td>
26.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
STON/O2. 3101282
</td>
<td>
7.9250
</td>
<td>
NaN
</td>
<td>
S
</td>
</tr>
<tr>
<th>
3
</th>
<td>
4
</td>
<td>
1
</td>
<td>
1
</td>
<td>
Futrelle, Mrs. Jacques Heath (Lily May Peel)
</td>
<td>
female
</td>
<td>
35.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
113803
</td>
<td>
53.1000
</td>
<td>
C123
</td>
<td>
S
</td>
</tr>
<tr>
<th>
4
</th>
<td>
5
</td>
<td>
0
</td>
<td>
3
</td>
<td>
Allen, Mr. William Henry
</td>
<td>
male
</td>
<td>
35.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
373450
</td>
<td>
8.0500
</td>
<td>
NaN
</td>
<td>
S
</td>
</tr>
</tbody>
</table>
</div>
<h4 id="任务四使用dataframe自带的方法join方法和append完成任务二和任务三的任务">2.4.4
任务四：使用DataFrame自带的方法join方法和append：完成任务二和任务三的任务</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line">resul_up = text_left_up.join(text_right_up)</span><br><span class="line">result_down = text_left_down.join(text_right_down)</span><br><span class="line">result = result_up.append(result_down)</span><br><span class="line">result.head()</span><br></pre></td></tr></table></figure>
<h4 id="任务五使用panads的merge方法和dataframe的append方法完成任务二和任务三的任务">2.4.5
任务五：使用Panads的merge方法和DataFrame的append方法：完成任务二和任务三的任务</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">该代码使用 pandas.merge 方法，以索引（index）为键，将两个 DataFrame (text_left_up 和 text_right_up) 横向合并。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">result_up = pd.merge(text_left_up,text_right_up,left_index=<span class="literal">True</span>,right_index=<span class="literal">True</span>)</span><br><span class="line">result_down = pd.merge(text_left_down,text_right_down,left_index=<span class="literal">True</span>,right_index=<span class="literal">True</span>)</span><br><span class="line">result = resul_up.append(result_down)</span><br><span class="line">result.head()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>【思考】对比merge、join以及concat的方法的不同以及相同。思考一下在任务四和任务五的情况下，为什么都要求使用DataFrame的append方法，如何只要求使用merge或者join可不可以完成任务四和任务五呢？</p>
<h4 id="任务六完成的数据保存为result.csv">2.4.6
任务六：完成的数据保存为result.csv</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line"></span><br><span class="line">result.to_csv(<span class="string">&#x27;result.csv&#x27;</span>)</span><br><span class="line">result.head()</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
PassengerId
</th>
<th>
Survived
</th>
<th>
Pclass
</th>
<th>
Name
</th>
<th>
Sex
</th>
<th>
Age
</th>
<th>
SibSp
</th>
<th>
Parch
</th>
<th>
Ticket
</th>
<th>
Fare
</th>
<th>
Cabin
</th>
<th>
Embarked
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
1
</td>
<td>
0
</td>
<td>
3
</td>
<td>
Braund, Mr. Owen Harris
</td>
<td>
male
</td>
<td>
22.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
A/5 21171
</td>
<td>
7.2500
</td>
<td>
NaN
</td>
<td>
S
</td>
</tr>
<tr>
<th>
1
</th>
<td>
2
</td>
<td>
1
</td>
<td>
1
</td>
<td>
Cumings, Mrs. John Bradley (Florence Briggs Th…
</td>
<td>
female
</td>
<td>
38.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
PC 17599
</td>
<td>
71.2833
</td>
<td>
C85
</td>
<td>
C
</td>
</tr>
<tr>
<th>
2
</th>
<td>
3
</td>
<td>
1
</td>
<td>
3
</td>
<td>
Heikkinen, Miss. Laina
</td>
<td>
female
</td>
<td>
26.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
STON/O2. 3101282
</td>
<td>
7.9250
</td>
<td>
NaN
</td>
<td>
S
</td>
</tr>
<tr>
<th>
3
</th>
<td>
4
</td>
<td>
1
</td>
<td>
1
</td>
<td>
Futrelle, Mrs. Jacques Heath (Lily May Peel)
</td>
<td>
female
</td>
<td>
35.0
</td>
<td>
1
</td>
<td>
0
</td>
<td>
113803
</td>
<td>
53.1000
</td>
<td>
C123
</td>
<td>
S
</td>
</tr>
<tr>
<th>
4
</th>
<td>
5
</td>
<td>
0
</td>
<td>
3
</td>
<td>
Allen, Mr. William Henry
</td>
<td>
male
</td>
<td>
35.0
</td>
<td>
0
</td>
<td>
0
</td>
<td>
373450
</td>
<td>
8.0500
</td>
<td>
NaN
</td>
<td>
S
</td>
</tr>
</tbody>
</table>
</div>
<h3 id="换一种角度看数据">2.5 换一种角度看数据</h3>
<h4 id="任务一将我们的数据变为series类型的数据">2.5.1
任务一：将我们的数据变为Series类型的数据</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line"><span class="comment">#text.stack() 是 Pandas 中用于将 DataFrame 的列旋转为行的方法</span></span><br><span class="line">text = pd.read_csv(<span class="string">&#x27;result.csv&#x27;</span>)</span><br><span class="line">unit_result=text.stack().head(<span class="number">20</span>)</span><br><span class="line">unit_result.head()</span><br></pre></td></tr></table></figure>
<pre><code>0  Unnamed: 0                           0
   PassengerId                          1
   Survived                             0
   Pclass                               3
   Name           Braund, Mr. Owen Harris
dtype: object</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入代码</span></span><br><span class="line"></span><br><span class="line">unit_result.to_csv(<span class="string">&#x27;unit_result.csv&#x27;</span>)</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="上一页" aria-label="上一页" href="/page/5/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><span class="page-number current">6</span><a class="page-number" href="/page/7/">7</a><span class="space">&hellip;</span><a class="page-number" href="/page/14/">14</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/7/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">张熙浚</span>
  </div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="本站访问数 fa fa-user 次"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="本站总访问量 fa fa-eye 次"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script size="400" alpha="0.6" zIndex="-1" src="https://cdnjs.cloudflare.com/ajax/libs/ribbon.js/1.0.2/ribbon.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.1.0/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script src="/js/pjax.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  <script src="/js/third-party/pace.js"></script>


  
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
